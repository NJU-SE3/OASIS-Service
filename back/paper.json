{"_id":{"$oid":"5e5b9113eee8435e8e7d3382"},"title":"Statistical Errors in Software Engineering Experiments: A Preliminary Literature Review","abstract":"Background: Statistical concepts and techniques are often applied incorrectly, even in mature disciplines such as medicine or psychology. Surprisingly, there are very few works that study statistical problems in software engineering (SE). Aim: Assess the existence of statistical errors in SE experiments. Method: Compile the most common statistical errors in experimental disciplines. Survey experiments published in ICSE to assess whether errors occur in high quality SE publications. Results: The same errors as identified in others disciplines were found in ICSE experiments, where 30 of the reviewed papers included several error types such as: a) missing statistical hypotheses, b) missing sample size calculation, c) failure to assess statistical test assumptions, and d) uncorrected multiple testing. This rather large error rate is greater for research papers where experiments are confined to the validation section. The origin of the errors can be traced back to: a) researchers not having sufficient statistical training, and b) a profusion of exploratory research. Conclusions: This paper provides preliminary evidence that SE research suffers from the same statistical problems as other experimental disciplines. However, the SE community appears to be unaware of any shortcomings in its experiments, whereas other disciplines work hard to avoid these threats. Further research is necessary to find the underlying causes and set up corrective measures, but there are some potentially effective actions and are a priori easy to implement: a) improve the statistical training of SE researchers, and b) enforce quality assessment and reporting guidelines in SE publications.","conference":"IEEE","terms":"Software engineering;Bibliographies;Training;Guidelines;Error analysis;Back,psychology;research and development;software engineering;statistical testing,software engineering experiments;SE experiments;high quality SE publications;ICSE experiments;statistical hypotheses;statistical test assumptions;SE community;SE researchers;quality assessment;statistical errors;statistical training;SE research","keywords":"Literature review;Survey;Prevalence;Statistical errors","startPage":"1195","endPage":"1206","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453201","citationCount":0,"referenceCount":0,"year":2018,"authors":"R. P. Reyes Ch.; O. Dieste; E. R. Fonseca C.; N. Juristo","affiliations":"Univ. Politec. de Madrid, Madrid, Spain; Univ. Politec. de Madrid, Madrid, Spain; NA; Univ. Politec. de Madrid, Madrid, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9113eee8435e8e7d3383"},"title":"3rd FME Workshop on Formal Methods in Software Engineering (FormaliSE 2015)","abstract":"Despite their significant advantages, formal methods are not widely used in industrial software development. Following the successful workshops we organized at ICSE 2103 in San Francisco, and ICSE 2014 in Hyderabad, we organize a third edition of the FormaliSE workshop with the main goal to promote the integration between the formal methods and the software engineering communities.","conference":"IEEE","terms":"Software;Conferences;Software engineering;Security;Committees;Industries;Collaboration,,","keywords":"Formal methods;Software engineering","startPage":"977","endPage":"978","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203136","citationCount":0,"referenceCount":0,"year":2015,"authors":"S. Gnesi; N. Plat","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3384"},"title":"A Novel Neural Source Code Representation Based on Abstract Syntax Tree","abstract":"Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.","conference":"IEEE","terms":"Syntactics;Cloning;Semantics;Neural networks;Task analysis;Binary trees;Natural languages,information retrieval;learning (artificial intelligence);natural language processing;program diagnostics;recurrent neural nets;text analysis;tree data structures,abstract syntax tree;code fragment;natural language texts;ASTNN;statement trees;statement vectors;bidirectional RNN model;vector representation;source code representation method;source code classification;code clone detection;program analysis;program comprehension tasks;AST-based Neural Network;neural source code representation;information retrieval;machine learning","keywords":"Abstract Syntax Tree, source code representation, neural network, code classification, code clone detection","startPage":"783","endPage":"794","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812062","citationCount":1,"referenceCount":66,"year":2019,"authors":"J. Zhang; X. Wang; H. Zhang; H. Sun; K. Wang; X. Liu","affiliations":"Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, China; Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, China; The University of Newcastle, Australia; Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, China; Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, China; Beihang University, China; Beijing Advanced Innovation Center for Big Data and Brain Computing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3385"},"title":"Balancing Soundness and Efficiency for Practical Testing of Configurable Systems","abstract":"Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system - GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available.","conference":"IEEE","terms":"Testing;Computer bugs;Software product lines;Complexity theory;Reliability;Space exploration,program testing;software product lines,configurable system testing;S-SPLat technique;heuristic sampling;symbolic search;configuration space;software product lines;GCC","keywords":"sampling;testing;configuration","startPage":"632","endPage":"642","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985700","citationCount":1,"referenceCount":52,"year":2017,"authors":"S. Souto; M. D'Amorim; R. Gheyi","affiliations":"State Univ. of Paraiba, Paraíba, Brazil; Fed. Univ. of Pernambuco, Recife, Brazil; Fed. Univ. of Campina Grande, Campina Grande, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3386"},"title":"Stochastic Optimization of Program Obfuscation","abstract":"Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = 〈t1, t2, ..., tn〉 (∀i ∈ [1, n]. ti ∈ T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%.","conference":"IEEE","terms":"Optimization;Mathematical model;Reactive power;Markov processes;Google;Lenses;Software,Java;Markov processes;Monte Carlo methods;software engineering,stochastic optimization;program obfuscation;software development;source code;binary code;mathematical optimization problem;guided stochastic algorithm;obscurity language model;Markov chain Monte Carlo methods;JavaScript","keywords":"program obfuscation;obscurity language model;markov chain monte carlo methods","startPage":"221","endPage":"231","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985664","citationCount":2,"referenceCount":41,"year":2017,"authors":"H. Liu; C. Sun; Z. Su; Y. Jiang; M. Gu; J. Sun","affiliations":"Sch. of Software, Tsinghua Univ., Beijing, China; Univ. of California, Davis, Davis, CA, USA; Univ. of California, Davis, Davis, CA, USA; Sch. of Software, Tsinghua Univ., Beijing, China; Sch. of Software, Tsinghua Univ., Beijing, China; Sch. of Software, Tsinghua Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3387"},"title":"Search-Based Energy Testing of Android","abstract":"The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efficiently use the device's battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app's energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app's energy behavior, COBWEB generates a test suite that can effectively find energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efficiently test energy behavior of apps, but also its superiority over prior techniques by finding a wider and more diverse set of energy defects.","conference":"IEEE","terms":"Testing;Hardware;Global Positioning System;Graphical user interfaces;Smart phones;Receivers;Batteries,Android (operating system);mobile computing;program testing;search problems;smart phones,battery capacity;energy consumption;energy defects;energy properties;search-based energy testing technique;test suite;energy behavior;Android;COBWEB","keywords":"Energy Testing;Android;Software Testing","startPage":"1119","endPage":"1130","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812097","citationCount":1,"referenceCount":69,"year":2019,"authors":"R. Jabbarvand; J. Lin; S. Malek","affiliations":"University of California, Irvine; University of California, Irvine; University of California, Irvine","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3388"},"title":"Syntactic and Semantic Differencing for Combinatorial Models of Test Designs","abstract":"Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates. Manually reasoning about the differences between versions of real-world models following such updates is infeasible due to their complexity and size. Moreover, representing the differences is challenging. In this work, we propose a first syntactic and semantic differencing technique for combinatorial models of test designs. We define a concise and canonical representation for differences between two models, and suggest a scalable algorithm for automatically computing and presenting it. We use our differencing technique to analyze the evolution of 42 real-world industrial models, demonstrating its applicability and scalability. Further, a user study with 16 CTD practitioners shows that comprehension of differences between real-world combinatorial model versions is challenging and that our differencing tool significantly improves the performance of less experienced practitioners. The analysis and user study provide evidence for the potential usefulness of our differencing approach. Our work advances the state-of-the-art in CTD with better capabilities for change comprehension and management.","conference":"IEEE","terms":"Computational modeling;Semantics;Tools;Syntactics;Analytical models;Data models;Binary decision diagrams,program testing,combinatorial test design;automatic test plan generation;CTD practitioners","keywords":"","startPage":"621","endPage":"631","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985699","citationCount":2,"referenceCount":38,"year":2017,"authors":"R. Tzoref-Brill; S. Maoz","affiliations":"Sch. of Comput. Sci., Tel Aviv Univ., Tel Aviv, Israel; Sch. of Comput. Sci., Tel Aviv Univ., Tel Aviv, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3389"},"title":"A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches","abstract":"Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called DDU, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. Our experiments show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric.","conference":"IEEE","terms":"Cognition;Density measurement;Software;Computer bugs;Gain measurement;Measurement uncertainty,error detection;fault diagnosis;program debugging;program testing,test-suite diagnosability metric;spectrum-based fault localization techniques;fault isolation;DDU;error detection mechanisms;bugs;branch-coverage metric;density-diversity-uniqueness","keywords":"Testing;Coverage;Diagnosability","startPage":"654","endPage":"664","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985702","citationCount":6,"referenceCount":39,"year":2017,"authors":"A. Perez; R. Abreu; A. van Deursen","affiliations":"HASLab, Univ. of Porto, Porto, Portugal; HASLab, Univ. of Porto, Porto, Portugal; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d338a"},"title":"Statically Checking Web API Requests in JavaScript","abstract":"Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.","conference":"IEEE","terms":"Uniform resource locators;Data mining;Reactive power;Payloads;Tools;Media;Writing,application program interfaces;Java;program diagnostics,Web API requests;JavaScript;HTTP requests;compile-time error checking;URL string;interprocedural string analysis;GitHub","keywords":"Static analysis;JavaScript;Web APIs","startPage":"244","endPage":"254","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985666","citationCount":7,"referenceCount":31,"year":2017,"authors":"E. Wittern; A. T. T. Ying; Y. Zheng; J. Dolby; J. A. Laredo","affiliations":"IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d338b"},"title":"Repairing Event Race Errors by Controlling Nondeterminism","abstract":"Modern web applications are written in an event-driven style, in which event handlers execute asynchronously in response to user or system events. The nondeterminism arising from this programming style can lead to pernicious errors. Recent work focuses on detecting event races and classifying them as harmful or harmless. However, since modifying the source code to prevent harmful races can be a difficult and error-prone task, it may be preferable to steer away from the bad executions. In this paper, we present a technique for automated repair of event race errors in JavaScript web applications. Our approach relies on an event controller that restricts event handler scheduling in the browser according to a specified repair policy, by intercepting and carefully postponing or discarding selected events. We have implemented the technique in a tool called EventRaceCommander, which relies entirely on source code instrumentation, and evaluated it by repairing more than 100 event race errors that occur in the web applications from the largest 20 of the Fortune 500 companies. Our results show that application-independent repair policies usually suffice to repair event race errors without excessive negative impact on performance or user experience, though application-specific repair policies that target specific event races are sometimes desirable.","conference":"IEEE","terms":"Maintenance engineering;Thumb;Tools;Browsers;Instruments;Programming;Schedules,high level languages;Internet;object-oriented programming;online front-ends;software maintenance,event race error repair;JavaScript Web applications;event handler scheduling;browser;EventRaceCommander;source code instrumentation;application-independent repair policies;application-specific repair policies","keywords":"JavaScript;event-driven programming;automated repair","startPage":"289","endPage":"299","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985670","citationCount":2,"referenceCount":34,"year":2017,"authors":"C. Q. Adamsen; A. Møller; R. Karim; M. Sridharan; F. Tip; K. Sen","affiliations":"Aarhus Univ., Aarhus, Denmark; Aarhus Univ., Aarhus, Denmark; Samsung Res. America, Mountain View, CA, USA; Samsung Res. America, Mountain View, CA, USA; Northeastern Univ., Boston, MA, USA; EECS Dept., UC Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d338c"},"title":"Automatically Generating Precise Oracles from Structured Natural Language Specifications","abstract":"Software specifications often use natural language to describe the desired behavior, but such specifications are difficult to verify automatically. We present Swami, an automated technique that extracts test oracles and generates executable tests from structured natural language specifications. Swami focuses on exceptional behavior and boundary conditions that often cause field failures but that developers often fail to manually write tests for. Evaluated on the official JavaScript specification (ECMA-262), 98.4% of the tests Swami generated were precise to the specification. Using Swami to augment developer-written test suites improved coverage and identified 1 previously unknown defect and 15 missing JavaScript features in Rhino, 1 previously unknown defect in Node.js, and 18 semantic ambiguities in the ECMA-262 specification.","conference":"IEEE","terms":"Natural languages;Documentation;Lenses;Software;Boundary conditions;Semantics;Prototypes,formal specification;Java;natural language processing;program testing,structured natural language specifications;developer-written test suites;ECMA-262 specification;software specifications;test oracles;Swami technique;JavaScript features;JavaScript specification","keywords":"oracle;test oracle;test generation;natural language specification","startPage":"188","endPage":"199","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812070","citationCount":0,"referenceCount":80,"year":2019,"authors":"M. Motwani; Y. Brun","affiliations":"University of Massachusetts Amherst; University of Massachusetts Amherst","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d338d"},"title":"Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game","abstract":"Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program, automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.","conference":"IEEE","terms":"Games;Tools;Testing;Crowdsourcing;Writing;Software;Computer bugs,computer games;crowdsourcing;Internet;program testing,Code Defenders;mutation testing game;gamification;crowdsourcing;software tests;Web-based game","keywords":"gamification;crowdsourcing;software testing;mutation testing","startPage":"677","endPage":"688","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985704","citationCount":2,"referenceCount":53,"year":2017,"authors":"J. M. Rojas; T. D. White; B. S. Clegg; G. Fraser","affiliations":"Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d338e"},"title":"Leveraging Artifact Trees to Evolve and Reuse Safety Cases","abstract":"Safety Assurance Cases (SACs) are increasingly used to guide and evaluate the safety of software-intensive systems. They are used to construct a hierarchically organized set of claims, arguments, and evidence in order to provide a structured argument that a system is safe for use. However, as the system evolves and grows in size, a SAC can be difficult to maintain. In this paper we utilize design science to develop a novel solution for identifying areas of a SAC that are affected by changes to the system. Moreover, we generate actionable recommendations for updating the SAC, including its underlying artifacts and trace links, in order to evolve an existing safety case for use in a new version of the system. Our approach, Safety Artifact Forest Analysis (SAFA), leverages traceability to automatically compare software artifacts from a previously approved or certified version with a new version of the system. We identify, visualize, and explain changes in a Delta Tree. We evaluate our approach using the Dronology system for monitoring and coordinating the actions of cooperating, small Unmanned Aerial Vehicles. Results from a user study show that SAFA helped users to identify changes that potentially impacted system safety and provided information that could be used to help maintain and evolve a SAC.","conference":"IEEE","terms":"Vegetation;Hazards;Monitoring;Thermostats;Computer science;Forestry,safety-critical software;software reliability;trees (mathematics),artifact trees;software-intensive systems;structured argument;trace links;software artifacts;safety assurance cases;dronology system;safety artifact forest analysis","keywords":"Change Impact, Safety Assurance Cases, Evolution, Traceability","startPage":"1222","endPage":"1233","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812137","citationCount":0,"referenceCount":63,"year":2019,"authors":"A. Agrawal; S. Khoshmanesh; M. Vierhauser; M. Rahimi; J. Cleland-Huang; R. Lutz","affiliations":"University of Notre Dame; Iowa State University; University of Notre Dame; Northern Illinois University; University of Notre Dame; Iowa State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d338f"},"title":"BugSwarm: Mining and Continuously Growing a Dataset of Reproducible Failures and Fixes","abstract":"Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.","conference":"IEEE","terms":"Containers;Tools;Maintenance engineering;History;Software;Libraries;Currencies,data mining;Java;program debugging;program testing;Python;software fault tolerance;software maintenance;software quality,reproducible failures;fault-detection;repair methods;software quality;realistic datasets;durably-reproducible faults;TRAVIS-CI;BUGSWARM toolkit;fail-pass pairs;fully reproducible containers;fail-pass activities;continuous-integration approach;mining;fault localization;Java;Python","keywords":"Bug Database;Reproducibility;Software Testing;Program Analysis;Experiment Infrastructure","startPage":"339","endPage":"349","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812141","citationCount":1,"referenceCount":32,"year":2019,"authors":"D. A. Tomassi; N. Dmeiri; Y. Wang; A. Bhowmick; Y. Liu; P. T. Devanbu; B. Vasilescu; C. Rubio-González","affiliations":"University of California, Davis; University of California, Davis; University of California, Davis; University of California, Davis; University of California, Davis; University of California, Davis; Carnegie Mellon University; University of California, Davis","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3390"},"title":"Efficient Detection of Thread Safety Violations via Coverage-Guided Generation of Concurrent Tests","abstract":"As writing concurrent programs is challenging, developers often rely on thread-safe classes, which encapsulate most synchronization issues. Testing such classes is crucial to ensure the correctness of concurrent programs. An effective approach to uncover otherwise missed concurrency bugs is to automatically generate concurrent tests. Existing approaches either create tests randomly, which is inefficient, build on a computationally expensive analysis of potential concurrency bugs exposed by sequential tests, or focus on exposing a particular kind of concurrency bugs, such as atomicity violations. This paper presents CovCon, a coverage-guided approach to generate concurrent tests. The key idea is to measure how often pairs of methods have already been executed concurrently and to focus the test generation on infrequently or not at all covered pairs of methods. The approach is independent of any particular bug pattern, allowing it to find arbitrary concurrency bugs, and is computationally inexpensive, allowing it to generate many tests in short time. We apply CovCon to 18 thread-safe Java classes, and it detects concurrency bugs in 17 of them. Compared to five state of the art approaches, CovCon detects more bugs than any other approach while requiring less time. Specifically, our approach finds bugs faster in 38 of 47 cases, with speedups of at least 4x for 22 of 47 cases.","conference":"IEEE","terms":"Instruction sets;Computer bugs;Concurrent computing;Testing;Synchronization;Schedules,concurrency (computers);Java;program debugging;program testing;synchronisation,thread safety violation detection;coverage-guided generation;concurrent tests;concurrent program writing;synchronization;missed concurrency bugs;sequential tests;CovCon;bug pattern;thread-safe Java classes;test generation","keywords":"test generation;coverage;concurrency","startPage":"266","endPage":"277","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985668","citationCount":6,"referenceCount":67,"year":2017,"authors":"A. Choudhary; S. Lu; M. Pradel","affiliations":"Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany; Dept. of Comput. Sci., Univ. of Chicago, Chicago, IL, USA; Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3391"},"title":"A SEALANT for Inter-App Security Holes in Android","abstract":"Android's communication model has a major security weakness: malicious apps can manipulate other apps into performing unintended operations and can steal end-user data, while appearing ordinary and harmless. This paper presents SEALANT, a technique that combines static analysis of app code, which infers vulnerable communication channels, with runtime monitoring of inter-app communication through those channels, which helps to prevent attacks. SEALANT's extensive evaluation demonstrates that (1) it detects and blocks inter-app attacks with high accuracy in a corpus of over 1,100 real-world apps, (2) it suffers from fewer false alarms than existing techniques in several representative scenarios, (3) its performance overhead is negligible, and (4) end-users do not find it challenging to adopt.","conference":"IEEE","terms":"Sealing materials;Androids;Humanoid robots;Runtime;Security;Monitoring;Focusing,Android (operating system);program diagnostics;security of data,SEALANT technique;inter-app security holes;Android communication model;static analysis;app code;vulnerable communication channels;inter-app communication runtime monitoring","keywords":"Android;Security;Inter-app vulnerability","startPage":"312","endPage":"323","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985672","citationCount":8,"referenceCount":95,"year":2017,"authors":"Y. K. Lee; J. Y. Bang; G. Safi; A. Shahbazian; Y. Zhao; N. Medvidovic","affiliations":"Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA; Kakao Corp., Seongnam, South Korea; Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA; Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA; Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA; Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3392"},"title":"Mining Software Defects: Should We Consider Affected Releases?","abstract":"With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models.","conference":"IEEE","terms":"Predictive models;Object oriented modeling;Software quality;Feature extraction;Data mining;Control systems,data mining;pattern classification;software maintenance;software quality,defect count models;defect classification models;software quality;defect data preparation;post-release defects;software development team;software repositories;defect identification;software defect mining","keywords":"Mining Software Repositories;Empirical Software Engineering;Software Quality;Defect Prediction Models","startPage":"654","endPage":"665","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811982","citationCount":0,"referenceCount":76,"year":2019,"authors":"S. Yatish; J. Jiarpakdee; P. Thongtanunam; C. Tantithamthavorn","affiliations":"The University of Adelaide; Monash University; The University of Melbourne; Monash University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3393"},"title":"LEOPARD: Identifying Vulnerable Code for Vulnerability Assessment Through Program Metrics","abstract":"Identifying potentially vulnerable locations in a code base is critical as a pre-step for effective vulnerability assessment; i.e., it can greatly help security experts put their time and effort to where it is needed most. Metric-based and pattern-based methods have been presented for identifying vulnerable code. The former relies on machine learning and cannot work well due to the severe imbalance between non-vulnerable and vulnerable code or lack of features to characterize vulnerabilities. The latter needs the prior knowledge of known vulnerabilities and can only identify similar but not new types of vulnerabilities. In this paper, we propose and implement a generic, lightweight and extensible framework, LEOPARD, to identify potentially vulnerable functions through program metrics. LEOPARD requires no prior knowledge about known vulnerabilities. It has two steps by combining two sets of systematically derived metrics. First, it uses complexity metrics to group the functions in a target application into a set of bins. Then, it uses vulnerability metrics to rank the functions in each bin and identifies the top ones as potentially vulnerable. Our experimental results on 11 real-world projects have demonstrated that, LEOPARD can cover 74.0% of vulnerable functions by identifying 20% of functions as vulnerable and outperform machine learning-based and static analysis-based techniques. We further propose three applications of LEOPARD for manual code review and fuzzing, through which we discovered 22 new bugs in real applications like PHP, radare2 and FFmpeg, and eight of them are new vulnerabilities.","conference":"IEEE","terms":"Measurement;Complexity theory;Security;Computer bugs;Fuzzing;Machine learning;Manuals,learning (artificial intelligence);program diagnostics;security of data;software metrics,pattern-based methods;LEOPARD;program metrics;systematically derived metrics;complexity metrics;vulnerability metrics;manual code review;vulnerability assessment;metric-based method;vulnerable code identification;machine learning-based technique","keywords":"Program Metric, Vulnerability, Fuzzing","startPage":"60","endPage":"71","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812029","citationCount":0,"referenceCount":82,"year":2019,"authors":"X. Du; B. Chen; Y. Li; J. Guo; Y. Zhou; Y. Liu; Y. Jiang","affiliations":"Nanyang Technological University, Singapore; Fudan University, China; Nanyang Technological University, Singapore; Tsinghua University, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Tsinghua University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3394"},"title":"On Learning Meaningful Code Changes Via Neural Machine Translation","abstract":"Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.","conference":"IEEE","terms":"Vocabulary;Crawlers;Computer bugs;Java;Data mining;Software engineering;Task analysis,language translation;learning (artificial intelligence);program debugging;program testing;public domain software;software maintenance;statistical analysis,pull requests;pull request changes;bug-fixing activities;source code;maintenance tasks;nontrivial coding activities;NMT model;code components;code changes;vulnerabilities detection;neural machine translation model;qualitative analysis","keywords":"Neural-Machine Translation;Empirical Study","startPage":"25","endPage":"36","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811910","citationCount":0,"referenceCount":67,"year":2019,"authors":"M. Tufano; J. Pantiuchina; C. Watson; G. Bavota; D. Poshyvanyk","affiliations":"The College of William and Mary; Università della Svizzera italiana (USI); The College of William and Mary; Università della Svizzera italiana (USI); The College of William and Mary","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3395"},"title":"FastLane: Test Minimization for Rapidly Deployed Large-Scale Online Services","abstract":"Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases. This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%.","conference":"IEEE","terms":"Testing;Correlation;Minimization;Predictive models;Complexity theory;Prediction algorithms;Machine learning,electronic mail;groupware;Internet;learning (artificial intelligence);program testing;software quality,FastLane;rapidly deployed large-scale online services;code-base;resource-intensive tasks;code-quality;light-weight machine-learning models;large-scale email;collaboration platform service;time-critical tasks;data-driven test minimization;continuous integration-continuous deployment processes;CI-CD processes;commit logs","keywords":"test prioritization;commit risk;machine learning","startPage":"408","endPage":"418","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812033","citationCount":0,"referenceCount":39,"year":2019,"authors":"A. A. Philip; R. Bhagwan; R. Kumar; C. S. Maddila; N. Nagppan","affiliations":"Microsoft Research, India; Microsoft Research, India; Microsoft Research, India; Microsoft Research, India; Microsoft Research, Redmond","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3396"},"title":"DLFinder: Characterizing and Detecting Duplicate Logging Code Smells","abstract":"Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed.","conference":"IEEE","terms":"Manuals;Static analysis;Cloud computing;Debugging;Tools;Semantics;Java,cloud computing;data handling;parallel processing;program debugging;program diagnostics;public domain software,DLFinder;duplicate logging code smells;software logs;single logging statement;duplicate logging statements;problematic duplicate logging code;static text message;dynamic view;open source systems;Hadoop;CloudStack;ElasticSearch;Cassandra;developers feedback","keywords":"log;code smell;duplicate log;static analysis;empirical study","startPage":"152","endPage":"163","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811945","citationCount":1,"referenceCount":54,"year":2019,"authors":"Z. Li; T. Chen; J. Yang; W. Shang","affiliations":"Concordia University, Canada; Concordia University, Canada; Concordia University, Canada; Concordia University, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3397"},"title":"FLOSS Participants' Perceptions About Gender and Inclusiveness: A Survey","abstract":"Background: While FLOSS projects espouse openness and acceptance for all, in practice, female contributors often face discriminatory barriers to contribution. Aims: In this paper, we examine the extent to which these problems still exist. We also study male and female contributors' perceptions of other contributors. Method: We surveyed participants from 15 FLOSS projects, asking a series of open-ended, closed-ended, and behavioral scale questions to gather information about the issue of gender in FLOSS projects. Results: Though many of those we surveyed expressed a positive sentiment towards females who participate in FLOSS projects, some were still strongly against their inclusion. Often, the respondents who were against inclusiveness also believed their own sentiments were the prevailing belief in the community, contrary to our findings. Others did not see the purpose of attempting to be inclusive, expressing the sentiment that a discussion of gender has no place in FLOSS. Conclusions: FLOSS projects have started to move forwards in terms of gender acceptance. However, there is still a need for more progress in the inclusion of gender-diverse contributors.","conference":"IEEE","terms":"Computer science;Software;Face;Data mining;Software engineering;Cognition;IEEE Fellows,gender issues;public domain software;sentiment analysis;social aspects of automation,gender-diverse contributors;inclusiveness;female contributors;FLOSS projects;gender acceptance;FLOSS participant perceptions;positive sentiment;male contributor perceptions;female contributor perceptions","keywords":"FLOSS;gender;survey;Open Source","startPage":"677","endPage":"687","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812068","citationCount":0,"referenceCount":27,"year":2019,"authors":"A. Lee; J. C. Carver","affiliations":"The University of Alabama, United States of America; The University of Alabama, United States of America","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3398"},"title":"Class Imbalance Evolution and Verification Latency in Just-in-Time Software Defect Prediction","abstract":"Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that re-build classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency -- the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.","conference":"IEEE","terms":"Software;Training;Machine learning algorithms;Machine learning;Prediction algorithms;Delays,learning (artificial intelligence);pattern classification;safety-critical software;sampling methods;software fault tolerance,SDP approach;predictive performance;class imbalance evolution approach;just-in-time software defect prediction;JIT-SDP approaches;top ranked g-means","keywords":"Software defect prediction;class imbalance;verification latency;online learning;concept drift;ensembles","startPage":"666","endPage":"676","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812072","citationCount":0,"referenceCount":36,"year":2019,"authors":"G. G. Cabral; L. L. Minku; E. Shihab; S. Mujahid","affiliations":"University of Birmingham (UK) and Federal Rural University of Pernambuco (Brazil); University of Birmingham (UK); Concordia University (Canada); Concordia University (Canada)","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d3399"},"title":"Learning to Prioritize Test Programs for Compiler Testing","abstract":"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.","conference":"IEEE","terms":"Computer bugs;Testing;Program processors;Life estimation;Feature extraction;Training;Predictive models,learning (artificial intelligence);program compilers;program testing;scheduling;software reliability,compiler reliability;software systems;automated compiler testing;test-generation tools;compiler bugs;bug-revealing test programs;LET;learning process;scheduling process;time model;bug-revealing probabilities","keywords":"","startPage":"700","endPage":"711","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985706","citationCount":2,"referenceCount":79,"year":2017,"authors":"J. Chen; Y. Bai; D. Hao; Y. Xiong; H. Zhang; B. Xie","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Univ. of Newcastle, Newcastle, NSW, Australia; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d339a"},"title":"Fuzzy Fine-Grained Code-History Analysis","abstract":"Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent revisions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by representing code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi-revision code-history analysis - fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable balance of precision and recall, and an overall improved accuracy over existing code-evolution models. Furthermore, we found that the use of such a fuzzy model of history provided improved accuracy for code-history analysis tasks.","conference":"IEEE","terms":"History;Analytical models;Solids;Computer bugs;Measurement;Computational modeling;Cloning,fuzzy set theory;graph theory;program slicing;software maintenance,fuzzy fine-grained code-history analysis;code lineage;novel multirevision code-history analysis;fuzzy history graph;fuzzy history slicing","keywords":"software engineering;computer aided software engineering;software maintenance;reasoning about programs","startPage":"746","endPage":"757","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985710","citationCount":1,"referenceCount":55,"year":2017,"authors":"F. Servant; J. A. Jones","affiliations":"NA; Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d339b"},"title":"Resource-Aware Program Analysis Via Online Abstraction Coarsening","abstract":"We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity to meet a constraint on peak memory consumption. The experimental results with 18 real-world programs show that our algorithm can learn a good controller and the analysis with this controller meets the constraint and utilizes available memory effectively.","conference":"IEEE","terms":"Memory management;Static analysis;Reinforcement learning;Task analysis;Software engineering;Benchmark testing;Indexes,C language;program diagnostics;program verification,fixpoint computation;benchmark programs;static analysis;real-world programs;resource-aware program analysis;online abstraction coarsening;program abstraction;C programs;physical resources","keywords":"static analysis;resource constraint;learning","startPage":"94","endPage":"104","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812143","citationCount":0,"referenceCount":37,"year":2019,"authors":"K. Heo; H. Oh; H. Yang","affiliations":"University of Pennsylvania; Korea University; KAIST","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d339c"},"title":"LibD: Scalable and Precise Third-Party Library Detection in Android Markets","abstract":"With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis. According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-to-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.","conference":"IEEE","terms":"Libraries;Androids;Humanoid robots;Tools;Mobile communication;Security;Java,Android (operating system);mobile computing;pattern classification;security of data;software libraries;software tools,LibD tool;precise third-party library detection;Android markets;mobile app markets;security problems;mobile ecosystem;third-party Android library identification;malware analysis;repackaging detection;internal code dependencies;library candidate classification;feature hashing;multipackage third-party libraries;name-based obfuscation","keywords":"Android;third-party library;software mining","startPage":"335","endPage":"346","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985674","citationCount":12,"referenceCount":46,"year":2017,"authors":"M. Li; W. Wang; P. Wang; S. Wang; D. Wu; J. Liu; R. Xue; W. Huo","affiliations":"Key Lab. of Network Assessment Technol., Inst. of Inf. Eng., Beijing, China; Key Lab. of Network Assessment Technol., Inst. of Inf. Eng., Beijing, China; Coll. of Inf. Sci. \u0026 Technol., Pennsylvania State Univ., University Park, PA, USA; Coll. of Inf. Sci. \u0026 Technol., Pennsylvania State Univ., University Park, PA, USA; Coll. of Inf. Sci. \u0026 Technol., Pennsylvania State Univ., University Park, PA, USA; Key Lab. of Network Assessment Technol., Inst. of Inf. Eng., Beijing, China; State Key Lab. of Inf. Security, Inst. of Inf. Eng., Beijing, China; Key Lab. of Network Assessment Technol., Inst. of Inf. Eng., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d339d"},"title":"Zero-Overhead Path Prediction with Progressive Symbolic Execution","abstract":"In previous work, we introduced zero-overhead profiling (ZOP), a technique that leverages the electromagnetic emissions generated by the computer hardware to profile a program without instrumenting it. Although effective, ZOP has several shortcomings: it requires test inputs that achieve extensive code coverage for its training phase; it predicts path profiles instead of complete execution traces; and its predictions can suffer unrecoverable accuracy losses. In this paper, we present zero-overhead path prediction (ZOP-2), an approach that extends ZOP and addresses its limitations. First, ZOP-2 achieves high coverage during training through progressive symbolic execution (PSE)-symbolic execution of increasingly small program fragments. Second, ZOP-2 predicts complete execution traces, rather than path profiles. Finally, ZOP-2 mitigates the problem of path mispredictions by using a stateless approach that can recover from prediction errors. We evaluated our approach on a set of benchmarks with promising results; for the cases considered, (1) ZOP-2 achieved over 90% path prediction accuracy, and (2) PSE covered feasible paths missed by traditional symbolic execution, thus boosting ZOP-2's accuracy.","conference":"IEEE","terms":"Training;Predictive models;Instruments;Electromagnetics;Benchmark testing;Task analysis;Microsoft Windows,program testing;symbol manipulation,symbolic execution;path prediction accuracy;electromagnetic emissions;computer hardware;extensive code coverage;path mispredictions;progressive symbolic execution;zero-overhead profiling;prediction errors;zero-overhead path prediction;complete execution traces;path profiles","keywords":"symbolic execution;path profiling;tracing","startPage":"234","endPage":"245","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812035","citationCount":0,"referenceCount":49,"year":2019,"authors":"R. Rutledge; S. Park; H. Khan; A. Orso; M. Prvulovic; A. Zajic","affiliations":"Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology; Georgia Institute of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d339e"},"title":"Tool Choice Matters: JavaScript Quality Assurance Tools and Usage Outcomes in GitHub Projects","abstract":"Quality assurance automation is essential in modern software development. In practice, this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context. Data and analytics of the pros and cons can inform these decisions. Yet, in most cases, there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices. We propose a general methodology to model the time-dependent effect of automation tool choice on four outcomes of interest: prevalence of issues, code churn, number of pull requests, and number of contributors, all with a multitude of controls. On a large data set of npm JavaScript projects, we extract the adoption events for popular tools in three task classes: linters, dependency managers, and coverage reporters. Using mixed methods approaches, we study the reasons for the adoptions and compare the adoption effects within each class, and sequential tool adoptions across classes. We find that some tools within each group are associated with more beneficial outcomes than others, providing an empirical perspective for the benefits of each. We also find that the order in which some tools are implemented is associated with varying outcomes.","conference":"IEEE","terms":"Tools;Task analysis;Pipelines;Quality assurance;Automation;Software;Switches,decision making;Internet;Java;program debugging;program diagnostics;program testing;project management;public domain software;quality assurance;software quality,adoption effects;sequential tool adoptions;beneficial outcomes;varying outcomes;tool choice matters;JavaScript quality assurance tools;GitHub projects;quality assurance automation;empirical evidence;time-dependent effect;automation tool choice;data set;npm JavaScript projects;adoption events;task classes;dependency managers;software development","keywords":"quality assurance tools;empirical study","startPage":"476","endPage":"487","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812106","citationCount":1,"referenceCount":86,"year":2019,"authors":"D. Kavaler; A. Trockman; B. Vasilescu; V. Filkov","affiliations":"UC Davis; University of Evansville; CMU; UC Davis","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d339f"},"title":"Investigating the Effects of Gender Bias on GitHub","abstract":"Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature.We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub,then evaluate those hypotheses quantitatively. While our results how that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.","conference":"IEEE","terms":"Software;Software engineering;Companies;Correlation;Encoding;Computer science,gender issues;software development management;software metrics,gender bias;gender diversity;software development organizations;gender studies;GitHub","keywords":"gender bias;software engineering","startPage":"700","endPage":"711","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812110","citationCount":0,"referenceCount":50,"year":2019,"authors":"N. Imtiaz; J. Middleton; J. Chakraborty; N. Robson; G. Bai; E. Murphy-Hill","affiliations":"North Carolina State University; North Carolina State University; North Carolina State University; North Carolina State University; North Carolina State University; Google LLC","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a0"},"title":"Training Binary Classifiers as Data Structure Invariants","abstract":"We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artificial neural network, more precisely a binary classifier, and training it to identify valid and invalid instances of a data structure. The obtained classifier can then be used in place of the data structure's invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object fields that \"break\" the collected valid values, i.e., that assign values to object fields that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classifiers that achieve significantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug finding.","conference":"IEEE","terms":"Data structures;Tools;Java;Computer bugs;Software;Neural networks;Test pattern generators,data structures;learning (artificial intelligence);neural nets;pattern classification;program debugging;program testing,data structure invariants;invalid data structure objects;artificial neural network;invalid instances;object building routines;assign values;learning technique;dynamic invariant detection;valid instances;binary classifiers training;bug finding;valid data structure objects","keywords":"Specification inference;Machine learning;Bug finding","startPage":"759","endPage":"770","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811951","citationCount":0,"referenceCount":43,"year":2019,"authors":"F. Molina; R. Degiovanni; P. Ponzio; G. Regis; N. Aguirre; M. Frias","affiliations":"CONICET and University of Rio Cuarto, Argentina; SnT, University of Luxembourg, Luxembourg; CONICET and University of Rio Cuarto, Argentina; University of Rio Cuarto, Argentina; CONICET and University of Rio Cuarto, Argentina; CONICET and Buenos Aires Institute of Technology, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a1"},"title":"Symbolic Model Extraction for Web Application Verification","abstract":"Modern web applications use complex data models and access control rules which lead to data integrity and access control errors. One approach to find such errors is to use formal verification techniques. However, as a first step, most formal verification techniques require extraction of a formal model which is a difficult problem in itself due to dynamic features of modern languages, and it is typically done either manually, or using ad hoc techniques. In this paper, we present a technique called symbolic model extraction for extracting formal data models from web applications. The key ideas of symbolic model extraction are 1) to use the source language interpreter for model extraction, which enables us to handle dynamic features of the language, 2) to use code instrumentation so that execution of each instrumented piece of code returns the formal model that corresponds to that piece of code, 3) to instrument the code dynamically so that the models of methods that are created at runtime can also be extracted, and 4) to execute both sides of branches during instrumented execution so that all program behaviors can be covered in a single instrumented execution. We implemented the symbolic model extraction technique for the Rails framework and used it to extract data and access control models from web applications. Our experiments demonstrate that symbolic model extraction is scalable and extracts formal models that are precise enough to find bugs in real-world applications without reporting too many false positives.","conference":"IEEE","terms":"Instruments;Feature extraction;Data models;Data mining;Rails;Runtime;Load modeling,authorisation;data integrity;formal verification,symbolic model extraction technique;program behaviors;source language interpreter;formal verification techniques;access control rules;complex data models;Web application verification","keywords":"Formal Verification;Model Extraction;Web Applications","startPage":"724","endPage":"734","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985708","citationCount":0,"referenceCount":31,"year":2017,"authors":"I. Bocic; T. Bultan","affiliations":"Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a2"},"title":"The Evolution of Continuous Experimentation in Software Product Development: From Data to a Data-Driven Organization at Scale","abstract":"Software development companies are increasingly aiming to become data-driven by trying to continuously experiment with the products used by their customers. Although familiar with the competitive edge that the A/B testing technology delivers, they seldom succeed in evolving and adopting the methodology. In this paper, and based on an exhaustive and collaborative case study research in a large software-intense company with highly developed experimentation culture, we present the evolution process of moving from ad-hoc customer data analysis towards continuous controlled experimentation at scale. Our main contribution is the \"Experimentation Evolution Model\" in which we detail three phases of evolution: technical, organizational and business evolution. With our contribution, we aim to provide guidance to practitioners on how to develop and scale continuous experimentation in software organizations with the purpose of becoming data-driven at scale.","conference":"IEEE","terms":"Software engineering,organisational aspects;product development;software engineering,continuous experimentation;software product development;data-driven organization;A/B testing technology;software-intense company;ad-hoc customer data analysis;experimentation evolution model;business evolution;organizational evolution;technical evolution;software organizations","keywords":"A/B testing;continuous experimentation;data science;customer feedback;continuous product innovation;Experimentation Evolution Model;product value;Experiment Owner","startPage":"770","endPage":"780","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985712","citationCount":12,"referenceCount":43,"year":2017,"authors":"A. Fabijan; P. Dmitriev; H. H. Olsson; J. Bosch","affiliations":"Fac. of Technol. \u0026 Soc., Malmo Univ., Malmo, Sweden; Microsoft Anal. \u0026 Experimentation, Microsoft, Redmond, WA, USA; Fac. of Technol. \u0026 Soc., Malmo Univ., Malmo, Sweden; Dept. of Comput. Sci. \u0026 Eng., Chalmers Univ. of Technol., Goteborg, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a3"},"title":"Adaptive Unpacking of Android Apps","abstract":"More and more app developers use the packing services (or packers) to prevent attackers from reverse engineering and modifying the executable (or Dex files) of their apps. At the same time, malware authors also use the packers to hide the malicious component and evade the signature-based detection. Although there are a few recent studies on unpacking Android apps, it has been shown that the evolving packers can easily circumvent them because they are not adaptive to the changes of packers. In this paper, we propose a novel adaptive approach and develop a new system, named PackerGrind, to unpack Android apps. We also evaluate PackerGrind with real packed apps, and the results show that PackerGrind can successfully reveal the packers' protection mechanisms and recover the Dex files with low overhead, showing that our approach can effectively handle the evolution of packers.","conference":"IEEE","terms":"Androids;Humanoid robots;Monitoring;Subspace constraints;Data collection;Runtime;Loading,Android (operating system);invasive software;reverse engineering,adaptive unpacking;Android apps;signature-based detection;reverse engineering;PackerGrind;malware authors","keywords":"Dynamic Analysis;App Unpacking","startPage":"358","endPage":"369","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985676","citationCount":12,"referenceCount":61,"year":2017,"authors":"L. Xue; X. Luo; L. Yu; S. Wang; D. Wu","affiliations":"Dept. of Comput., Hong Kong Polytech. Univ., Hong Kong, China; Dept. of Comput., Hong Kong Polytech. Univ., Hong Kong, China; Dept. of Comput., Hong Kong Polytech. Univ., Hong Kong, China; Dept. of Comput., Hong Kong Polytech. Univ., Hong Kong, China; Coll. of Inf. Sci. \u0026 Technol, Pennsylvania State Univ., University Park, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a4"},"title":"Latent Patterns in Activities: A Field Study of How Developers Manage Context","abstract":"In order to build efficient tools that support complex programming tasks, it is imperative that we understand how developers program. We know that developers create a context around their programming task by gathering relevant information. We also know that developers decompose their tasks recursively into smaller units. However, important gaps exist in our knowledge about: (1) the role that context plays in supporting smaller units of tasks, (2) the relationship that exists among these smaller units, and (3) how context flows across them. The goal of this research is to gain a better understanding of how developers structure their tasks and manage context through a field study of ten professional developers in an industrial setting. Our analysis reveals that developers decompose their tasks into smaller units with distinct goals, that specific patterns exist in how they sequence these smaller units, and that developers may maintain context between those smaller units with related goals.","conference":"IEEE","terms":"Task analysis;Software;Encoding;Tools;Java;Programming profession,formal specification;software development management;software quality,programming task;professional developers;complex programming tasks;latent patterns","keywords":"context, task decomposition, field study","startPage":"373","endPage":"383","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811986","citationCount":0,"referenceCount":34,"year":2019,"authors":"S. Chattopadhyay; N. Nelson; Y. Ramirez Gonzalez; A. Amelia Leon; R. Pandita; A. Sarma","affiliations":"Oregon State University; Oregon State University; Oregon State University; Oregon State University; Phase Change Software; Oregon State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a5"},"title":"Learning Syntactic Program Transformations from Examples","abstract":"Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.","conference":"IEEE","terms":"DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software,automatic programming;program processors,syntactic program transformations learning;REFAZER;programming-by-example methodology;domain-specific language;domain-specific deductive algorithms;DSL;code edits;C# open-source projects","keywords":"Program transformation;program synthesis;tutoring systems;refactoring","startPage":"404","endPage":"415","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985680","citationCount":10,"referenceCount":50,"year":2017,"authors":"R. Rolim; G. Soares; L. D'Antoni; O. Polozov; S. Gulwani; R. Gheyi; R. Suzuki; B. Hartmann","affiliations":"UFCG, Brazil; UFCG, Brazil; Univ. of Wisconsin - Madison, Madison, WI, USA; Univ. of Washington, Seattle, WA, USA; Microsoft, Redmond, WA, USA; UFCG, Brazil; Univ. of Colorado Boulder, Boulder, CO, USA; UC Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a6"},"title":"Why Does Code Review Work for Open Source Software Communities?","abstract":"Open source software communities have demonstrated that they can produce high quality results. The overall success of peer code review, commonly used in open source projects, has likely contributed strongly to this success. Code review is an emotionally loaded practice, with public exposure of reputation and ample opportunities for conflict. We set off to ask why code review works for open source communities, despite this inherent challenge. We interviewed 21 open source contributors from four communities and participated in meetings of ROS community devoted to implementation of the code review process. It appears that the hacker ethic is a key reason behind the success of code review in FOSS communities. It is built around the ethic of passion and the ethic of caring. Furthermore, we observed that tasks of code review are performed with strong intrinsic motivation, supported by many non-material extrinsic motivation mechanisms, such as desire to learn, to grow reputation, or to improve one's positioning on the job market. In the paper, we describe the study design, analyze the collected data and formulate 20 proposals for how what we know about hacker ethics and human and social aspects of code review, could be exploited to improve the effectiveness of the practice in software projects.","conference":"IEEE","terms":"Interviews;Software;Standards;Ethics;Guidelines;Robot kinematics,human factors;public domain software;software engineering,open source projects;code review process;code review work;open source software communities;peer code review;open source contributors;ROS community;FOSS communities;nonmaterial extrinsic motivation mechanisms","keywords":"Open Source, Code Review, Motivation","startPage":"1073","endPage":"1083","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812037","citationCount":0,"referenceCount":46,"year":2019,"authors":"A. Alami; M. Leavitt Cohn; A. Wąsowski","affiliations":"IT University of Copenhagen; IT University of Copenhagen, Denmark; IT University of Copenhagen, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a7"},"title":"The Seven Sins: Security Smells in Infrastructure as Code Scripts","abstract":"Practitioners use infrastructure as code (IaC) scripts to provision servers and development environments. While developing IaC scripts, practitioners may inadvertently introduce security smells. Security smells are recurring coding patterns that are indicative of security weakness and can potentially lead to security breaches. The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code (IaC) scripts through an empirical study of security smells in IaC scripts. We apply qualitative analysis on 1,726 IaC scripts to identify seven security smells. Next, we implement and validate a static analysis tool called Security Linter for Infrastructure as Code scripts (SLIC) to identify the occurrence of each smell in 15,232 IaC scripts collected from 293 open source repositories. We identify 21,201 occurrences of security smells that include 1,326 occurrences of hard-coded passwords. We submitted bug reports for 1,000 randomly-selected security smell occurrences. We obtain 212 responses to these bug reports, of which 148 occurrences were accepted by the development teams to be fixed. We observe security smells can have a long lifetime, e.g., a hard-coded secret can persist for as long as 98 months, with a median lifetime of 20 months.","conference":"IEEE","terms":"Password;Encoding;Tools;Software;Servers;Static analysis,program debugging;program diagnostics;security of data,hard-coded passwords;security smells;hard-coded secret;security weakness;security breaches;IaC scripts;security linter tool;security linter for infrastructure as code scripts","keywords":"devops, devsecops, empirical study, infrastructure as code, puppet, security, smell, static analysis","startPage":"164","endPage":"175","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812041","citationCount":6,"referenceCount":56,"year":2019,"authors":"A. Rahman; C. Parnin; L. Williams","affiliations":"North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a8"},"title":"IconIntent: Automatic Identification of Sensitive UI Widgets Based on Icon Classification for Android Apps","abstract":"Many mobile applications (i.e., apps) include UI widgets to use or collect users' sensitive data. Thus, to identify suspicious sensitive data usage such as UI-permission mismatch, it is crucial to understand the intentions of UI widgets. However, many UI widgets leverage icons of specific shapes (object icons) and icons embedded with text (text icons) to express their intentions, posing challenges for existing detection techniques that analyze only textual data to identify sensitive UI widgets. In this work, we propose a novel app analysis framework, ICONINTENT, that synergistically combines program analysis and icon classification to identify sensitive UI widgets in Android apps. ICONINTENT automatically associates UI widgets and icons via static analysis on app's UI layout files and code, and then adapts computer vision techniques to classify the associated icons into eight categories of sensitive data. Our evaluations of ICONINTENT on 150 apps from Google Play show that ICONINTENT can detect 248 sensitive UI widgets in 97 apps, achieving a precision of 82.4%. When combined with SUPOR, the state-of-the-art sensitive UI widget identification technique based on text analysis, SUPOR +ICONINTENT can detect 487 sensitive UI widgets (101.2% improvement over SUPOR only), and reduces suspicious permissions to be inspected by 50.7% (129.4% improvement over SUPOR only).","conference":"IEEE","terms":"Layout;Image color analysis;Optical character recognition software;Static analysis;Global Positioning System;Feature extraction;Shape,Android (operating system);computer vision;graphical user interfaces;image classification;mobile computing;program diagnostics;text analysis;user interfaces,icon classification;android apps;suspicious sensitive data usage;UI widget identification technique;sensitive UI widgets;ICONINTENT;SUPOR;suspicious permissions;Google Play;mobile applications;user sensitive data;textual data;computer vision techniques","keywords":"Mobile Security;Program Analysis;Computer Vision;Icon Recognition","startPage":"257","endPage":"268","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812108","citationCount":1,"referenceCount":45,"year":2019,"authors":"X. Xiao; X. Wang; Z. Cao; H. Wang; P. Gao","affiliations":"Case Western Reserve University; The University of Texas at San Antonio; Case Western Reserve University; Case Western Reserve University; Princeton University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33a9"},"title":"Detecting Atomicity Violations for Event-Driven Node.js Applications","abstract":"Node.js has been widely-used as an event-driven server-side architecture. To improve performance, a task in a Node.js application is usually divided into a group of events, which are non-deterministically scheduled by Node.js. Developers may assume that the group of events (named atomic event group) should be atomically processed, without interruption. However, the atomicity of an atomic event group is not guaranteed by Node.js, and thus other events may interrupt the execution of the atomic event group, break down the atomicity and cause unexpected results. Existing approaches mainly focus on event race among two events, and cannot detect high-level atomicity violations among a group of events. In this paper, we propose NodeAV, which can predictively detect atomicity violations in Node.js applications based on an execution trace. Based on happens-before relations among events in an execution trace, we automatically identify a pair of events that should be atomically processed, and use predefined atomicity violation patterns to detect atomicity violations. We have evaluated NodeAV on real-world Node.js applications. The experimental results show that NodeAV can effectively detect atomicity violations in these Node.js applications.","conference":"IEEE","terms":"Task analysis;Programming;Instruction sets;Computer architecture;Message systems;Concurrent computing;Computer bugs,Java;program testing;scheduling,event-driven server-side architecture;Node.js application;atomic event group;event race;atomicity violation patterns;event-driven Node.js applications;NodeAV","keywords":"Node.js;event-driven architecture;atomicity violation;happens-before","startPage":"631","endPage":"642","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811949","citationCount":0,"referenceCount":48,"year":2019,"authors":"X. Chang; W. Dou; Y. Gao; J. Wang; J. Wei; T. Huang","affiliations":"State Key Lab of Computer Sciences; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33aa"},"title":"Dynamic Slicing for Android","abstract":"Dynamic program slicing is useful for a variety of tasks, from testing to debugging to security. Prior slicing approaches have targeted traditional desktop/server platforms, rather than mobile platforms such as Android. Slicing mobile, event-based systems is challenging due to their asynchronous callback construction and the IPC (interprocess communication)- heavy, sensor-driven, timing-sensitive nature of the platform. To address these problems, we introduce AndroidSlicer1, the first slicing approach for Android. AndroidSlicer combines a novel asynchronous slicing approach for modeling data and control dependences in the presence of callbacks with lightweight and precise instrumentation; this allows slicing for apps running on actual phones, and without requiring the app's source code. Our slicer is capable of handling a wide array of inputs that Android supports without adding any noticeable overhead. Experiments on 60 apps from Google Play show that AndroidSlicer is effective (reducing the number of instructions to be examined to 0.3% of executed instructions) and efficient (app instrumentation and post-processing combined takes 31 seconds); all while imposing a runtime overhead of just 4%. We present three applications of AndroidSlicer that are particularly relevant in the mobile domain: (1) finding and tracking input parts responsible for an error/crash, (2) fault localization, i.e., finding the instructions responsible for an error/crash, and (3) reducing the regression test suite. Experiments with these applications on an additional set of 18 popular apps indicate that AndroidSlicer is effective for Android testing and debugging.","conference":"IEEE","terms":"Smart phones;Registers;Computer crashes;Testing;Runtime;Debugging;Pins,Android (operating system);mobile computing;program debugging;program slicing;program testing,timing-sensitive nature;AndroidSlicer1;modeling data;control dependences;lightweight instrumentation;precise instrumentation;executed instructions;mobile domain;tracking input parts;regression test suite;Android testing;debugging;dynamic program slicing;mobile platforms;mobile event-based systems;asynchronous callback construction;IPC;interprocess communication;asynchronous slicing approach","keywords":"Mobile apps, Android, Dynamic analysis","startPage":"1154","endPage":"1164","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811953","citationCount":0,"referenceCount":32,"year":2019,"authors":"T. Azim; A. Alavi; I. Neamtiu; R. Gupta","affiliations":"University of California, Riverside; University of California, Riverside; New Jersey Institute of Technology; University of California, Riverside","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33ab"},"title":"The Product Backlog","abstract":"Context: One of the most common artifacts in contemporary software projects is a product backlog comprising user stories, bugs, chores or other work items. However, little research has investigated how the backlog is generated or the precise role it plays in a project. Objective: The purpose of this paper is to determine what is a product backlog, what is its role, and how does it emerge? Method: Following Constructivist Grounded Theory, we conducted a two-year, five-month participant-observation study of eight software development projects at Pivotal, a large, international software company. We interviewed 56 software engineers, product designers, and product managers.We conducted a survey of 27 product designers. We alternated between analysis and theoretical sampling until achieving theoretical saturation. Results: We observed 13 practices and 6 obstacles related to product backlog generation. Limitations: Grounded Theory does not support statistical generalization. While the proposed theory of product backlogs appears widely applicable, organizations with different software development cultures may use different practices. Conclusion: The product backlog is simultaneously a model of work to be done and a boundary object that helps bridge the gap between the processes of generating user stories and realizing them in working code. It emerges from sensemaking (the team making sense of the project context) and coevolution (a cognitive process where the team simultaneously refines its understanding of the problematic context and fledgling solution concepts).","conference":"IEEE","terms":"Software;Interviews;Programming;Companies;Buildings;Stakeholders;Data collection,project management,software development projects;product backlog generation;puser stories;software development cultures;constructivist grounded theory","keywords":"Product-backlog;dual-track-agile;scrum;lean;extreme-programming;user-stories;design-thinking;user-centered-design;feature-engineering","startPage":"200","endPage":"211","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812076","citationCount":0,"referenceCount":58,"year":2019,"authors":"T. Sedano; P. Ralph; C. Péraire","affiliations":"Carnegie Mellon University, Silicon Valley Campus; University of Auckland; Carnegie Mellon University, Silicon Valley Campus","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33ac"},"title":"How Do Developers Fix Cross-Project Correlated Bugs? A Case Study on the GitHub Scientific Python Ecosystem","abstract":"GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects, and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of ecosystem, as well as shed light on the requirements of issue trackers for such bugs.","conference":"IEEE","terms":"Computer bugs;Ecosystems;Software;Collaboration;Maintenance engineering;Tools;Software engineering,configuration management;object-oriented languages;program debugging;project management;software development management,cross-project correlated bugs;GitHub scientific Python ecosystem;upstream bugs;bug reports","keywords":"GitHub ecosystems;cross-project correlated bugs;root causes tracking;coordinate","startPage":"381","endPage":"392","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985678","citationCount":9,"referenceCount":39,"year":2017,"authors":"W. Ma; L. Chen; X. Zhang; Y. Zhou; B. Xu","affiliations":"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33ad"},"title":"DeepPerf: Performance Prediction for Configurable Software with Deep Sparse Neural Network","abstract":"Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.","conference":"IEEE","terms":"Software systems;Predictive models;Software performance;Tuning;System performance;Biological neural networks;Data models,feedforward neural nets;learning (artificial intelligence),deep feedforward neural network;network hyperparameters;performance values;binary configuration options;numeric configuration options;deep sparse neural network;system performance;performance prediction model;performance data;configurable software system;configurable software systems;runtime performance","keywords":"software performance prediction, deep sparse feedforward neural network, highly configurable systems, sparsity regularization","startPage":"1095","endPage":"1106","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811988","citationCount":3,"referenceCount":55,"year":2019,"authors":"H. Ha; H. Zhang","affiliations":"The University of Newcastle, Australia; The University of Newcastle, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33ae"},"title":"Heuristically Matching Solution Spaces of Arithmetic Formulas to Efficiently Reuse Solutions","abstract":"Many symbolic program analysis techniques rely on SMT solvers to verify properties of programs. Despite the remarkable progress made in the development of such tools, SMT solvers still represent a main bottleneck to the scalability of these techniques. Recent approaches tackle this bottleneck by reusing solutions of formulas that recur during program analysis, thus reducing the number of queries to SMT solvers. Current approaches only reuse solutions across formulas that are equivalent to, contained in or implied by other formulas, as identified through a set of predefined rules, and cannot reuse solutions across formulas that differ in their structure, even if they share some potentially reusable solutions. In this paper, we propose a novel approach that can reuse solutions across formulas that share at least one solution, regardless of their structural resemblance. Our approach exploits a novel heuristic to efficiently identify solutions computed for previously solved formulas and most likely shared by new formulas. The results of an empirical evaluation of our approach on two different logics show that our approach can identify on average more reuse opportunities and is markedly faster than competing approaches.","conference":"IEEE","terms":"Scalability;Terminology;Heuristic algorithms;Software engineering;Tools;Operating systems;Mission critical systems,computability;program diagnostics,heuristically matching solution spaces;arithmetic formulas;symbolic program analysis techniques;SMT solvers;structural resemblance","keywords":"SMT-based program analysis;symbolic execution;SMT solvers;solution reuse","startPage":"427","endPage":"437","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985682","citationCount":1,"referenceCount":19,"year":2017,"authors":"A. Aquino; G. Denaro; M. Pezzè","affiliations":"Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. of Milano-Bicocca, Milan, Italy; Univ. della Svizzera Italiana, Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33af"},"title":"Developer Reading Behavior While Summarizing Java Methods: Size and Context Matters","abstract":"An eye-tracking study of 18 developers reading and summarizing Java methods is presented. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. Previous studies on this topic use only short methods presented in isolation usually as images. In contrast, this work presents the study in the Eclipse IDE allowing access to all the source code in the system. The developer can navigate via scrolling and switching files while writing the summary. New eye-tracking infrastructure allows for this improvement in the study environment. Data collected includes eye gazes on source code, written summaries, and time to complete each summary. Unlike prior work that concluded developers focus on the signature the most, these results indicate that they tend to focus on the method body more than the signature. Moreover, both experts and novices tend to revisit control flow terms rather than reading them for a long period. They also spend a significant amount of gaze time and have higher gaze visits when they read call terms. Experts tend to revisit the body of the method significantly more frequently than its signature as the size of the method increases. Moreover, experts tend to write their summaries from source code lines that they read the most.","conference":"IEEE","terms":"Java;Gaze tracking;Task analysis;Natural languages;Software engineering;Switches,gaze tracking;human factors;Java,Java methods;written summary;gaze time;source code lines;eye gazes;eye tracking infrastructure;developer reading behavior;Eclipse IDE;control flow","keywords":"source code summarization;eye tracking;program comprehension;empirical study","startPage":"384","endPage":"395","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812039","citationCount":2,"referenceCount":48,"year":2019,"authors":"N. J. Abid; B. Sharif; N. Dragan; H. Alrasheed; J. I. Maletic","affiliations":"Taibah University, Saudi Arabia; University of Nebraska - Lincoln, USA; Kent State University, USA; King Saud University, Saudi Arabia; Kent State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b0"},"title":"SafeCheck: Safety Enhancement of Java Unsafe API","abstract":"Java is a safe programming language by providing bytecode verification and enforcing memory protection. For instance, programmers cannot directly access the memory but have to use object references. Yet, the Java runtime provides an Unsafe API as a backdoor for the developers to access the low- level system code. Whereas the Unsafe API is designed to be used by the Java core library, a growing community of third-party libraries use it to achieve high performance. The Unsafe API is powerful, but dangerous, which leads to data corruption, resource leaks and difficult-to-diagnose JVM crash if used improperly. In this work, we study the Unsafe crash patterns and propose a memory checker to enforce memory safety, thus avoiding the JVM crash caused by the misuse of the Unsafe API at the bytecode level. We evaluate our technique on real crash cases from the openJDK bug system and real-world applications from AJDK. Our tool reduces the efforts from several days to a few minutes for the developers to diagnose the Unsafe related crashes. We also evaluate the runtime overhead of our tool on projects using intensive Unsafe operations, and the result shows that our tool causes a negligible perturbation to the execution of the applications.","conference":"IEEE","terms":"Java;Safety;Tools;Computer bugs;Runtime;Libraries,application program interfaces;Java;program debugging;security of data;storage management,memory safety;bytecode verification;Java runtime;low- level system code;Java core library;memory checker;memory protection;safety enhancement;Java unsafe API;programming language;unsafe crash patterns;openJDK bug system;JVM crash","keywords":"Java Unsafe API;Dynamic Analysis;Bytecode;Memoey Safety","startPage":"889","endPage":"899","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811920","citationCount":0,"referenceCount":28,"year":2019,"authors":"S. Huang; J. Guo; S. Li; X. Li; Y. Qi; K. Chow; J. Huang","affiliations":"Department of Computer Science, Texas A\u0026M University; Alibaba Group, China; Alibaba Group, China; Alibaba Group, US.; Alibaba Group, US.; Alibaba Group, China; Department of Computer Science, Texas A\u0026M University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b1"},"title":"StoryDroid: Automated Generation of Storyboard for Android Apps","abstract":"Mobile apps are now ubiquitous. Before developing a new app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspiration for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer) in a development team can be ineffective. For example, it is difficult to completely explore all the functionalities of the app in a short period of time. Inspired by the conception of storyboard in movie production, we propose a system, StoryDroid, to automatically generate the storyboard for Android apps, and assist different roles to review apps efficiently. Specifically, StoryDroid extracts the activity transition graph and leverages static analysis techniques to render UI pages to visualize the storyboard with the rendered pages. The mapping relations between UI pages and the corresponding implementation code (e.g., layout code, activity code, and method hierarchy) are also provided to users. Our comprehensive experiments unveil that StoryDroid is effective and indeed useful to assist app development. The outputs of StoryDroid enable several potential applications, such as the recommendation of UI design and layout code.","conference":"IEEE","terms":"Layout;Google;User interfaces;Semantics;Motion pictures;Production;Visualization,Android (operating system);graphical user interfaces;mobile computing;program diagnostics,storyboard;Android apps;mobile apps;StoryDroid;activity transition graph;UI pages;automated generation;static analysis techniques","keywords":"Android app;Storyboard;Competitive analysis;App review","startPage":"596","endPage":"607","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812043","citationCount":2,"referenceCount":70,"year":2019,"authors":"S. Chen; L. Fan; C. Chen; T. Su; W. Li; Y. Liu; L. Xu","affiliations":"East China Normal University, China; East China Normal University, China; Monash University, Australia; Nanyang Technological University, Singapore; New York University Shanghai, China; Nanyang Technological University, Singapore; New York University Shanghai, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b2"},"title":"Exposing Library API Misuses Via Mutation Analysis","abstract":"Misuses of library APIs are pervasive and often lead to software crashes and vulnerability issues. Various static analysis tools have been proposed to detect library API misuses. They often involve mining frequent patterns from a large number of correct API usage examples, which can be hard to obtain in practice. They also suffer from low precision due to an over-simplified assumption that a deviation from frequent usage patterns indicates a misuse. We make two observations on the discovery of API misuse patterns. First, API misuses can be represented as mutants of the corresponding correct usages. Second, whether a mutant will introduce a misuse can be validated via executing it against a test suite and analyzing the execution information. Based on these observations, we propose MutApi, the first approach to discovering API misuse patterns via mutation analysis. To effectively mimic API misuses based on correct usages, we first design eight effective mutation operators inspired by the common characteristics of API misuses. MutApi generates mutants by applying these mutation operators on a set of client projects and collects mutant-killing tests as well as the associated stack traces. Misuse patterns are discovered from the killed mutants that are prioritized according to their likelihood of causing API misuses based on the collected information. We applied MutApi on 16 client projects with respect to 73 popular Java APIs. The results show that MutApi is able to discover substantial API misuse patterns with a high precision of 0.78. It also achieves a recall of $0.49$ on the MuBench benchmark, which outperforms the state-of-the-art techniques.","conference":"IEEE","terms":"Libraries;Computer bugs;Java;Software;Tools;Detectors,application program interfaces;data mining;Java;program diagnostics;program testing;security of data,substantial API misuse patterns;library API misuses;mutation analysis;frequent usage patterns;Java API;static analysis tools;MutApi;mutant-killing tests;frequent pattern mining","keywords":"Mutation Analysis;Library API Misuse","startPage":"866","endPage":"877","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812114","citationCount":0,"referenceCount":60,"year":2019,"authors":"M. Wen; Y. Liu; R. Wu; X. Xie; S. Cheung; Z. Su","affiliations":"The Hong Kong University of Science and Technology, Hong Kong, China; Shenzhen Key Laboratory of Computational Intelligence Southern University of Science and Technology, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, China; Sun Yat-sen University; The Hong Kong University of Science and Technology, Hong Kong, China; ETH Zurich, Switzerland \u0026 UC Davis, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b3"},"title":"Semantically Enhanced Software Traceability Using Deep Learning Techniques","abstract":"In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.","conference":"IEEE","terms":"Semantics;Machine learning;Recurrent neural networks;Standards;Training;Natural language processing;Software,information retrieval;knowledge representation;learning (artificial intelligence);program diagnostics;program testing;programming language semantics;recurrent neural nets;safety-critical software;source code (software);vectors,semantically enhanced software traceability;deep learning techniques;safety-critical domains;source code;test cases;information retrieval technique;machine learning technique;trace link generation;requirements artifact semantics;domain knowledge;tracing network architecture;word embedding model;recurrent neural network model;word vectors;knowledge representation;domain corpus;sentence semantics;requirements artifacts;positive train control domain;bidirectional gated recurrent unit;BI-GRU;vector space model;latent semantic indexing","keywords":"Traceability;Deep Learning;Recurrent Neural Network;Semantic Representation","startPage":"3","endPage":"14","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985645","citationCount":28,"referenceCount":70,"year":2017,"authors":"J. Guo; J. Cheng; J. Cleland-Huang","affiliations":"Univ. of Notre Dame, Notre Dame, IN, USA; Univ. of Notre Dame, Notre Dame, IN, USA; Univ. of Notre Dame, Notre Dame, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b4"},"title":"A System Identification Based Oracle for Control-CPS Software Fault Localization","abstract":"Control-CPS software fault localization (SFL, aka bug localization) is of critical importance as bugs may cause major failures, even injuries/deaths. To locate the bugs in control-CPSs, SFL tools often demand many labeled (\"correct\"/\"incorrect\") source code execution traces as inputs. To label the correctness of these traces, we must judge the corresponding control-CPS physical trajectories' correctness. However, unlike discrete outputs, the boundaries between correct and incorrect physical trajectories are often vague. The mechanism (aka oracle) to judge the physical trajectories' correctness thus becomes a major challenge. So far, the ad hoc practice of ``human oracles'' is still widely used, whose qualities heavily depend on the human experts' expertise and availability. This paper proposes an oracle based on the well adopted autoregressive system identification (AR-SI). With proven success for controlling black-box physical systems, AR-SI is adapted by us to identify the buggy control-CPS as a black-box. We use this identification result as an oracle to judge the control-CPS's behaviors, and propose a methodology to prepare traces for control-CPS debugging. Comprehensive evaluations on classic control-CPSs with injected real-life and artificial bugs show that our proposed approach significantly outperforms the human oracle approach in SFL accuracy (recall) and latency, and in oracle false positive/negative rates. Our approach also helps discover a new real-life bug in a consumer-grade control-CPS.","conference":"IEEE","terms":"Trajectory;Computer bugs;Tools;Software;Emulation;Debugging,autoregressive processes;cyber-physical systems;program debugging;software fault tolerance;software tools;source code (software),buggy control-CPS;control-CPS debugging;control-CPS software fault localization;aka bug localization;aka oracle;human oracles;autoregressive system identification;black-box physical systems;consumer-grade control;SFL tools;source code execution;AR-SI","keywords":"Oracle;Cyber-Physical System;Debug;Testing","startPage":"116","endPage":"127","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811955","citationCount":0,"referenceCount":101,"year":2019,"authors":"Z. He; Y. Chen; E. Huang; Q. Wang; Y. Pei; H. Yuan","affiliations":"The Hong Kong Polytechnic University, Hong Kong SAR of China; The Hong Kong Polytechnic University, Hong Kong SAR of China; The Hong Kong Polytechnic University, Hong Kong SAR of China; The Hong Kong Polytechnic University, Hong Kong SAR of China; The Hong Kong Polytechnic University, Hong Kong SAR of China; The Chinese University of Hong Kong, Hong Kong SAR of China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b5"},"title":"Detecting Incorrect Build Rules","abstract":"Automated build systems are routinely used by software engineers to minimize the number of objects that need to be recompiled after incremental changes to the source files of a project. In order to achieve efficient and correct builds, developers must provide the build tools with dependency information between the files and modules of a project, usually expressed in a macro language specific to each build tool. In order to guarantee correctness, the authors of these tools are responsible for enumerating all the files whose contents an output depends on. Unfortunately, this is a tedious process and not all dependencies are captured in practice, which leads to incorrect builds. We automatically uncover such missing dependencies through a novel method that we call build fuzzing. The correctness of build definitions is verified by modifying files in a project, triggering incremental builds and comparing the set of changed files to the set of expected changes. These sets are determined using a dependency graph inferred by tracing the system calls executed during a clean build. We evaluate our method by exhaustively testing build rules of open-source projects, uncovering issues leading to race conditions and faulty builds in 31 of them. We provide a discussion of the bugs we detect, identifying anti-patterns in the use of the macro languages. We fix some of the issues in projects where the features of build systems allow a clean solution.","conference":"IEEE","terms":"Tools;Generators;Computer bugs;Linux;C++ languages;Open source software;Kernel,graph theory;program debugging;program testing;public domain software;software quality,build definitions;changed files;dependency graph;open-source projects;software engineers;dependency information","keywords":"build tools;exhaustive testing;verification","startPage":"1234","endPage":"1244","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812082","citationCount":1,"referenceCount":25,"year":2019,"authors":"N. Licker; A. Rice","affiliations":"University of Cambridge; University of Cambridge","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b6"},"title":"Unsupervised Software-Specific Morphological Forms Inference from Informal Discussions","abstract":"Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.","conference":"IEEE","terms":"Software engineering;Encyclopedias;Electronic publishing;Internet;Thesauri;Dictionaries,natural language processing;software engineering;text analysis;thesauri,unsupervised software-specific morphological forms inference;informal discussions;natural language process;software engineering tasks;NLP techniques;software-specific terms;thesaurus;domain-specific lexical rules","keywords":"abbreviation;synonym;morphological form;word embedding;Stack Overflow","startPage":"450","endPage":"461","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985684","citationCount":4,"referenceCount":60,"year":2017,"authors":"C. Chen; Z. Xing; X. Wang","affiliations":"Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Res. Sch. of Comput. Sci., Australian Nat. Univ., Canberra, ACT, Australia; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b7"},"title":"ActionNet: Vision-Based Workflow Action Recognition From Programming Screencasts","abstract":"Programming screencasts have two important applications in software engineering context: study developer behaviors, information needs and disseminate software engineering knowledge. Although programming screencasts are easy to produce, they are not easy to analyze or index due to the image nature of the data. Existing techniques extract only content from screencasts, but ignore workflow actions by which developers accomplish programming tasks. This significantly limits the effective use of programming screencasts in downstream applications. In this paper, we are the first to present a novel technique for recognizing workflow actions in programming screencasts. Our technique exploits image differencing and Convolutional Neural Network (CNN) to analyze the correspondence and change of consecutive frames, based on which nine classes of frequent developer actions can be recognized from programming screencasts. Using programming screencasts from Youtube, we evaluate different configurations of our CNN model and the performance of our technique for developer action recognition across developers, working environments and programming languages. Using screencasts of developers' real work, we demonstrate the usefulness of our technique in a practical application for actionaware extraction of key-code frames in developers' work.","conference":"IEEE","terms":"Programming;Microsoft Windows;Feature extraction;Software engineering;Tutorials;Mice,convolutional neural nets;feature extraction;image recognition;software engineering,programming screencasts;vision-based workflow action recognition;ActionNet;image differencing;convolutional neural network;CNN model;programming languages;software engineering;developer behaviors","keywords":"Programming Screencast;Action Recognition;Deep learning","startPage":"350","endPage":"361","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811922","citationCount":1,"referenceCount":69,"year":2019,"authors":"D. Zhao; Z. Xing; C. Chen; X. Xia; G. Li","affiliations":"Australian National University; Australian National University; Monash University; Monash University; Shanghai Jiao Tong University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b8"},"title":"Hunting for Bugs in Code Coverage Tools via Randomized Differential Testing","abstract":"Reliable code coverage tools are critically important as it is heavily used to facilitate many quality assurance activities, such as software testing, fuzzing, and debugging. However, little attention has been devoted to assessing the reliability of code coverage tools. In this study, we propose a randomized differential testing approach to hunting for bugs in the most widely used C code coverage tools. Specifically, by generating random input programs, our approach seeks for inconsistencies in code coverage reports produced by different code coverage tools, and then identifies inconsistencies as potential code coverage bugs. To effectively report code coverage bugs, we addressed three specific challenges: (1) How to filter out duplicate test programs as many of them triggering the same bugs in code coverage tools; (2) how to automatically reduce large test programs to much smaller ones that have the same properties; and (3) how to determine which code coverage tools have bugs? The extensive evaluations validate the effectiveness of our approach, resulting in 42 and 28 confirmed/fixed bugs for gcov and llvm-cov, respectively. This case study indicates that code coverage tools are not as reliable as it might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of code coverage tools. This work opens up a new direction in code coverage validation which calls for more attention in this area.","conference":"IEEE","terms":"Computer bugs;Tools;Software;Reliability;Fuzzing;Debugging,program debugging;program testing;software tools,random input programs;C code coverage tools;randomized differential testing;code coverage validation;potential code coverage bugs;code coverage reports;reliable code coverage tools","keywords":"Code Coverage;Differential Testing;Coverage Tools;Bug Detection.","startPage":"488","endPage":"499","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812045","citationCount":2,"referenceCount":60,"year":2019,"authors":"Y. Yang; Y. Zhou; H. Sun; Z. Su; Z. Zuo; L. Xu; B. Xu","affiliations":"Nanjing University, Nanjing, China; Nanjing University, Nanjing, China; Unaffiliated; ETH Zurich, Switzerland; Nanjing University, Nanjing, China; Nanjing University, Nanjing, China; Nanjing University, Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33b9"},"title":"When Code Completion Fails: A Case Study on Real-World Completions","abstract":"Code completion is commonly used by software developers and is integrated into all major IDE's. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientific understanding of developer needs and of the efficacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artificial completions to inform future research and tools in this area. We find that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data. Worse, on the few completions that consumed most of the developers' time, prediction accuracy was less than 20% -- an effect that is invisible in synthetic benchmarks. Our findings have ramifications for future benchmarks, tool design and real-world efficacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artificial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249].","conference":"IEEE","terms":"Tools;Benchmark testing;Data models;Context modeling;Syntactics;Vocabulary;Training,data handling;program testing;software engineering;software tools,code completion;real-world completions;software developers;synthetic benchmarks;real-world data;completion models;artificial completions;tested completion tools;tool design;real-world efficacy;real-world developer trials;predictable completions","keywords":"Code completion;Benchmark;Language models","startPage":"960","endPage":"970","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812116","citationCount":1,"referenceCount":29,"year":2019,"authors":"V. J. Hellendoorn; S. Proksch; H. C. Gall; A. Bacchelli","affiliations":"UC Davis; University of Zurich; University of Zurich; University of Zurich","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33ba"},"title":"Analyzing APIs Documentation and Code to Detect Directive Defects","abstract":"Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility.","conference":"IEEE","terms":"Documentation;Natural language processing;Null value;Software;Data mining;Feature extraction;Computer science,application program interfaces;formal logic;natural language processing;program diagnostics;software maintenance;software quality;system documentation,API documentation analysis;directive defect detection;API code analysis;application programming interface documents;API comprehension;software quality;natural language processing;program comprehension;first-order logic based constraint solver;parameter constraints;exception throwing declarations;JDK 1.8 API;API usage constraints;defective document directives;precision rate;recall rate","keywords":"API documentation;static analysis;natural language processing","startPage":"27","endPage":"37","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985647","citationCount":14,"referenceCount":41,"year":2017,"authors":"Y. Zhou; R. Gu; T. Chen; Z. Huang; S. Panichella; H. Gall","affiliations":"Coll. of Comput. Sci., Nanjing Univ. of Aeronaut. \u0026 Astronaut., Nanjing, China; Coll. of Comput. Sci., Nanjing Univ. of Aeronaut. \u0026 Astronaut., Nanjing, China; Dept. of Comput. Sci., Middlesex Univ., London, UK; Coll. of Comput. Sci., Nanjing Univ. of Aeronaut. \u0026 Astronaut., Nanjing, China; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33bb"},"title":"Deep Differential Testing of JVM Implementations","abstract":"The Java Virtual Machine (JVM) is the cornerstone of the widely-used Java platform. Thus, it is critical to ensure the reliability and robustness of popular JVM implementations. However, little research exists on validating production JVMs. One notable effort is classfuzz, which mutates Java bytecode syntactically to stress-test different JVMs. It is shown that classfuzz mainly produces illegal bytecode files and uncovers defects in JVMs' startup processes. It remains a challenge to effectively test JVMs' bytecode verifiers and execution engines to expose deeper bugs. This paper tackles this challenge by introducing classming, a novel, effective approach to performing deep, differential JVM testing. The key of classming is a technique, live bytecode mutation, to generate, from a seed bytecode file f, likely valid, executable (live) bytecode files: (1) capture the seed f's live bytecode, the sequence of its executed bytecode instructions; (2) repeatedly manipulate the control- and data-flow in f's live bytecode to generate semantically different mutants; and (3) selectively accept the generated mutants to steer the mutation process toward live, diverse mutants. The generated mutants are then employed to differentially test JVMs. We have evaluated classming on mainstream JVM implementations, including OpenJDK's HotSpot and IBM's J9, by mutating the DaCapo benchmarks. Our results show that classming is very effective in uncovering deep JVM differences. More than 1,800 of the generated classes exposed JVM differences, and more than 30 triggered JVM crashes. We analyzed and reported the JVM runtime differences and crashes, of which 14 have already been confirmed/fixed, including a highly critical security vulnerability in J9 that allowed untrusted code to disable the security manager and elevate its privileges (CVE-2017-1376).","conference":"IEEE","terms":"Monitoring;Testing;Java;Engines;Security;Computer crashes;Runtime,Java;program testing;program verification;security of data;source code (software);virtual machines,Java Virtual Machine;Java platform;classfuzz;classming;differential JVM testing;live bytecode mutation;mutation process;deep JVM differences;JVM crashes;bytecode instructions;deep differential testing;OpenJDK HotSpot;IBM J9;DaCapo benchmarks;deeper bugs","keywords":"Differential JVM testing;live bytecode mutation;semantically different mutants","startPage":"1257","endPage":"1268","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811957","citationCount":0,"referenceCount":49,"year":2019,"authors":"Y. Chen; T. Su; Z. Su","affiliations":"Shanghai Jiao Tong University; Nanyang Technological University; ETH Zurich/University of California, Davis","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9114eee8435e8e7d33bc"},"title":"Characterizing and Detecting Anti-Patterns in the Logging Code","abstract":"Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code). In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.","conference":"IEEE","terms":"Tools;Open source software;Software systems;Runtime;Computer crashes;Data mining,data handling;parallel processing;program diagnostics;public domain software;software maintenance;source code (software),anti-pattern detection;anti-pattern characterization;system behavior;maintenance overhead;high disk I/O bandwidth;logging code snippets;ActiveMQ;Hadoop;Maven;static code analysis tool;LCAnalyzer;source code;open source software systems;how-to-log problem","keywords":"anti-patterns;logging code;logging practices;empirical studies;software maintenance","startPage":"71","endPage":"81","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985651","citationCount":12,"referenceCount":47,"year":2017,"authors":"B. Chen; Z. M. Jiang","affiliations":"AnaLytics \u0026 Evaluation (SCALE) Lab., York Univ., Toronto, ON, Canada; AnaLytics \u0026 Evaluation (SCALE) Lab., York Univ., Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33bd"},"title":"RESTler: Stateful REST API Fuzzing","abstract":"This paper introduces RESTler, the first stateful REST API fuzzer. RESTler analyzes the API specification of a cloud service and generates sequences of requests that automatically test the service through its API. RESTler generates test sequences by (1) inferring producer-consumer dependencies among request types declared in the specification (e.g., inferring that \"a request B should be executed after request A\" because B takes as an input a resource-id x produced by A) and by (2) analyzing dynamic feedback from responses observed during prior test executions in order to generate new tests (e.g., learning that \"a request C after a request sequence A;B is refused by the service\" and therefore avoiding this combination in the future). We present experimental results showing that these two techniques are necessary to thoroughly exercise a service under test while pruning the large search space of possible request sequences. We used RESTler to test GitLab, an open-source Git service, as well as several Microsoft Azure and Office365 cloud services. RESTler found 28 bugs in GitLab and several bugs in each of the Azure and Office365 cloud services tested so far. These bugs have been confirmed and fixed by the service owners.","conference":"IEEE","terms":"Computer bugs;Tools;Fuzzing;Dictionaries;Open source software;Test pattern generators,application program interfaces;cloud computing;fuzzy set theory;program debugging;program testing,prior test executions;request C;test GitLab;open-source Git service;Office365 cloud services;service owners;stateful REST API fuzzing;stateful REST API fuzzer;RESTler analyzes;API specification;cloud service;test sequences;request types;request B;request sequences;Microsoft Azure cloud services;bugs","keywords":"REST API;Fuzzing;cloud services;fuzzer;testing;bug finding","startPage":"748","endPage":"758","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811961","citationCount":1,"referenceCount":43,"year":2019,"authors":"V. Atlidakis; P. Godefroid; M. Polishchuk","affiliations":"Columbia University; Microsoft Research; Microsoft Research","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33be"},"title":"Supporting Analysts by Dynamic Extraction and Classification of Requirements-Related Knowledge","abstract":"In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks.","conference":"IEEE","terms":"Feature extraction;Transducers;Stakeholders;Data mining;Real-time systems;Task analysis;Support vector machines,computational linguistics;feature extraction;information retrieval;natural language processing;pattern classification;project management;software development management;support vector machines;text analysis,natural language processing tasks;variable-sized feature vectors;requirements-related knowledge;software development projects;weighted finite state transducers;support vector machines;requirements knowledge;phrasal units;project documents;edit distance;precision/recall metrics;extraction tasks;classification tasks","keywords":"Requirements elicitation;Natural Language Processing;Requirements classification;Weighted Finite State Transducer;Dynamic language models;Information extraction","startPage":"442","endPage":"453","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812084","citationCount":0,"referenceCount":80,"year":2019,"authors":"Z. Shakeri Hossein Abad; V. Gervasi; D. Zowghi; B. H. Far","affiliations":"University of Calgary, Canada; University of Pisa, Italy; University of Technology Sydney, Australia; University of Calgary, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33bf"},"title":"Travioli: A Dynamic Analysis for Detecting Data-Structure Traversals","abstract":"Traversal is one of the most fundamental operations on data structures, in which an algorithm systematically visits some or all of the data items of a data structure. We propose a dynamic analysis technique, called Travioli, for detecting data-structure traversals. We introduce the concept of acyclic execution contexts, which enables precise detection of traversals of arrays and linked data structures such as lists and trees in the presence of both loops and recursion. We describe how the information reported by Travioli can be used for visualizing data-structure traversals, manually generating performance regression tests, and for discovering performance bugs caused by redundant traversals. We evaluate Travioli on five real-world JavaScript programs. In our experiments, Travioli produced fewer than 4% false positives. We were able to construct performance tests for 93.75% of the reported true traversals. Travioli also found two asymptotic performance bugs in widely used JavaScript frameworks D3 and express.","conference":"IEEE","terms":"Reactive power;Arrays;Computer bugs;Lenses;Data visualization;Performance analysis,data structures;Java;system monitoring,dynamic analysis;data-structure traversals;acyclic execution contexts;real-world JavaScript programs;Travioli;regression tests;performance bugs","keywords":"","startPage":"473","endPage":"483","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985686","citationCount":1,"referenceCount":32,"year":2017,"authors":"R. Padhye; K. Sen","affiliations":"EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c0"},"title":"Machine-Learning-Guided Selectively Unsound Static Analysis","abstract":"We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use an anomaly-detection technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.","conference":"IEEE","terms":"Computer bugs;Libraries;Benchmark testing;Tools;Software engineering;Scalability;Support vector machines,learning (artificial intelligence);program diagnostics,machine-learning-guided selectively unsound static Analysis;bug-finding static analyzers;anomaly-detection technique;taint analysis;interval analysis;buffer-overflow detection","keywords":"Static Analysis;Machine Learning;Bug-finding","startPage":"519","endPage":"529","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985690","citationCount":2,"referenceCount":24,"year":2017,"authors":"K. Heo; H. Oh; K. Yi","affiliations":"Seoul Nat. Univ., Seoul, South Korea; Korea Univ., Seoul, South Korea; Seoul Nat. Univ., Seoul, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c1"},"title":"Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing","abstract":"Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of 'sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately.","conference":"IEEE","terms":"Testing;Perturbation methods;Sensitivity;Runtime;Training;Biological neural networks,learning (artificial intelligence);neural nets;program testing;security of data;software fault tolerance;statistical analysis,adversarial sample detection;deep neural network;adversarial samples;normal sample;input sample;model mutation testing;DNN;MNIST dataset;CIFAR10 dataset;sensitivity measure","keywords":"adversarial sample;detection;deep neural network;mutation testing;sensitivity","startPage":"1245","endPage":"1256","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812047","citationCount":3,"referenceCount":56,"year":2019,"authors":"J. Wang; G. Dong; J. Sun; X. Wang; P. Zhang","affiliations":"Shenzhen University, China; Singapore University of Technology and Design, Singapore; Zhejiang University, China; Singapore University of Technology and Design, Singapore; Zhejiang University, China; Zhejiang University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c2"},"title":"FOCUS: A Recommender System for Mining API Function Calls and Usage Patterns","abstract":"Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate, accuracy, and execution time. Results indicate the suitability of context-aware collaborative-filtering recommender systems to provide API usage patterns.","conference":"IEEE","terms":"Recommender systems;Collaboration;Java;Libraries;Documentation;Data mining;Tools,application program interfaces;collaborative filtering;data mining;Java;public domain software;recommender systems;software libraries;ubiquitous computing,API function calls;software developers interact;learning process;usage pattern recommendation;open-source project repositories;API method invocations;API usage patterns;collaborative-filtering recommender system;Maven Central;GitHub;Java projects;FOCUS","keywords":"recommender system;api mining;api usage pattern;api recommendation","startPage":"1050","endPage":"1060","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812051","citationCount":2,"referenceCount":45,"year":2019,"authors":"P. T. Nguyen; J. Di Rocco; D. Di Ruscio; L. Ochoa; T. Degueule; M. Di Penta","affiliations":"Università degli Studi dell'Aquila, Italy; Università degli Studi dell'Aquila, Italy; Università degli Studi dell'Aquila, Italy; Centrum Wiskunde \u0026 Informatica, Netherlands; Centrum Wiskunde \u0026 Informatica, Netherlands; Università degli Studi del Sannio, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c3"},"title":"Why Do Episodic Volunteers Stay in FLOSS Communities?","abstract":"Successful Free/Libre and Open Source Software (FLOSS) projects incorporate both habitual and infrequent, or episodic, contributors. Using the concept of episodic volunteering (EV) from the general volunteering literature, we derive a model consisting of five key constructs that we hypothesize affect episodic volunteers' retention in FLOSS communities. To evaluate the model we conducted a survey with over 100 FLOSS episodic volunteers. We observe that three of our model constructs (social norms, satisfaction and community commitment) are all positively associated with volunteers' intention to remain, while the two other constructs (psychological sense of community and contributor benefit motivations) are not. Furthermore, exploratory clustering on unobserved heterogeneity suggests that there are four distinct categories of volunteers: satisfied, classic, social and obligated. Based on our findings, we offer suggestions for projects to incorporate and manage episodic volunteers, so as to better leverage this type of contributors and potentially improve projects' sustainability.","conference":"IEEE","terms":"Psychology;Analytical models;Sustainable development;Open source software;Computer science;Face,public domain software;software development management;software quality,FLOSS communities;habitual contributors;episodic contributors;episodic volunteering;general volunteering literature;community commitment;FLOSS episodic volunteers;free-libre and open source software","keywords":"community management;episodic volunteering;open source software;volunteer management","startPage":"948","endPage":"959","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811892","citationCount":1,"referenceCount":88,"year":2019,"authors":"A. Barcomb; K. Stol; D. Riehle; B. Fitzgerald","affiliations":"Lero - University of Limerick, Ireland; Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany; Lero - University College Cork, Ireland; Friedrich-Alexander-Universität Erlangen-Nürnberg, Germany; Lero - University of Limerick, Ireland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c4"},"title":"Detecting User Story Information in Developer-Client Conversations to Generate Extractive Summaries","abstract":"User stories are descriptions of functionality that a software user needs. They play an important role in determining which software requirements and bug fixes should be handled and in what order. Developers elicit user stories through meetings with customers. But user story elicitation is complex, and involves many passes to accommodate shifting and unclear customer needs. The result is that developers must take detailed notes during meetings or risk missing important information. Ideally, developers would be freed of the need to take notes themselves, and instead speak naturally with their customers. This paper is a step towards that ideal. We present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers. We perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically. From this, we found that roughly 10.2% of these conversations contained user story information. Then, we test our technique in a quantitative study to determine the degree to which our technique can extract user story information. In our experiment, our process obtained about 70.8% precision and 18.3% recall on the information.","conference":"IEEE","terms":"Data mining;Software;Computer bugs;Electronic mail;Software algorithms;Software engineering;Algorithm design and analysis,software engineering,user story information detection;developer-client conversations;extractive summaries;software functionality","keywords":"developer communication;user story generation;software engineering;productivity;transcripts","startPage":"49","endPage":"59","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985649","citationCount":6,"referenceCount":39,"year":2017,"authors":"P. Rodeghero; S. Jiang; A. Armaly; C. McMillan","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Univ. of Notre Dame, Notre Dame, IN, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Notre Dame, Notre Dame, IN, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Notre Dame, Notre Dame, IN, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Notre Dame, Notre Dame, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c5"},"title":"Supporting Software Developers with a Holistic Recommender System","abstract":"The promise of recommender systems is to provide intelligent support to developers during their programming tasks. Such support ranges from suggesting program entities to taking into account pertinent Q\u0026A pages. However, current recommender systems limit the context analysis to change history and developers' activities in the IDE, without considering what a developer has already consulted or perused, e.g., by performing searches from the Web browser. Given the faceted nature of many programming tasks, and the incompleteness of the information provided by a single artifact, several heterogeneous resources are required to obtain the broader picture needed by a developer to accomplish a task. We present Libra, a holistic recommender system. It supports the process of searching and navigating the information needed by constructing a holistic meta-information model of the resources perused by a developer, analyzing their semantic relationships, and augmenting the web browser with a dedicated interactive navigation chart. The quantitative and qualitative evaluation of Libra provides evidence that a holistic analysis of a developer's information context can indeed offer comprehensive and contextualized support to information navigation and retrieval during software development.","conference":"IEEE","terms":"Browsers;Uniform resource locators;Recommender systems;Navigation;Web search;User interfaces;Web pages,information analysis;information retrieval;interactive programming;online front-ends;recommender systems;software engineering,software developers;holistic recommender system;intelligent support;programming tasks;program entities;pertinent Q\u0026A pages;heterogeneous resources;Libra;information search;information navigation;holistic meta-information model;Web browser;interactive navigation chart;information context analysis;information retrieval;software development","keywords":"Mining unstructured data;Recommender systems","startPage":"94","endPage":"105","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985653","citationCount":8,"referenceCount":56,"year":2017,"authors":"L. Ponzanelli; S. Scalabrino; G. Bavota; A. Mocci; R. Oliveto; M. Di Penta; M. Lanza","affiliations":"Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. of Molise, Italy; Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. of Molise, Italy; Univ. of Sannio, Benevento, Italy; Univ. della Svizzera Italiana, Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c6"},"title":"Distilling Neural Representations of Data Structure Manipulation using fMRI and fNIRS","abstract":"Data structures permeate many aspects of software engineering, but their associated human cognitive processes are not thoroughly understood. We leverage medical imaging and insights from the psychological notion of spatial ability to decode the neural representations of several fundamental data structures and their manipulations. In a human study involving 76 participants, we examine list, array, tree, and mental rotation tasks using both functional near-infrared spectroscopy (fNIRS) and functional magnetic resonance imaging (fMRI). We find a nuanced relationship: data structure and spatial operations use the same focal regions of the brain but to different degrees. They are related but distinct neural tasks. In addition, more difficult computer science problems induce higher cognitive load than do problems of pure spatial reasoning. Finally, while fNIRS is less expensive and more permissive, there are some computing-relevant brain regions that only fMRI can reach.","conference":"IEEE","terms":"Functional magnetic resonance imaging;Data structures;Task analysis;Software engineering;Biomedical imaging;Neuroimaging;Brain,biomedical MRI;biomedical optical imaging;brain;cognition;data structures;infrared spectroscopy;medical image processing;neurophysiology;psychology;spatial reasoning,computing-relevant brain regions;pure spatial reasoning;higher cognitive load;difficult computer science problems;distinct neural tasks;spatial operations;functional magnetic resonance imaging;near-infrared spectroscopy;mental rotation tasks;tree;human study;manipulations;fundamental data structures;spatial ability;psychological notion;leverage medical imaging;associated human cognitive processes;software engineering;fNIRS;fMRI;data structure manipulation;neural representations","keywords":"medical imaging;data structures;spatial ability","startPage":"396","endPage":"407","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812086","citationCount":0,"referenceCount":92,"year":2019,"authors":"Y. Huang; X. Liu; R. Krueger; T. Santander; X. Hu; K. Leach; W. Weimer","affiliations":"Univeristy of Michigan; Univeristy of Michigan; University of Michigan; University of California, Santa Barbara; Univeristy of Michigan; University of Michigan; University of Michigan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c7"},"title":"Glacier: Transitive Class Immutability for Java","abstract":"Though immutability has been long-proposed as a way to prevent bugs in software, little is known about how to make immutability support in programming languages effective for software engineers. We designed a new formalism that extends Java to support transitive class immutability, the form of immutability for which there is the strongest empirical support, and implemented that formalism in a tool called Glacier. We applied Glacier successfully to two real-world systems. We also compared Glacier to Java's final in a user study of twenty participants. We found that even after being given instructions on how to express immutability with final, participants who used final were unable to express immutability correctly, whereas almost all participants who used Glacier succeeded. We also asked participants to make specific changes to immutable classes and found that participants who used final all incorrectly mutated immutable state, whereas almost all of the participants who used Glacier succeeded. Glacier represents a promising approach to enforcing immutability in Java and provides a model for enforcement in other languages.","conference":"IEEE","terms":"Java;Computer bugs;Runtime;Usability;Tools,Java;program debugging;software tools,Glacier tool;Java;transitive class immutability;software bugs prevention;programming languages","keywords":"immutability;programming language usability;empirical studies of programmers","startPage":"496","endPage":"506","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985688","citationCount":7,"referenceCount":27,"year":2017,"authors":"M. Coblenz; W. Nelson; J. Aldrich; B. Myers; J. Sunshine","affiliations":"Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Hampton Univ., Hampton, VA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c8"},"title":"Adaptive Coverage and Operational Profile-Based Testing for Reliability Improvement","abstract":"We introduce covrel, an adaptive software testing approach based on the combined use of operational profile and coverage spectrum, with the ultimate goal of improving the delivered reliability of the program under test. Operational profile-based testing is a black-box technique that selects test cases having the largest impact on failure probability in operation, as such, it is considered well suited when reliability is a major concern. Program spectrum is a characterization of a program's behavior in terms of the code entities (e.g., branches, statements, functions) that are covered as the program executes. The driving idea of covrel is to complement operational profile information with white-box coverage measures based on count spectra, so as to dynamically select the most effective test cases for reliability improvement. In particular, we bias operational profile-based test selection towards those entities covered less frequently. We assess the approach by experiments with 18 versions from 4 subjects commonly used in software testing research, comparing results with traditional operational and coverage testing. Results show that exploiting operational and coverage data in a combined adaptive way actually pays in terms of reliability improvement, with covrel overcoming conventional operational testing in more than 80% of the cases.","conference":"IEEE","terms":"Software reliability;Resource management;Software testing;Reliability engineering;Optimization,probability;program testing;software reliability,adaptive coverage testing;operational profile-based testing;reliability improvement;adaptive software testing approach;operational profile;coverage spectrum;program-under-test;black-box technique;failure probability;program spectrum;program behavior;code entities;white-box coverage measures;count spectra;operational profile-based test selection;operational data;coverage data;covrel","keywords":"Testing;Reliability;Operational profile;Program count spectrum;Operational coverage;Test case selection","startPage":"541","endPage":"551","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985692","citationCount":1,"referenceCount":36,"year":2017,"authors":"A. Bertolino; B. Miranda; R. Pietrantuono; S. Russo","affiliations":"ISTI, Pisa, Italy; ISTI, Pisa, Italy; Univ. degli Studi di Napoli Federico II, Naples, Italy; Univ. degli Studi di Napoli Federico II, Naples, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33c9"},"title":"Distance-Based Sampling of Software Configuration Spaces","abstract":"Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.","conference":"IEEE","terms":"Software systems;Sociology;Statistics;Probability distribution;Task analysis;Performance evaluation,learning (artificial intelligence);sampling methods;software maintenance;software performance evaluation;statistical distributions,sampling strategy;configuration options;t-wise sampling;real-world configurable software systems;software configuration;brute-force strategy;representative sample set;distance-based sampling;sampling strategies;software configuration space;probability distribution","keywords":"Distance Based Sampling;Configuration Sampling;Configurable Systems;Performance Modeling","startPage":"1084","endPage":"1094","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812049","citationCount":3,"referenceCount":36,"year":2019,"authors":"C. Kaltenecker; A. Grebhahn; N. Siegmund; J. Guo; S. Apel","affiliations":"University of Passau; University of Passau; University of Weimar; Alibaba Group; University of Passau","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ca"},"title":"DifFuzz: Differential Fuzzing for Side-Channel Analysis","abstract":"Side-channel attacks allow an adversary to uncover secret program data by observing the behavior of a program with respect to a resource, such as execution time, consumed memory or response size. Side-channel vulnerabilities are difficult to reason about as they involve analyzing the correlations between resource usage over multiple program paths. We present DifFuzz, a fuzzing-based approach for detecting side-channel vulnerabilities related to time and space. DifFuzz automatically detects these vulnerabilities by analyzing two versions of the program and using resource-guided heuristics to find inputs that maximize the difference in resource consumption between secret-dependent paths. The methodology of DifFuzz is general and can be applied to programs written in any language. For this paper, we present an implementation that targets analysis of Java programs, and uses and extends the Kelinci and AFL fuzzers. We evaluate DifFuzz on a large number of Java programs and demonstrate that it can reveal unknown side-channel vulnerabilities in popular applications. We also show that DifFuzz compares favorably against Blazer and Themis, two state-of-the-art analysis tools for finding side-channels in Java programs.","conference":"IEEE","terms":"Fuzzing;Java;Tools;Time factors;Instruments;Correlation;Performance analysis,cryptography;Java;program diagnostics;security of data,DifFuzz;differential fuzzing;side-channel analysis;side-channel attacks;secret program data;execution time;resource usage;fuzzing-based approach;resource consumption;secret-dependent paths;Java programs;unknown side-channel vulnerabilities;resource-guided heuristics","keywords":"vulnerability detection;side-channel;dynamic analysis;fuzzing","startPage":"176","endPage":"187","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812124","citationCount":0,"referenceCount":42,"year":2019,"authors":"S. Nilizadeh; Y. Noller; C. S. Pasareanu","affiliations":"University of Texas at Arlington; Humboldt-Universität zu Berlin; Carnegie Mellon University Silicon Valley, NASA Ames Research Center","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33cb"},"title":"Machine Learning-Based Detection of Open Source License Exceptions","abstract":"From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.","conference":"IEEE","terms":"Software engineering,learning (artificial intelligence);pattern classification;public domain software;software reusability;source code (software),sensitivity analysis;supervised learners;license exception classification;license compliance analysis;FOSS licenses;free and open source software;source code;binary code;software reuse;software redistribution;software modification;software licenses;open source license exceptions;machine learning-based detection","keywords":"Software Licenses;Empirical Studies;Classifiers","startPage":"118","endPage":"129","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985655","citationCount":3,"referenceCount":49,"year":2017,"authors":"C. Vendome; M. Linares-Vásquez; G. Bavota; M. Di Penta; D. German; D. Poshyvanyk","affiliations":"Coll. of William \u0026 Mary, Williamsburg, VA, USA; Univ. de los Andes, Bogota, Colombia; Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. of Sannio, Benevento, Italy; Univ. of Victoria, Victoria, BC, Canada; Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33cc"},"title":"Investigating the Impact of Multiple Dependency Structures on Software Defects","abstract":"Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types.","conference":"IEEE","terms":"History;Syntactics;Semantics;Software;Computer bugs;Reverse engineering;Prediction algorithms,program debugging;public domain software;software architecture;software maintenance;software quality,dependency type;DRSpace;defective files;software quality;multiple dependency structures;software defects;syntactic dependency;history co-change relation;semantic similarity;software architecture;design space;Apache open source projects;dependency relations","keywords":"Software Structure;Software Maintenance;Software Quality","startPage":"584","endPage":"595","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812092","citationCount":0,"referenceCount":35,"year":2019,"authors":"D. Cui; T. Liu; Y. Cai; Q. Zheng; Q. Feng; W. Jin; J. Guo; Y. Qu","affiliations":"Xi’an Jiaotong University, China; Xi’an Jiaotong University, China; Drexel University, United States; Xi'an Jiaotong University, China; Drexel University, United States; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33cd"},"title":"PEoPL: Projectional Editing of Product Lines","abstract":"The features of a software product line - a portfolio of system variants - can be realized using various implementation techniques (a. k. a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts. We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (\u003c;45ms on average for our largest subject Berkeley DB).","conference":"IEEE","terms":"Latches;Java;Switches;Visualization;Bars;Software;Clutter,Java;software product lines,PEoPL;projectional editing of product lines;software product line;software artifacts;modular representations;programming-language-independent internal representation;Java-based product lines","keywords":"","startPage":"563","endPage":"574","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985694","citationCount":2,"referenceCount":65,"year":2017,"authors":"B. Behringer; J. Palz; T. Berger","affiliations":"Univ. of Luxembourg, Luxembourg City, Luxembourg; htw saar, Germany; Univ. of Gothenburg, Gothenburg, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ce"},"title":"Interactive Production Performance Feedback in the IDE","abstract":"Because of differences between development and production environments, many software performance problems are detected only after software enters production. We present PerformanceHat, a new system that uses profiling information from production executions to develop a global performance model suitable for integration into interactive development environments. PerformanceHat's ability to incrementally update this global model as the software is changed in the development environment enables it to deliver near real-time predictions of performance consequences reflecting the impact on the production environment. We implement PerformanceHat as an Eclipse plugin and evaluate it in a controlled experiment with 20 professional software developers implementing several software maintenance tasks using our approach and a representative baseline (Kibana). Our results indicate that developers using PerformanceHat were significantly faster in (1) detecting the performance problem, and (2) finding the root-cause of the problem. These results provide encouraging evidence that our approach helps developers detect, prevent, and debug production performance problems during development before the problem manifests in production.","conference":"IEEE","terms":"Monitoring;Software;Runtime;Tools;Data models;Task analysis,program debugging;programming environments;software maintenance;software performance evaluation,interactive production performance feedback;production environment;software performance problems;production executions;global performance model;interactive development environments;development environment;software maintenance tasks;performance problem;debug production performance problems;PerformanceHat;Eclipse plugin;software maintenance","keywords":"software performance engineering, IDE, user study","startPage":"971","endPage":"981","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811928","citationCount":0,"referenceCount":33,"year":2019,"authors":"J. Cito; P. Leitner; M. Rinard; H. C. Gall","affiliations":"MIT; Chalmers and University of Gothenburg; MIT; University of Zurich","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33cf"},"title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines","abstract":"Source code summarization -- creating natural language descriptions of source code behavior -- is a rapidly-growing research topic with applications to automatic documentation generation, program comprehension, and software maintenance. Traditional techniques relied on heuristics and templates built manually by human experts. Recently, data-driven approaches based on neural machine translation have largely overtaken template-based systems. But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names, the models fail to create good summaries. In this paper, we present a neural model that combines words from code with code structure from an AST. Unlike previous approaches, our model processes each data source as a separate input, which allows the model to learn code structure independent of the text in code. This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided. We evaluate our technique with a dataset we created from 2.1m Java methods. We find improvement over two baseline techniques from SE literature and one from NLP literature.","conference":"IEEE","terms":"Algorithms;Documentation;Natural languages;Java;Software engineering;Standards;Task analysis,language translation;natural language processing;neural nets;software engineering;source code (software);system documentation,neural machine translation;neural model;program subroutines;source code summarization;natural language descriptions;natural language summaries;data-driven approaches;software maintenance;program comprehension;automatic documentation generation;source code behavior","keywords":"automatic documentation generation;source code summarization;code comment generation","startPage":"795","endPage":"806","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811932","citationCount":1,"referenceCount":60,"year":2019,"authors":"A. LeClair; S. Jiang; C. McMillan","affiliations":"University of Notre Dame; Eastern Michigan University; University of Notre Dame","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d0"},"title":"Becoming Agile: A Grounded Theory of Agile Transitions in Practice","abstract":"Agile adoption is typically understood as a one-off organizational process involving a staged selection of agile development practices. This view of agility fails to explain the differences in the pace and effectiveness of individual teams transitioning to agile development. Based on a Grounded Theory study of 31 agile practitioners drawn from 18 teams across five countries, we present a grounded theory of becoming agile as a network of on-going transitions across five dimensions: software development practices, team practices, management approach, reflective practices, and culture. The unique position of a software team through this network, and their pace of progress along the five dimensions, explains why individual agile teams present distinct manifestations of agility and unique transition experiences. The theory expands the current understanding of agility as a holistic and complex network of on-going multidimensional transitions, and will help software teams, their managers, and organizations better navigate their individual agile journeys.","conference":"IEEE","terms":"Software;Interviews;Telecommunications;Banking;Encoding;Finance,cultural aspects;software development management;software prototyping;team working,agile development practices;grounded theory;software development practices;team practices;management approach;reflective practices;culture;software team","keywords":"agile software development;transition;selforganizing;teams;management;culture;theory;grounded theory","startPage":"141","endPage":"151","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985657","citationCount":6,"referenceCount":44,"year":2017,"authors":"R. Hoda; J. Noble","affiliations":"Dept. of Electr. \u0026 Comput. Eng., Univ. of Auckland, Auckland, New Zealand; Sch. of Eng. \u0026 Comput. Sci., Victoria Univ. of Wellington, Wellington, New Zealand","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d1"},"title":"Understanding the Impressions, Motivations, and Barriers of One Time Code Contributors to FLOSS Projects: A Survey","abstract":"Successful Free/Libre Open Source Software (FLOSS) projects must attract and retain high-quality talent. Researchers have invested considerable effort in the study of core and peripheral FLOSS developers. To this point, one critical subset of developers that have not been studied are One-Time code Contributors (OTC) - those that have had exactly one patch accepted. To understand why OTCs have not contributed another patch and provide guidance to FLOSS projects on retaining OTCs, this study seeks to understand the impressions, motivations, and barriers experienced by OTCs. We conducted an online survey of OTCs from 23 popular FLOSS projects. Based on the 184 responses received, we observed that OTCs generally have positive impressions of their FLOSS project and are driven by a variety of motivations. Most OTCs primarily made contributions to fix bugs that impeded their work and did not plan on becoming long term contributors. Furthermore, OTCs encounter a number of barriers that prevent them from continuing to contribute to the project. Based on our findings, there are some concrete actions FLOSS projects can take to increase the chances of converting OTCs into long-term contributors.","conference":"IEEE","terms":"Computer bugs;Electronic mail;Encoding;Computer science;Open source software;Concrete;Tools,project management;public domain software;software engineering,one time code contributors;FLOSS projects;free-Libre open source software project;OTC","keywords":"FLOSS;Open source;OSS;Newcomers;One Time Contributors;Survey;Qualitative Research","startPage":"187","endPage":"197","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985661","citationCount":7,"referenceCount":31,"year":2017,"authors":"A. Lee; J. C. Carver; A. Bosu","affiliations":"Comput. Sci. Dept., Univ. of Alabama, Tuscaloosa, AL, USA; Comput. Sci. Dept., Univ. of Alabama, Tuscaloosa, AL, USA; Dept. of Comput. Sci., Southern Illinois Univ., Carbondale, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d2"},"title":"Statistical Algorithmic Profiling for Randomized Approximate Programs","abstract":"Many modern applications require low-latency processing of large data sets, often by using approximate algorithms that trade accuracy of the results for faster execution or reduced memory consumption. Although the algorithms provide probabilistic accuracy and performance guarantees, a software developer who implements these algorithms has little support from existing tools. Standard profilers do not consider accuracy of the computation and do not check whether the outputs of these programs satisfy their accuracy specifications. We present AXPROF, an algorithmic profiling framework for analyzing randomized approximate programs. The developer provides the accuracy specification as a formula in a mathematical notation, using probability or expected value predicates. AXPROF automatically generates statistical reasoning code. It first constructs the empirical models of accuracy, time, and memory consumption. It then selects and runs appropriate statistical tests that can, with high confidence, determine if the implementation satisfies the specification. We used AXPROF to profile 15 approximate applications from three domains - data analytics, numerical linear algebra, and approximate computing. AXPROF was effective in finding bugs and identifying various performance optimizations. In particular, we discovered five previously unknown bugs in the implementations of the algorithms and created fixes, guided by AXPROF.","conference":"IEEE","terms":"Approximation algorithms;Computer bugs;Software algorithms;Hash functions;Probabilistic logic;Heuristic algorithms;Memory management,approximation theory;data analysis;linear algebra;optimisation;probability;program verification;randomised algorithms;statistical testing,AXPROF;statistical reasoning code;memory consumption;approximate computing;statistical algorithmic profiling;randomized approximate programs;low-latency processing;data sets;approximate algorithms;probabilistic accuracy;software developer;statistical tests;data analytics;bugs finding;performance optimizations;linear algebra;mathematical notation","keywords":"profiler;randomized algorithms","startPage":"608","endPage":"618","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811971","citationCount":0,"referenceCount":67,"year":2019,"authors":"K. Joshi; V. Fernando; S. Misailovic","affiliations":"University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d3"},"title":"A General Framework for Dynamic Stub Injection","abstract":"Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.","conference":"IEEE","terms":"Instruments;DSL;Computer bugs;Testing;Runtime;Debugging,binary codes;program diagnostics;program testing;specification languages,stub testing;fault injection;binary code;declarative rules;stub injection strategies;domain specific language","keywords":"","startPage":"586","endPage":"596","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985696","citationCount":1,"referenceCount":26,"year":2017,"authors":"M. Christakis; P. Emmisberger; P. Godefroid; P. Müller","affiliations":"Microsoft Res., Redmond, WA, USA; Dept. of Comput. Sci., ETH Zurich, Zürich, Switzerland; Microsoft Res., Redmond, WA, USA; Dept. of Comput. Sci., ETH Zurich, Zürich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d4"},"title":"Multifaceted Automated Analyses for Variability-Intensive Embedded Systems","abstract":"Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.","conference":"IEEE","terms":"Task analysis;Random access memory;Hardware;Graphics processing units;Rendering (computer graphics);Manufacturing,embedded systems;formal specification;program verification,possibly-antagonistic quality;model-driven framework;high-level specifications;variable dataflows;configurable hardware platforms;mapping algorithm;design space;variability-aware executable model;optimal system;automotive industry;multifaceted automated analyses;variability-intensive embedded systems;automotive domain;nonfunctional requirements;hardware level;verification algorithms","keywords":"Embedded system design engineering;variability modeling;model checking;non functional property;multi objective optimization","startPage":"854","endPage":"865","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812057","citationCount":1,"referenceCount":81,"year":2019,"authors":"S. Lazreg; M. Cordy; P. Collet; P. Heymans; S. Mosser","affiliations":"Visteon Electronics, Université Côte d'Azur, CNRS, I3S, France; SnT, University of Luxembourg; Université Côte d'Azur, CNRS, I3S, France; University of Namur; Université Côte d'Azur, CNRS, I3S, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d5"},"title":"Type Migration in Ultra-Large-Scale Codebases","abstract":"Type migration is a refactoring activity in which an existing type is replaced with another one throughout the source code. Manually performing type migration is tedious as programmers need to find all instances of the type to be migrated, along with its dependencies that propagate over assignment operations, method hierarchies, and subtypes. Existing automated approaches for type migration are not adequate for ultra-large-codebases - they perform an intensive whole-program analysis that does not scale. If we could represent the type structure of the program as graphs, then we could employ a MAPREDUCE parallel and distributed process that scales to hundreds of millions of LOC. We implemented this approach as an IDE-independent tool called T2R, which integrates with most build systems. We evaluated T2R's accuracy, usefulness and scalability on seven open source projects and one proprietary codebase of 300M LOC. T2R generated 130 type migration patches, of which the original developers accepted 98%.","conference":"IEEE","terms":"Java;Google;Open source software;Tools;Scalability;Task analysis;Complexity theory,data mining;parallel processing;program diagnostics;public domain software;software maintenance,ultra-large-scale codebases;type migration patches;refactoring activity;whole-program analysis;MAPREDUCE parallel and distributed process;IDE-independent tool;T2R tool","keywords":"Refactoring;Type Migration;MapReduce","startPage":"1142","endPage":"1153","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812061","citationCount":1,"referenceCount":47,"year":2019,"authors":"A. Ketkar; A. Mesbah; D. Mazinanian; D. Dig; E. Aftandilian","affiliations":"Oregon State University, U.S.A.; University of British Columbia, Canada; University of British Columbia, Canada; Oregon State University, U.S.A.; Google Inc., U.S.A.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d6"},"title":"PIVOT: Learning API-Device Correlations to Facilitate Android Compatibility Issue Detection","abstract":"The heavily fragmented Android ecosystem has induced various compatibility issues in Android apps. The search space for such fragmentation-induced compatibility issues (FIC issues) is huge, comprising three dimensions: device models, Android OS versions, and Android APIs. FIC issues, especially those arising from device models, evolve quickly with the frequent release of new device models to the market. As a result, an automated technique is desired to maintain timely knowledge of such FIC issues, which are mostly undocumented. In this paper, we propose such a technique, PIVOT, that automatically learns API-device correlations of FIC issues from existing Android apps. PIVOT extracts and prioritizes API-device correlations from a given corpus of Android apps. We evaluated PIVOT with popular Android apps on Google Play. Evaluation results show that PIVOT can effectively prioritize valid API-device correlations for app corpora collected at different time. Leveraging the knowledge in the learned API-device correlations, we further conducted a case study and successfully uncovered ten previously-undetected FIC issues in open-source Android apps.","conference":"IEEE","terms":"Correlation;Biological system modeling;Cameras;Ecosystems;Testing;Google;Open source software,Android (operating system);application program interfaces;learning (artificial intelligence);mobile computing;smart phones,PIVOT;heavily fragmented Android ecosystem;fragmentation-induced compatibility issues;device models;Android OS versions;Android APIs;popular Android apps;valid API-device correlations;learned API-device correlations;open-source Android apps;Android compatibility issue detection","keywords":"Android fragmentation, compatibility, static analysis, learning","startPage":"878","endPage":"888","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811898","citationCount":0,"referenceCount":63,"year":2019,"authors":"L. Wei; Y. Liu; S. Cheung","affiliations":"The Hong Kong University of Science and Technology, Hong Kong, China; Southern University of Science and Technology, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, Chine","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d7"},"title":"Could I Have a Stack Trace to Examine the Dependency Conflict Issue?","abstract":"Intensive use of libraries in Java projects brings potential risk of dependency conflicts, which occur when a project directly or indirectly depends on multiple versions of the same library or class. When this happens, JVM loads one version and shadows the others. Runtime exceptions can occur when methods in the shadowed versions are referenced. Although project management tools such as Maven are able to give warnings of potential dependency conflicts when a project is built, developers often ask for crashing stack traces before examining these warnings. It motivates us to develop Riddle, an automated approach that generates tests and collects crashing stack traces for projects subject to risk of dependency conflicts. Riddle, built on top of Asm and Evosuite, combines condition mutation, search strategies and condition restoration. We applied Riddle on 19 real-world Java projects with duplicate libraries or classes. We reported 20 identified dependency conflicts including their induced crashing stack traces and the details of generated tests. Among them, 15 conflicts were confirmed by developers as real issues, and 10 were readily fixed. The evaluation results demonstrate the effectiveness and usefulness of Riddle.","conference":"IEEE","terms":"Libraries;Runtime;Java;Computer crashes;Tools;Test pattern generators;Static analysis,Java;program debugging;program testing;project management;public domain software;system recovery,stack trace;Riddle;real-world Java;induced crashing stack traces;dependency conflict issue;Java projects;runtime exceptions;shadowed versions;project management tools;dependency conflicts;condition mutation;search strategies;condition restoration","keywords":"test generation;third party-library;mutation","startPage":"572","endPage":"583","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812128","citationCount":0,"referenceCount":105,"year":2019,"authors":"Y. Wang; M. Wen; R. Wu; Z. Liu; S. H. Tan; Z. Zhu; H. Yu; S. Cheung","affiliations":"Northeastern University; The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology; Northeastern University; Southern University of Science and Technology; Northeastern University; Northeastern University; The Hong Kong University of Science and Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d8"},"title":"Practical GUI Testing of Android Applications Via Model Abstraction and Refinement","abstract":"This paper introduces a new, fully automated modelbased approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms.We have realized our technique in a practical tool, APE. On 15 large, widely-used apps from the Google Play Store, APE outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate APE's effectiveness and usability, we conduct another evaluation of APE on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Tools;Computer crashes;Google;Indexes;Runtime,Android (operating system);graphical user interfaces;mobile computing;program testing,Android apps;static GUI model;testing effectiveness;APE;Android applications;GUI testing;Android GUI testing tools","keywords":"GUI testing;mobile app testing;CEGAR","startPage":"269","endPage":"280","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812132","citationCount":1,"referenceCount":54,"year":2019,"authors":"T. Gu; C. Sun; X. Ma; C. Cao; C. Xu; Y. Yao; Q. Zhang; J. Lu; Z. Su","affiliations":"University of California, Davis, United States; University of California, Davis, United States; Nanjing University, China; Nanjing University, China; Nanjing University, China; Nanjing University, China; Georgia Institute of Technology, United States; Nanjing University, China; University of California, Davis, United States and ETH Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33d9"},"title":"Classifying Developers into Core and Peripheral: An Empirical Study on Count and Network Metrics","abstract":"Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work.","conference":"IEEE","terms":"Measurement;Collaboration;Stability analysis;Open source software;Systems architecture;Computer bugs,software engineering,software project;count-based operationalizations;fine-grained developer network;open-source projects;network-based core-peripheral operationalizations","keywords":"Developer roles;developer networks;classification;mining software repositories","startPage":"164","endPage":"174","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985659","citationCount":9,"referenceCount":34,"year":2017,"authors":"M. Joblin; S. Apel; C. Hunsen; W. Mauerer","affiliations":"Siemens AG, Erlangen, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; OTH Regensburg Munich, Siemens AG, Munich, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33da"},"title":"A Guided Genetic Algorithm for Automated Crash Reproduction","abstract":"To reduce the effort developers have to make for crash debugging, researchers have proposed several solutions for automatic failure reproduction. Recent advances proposed the use of symbolic execution, mutation analysis, and directed model checking as underling techniques for post-failure analysis of crash stack traces. However, existing approaches still cannot reproduce many real-world crashes due to such limitations as environment dependencies, path explosion, and time complexity. To address these challenges, we present EvoCrash, a post-failure approach which uses a novel Guided Genetic Algorithm (GGA) to cope with the large search space characterizing real-world software programs. Our empirical study on three open-source systems shows that EvoCrash can replicate 41 (82%) of real-world crashes, 34 (89%) of which are useful reproductions for debugging purposes, outperforming the state-of-the-art in crash replication.","conference":"IEEE","terms":"Tools;Genetic algorithms;Software;Model checking;Computer bugs;Debugging,genetic algorithms;program debugging,guided genetic algorithm;automated crash reproduction;crash debugging;automatic failure reproduction;EvoCrash;post-failure approach;GGA","keywords":"Search-Based Software Testing;Genetic Algorithms;Automated Crash Reproduction","startPage":"209","endPage":"220","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985663","citationCount":6,"referenceCount":40,"year":2017,"authors":"M. Soltani; A. Panichella; A. van Deursen","affiliations":"Delft Univ. of Technol., Delft, Netherlands; SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33db"},"title":"Evaluating and Improving Fault Localization","abstract":"Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better. Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults. We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults. In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports.","conference":"IEEE","terms":"Maintenance engineering;Tools;Debugging;Java;Computer bugs;Focusing;Manuals,program testing;source code (software),fault localization techniques;faulty program;suspicious code locations;defective code;replication study;spectrum-based families;mutation-based families;design space","keywords":"","startPage":"609","endPage":"620","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985698","citationCount":35,"referenceCount":50,"year":2017,"authors":"S. Pearson; J. Campos; R. Just; G. Fraser; R. Abreu; M. D. Ernst; D. Pang; B. Keller","affiliations":"Univ. of Washington, St. Louis, MO, USA; Univ. of Sheffield, Sheffield, UK; Univ. of Massachusetts, Amherst, MA, USA; Univ. of Sheffield, Sheffield, UK; Palo Alto Res. Center, Palo Alto, CA, USA; Univ. of Washington, St. Louis, MO, USA; Univ. of Washington, St. Louis, MO, USA; Univ. of Washington, St. Louis, MO, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33dc"},"title":"Analysis and Detection of Information Types of Open Source Software Issue Discussions","abstract":"Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.","conference":"IEEE","terms":"Stakeholders;Message systems;Software engineering;Computer bugs;Task analysis;Open source software,learning (artificial intelligence);pattern classification;project management;public domain software;regression analysis;software development management,ITSs;open source software projects;software project;OSS stakeholders;OSS issue discussions;qualitative content analysis;sentence types;software engineering activities;information types;open source software issue discussions;random forest;logistic regression","keywords":"collaborative software engineering;issue tracking system;issue discussion analysis","startPage":"454","endPage":"464","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811936","citationCount":0,"referenceCount":29,"year":2019,"authors":"D. Arya; W. Wang; J. L. C. Guo; J. Cheng","affiliations":"McGill University; McGill University; McGill University; Polytechnique Montreal","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33dd"},"title":"Natural Software Revisited","abstract":"Recent works have concluded that software code is more repetitive and predictable, i.e. more natural, than English texts. On re-examination, we find that much of the apparent \"naturalness\" of source code is due to the presence of language specific syntax, especially separators, such as semi-colons and brackets. For example, separators account for 44% of all tokens in our Java corpus. When we follow the NLP practices of eliminating punctuation (e.g., separators) and stopwords (e.g., keywords), we find that code is still repetitive and predictable, but to a lesser degree than previously thought. We suggest that SyntaxTokens be filtered to reduce noise in code recommenders. Unlike the code written for a particular project, API code usage is similar across projects: a file is opened and closed in the same manner regardless of domain. When we restrict our n-grams to those contained in the Java API, we find that API usages are highly repetitive. Since API calls are common across programs, researchers have made reliable statistical models to recommend sophisticated API call sequences. Sequential n-gram models were developed for natural languages. Code is usually represented by an AST which contains control and data flow, making n-grams models a poor representation of code. Comparing n-grams to statistical graph representations of the same codebase, we find that graphs are more repetitive and contain higherlevel patterns than n-grams. We suggest that future work focus on statistical code graphs models that accurately capture complex coding patterns. Our replication package makes our scripts and data available to future researchers[1].","conference":"IEEE","terms":"Java;Mathematical model;Syntactics;Particle separators;Programming;Python,application program interfaces;graph theory;Java;natural language processing;source code (software);statistical analysis;text analysis,natural software;software code;English texts;re-examination;apparent naturalness;source code;language specific syntax;separators;brackets;Java corpus;NLP practices;code recommenders;API code usage;Java API;API usages;sophisticated API call sequences;sequential n-gram models;natural languages;statistical graph representations;statistical code graphs models;complex coding patterns","keywords":"Basic Science;Entropy;Language Models;Statistical Code Graphs;StackOverflow","startPage":"37","endPage":"48","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811940","citationCount":2,"referenceCount":58,"year":2019,"authors":"M. Rahman; D. Palani; P. C. Rigby","affiliations":"Concordia University; Concordia University; Concordia University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33de"},"title":"Automatic Text Input Generation for Mobile Testing","abstract":"Many designs have been proposed to improve the automated mobile testing. Despite these improvements, providing appropriate text inputs remains a prominent obstacle, which hinders the large-scale adoption of automated testing approaches. The key challenge is how to automatically produce the most relevant text in a use case context. For example, a valid website address should be entered in the address bar of a mobile browser app to continue the testing of the app, a singer's name should be entered in the search bar of a music recommendation app. Without the proper text inputs, the testing would get stuck. We propose a novel deep learning based approach to address the challenge, which reduces the problem to a minimization problem. Another challenge is how to make the approach generally applicable to both the trained apps and the untrained apps. We leverage the Word2Vec model to address the challenge. We have built our approaches as a tool and evaluated it with 50 iOS mobile apps including Firefox and Wikipedia. The results show that our approach significantly outperforms existing automatic text input generation methods.","conference":"IEEE","terms":"Testing;Mobile communication;Neurons;Biological neural networks;Predictive models;Training;Context modeling,learning (artificial intelligence);mobile computing;program testing;text analysis,Wikipedia;Firefox;iOS mobile apps;Word2Vec model;minimization problem;deep learning based approach;music recommendation app;mobile browser app;Web site address;automated mobile testing;automatic text input generation method","keywords":"","startPage":"643","endPage":"653","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985701","citationCount":7,"referenceCount":37,"year":2017,"authors":"P. Liu; X. Zhang; M. Pistoia; Y. Zheng; M. Marques; L. Zeng","affiliations":"IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; Purdue Univ., West Lafayette, IN, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; IBM T. J. Watson Res. Center, Yorktown Heights, NY, USA; Purdue Univ., West Lafayette, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33df"},"title":"Learning to Spot and Refactor Inconsistent Method Names","abstract":"To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.","conference":"IEEE","terms":"Feature extraction;Semantics;Neural networks;Training;Computational modeling;Debugging;Software,Java;program debugging;software maintenance,refactor inconsistent method names;program methods;debugging method names;method code;Java methods;code readability;software maintenance","keywords":"Code refactoring, inconsistent method names, deep neural networks, code embedding","startPage":"1","endPage":"12","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812134","citationCount":4,"referenceCount":86,"year":2019,"authors":"K. Liu; D. Kim; T. F. Bissyandé; T. Kim; K. Kim; A. Koyuncu; S. Kim; Y. Le Traon","affiliations":"Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Department of Software Engineering, Chonbuk National University, South Korea; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Department of Software Engineering, Chonbuk National University, South Korea; Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e0"},"title":"ZenIDS: Introspective Intrusion Detection for PHP Applications","abstract":"Since its first appearance more than 20 years ago, PHP has steadily increased in popularity, and has become the foundation of the Internet's most popular content management systems (CMS). Of the world's 1 million most visited websites, nearly half use a CMS, and WordPress alone claims 25% market share of all websites. While their easy-to-use templates and components have greatly simplified the work of developing high quality websites, it comes at the cost of software vulnerabilities that are inevitable in such large and rapidly evolving frameworks. Intrusion Detection Systems (IDS) are often used to protect Internet-facing applications, but conventional techniques struggle to keep up with the fast pace of development in today's web applications. Rapid changes to application interfaces increase the workload of maintaining an IDS whitelist, yet the broad attack surface of a web application makes for a similarly verbose blacklist. We developed ZenIDS to dynamically learn the trusted execution paths of an application during a short online training period and report execution anomalies as potential intrusions. We implement ZenIDS as a PHP extension supported by 8 hooks instrumented in the PHP interpreter. Our experiments demonstrate its effectiveness monitoring live web traffic for one year to 3 large PHP applications, detecting malicious requests with a false positive rate of less than .01% after training on fewer than 4,000 requests. ZenIDS excludes the vast majority of deployed PHP code from the whitelist because it is never used for valid requests-yet could potentially be exploited by a remote adversary. We observe 5% performance overhead (or less) for our applications vs. an optimized vanilla LAMP stack.","conference":"IEEE","terms":"Training;Monitoring;Authentication;Intrusion detection;Internet;Content management,authoring languages;security of data;system monitoring,PHP applications;ZENIDS;PHP extension;PHP interpreter;malicious requests detection;live Web traffic monitoring;vanilla LAMP stack;introspective intrusion detection;intrusion detection systems","keywords":"","startPage":"232","endPage":"243","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985665","citationCount":0,"referenceCount":54,"year":2017,"authors":"B. Hawkins; B. Demsky","affiliations":"Univ. of California, Irvine, Irvine, CA, USA; Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e1"},"title":"VFix: Value-Flow-Guided Precise Program Repair for Null Pointer Dereferences","abstract":"Automated Program Repair (APR) faces a key challenge in efficiently generating correct patches from a potentially infinite solution space. Existing approaches, which attempt to reason about the entire solution space, can be ineffective (by often producing no plausible patches at all) and imprecise (by often producing plausible but incorrect patches). We present VFIX, a new value-flow-guided APR approach, to fix null pointer exception (NPE) bugs by considering a substantially reduced solution space in order to greatly increase the number of correct patches generated. By reasoning about the data and control dependences in the program, VFIX can identify bug-relevant repair statements more accurately and generate more correct repairs than before. VFIX outperforms a set of 8 state-of-the-art APR tools in fixing the NPE bugs in Defects4j in terms of both precision (by correctly fixing 3 times as many bugs as the most precise one and 50% more than all the bugs correctly fixed by these 8 tools altogether) and efficiency (by producing a correct patch in minutes instead of hours).","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;Tools;Aerospace electronics;Software;Australia,program debugging;program diagnostics;software maintenance,VFix;value-flow-guided precise program repair;null pointer dereferences;value-flow-guided APR approach;null pointer exception bugs;bug-relevant repair statements;NPE bugs;automated program repair","keywords":"program repair, static analysis, null dereference","startPage":"512","endPage":"523","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812101","citationCount":1,"referenceCount":92,"year":2019,"authors":"X. Xu; Y. Sui; H. Yan; J. Xue","affiliations":"University of New South Wales, Australia; University of Technology Sydney, Australia; University of New South Wales, Australia; University of New South Wales, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e2"},"title":"View-Centric Performance Optimization for Database-Backed Web Applications","abstract":"Web developers face the stringent task of designing informative web pages while keeping the page-load time low. This task has become increasingly challenging as most web contents are now generated by processing ever-growing amount of user data stored in back-end databases. It is difficult for developers to understand the cost of generating every web-page element, not to mention explore and pick the web design with the best trade-off between performance and functionality. In this paper, we present Panorama, a view-centric and database-aware development environment for web developers. Using database-aware program analysis and novel IDE design, Panorama provides developers with intuitive information about the cost and the performance-enhancing opportunities behind every HTML element, as well as suggesting various global code refactorings that enable developers to easily explore a wide spectrum of performance and functionality trade-offs.","conference":"IEEE","terms":"Databases;Rails;Web pages;Data processing;Tools;Servers;Task analysis,database management systems;hypermedia markup languages;program diagnostics;Web design,view-centric performance optimization;page-load time;back-end databases;database-aware development environment;database-aware program analysis;performance-enhancing opportunities;database-backed Web applications;Web design;Web-page element;informative Web pages","keywords":"database backed web applications;ORM framework;view centric","startPage":"994","endPage":"1004","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811938","citationCount":2,"referenceCount":43,"year":2019,"authors":"J. Yang; C. Yan; C. Wan; S. Lu; A. Cheung","affiliations":"University of Chicago, USA; University of Washington, USA; University of Chicago, USA; University of Chicago, USA; University of Washington, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e3"},"title":"ReCDroid: Automatically Reproducing Android Application Crashes from Bug Reports","abstract":"The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid uses a combination of natural language processing (NLP) and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid on 51 original bug reports from 33 Android apps. The results show that ReCDroid successfully reproduced 33 crashes (63.5% success rate) directly from the textual description of bug reports. A user study involving 12 participants demonstrates that ReCDroid can improve the productivity of developers when resolving crash bug reports.","conference":"IEEE","terms":"Computer bugs;Graphical user interfaces;Data mining;Natural language processing;Google,graphical user interfaces;mobile computing;natural language processing;program debugging;smart phones;software maintenance,reported crash;ReCDroid;bug reports;crash bug reports;Android application crash;Android apps;natural language processing;dynamic GUI exploration","keywords":"Bug reproduction;Android;Natural language processing","startPage":"128","endPage":"139","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811942","citationCount":0,"referenceCount":57,"year":2019,"authors":"Y. Zhao; T. Yu; T. Su; Y. Liu; W. Zheng; J. Zhang; W. G.J. Halfond","affiliations":"University of Kentucky; University of Kentucky; Nanyang Technological University; Nanyang Technological University; Northwestern Polytechnical University; Northwestern Polytechnical University; University of Southern California","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e4"},"title":"Automated Transplantation and Differential Testing for Clones","abstract":"Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of G RAFTER, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders.","conference":"IEEE","terms":"Cloning;Computer bugs;Testing;Runtime;Software;Safety;Java,program diagnostics;program testing;public domain software;software reusability,automated transplantation testing;automated differential testing;code clones;runtime behavior;Grafter;variation identification;identifier names;targets;compilation errors;code transformation;stub code insertion;input data transfer;intermediate output value transfer;test outcome level;intermediate program state level;open source projects;test reuse;clone pairs;mutation testing tool;Major;fault injection;behavioral differences detection;test-level comparison;state-level comparison;static cloning bug finders","keywords":"Test Reuse;Code Transplantation;Differential Testing;Code Clones","startPage":"665","endPage":"676","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985703","citationCount":5,"referenceCount":52,"year":2017,"authors":"T. Zhang; M. Kim","affiliations":"Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e5"},"title":"Parallel Refinement for Multi-Threaded Program Verification","abstract":"Program verification is one of the most important methods to ensuring the correctness of concurrent programs. However, due to the path explosion problem, concurrent program verification is usually time consuming, which hinders its scalability to industrial programs. Parallel processing is a mainstream technique to deal with those problems which require mass computing. Hence, designing parallel algorithms to improve the performance of concurrent program verification is highly desired. This paper focuses on parallelization of the abstraction refinement technique, one of the most efficient techniques for concurrent program verification. We present a parallel refinement framework which employs multiple engines to refine the abstraction in parallel. Different from existing work which parallelizes the search process, our method achieves the effect of parallelization by refinement constraint and learnt clause sharing, so that the number of required iterations can be significantly reduced. We have implemented this framework on the scheduling constraint based abstraction refinement method, one of the best methods for concurrent program verification. Experiments on SV-COMP 2018 show the encouraging results of our method. For those complex programs requiring a large number of iterations, our method can obtain a linear reduction of the iteration number and significantly improve the verification performance.","conference":"IEEE","terms":"Engines;Concurrent computing;Programming;Encoding;Instruction sets;Software engineering;Job shop scheduling,iterative methods;multi-threading;parallel algorithms;program diagnostics;program verification;scheduling,parallel processing;parallel algorithms;concurrent program verification;abstraction refinement technique;parallel refinement framework;scheduling constraint based abstraction refinement method;complex programs;multithreaded program;concurrent programs","keywords":"Concurrent Program;Abstraction Refinement;Scheduling Constraint;Parallel Verification","startPage":"643","endPage":"653","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812136","citationCount":0,"referenceCount":39,"year":2019,"authors":"L. Yin; W. Dong; W. Liu; J. Wang","affiliations":"National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e6"},"title":"On Cross-Stack Configuration Errors","abstract":"Today's web applications are deployed on powerful software stacks such as MEAN (JavaScript) or LAMP (PHP), which consist of multiple layers such as an operating system, web server, database, execution engine and application framework, each of which provide resources to the layer just above it. These powerful software stacks unfortunately are plagued by so-called cross-stack configuration errors (CsCEs), where a higher layer in the stack suddenly starts to behave incorrectly or even crash due to incorrect configuration choices in lower layers. Due to differences in programming languages and lack of explicit links between configuration options of different layers, sysadmins and developers have a hard time identifying the cause of a CsCE, which is why this paper (1) performs a qualitative analysis of 1,082 configuration errors to understand the impact, effort and complexity of dealing with CsCEs, then (2) proposes a modular approach that plugs existing source code analysis (slicing) techniques, in order to recommend the culprit configuration option. Empirical evaluation of this approach on 36 real CsCEs of the top 3 LAMP stack layers shows that our approach reports the misconfigured option with an average rank of 2.18 for 32 of the CsCEs, and takes only few minutes, making it practically useful.","conference":"IEEE","terms":"Databases;Web servers;Testing;Debugging;Operating systems,Internet;program slicing;source code (software),Web applications;software stacks;cross-stack configuration errors;programming languages;CsCE;qualitative analysis;configuration errors;source code analysis;source code slicing;culprit configuration option;LAMP stack layers","keywords":"Software Configuration;Software Stack;Qualitative Study;Multi-layer Systems;Empirical Study;Slicing;PHP","startPage":"255","endPage":"265","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985667","citationCount":4,"referenceCount":53,"year":2017,"authors":"M. Sayagh; N. Kerzazi; B. Adams","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e7"},"title":"Supporting the Statistical Analysis of Variability Models","abstract":"Variability models are broadly used to specify the configurable features of highly customizable software. In practice, they can be large, defining thousands of features with their dependencies and conflicts. In such cases, visualization techniques and automated analysis support are crucial for understanding the models. This paper contributes to this line of research by presenting a novel, probabilistic foundation for statistical reasoning about variability models. Our approach not only provides a new way to visualize, describe and interpret variability models, but it also supports the improvement of additional state-of-the-art methods for software product lines; for instance, providing exact computations where only approximations were available before, and increasing the sensitivity of existing analysis operations for variability models. We demonstrate the benefits of our approach using real case studies with up to 17,365 features, and written in two different languages (KConfig and feature models).","conference":"IEEE","terms":"Visualization;Computational modeling;Analytical models;Software;Cognition;Feature extraction;Complexity theory,data visualisation;probability;public domain software;software product lines;software reusability;statistical analysis,statistical analysis;variability models;automated analysis support;visualization techniques;software product lines;feature model language;KConfig language","keywords":"Variability modeling;feature modeling;software product lines;software visualization;binary decision diagrams","startPage":"843","endPage":"853","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811977","citationCount":0,"referenceCount":41,"year":2019,"authors":"R. Heradio; D. Fernandez-Amoros; C. Mayr-Dorn; A. Egyed","affiliations":"Universidad Nacional de Educacion a Distancia, Spain; Universidad Nacional de Educacion a Distancia, Spain; Johannes Kepler University, Austria; Johannes Kepler University, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e8"},"title":"Making Malory Behave Maliciously: Targeted Fuzzing of Android Execution Environments","abstract":"Android applications, or apps, provide useful features to end-users, but many apps also contain malicious behavior. Modern malware makes understanding such behavior challenging by behaving maliciously only under particular conditions. For example, a malware app may check whether it runs on a real device and not an emulator, in a particular country, and alongside a specific target app, such as a vulnerable banking app. To observe the malicious behavior, a security analyst must find out and emulate all these app-specific constraints. This paper presents FuzzDroid, a framework for automatically generating an Android execution environment where an app exposes its malicious behavior. The key idea is to combine an extensible set of static and dynamic analyses through a search-based algorithm that steers the app toward a configurable target location. On recent malware, the approach reaches the target location in 75% of the apps. In total, we reach 240 code locations within an average time of only one minute. To reach these code locations, FuzzDroid generates 106 different environments, too many for a human analyst to create manually.","conference":"IEEE","terms":"Malware;Smart phones;Heuristic algorithms;Algorithm design and analysis;Mobile communication;Security;Instruments,invasive software;smart phones,malory;Android execution environment targeted fuzzing;Android applications;malicious behavior;malware;security analyst;app-specific constraints;FuzzDroid;static analysis;dynamic analysis;search-based algorithm;code locations","keywords":"","startPage":"300","endPage":"311","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985671","citationCount":9,"referenceCount":44,"year":2017,"authors":"S. Rasthofer; S. Arzt; S. Triller; M. Pradel","affiliations":"Fraunhofer SIT, Tech. Univ. Darmstadt, Darmstadt, Germany; Fraunhofer SIT, Tech. Univ. Darmstadt, Darmstadt, Germany; Fraunhofer SIT, Germany; Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33e9"},"title":"Gigahorse: Thorough, Declarative Decompilation of Smart Contracts","abstract":"The rise of smart contracts - autonomous applications running on blockchains - has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a “batteries included” approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.","conference":"IEEE","terms":"Smart contracts;Blockchain;Virtual machining;Task analysis;Java;Security,contracts;cryptocurrencies;distributed databases;formal specification;program compilers;program diagnostics;virtual machines,Gigahorse;low-level decompilation;program analysis;blockchain;logic-based specification;cryptocurrencies;Ethereum virtual machine bytecode;EVM bytecode;Porosity decompiler;Vandal decompiler;Ethereum smart contracts","keywords":"Ethereum;Blockchain;Decompilation;Program Analysis;Security","startPage":"1176","endPage":"1186","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811905","citationCount":0,"referenceCount":36,"year":2019,"authors":"N. Grech; L. Brent; B. Scholz; Y. Smaragdakis","affiliations":"University of Athens, Greece and University of Malta, Malta; The University of Sydney, Australia; The University of Sydney, Australia; University of Athens, Greece","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ea"},"title":"Analyzing and Supporting Adaptation of Online Code Examples","abstract":"Developers often resort to online Q\u0026A forums such as Stack Overflow (SO) for filling their programming needs. Although code examples on those forums are good starting points, they are often incomplete and inadequate for developers' local program contexts; adaptation of those examples is necessary to integrate them to production code. As a consequence, the process of adapting online code examples is done over and over again, by multiple developers independently. Our work extensively studies these adaptations and variations, serving as the basis for a tool that helps integrate these online code examples in a target context in an interactive manner. We perform a large-scale empirical study about the nature and extent of adaptations and variations of SO snippets. We construct a comprehensive dataset linking SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. We then qualitatively inspect 400 SO examples and their GitHub counterparts and develop a taxonomy of 24 adaptation types. Using this taxonomy, we build an automated adaptation analysis technique on top of GumTree to classify the entire dataset into these types. We build a Chrome extension called ExampleStack that automatically lifts an adaptation-aware template from each SO example and its GitHub counterparts to identify hot spots where most changes happen. A user study with sixteen programmers shows that seeing the commonalities and variations in similar GitHub counterparts increases their confidence about the given SO example, and helps them grasp a more comprehensive view about how to reuse the example differently and avoid common pitfalls.","conference":"IEEE","terms":"Cloning;Taxonomy;Java;Tools;Programming;Software engineering;Production,data mining;program debugging;program diagnostics;software maintenance,automated adaptation analysis technique;adaptation-aware template;production code;online Q\u0026A forums;SO snippets;clone detection;time stamp analysis;explicit URL references;Chrome extension;ExampleStack","keywords":"online code examples;code adaptation","startPage":"316","endPage":"327","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812028","citationCount":0,"referenceCount":68,"year":2019,"authors":"T. Zhang; D. Yang; C. Lopes; M. Kim","affiliations":"University of California, Los Angeles; University of California, Irvine; University of California, Irvine; University of California, Los Angeles","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33eb"},"title":"Graph-Based Mining of In-the-Wild, Fine-Grained, Semantic Code Change Patterns","abstract":"Prior research exploited the repetitiveness of code changes to enable several tasks such as code completion, bug-fix recommendation, library adaption, etc. These and other novel applications require accurate detection of semantic changes, but the state-of-the-art methods are limited to algorithms that detect specific kinds of changes at the syntactic level. Existing algorithms relying on syntactic similarity have lower accuracy, and cannot effectively detect semantic change patterns. We introduce a novel graph-based mining approach, CPatMiner, to detect previously unknown repetitive changes in the wild, by mining fine-grained semantic code change patterns from a large number of repositories. To overcome unique challenges such as detecting meaningful change patterns and scaling to large repositories, we rely on fine-grained change graphs to capture program dependencies. We evaluate CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub across a population of 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to 108 open-source developers. We found that 70% of respondents recognized those patterns as their meaningful frequent changes. Moreover, 79% of respondents even named the patterns, and 44% wanted future IDEs to automate such repetitive changes. We found that the mined change patterns belong to various development activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%, including refactorings). Second, we compared our tool with the state-of-the-art, AST-based technique, and reported that it detects 2.1x more meaningful patterns. Third, we use CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K instances of change patterns from which we provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the change patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns are not intermittent but spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications.","conference":"IEEE","terms":"Semantics;Tools;Data mining;Syntactics;Computer science;Task analysis;Open source software,data mining;graph theory;program diagnostics;software engineering,fine-grained semantic code change patterns;fine-grained change;mined change patterns;community-based change pattern database;graph-based mining;code changes;semantic change patterns;GitHub projects;AST-based technique","keywords":"Semantic Change Pattern Mining, Graph Mining","startPage":"819","endPage":"830","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812071","citationCount":1,"referenceCount":48,"year":2019,"authors":"H. A. Nguyen; T. N. Nguyen; D. Dig; S. Nguyen; H. Tran; M. Hilton","affiliations":"Iowa State University, USA; University of Texas at Dallas, USA; Oregon State University; University of Texas at Dallas, USA; University of Texas at Dallas, USA; Carnegie Mellon University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ec"},"title":"Optimizing Test Placement for Module-Level Regression Testing","abstract":"Modern build systems help increase developer productivity by performing incremental building and testing. These build systems view a software project as a group of interdependent modules and perform regression test selection at the module level. However, many large software projects have imprecise dependency graphs that lead to wasteful test executions. If a test belongs to a module that has more dependencies than the actual dependencies of the test, then it is executed unnecessarily whenever a code change impacts those additional dependencies. In this paper, we formulate the problem of wasteful test executions due to suboptimal placement of tests in modules. We propose a greedy algorithm to reduce the number of test executions by suggesting test movements while considering historical build information and actual dependencies of tests. We have implemented our technique, called TestOptimizer, on top of CloudBuild, the build system developed within Microsoft over the last few years. We have evaluated the technique on five large proprietary projects. Our results show that the suggested test movements can lead to a reduction of 21.66 million test executions (17.09%) across all our subject projects. We received encouraging feedback from the developers of these projects; they accepted and intend to implement ≈80% of our reported suggestions.","conference":"IEEE","terms":"Software;Testing;Buildings;Greedy algorithms;Metadata;Productivity;Google,greedy algorithms;program testing;regression analysis,test placement optimization;module-level regression testing;software project;dependency graphs;greedy algorithm;TestOptimizer;CloudBuild","keywords":"regression test selection;module-level regression testing;build system","startPage":"689","endPage":"699","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985705","citationCount":2,"referenceCount":24,"year":2017,"authors":"A. Shi; S. Thummalapenta; S. K. Lahiri; N. Bjorner; J. Czerwonka","affiliations":"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Microsoft Corp., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ed"},"title":"RClassify: Classifying Race Conditions in Web Applications via Deterministic Replay","abstract":"Race conditions are common in web applications but are difficult to diagnose and repair. Although there exist tools for detecting races in web applications, they all report a large number of false positives. That is, the races they report are either bogus, meaning they can never occur in practice, or benign, meaning they do not lead to erroneous behaviors. Since manually diagnosing them is tedious and error prone, reporting these race warnings to developers would be counter-productive. We propose a platform-agnostic, deterministic replay-based method for identifying not only the real but also the truly harmful race conditions. It relies on executing each pair of racing events in two different orders and assessing their impact on the program state: we say a race is harmful only if (1) both of the two executions arefeasible and (2) they lead to different program states. We have evaluated our evidence-based classification method on a large set of real websites from Fortune-500 companies and demonstrated that it significantly outperforms all state-of-the-art techniques.","conference":"IEEE","terms":"Tools;HTML;Browsers;Companies;Robustness;Benchmark testing;Standards,Internet;pattern classification,RClassify;race condition classification;Web applications;platform-agnostic deterministic replay-based method;program state;evidence-based classification method;Web sites","keywords":"Race condition;web application;JavaScript;deterministic replay;program repair","startPage":"278","endPage":"288","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985669","citationCount":2,"referenceCount":33,"year":2017,"authors":"L. Zhang; C. Wang","affiliations":"Virginia Tech., Blacksburg, VA, USA; Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ee"},"title":"An Efficient, Robust, and Scalable Approach for Analyzing Interacting Android Apps","abstract":"When multiple apps on an Android platform interact, faults and security vulnerabilities can occur. Software engineers need to be able to analyze interacting apps to detect such problems. Current approaches for performing such analyses, however, do not scale to the numbers of apps that may need to be considered, and thus, are impractical for application to real-world scenarios. In this paper, we introduce JITANA, a program analysis framework designed to analyze multiple Android apps simultaneously. By using a classloader-based approach instead of a compiler-based approach such as SOOT, JITANA is able to simultaneously analyze large numbers of interacting apps, perform on-demand analysis of large libraries, and effectively analyze dynamically generated code. Empirical studies of JITANA show that it is substantially more efficient than a state-of-the-art approach, and that it can effectively and efficiently analyze complex apps including Facebook, Pokemon Go, and Pandora that the state-of-the-art approach cannot handle.","conference":"IEEE","terms":"Androids;Humanoid robots;Tools;Security;Java;Libraries;Robustness,Android (operating system);mobile computing;program compilers,Android Apps;JITANA;program analysis;classloader-based approach;SOOT;Facebook;Pokemon Go;Pandora","keywords":"Android;program analysis;inter-app communication","startPage":"324","endPage":"334","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985673","citationCount":3,"referenceCount":33,"year":2017,"authors":"Y. Tsutano; S. Bachala; W. Srisa-An; G. Rothermel; J. Dinh","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33ef"},"title":"Mimic: UI Compatibility Testing System for Android Apps","abstract":"This paper proposes Mimic, an automated UI compatibility testing system for Android apps. Mimic is designed specifically for comparing the UI behavior of an app across different devices, different Android versions, and different app versions. This design choice stems from a common problem that Android developers and researchers face-how to test whether or not an app behaves consistently across different environments or internal changes. Mimic allows Android app developers to easily perform backward and forward compatibility testing for their apps. It also enables a clear comparison between a stable version of app and a newer version of app. In doing so, Mimic allows multiple testing strategies to be used, such as randomized or sequential testing. Finally, Mimic programming model allows such tests to be scripted with much less developer effort than other comparable systems. Additionally, Mimic allows parallel testing with multiple testing devices and thereby speeds up testing time. To demonstrate these capabilities, we perform extensive tests for each of the scenarios described above. Our results show that Mimic is effective in detecting forward and backward compatibility issues, and verify runtime behavior of apps. Our evaluation also shows that Mimic significantly reduces the development burden for developers.","conference":"IEEE","terms":"Testing;Programming;Performance evaluation;Runtime;Libraries;Instruments;Google,Android (operating system);mobile computing;program testing;user interfaces,Android apps;Mimic programming model;parallel testing;multiple testing devices;UI compatibility testing system","keywords":"Mobile apps;UI compatibility testing;Parallel testing;Programming model","startPage":"246","endPage":"256","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811983","citationCount":1,"referenceCount":37,"year":2019,"authors":"T. Ki; C. M. Park; K. Dantu; S. Y. Ko; L. Ziarek","affiliations":"University at Buffalo, The State University of New York; University at Buffalo, The State University of New York; University at Buffalo, The State University of New York; University at Buffalo, The State University of New York; University at Buffalo, The State University of New York","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f0"},"title":"Test-Driven Code Review: An Empirical Study","abstract":"Test-Driven Code Review (TDR) is a code review practice in which a reviewer inspects a patch by examining the changed test code before the changed production code. Although this practice has been mentioned positively by practitioners in informal literature and interviews, there is no systematic knowledge of its effects, prevalence, problems, and advantages. In this paper, we aim at empirically understanding whether this practice has an effect on code review effectiveness and how developers' perceive TDR. We conduct (i) a controlled experiment with 93 developers that perform more than 150 reviews, and (ii) 9 semi-structured interviews and a survey with 103 respondents to gather information on how TDR is perceived. Key results from the experiment show that developers adopting TDR find the same proportion of defects in production code, but more in test code, at the expenses of fewer maintainability issues in production code. Furthermore, we found that most developers prefer to review production code as they deem it more critical and tests should follow from it. Moreover, general poor test code quality and no tool support hinder the adoption of TDR. Public preprint: [https: //doi.org/10.5281/zenodo.2551217], data and materials: [https:// doi.org/10.5281/zenodo.2553139].","conference":"IEEE","terms":"Production;Tools;Software;Inspection;Interviews;Computer bugs;Graphical user interfaces,program testing;software development management;software quality,test-driven code review;TDR;code review effectiveness;critical tests;production code;semistructured interviews;test code quality","keywords":"MSR;code review;TDR;software testing","startPage":"1061","endPage":"1072","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811911","citationCount":1,"referenceCount":56,"year":2019,"authors":"D. Spadini; F. Palomba; T. Baum; S. Hanenberg; M. Bruntink; A. Bacchelli","affiliations":"Delft University of Technology / SIG; University of Zurich; Leibniz Universitat Hannover; Paluno, University of Duisburg-Essen; Software Improvement Group; University of Zurich","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f1"},"title":"Recovering Variable Names for Minified Code with Usage Contexts","abstract":"To avoid the exposure of original source code in a Web application, the variable names in JS code deployed in the wild are often replaced by short, meaningless names, thus making the code extremely difficult to manually understand and analysis. This paper presents JSNeat, an information retrieval (IR)-based approach to recover the variable names in minified JS code. JSNeat follows a data-driven approach to recover names by searching for them in a large corpus of open-source JS code. We use three types of contexts to match a variable in given minified code against the corpus including the context of the properties and roles of the variable, the context of that variable and relations with other variables under recovery, and the context of the task of the function to which the variable contributes. We performed several empirical experiments to evaluate JSNeat on the dataset of more than 322K JS files with 1M functions, and 3.5M variables with 176K unique variable names. We found that JSNeat achieves a high accuracy of 69.1%, which is the relative improvements of 66.1% and 43% over two state-of-the-art approaches JSNice and JSNaughty, respectively. The time to recover for a file or a variable with JSNeat is twice as fast as with JSNice and 4x as fast as with JNaughty, respectively.","conference":"IEEE","terms":"Task analysis;Reactive power;Information retrieval;Tools;Databases;Static VAr compensators;Computer science,authoring languages;information retrieval;Internet;Java;public domain software;source code (software),usage contexts;JSNeat;information retrieval-based approach;minified JS code;open-source JS code;variable relations;variable contributes;data-driven approach;Web application;variable names;minified code;source code","keywords":"Minified JS Code, Variable Name Recovery, Naturalness of Code, Usage Contexts.","startPage":"1165","endPage":"1175","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812034","citationCount":1,"referenceCount":21,"year":2019,"authors":"H. Tran; N. Tran; S. Nguyen; H. Nguyen; T. N. Nguyen","affiliations":"University of Texas at Dallas, USA; University of Texas at Dallas, USA; University of Texas at Dallas, USA; Iowa State University, USA; University of Texas at Dallas, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f2"},"title":"SLF: Fuzzing without Valid Seed Inputs","abstract":"Fuzzing is an important technique to detect software bugs and vulnerabilities. It works by mutating a small set of seed inputs to generate a large number of new inputs. Fuzzers' performance often substantially degrades when valid seed inputs are not available. Although existing techniques such as symbolic execution can generate seed inputs from scratch, they have various limitations hindering their applications in real-world complex software. In this paper, we propose a novel fuzzing technique that features the capability of generating valid seed inputs. It piggy-backs on AFL to identify input validity checks and the input fields that have impact on such checks. It further classifies these checks according to their relations to the input. Such classes include arithmetic relation, object offset, data structure length and so on. A multi-goal search algorithm is developed to apply class-specific mutations in order to satisfy inter-dependent checks all together. We evaluate our technique on 20 popular benchmark programs collected from other fuzzing projects and the Google fuzzer test suite, and compare it with existing fuzzers AFL and AFLFast, symbolic execution engines KLEE and S2E, and a hybrid tool Driller that combines fuzzing with symbolic execution. The results show that our technique is highly effective and efficient, out-performing the other tools.","conference":"IEEE","terms":"Fuzzing;Software;Engines;Indexes;Tools;Libraries;Computer science,fuzzy set theory;program debugging;program testing,valid seed inputs;novel fuzzing technique;input validity checks;software bugs detection;software vulnerabilities;Google fuzzer test suite;multigoal search algorithm","keywords":"fuzzing;seed inputs","startPage":"712","endPage":"723","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812105","citationCount":0,"referenceCount":57,"year":2019,"authors":"W. You; X. Liu; S. Ma; D. Perry; X. Zhang; B. Liang","affiliations":"Purdue University; Zhejiang University; Purdue University; Purdue University; Purdue University; Renmin University of China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f3"},"title":"Guiding Deep Learning System Testing Using Surprise Adequacy","abstract":"Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.","conference":"IEEE","terms":"Neurons;Testing;Training;Training data;Deep learning;Autonomous vehicles;Density measurement,image classification;learning (artificial intelligence);program testing,DL system;training data;test adequacy criterion;deep learning system testing;Surprise Adequacy for Deep Learning Systems;autonomous driving car platforms;systematic sampling;coverage criteria","keywords":"Test Adequacy;Coverage Criteria;Deep Learning Systems","startPage":"1039","endPage":"1049","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812069","citationCount":4,"referenceCount":44,"year":2019,"authors":"J. Kim; R. Feldt; S. Yoo","affiliations":"KAIST, Republic of Korea; Chalmers University and Blekinge Inst. of Technology, Sweden; KAIST, Republic of Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f4"},"title":"Grey-Box Concolic Testing on Binary Code","abstract":"We present grey-box concolic testing, a novel path-based test case generation method that combines the best of both white-box and grey-box fuzzing. At a high level, our technique systematically explores execution paths of a program under test as in white-box fuzzing, a.k.a. concolic testing, while not giving up the simplicity of grey-box fuzzing: it only uses a lightweight instrumentation, and it does not rely on an SMT solver. We implemented our technique in a system called Eclipser, and compared it to the state-of-the-art grey-box fuzzers (including AFLFast, LAF-intel, Steelix, and VUzzer) as well as a symbolic executor (KLEE). In our experiments, we achieved higher code coverage and found more bugs than the other tools.","conference":"IEEE","terms":"Fuzzing;Computer bugs;Instruments;Binary codes;Tools;Security,binary codes;program debugging;program testing,grey-box concolic testing;grey-box fuzzing;white-box fuzzing;grey-box fuzzers;path-based test case generation method;binary code","keywords":"software testing;concolic testing;fuzzing","startPage":"736","endPage":"747","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811950","citationCount":0,"referenceCount":63,"year":2019,"authors":"J. Choi; J. Jang; C. Han; S. K. Cha","affiliations":"KAIST, Republic of Korea; Samsung Research, Republic of Korea; Naver Labs, Republic of Korea; KAIST, Republic of Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f5"},"title":"A Framework for Checking Regression Test Selection Tools","abstract":"Regression test selection (RTS) reduces regression testing costs by re-running only tests that can change behavior due to code changes. Researchers and large software organizations recently developed and adopted several RTS tools to deal with the rapidly growing costs of regression testing. As RTS tools gain adoption, it becomes critical to check that they are correct and efficient. Unfortunately, checking RTS tools currently relies solely on limited tests that RTS tool developers manually write. We present RTSCheck, the first framework for checking RTS tools. RTSCheck feeds evolving programs (i.e., sequences of program revisions) to an RTS tool and checks the output against rules inspired by existing RTS test suites. Violations of these rules are likely due to deviations from expected RTS tool behavior, and indicative of bugs in the tool. RTSCheck uses three components to obtain evolving programs: (1) AutoEP automatically generates evolving programs and corresponding tests, (2) DefectsEP uses buggy and fixed program revisions from bug databases, and (3) EvoEP uses sequences of program revisions from actual open-source projects' histories. We used RTSCheck to check three recently developed RTS tools for Java: Clover, Ekstazi, and STARTS. RTSCheck discovered 27 bugs in these three tools.","conference":"IEEE","terms":"Tools;Computer bugs;Testing;Safety;Java;Software;Feeds,Java;program debugging;program testing;software tools,program revisions;RTSCheck;checking regression test selection tools;regression testing costs;RTS tool developers;expected RTS tool behavior;RTS test suites;EvoEP;open-source project histories;Java;Clover;Ekstazi;STARTS","keywords":"regression test selection;program generation;evolution;checking software tools","startPage":"430","endPage":"441","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812073","citationCount":0,"referenceCount":80,"year":2019,"authors":"C. Zhu; O. Legunsen; A. Shi; M. Gligoric","affiliations":"The University of Texas at Austin; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; The University of Texas at Austin","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f6"},"title":"What Causes My Test Alarm? Automatic Cause Analysis for Test Alarms in System and Integration Testing","abstract":"Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.","conference":"IEEE","terms":"Software;Computer bugs;Product codes;Instruments;Software testing;Analytical models,information retrieval;program diagnostics;program testing,automatic cause analysis;system and integration testing;test alarms;cause analysis model;information retrieval techniques;test logs;SIT","keywords":"software testing;system and integration testing;test alarm analysis;multiclass classification","startPage":"712","endPage":"723","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985707","citationCount":7,"referenceCount":51,"year":2017,"authors":"H. Jiang; X. Li; Z. Yang; J. Xuan","affiliations":"Sch. of Software, Dalian Univ. of Technol., Dalian, China; Sch. of Software, Dalian Univ. of Technol., Dalian, China; Western Michigan Univ., Kalamazoo, MI, USA; State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f7"},"title":"To Type or Not to Type: Quantifying Detectable Bugs in JavaScript","abstract":"JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!.","conference":"IEEE","terms":"Computer bugs;History;Software;Surgery;Facebook;Measurement uncertainty;Documentation,Java;program debugging,JavaScript;TypeScript;static type systems;bug detection;Web domain","keywords":"JavaScript;static type systems;Flow;TypeScript;mining software repositories","startPage":"758","endPage":"769","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985711","citationCount":5,"referenceCount":53,"year":2017,"authors":"Z. Gao; C. Bird; E. T. Barr","affiliations":"Univ. Coll. London, London, UK; Microsoft Res., Redmond, WA, USA; Univ. Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9115eee8435e8e7d33f8"},"title":"Analysis and Testing of Notifications in Android Wear Applications","abstract":"Android Wear (AW) is Google's platform for developing applications for wearable devices. Our goal is to make a first step toward a foundation for analysis and testing of AW apps. We focus on a core feature of such apps: notifications issued by a handheld device (e.g., a smartphone) and displayed on a wearable device (e.g., a smartwatch). We first define a formal semantics of AW notifications in order to capture the core features and behavior of the notification mechanism. Next, we describe a constraint-based static analysis to build a model of this run-time behavior. We then use this model to develop a novel testing tool for AW apps. The tool contains a testing framework together with components to support AW-specific coverage criteria and to automate the generation of GUI events on the wearable. These contributions advance the state of the art in the increasingly important area of software for wearable devices.","conference":"IEEE","terms":"Semantics;Testing;Androids;Humanoid robots;Handheld computers;Software;Smart phones,Android (operating system);graphical user interfaces;program diagnostics;program testing;software tools;wearable computers,notification testing;notification analysis;Android wear applications;Google platform;wearable devices;AW apps testing;AW apps analysis;handheld device;constraint-based static analysis;AW-specific coverage criteria;GUI event generation","keywords":"","startPage":"347","endPage":"357","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985675","citationCount":8,"referenceCount":43,"year":2017,"authors":"H. Zhang; A. Rountev","affiliations":"Ohio State Univ., Columbus, OH, USA; Ohio State Univ., Columbus, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33f9"},"title":"Intention-Based Integration of Software Variants","abstract":"Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional soft- ware merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants. In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions-domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the pro- posed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.","conference":"IEEE","terms":"Tools;Merging;Task analysis;Software product lines;Open source software;Maintenance engineering,integrated software;programming environments;software maintenance,fine-grained code edits;integration intentions-domain-specific actions;code snippets;configurable integrated platform;intention-based integration;software variants;configurable platform;variable code;configurable code;IDE tool INCLINE","keywords":"software product line;variant integration;clone and own;re engineering variants;code merging;intention based integration","startPage":"831","endPage":"842","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811913","citationCount":0,"referenceCount":46,"year":2019,"authors":"M. Lillack; S. Stanciulescu; W. Hedman; T. Berger; A. Wąsowski","affiliations":"Leipzig University, Germany; IT University of Copenhagen, Denmark and Chalmers and University of Gothenburg, Sweden; Chalmers and University of Gothenburg, Sweden; Chalmers and University of Gothenburg, Sweden; IT University of Copenhagen, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33fa"},"title":"Rotten Green Tests","abstract":"Unit tests are a tenant of agile programming methodologies, and are widely used to improve code quality and prevent code regression. A green (passing) test is usually taken as a robust sign that the code under test is valid. However, some green tests contain assertions that are never executed. We call such tests Rotten Green Tests. Rotten Green Tests represent a case worse than a broken test: they report that the code under test is valid, but in fact do not test that validity. We describe an approach to identify rotten green tests by combining simple static and dynamic call-site analyses. Our approach takes into account test helper methods, inherited helpers, and trait compositions, and has been implemented in a tool called DrTest. DrTest reports no false negatives, yet it still reports some false positives due to conditional use or multiple test contexts. Using DrTest we conducted an empirical evaluation of 19,905 real test cases in mature projects of the Pharo ecosystem. The results of the evaluation show that the tool is effective; it detected 294 tests as rotten-green tests that contain assertions that are not executed. Some rotten tests have been “sleeping” in Pharo for at least 5 years.","conference":"IEEE","terms":"Testing;Software;Java;Tools;Writing;Computer bugs;Software engineering,object-oriented programming;program testing;software prototyping,rotten tests;unit tests;rotten green tests;agile programming methodologies;code quality;code regression;dynamic call-site analysis;static call-site analysis;DrTest;Pharo ecosystem","keywords":"","startPage":"500","endPage":"511","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812040","citationCount":0,"referenceCount":44,"year":2019,"authors":"J. Delplanque; S. Ducasse; G. Polito; A. P. Black; A. Etien","affiliations":"Univ. Lille, CNRS, France; RMOD - Inria Lille, France; Univ. Lille, CNRS, France; Portland State University, Oregon, USA; Univ. Lille, CNRS, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33fb"},"title":"Reasonably-Most-General Clients for JavaScript Library Analysis","abstract":"A well-known approach to statically analyze libraries without having access to their client code is to model all possible clients abstractly using a most-general client. In dynamic languages, however, a most-general client would be too general: it may interact with the library in ways that are not intended by the library developer and are not realistic in actual clients, resulting in useless analysis results. In this work, we explore the concept of a reasonably-most-general client, in the context of a new static analysis tool REAGENT that aims to detect errors in TypeScript declaration files for JavaScript libraries. By incorporating different variations of reasonably-most-general clients into an existing static analyzer for JavaScript, we use REAGENT to study how different assumptions of client behavior affect the analysis results. We also show how REAGENT is able to find type errors in real-world TypeScript declaration files, and, once the errors have been corrected, to guarantee that no remaining errors exist relative to the selected assumptions.","conference":"IEEE","terms":"Libraries;Tools;Static analysis;Semantics;Contracts;Analytical models;Testing,Java;program diagnostics,client behavior;JavaScript library analysis;client code;library developer;static analysis tool;REAGENT;reasonably-most-general clients","keywords":"program analysis;TypeScript;most general client","startPage":"83","endPage":"93","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812107","citationCount":0,"referenceCount":29,"year":2019,"authors":"E. K. Kristensen; A. Møller","affiliations":"Aarhus University; Aarhus University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33fc"},"title":"SMOKE: Scalable Path-Sensitive Memory Leak Detection for Millions of Lines of Code","abstract":"Detecting memory leak at industrial scale is still not well addressed, in spite of the tremendous effort from both industry and academia in the past decades. Existing work suffers from an unresolved paradox - a highly precise analysis limits its scalability and an imprecise one seriously hurts its precision or recall. In this work, we present SMOKE, a staged approach to resolve this paradox. In the first stage, instead of using a uniform precise analysis for all paths, we use a scalable but imprecise analysis to compute a succinct set of candidate memory leak paths. In the second stage, we leverage a more precise analysis to verify the feasibility of those candidates. The first stage is scalable, due to the design of a new sparse program representation, the use-flow graph (UFG), that models the problem as a polynomial-time state analysis. The second stage analysis is both precise and efficient, due to the smaller number of candidates and the design of a dedicated constraint solver. Experimental results show that SMOKE can finish checking industrial-sized projects, up to 8MLoC, in forty minutes with an average false positive rate of 24.4%. Besides, SMOKE is significantly faster than the state-of-the-art research techniques as well as the industrial tools, with the speedup ranging from 5.2X to 22.8X. In the twenty-nine mature and extensively checked benchmark projects, SMOKE has discovered thirty previously unknown memory leaks which were con?rmed by developers, and one even assigned a CVE ID.","conference":"IEEE","terms":"Scalability;Leak detection;Benchmark testing;Computer bugs;Complexity theory;Correlation;Tools,data flow analysis;program debugging,SMOKE;sparse program representation;polynomial-time state analysis;path-sensitive memory leak detection;static bug finding;value-flow graph;use-flow graph","keywords":"memory leak, static bug finding, use-flow graph, value-flow graph","startPage":"72","endPage":"82","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812075","citationCount":0,"referenceCount":36,"year":2019,"authors":"G. Fan; R. Wu; Q. Shi; X. Xiao; J. Zhou; C. Zhang","affiliations":"Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Hong Kong University of Science and Technology, Hong Kong, China; Sourcebrella Inc., China; Sourcebrella Inc., China; Hong Kong University of Science and Technology, Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33fd"},"title":"UML Diagram Refinement (Focusing on Class-and Use Case Diagrams)","abstract":"Large and complicated UML models are not useful, because they are difficult to understand. This problem can be solved by using several diagrams of the same system at different levels of abstraction. Unfortunately, UML does not define an explicit set of rules for ensuring that diagrams at different levels of abstraction are consistent. We define such a set of rules, that we call diagram refinement. Diagram refinement is intuitive, and applicable to several kinds of UML diagrams (mostly to structural diagrams but also to use case diagrams), yet it rests on a solid mathematical basis-the theory of graph homomorphisms. We illustrate its usefulness with a series of examples.","conference":"IEEE","terms":"Unified modeling language;Concrete;Lattices;Semantics;Software engineering;Electronic mail;Mathematical model,graph theory;Unified Modeling Language,UML diagram refinement;graph homomorphisms","keywords":"Refinement;UML;Class diagram;Use case diagram;Graph homomorphism;Design patterns","startPage":"735","endPage":"745","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985709","citationCount":3,"referenceCount":25,"year":2017,"authors":"D. Faitelson; S. Tyszberowicz","affiliations":"Software Eng. Dept., Afeka Tel Aviv Acad. Coll. of Eng., Israel; Sch. of Comput. Sci., Acad. Coll. of Tel Aviv-Yaffo, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33fe"},"title":"Performance Diagnosis for Inefficient Loops","abstract":"Writing efficient software is difficult. Design and implementation defects can cause severe performance degradation. Unfortunately, existing performance diagnosis techniques like profilers are still preliminary. They can locate code regions that consume resources, but not the ones that waste resources. In this paper, we first design a root-cause and fix-strategy taxonomy for inefficient loops, one of the most common performance problems in the field. We then design a static-dynamic hybrid analysis tool, LDoctor, to provide accurate performance diagnosis for loops. We further use sampling techniques to lower the run-time overhead without degrading the accuracy or latency of LDoctor diagnosis. Evaluation using real-world performance problems shows that LDoctor can provide better coverage and accuracy than existing techniques, with low overhead.","conference":"IEEE","terms":"Tools;Computer bugs;Taxonomy;Software;Redundancy;Anodes;Debugging,program diagnostics;software engineering,software performance degradation;root-cause and fix-strategy taxonomy;LDoctor diagnosis;static-dynamic hybrid analysis tool","keywords":"performance diagnosis;debugging;loop inefficiency","startPage":"370","endPage":"380","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985677","citationCount":5,"referenceCount":37,"year":2017,"authors":"L. Song; S. Lu","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d33ff"},"title":"CTRAS: Crowdsourced Test Report Aggregation and Summarization","abstract":"Crowdsourced testing has been widely adopted to improve the quality of various software products. Crowdsourced workers typically perform testing tasks and report their experiences through test reports. While the crowdsourced test reports provide feedbacks from real usage scenarios, inspecting such a large number of reports becomes a time-consuming yet inevitable task. To improve the efficiency of this task, existing widely used issue-tracking systems, such as JIRA, Bugzilla, and Mantis, have provided keyword-search-based methods to assist users in identifying duplicate test reports. However, on mobile devices (such as mobile phones), where the crowdsourced test reports often contain insufficient text descriptions but instead rich screenshots, these text-analysis-based methods become less effective because the data has fundamentally changed. In this paper, instead of focusing on only detecting duplicates based on textual descriptions, we present CTRAS: a novel approach to leveraging duplicates to enrich the content of bug descriptions and improve the efficiency of inspecting these reports. CTRAS is capable of automatically aggregating duplicates based on both textual information and screenshots, and further summarizes the duplicate test reports into a comprehensive and comprehensible report. To validate CTRAS, we conducted quantitative studies using more than 5000 test reports, collected from 12 industrial crowdsourced projects. The experimental results reveal that CTRAS can reach an accuracy of 0.87, on average, regarding automatically detecting duplicate reports, and it outperforms the classic Max-Coverage-based and MMR summarization methods under Jensen Shannon divergence metric. Moreover, we conducted a task-based user study with 30 participants, whose result indicates that CTRAS can save nearly 30% time cost on average without loss of correctness.","conference":"IEEE","terms":"Testing;Software;Computer bugs;Task analysis;Feature extraction;Mobile handsets;Debugging,information retrieval;natural language processing;program debugging;text analysis,Jensen Shannon divergence metric;industrial crowdsourced projects;comprehensible report;comprehensive report;text-analysis-based methods;duplicate test reports;keyword-search-based methods;crowdsourced test reports;crowdsourced workers;crowdsourced test report aggregation;task-based user study;duplicate reports;CTRAS","keywords":"crowdsourced testing;summarization;duplicate bug reports","startPage":"900","endPage":"911","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811987","citationCount":1,"referenceCount":45,"year":2019,"authors":"R. Hao; Y. Feng; J. A. Jones; Y. Li; Z. Chen","affiliations":"State Key Laboratory for Novel Software Technology Nanjing University, China; University of California, Irvine, USA; University of California, Irvine, USA; State Key Laboratory for Novel Software Technology Nanjing University, China; State Key Laboratory for Novel Software Technology Nanjing University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3400"},"title":"Precise Condition Synthesis for Program Repair","abstract":"Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an \"if\" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.","conference":"IEEE","terms":"Maintenance engineering;Text analysis;Benchmark testing;Java;Input variables;Software engineering;Software,application program interfaces;document handling;fault location;program testing;software maintenance;sorting,precise condition synthesis;program repair system;automatic defect repair;weak test suites;sorting method;dependency relations;document analysis technique;API document;ACS;faulty locations;repair operation;Defects4J","keywords":"","startPage":"416","endPage":"426","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985681","citationCount":28,"referenceCount":48,"year":2017,"authors":"Y. Xiong; J. Wang; R. Yan; J. Zhang; S. Han; G. Huang; L. Zhang","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; SISE, Univ. of Electron. Sci. \u0026 Technol. of China, Chengdu, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Microsoft Res., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3401"},"title":"Probabilistic Disassembly","abstract":"Disassembling stripped binaries is a prominent challenge for binary analysis, due to the interleaving of code segments and data, and the difficulties of resolving control transfer targets of indirect calls and jumps. As a result, most existing disassemblers have both false positives (FP) and false negatives (FN). We observe that uncertainty is inevitable in disassembly due to the information loss during compilation and code generation. Therefore, we propose to model such uncertainty using probabilities and propose a novel disassembly technique, which computes a probability for each address in the code space, indicating its likelihood of being a true positive instruction. The probability is computed from a set of features that are reachable to an address, including control flow and data flow features. Our experiments with more than two thousands binaries show that our technique does not have any FN and has only 3.7% FP. In comparison, a state-of-the-art superset disassembly technique has 85% FP. A rewriter built on our disassembly can generate binaries that are only half of the size of those by superset disassembly and run 3% faster. While many widely-used disassemblers such as IDA and BAP suffer from missing function entries, our experiment also shows that even without any function entry information, our disassembler can still achieve 0 FN and 6.8% FP.","conference":"IEEE","terms":"Probabilistic logic;Uncertainty;Runtime;Registers;Computer science;Instruments;Aggregates,design for disassembly;probability;program assemblers;program compilers;program diagnostics,disassembling stripped binaries;binary analysis;code segments;control transfer targets;indirect calls;false negatives;code generation;code space;control flow;data flow features;disassembler;probabilistic disassembly;superset disassembly technique;compilation;true positive instruction;probability","keywords":"binary;disassembly;binary rewrite;probabilistic disassembly","startPage":"1187","endPage":"1198","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812038","citationCount":0,"referenceCount":57,"year":2019,"authors":"K. Miller; Y. Kwon; Y. Sun; Z. Zhang; X. Zhang; Z. Lin","affiliations":"Purdue University, USA; University of Virginia, USA; Purdue University, USA; Purdue University, USA; Purdue University, USA; Ohio State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3402"},"title":"How Reliable is the Crowdsourced Knowledge of Security Implementation?","abstract":"Stack Overflow (SO) is the most popular online Q\u0026A site for developers to share their expertise in solving programming issues. Given multiple answers to a certain question, developers may take the accepted answer, the answer from a person with high reputation, or the one frequently suggested. However, researchers recently observed that SO contains exploitable security vulnerabilities in the suggested code of popular answers, which found their way into security-sensitive high-profile applications that millions of users install every day. This observation inspires us to explore the following questions: How much can we trust the security implementation suggestions on SO? If suggested answers are vulnerable, can developers rely on the community's dynamics to infer the vulnerability and identify a secure counterpart? To answer these highly important questions, we conducted a comprehensive study on security-related SO posts by contrasting secure and insecure advice with the community-given content evaluation. Thereby, we investigated whether SO's gamification approach on incentivizing users is effective in improving security properties of distributed code examples. Moreover, we traced the distribution of duplicated samples over given answers to test whether the community behavior facilitates or prevents propagation of secure and insecure code suggestions within SO. We compiled 953 different groups of similar security-related code examples and labeled their security, identifying 785 secure answer posts and 644 insecure answer posts. Compared with secure suggestions, insecure ones had higher view counts (36,508 vs. 18,713), received a higher score (14 vs. 5), and had significantly more duplicates (3.8 vs. 3.0) on average. 34% of the posts provided by highly reputable so-called trusted users were insecure. Our findings show that based on the distribution of secure and insecure code on SO, users being laymen in security rely on additional advice and guidance. However, the community-given feedback does not allow differentiating secure from insecure choices. The reputation mechanism fails in indicating trustworthy users with respect to security questions, ultimately leaving other users wandering around alone in a software security minefield.","conference":"IEEE","terms":"Security;Cloning;Message systems;Reliability;Encoding;Crowdsourcing;Software,question answering (information retrieval);security of data;Web sites,programming issues;exploitable security vulnerabilities;suggested code;security-sensitive high-profile applications;security implementation suggestions;suggested answers;vulnerability;secure counterpart;secure advice;insecure advice;community-given content evaluation;security properties;distributed code examples;community behavior facilitates;secure code suggestions;insecure code suggestions;similar security-related code examples;community-given feedback;insecure choices;trustworthy users;security questions;software security minefield;crowdsourced knowledge;Stack Overflow;answer posts;online Q\u0026A site;gamification approach","keywords":"Stack Overflow, crowdsourced knowledge, social dynamics, security implementation","startPage":"536","endPage":"547","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812042","citationCount":2,"referenceCount":73,"year":2019,"authors":"M. Chen; F. Fischer; N. Meng; X. Wang; J. Grossklags","affiliations":"Virginia Tech, United States; Technical University of Munich, Germany; Virginia Tech, United States; University of Texas at San Antonio, United States; Technical University of Munich, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3403"},"title":"iSENSE: Completion-Aware Crowdtesting Management","abstract":"Crowdtesting has become an effective alternative to traditional testing, especially for mobile applications. However, crowdtesting is hard to manage in nature. Given the complexity of mobile applications and unpredictability of distributed crowdtesting processes, it is difficult to estimate (a) remaining number of bugs yet to be detected or (b) required cost to find those bugs. Experience-based decisions may result in ineffective crowdtesting processes, e.g., there is an average of 32% wasteful spending in current crowdtesting practices. This paper aims at exploring automated decision support to effectively manage crowdtesting processes. It proposes an approach named ISENSE which applies incremental sampling technique to process crowdtesting reports arriving in chronological order, organizes them into fixed-size groups as dynamic inputs, and predicts two test completion indicators in an incremental manner. The two indicators are: 1) total number of bugs predicted with Capture-ReCapture model, and 2) required test cost for achieving certain test objectives predicted with AutoRegressive Integrated Moving Average model. The evaluation of ISENSE is conducted on 46,434 reports of 218 crowdtesting tasks from one of the largest crowdtesting platforms in China. Its effectiveness is demonstrated through two application studies for automating crowdtesting management and semi-automation of task closing trade-off analysis. The results show that ISENSE can provide managers with greater awareness of testing progress to achieve cost-effectiveness gains of crowdtesting. Specifically, a median of 100% bugs can be detected with 30% saved cost based on the automated close prediction.","conference":"IEEE","terms":"Computer bugs;Task analysis;Testing;Predictive models;Software;Mobile applications;Data models,autoregressive moving average processes;crowdsourcing;decision support systems;program debugging;program testing,distributed crowdtesting processes;bugs;experience-based decisions;automated decision support;incremental sampling technique;crowdtesting reports;test completion indicators;automating crowdtesting management;cost-effectiveness gains;automated close prediction;iSENSE;completion-aware crowdtesting management;mobile applications;autoregressive integrated moving average model;crowdtesting platforms;crowdtesting tasks","keywords":"Crowdtesting;automated close prediction;test completion;crowdtesting management","startPage":"912","endPage":"923","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812109","citationCount":0,"referenceCount":68,"year":2019,"authors":"J. Wang; Y. Yang; R. Krishna; T. Menzies; Q. Wang","affiliations":"Institute of Software Chinese Academy of Sciences; Stevens Institute of Technology, USA; North Carolina State University, USA; North Carolina State University, USA; Institute of Software Chinese Academy of Sciences, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3404"},"title":"Mining Historical Test Logs to Predict Bugs and Localize Faults in the Test Logs","abstract":"Software testing is an integral part of modern software development. However, test runs can produce thousands of lines of logged output that make it difficult to find the cause of a fault in the logs. This problem is exacerbated by environmental failures that distract from product faults. In this paper we present techniques with the goal of capturing the maximum number of product faults, while flagging the minimum number of log lines for inspection. We observe that the location of a fault in a log should be contained in the lines of a failing test log. In contrast, a passing test log should not contain the lines related to a failure. Lines that occur in both a passing and failing log introduce noise when attempting to find the fault in a failing log. We introduce an approach where we remove the lines that occur in the passing log from the failing log. After removing these lines, we use information retrieval techniques to flag the most probable lines for investigation. We modify TF-IDF to identify the most relevant log lines related to past product failures. We then vectorize the logs and develop an exclusive version of KNN to identify which logs are likely to lead to product faults and which lines are the most probable indication of the failure. Our best approach, LogFaultFlagger finds 89% of the total faults and flags less than 1% of the total failed log lines for inspection. LogFaultFlagger drastically outperforms the previous work CAM. We implemented LogFaultFlagger as a tool at Ericsson where it presents fault prediction summaries to base station testers.","conference":"IEEE","terms":"Testing;Inspection;Computer bugs;Fault diagnosis;Base stations;Software;Software engineering,data mining;probability;program debugging;program testing;software fault tolerance,mining historical test logs;software testing;logged output;product faults;passing test log;failing log;passing log;relevant log lines;fault prediction summaries;LogFaultFlagger approach;TF-IDF","keywords":"Testing;Logs;Faults;Industry","startPage":"140","endPage":"151","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812113","citationCount":1,"referenceCount":60,"year":2019,"authors":"A. Amar; P. C. Rigby","affiliations":"Concordia University, Canada; Concordia University, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3405"},"title":"Feedback-Based Debugging","abstract":"Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible. In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8% of the bugs and 65% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8% less time to locate the bugs.","conference":"IEEE","terms":"Computer bugs;Debugging;Tools;Reactive power;Inspection;Software debugging;Software engineering,formal specification;program debugging,feedback-based debugging;human feedbacks;buggy program;partial program specification;buggy execution;execution trace;light-weight feedback;Microbat;Whyline","keywords":"debugging;feedback;slicing;path pattern;approximation","startPage":"393","endPage":"403","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985679","citationCount":4,"referenceCount":38,"year":2017,"authors":"Y. Lin; J. Sun; Y. Xue; Y. Liu; J. Dong","affiliations":"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Singapore Univ. of Technol. \u0026 Design, Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3406"},"title":"Exploring API Embedding for API Usages and Applications","abstract":"Word2Vec is a class of neural network models that as being trainedfrom a large corpus of texts, they can produce for each unique word acorresponding vector in a continuous space in which linguisticcontexts of words can be observed. In this work, we study thecharacteristics of Word2Vec vectors, called API2VEC or API embeddings, for the API elements within the API sequences in source code. Ourempirical study shows that the close proximity of the API2VEC vectorsfor API elements reflects the similar usage contexts containing thesurrounding APIs of those API elements. Moreover, API2VEC can captureseveral similar semantic relations between API elements in API usagesvia vector offsets. We demonstrate the usefulness of API2VEC vectorsfor API elements in three applications. First, we build a tool thatmines the pairs of API elements that share the same usage relationsamong them. The other applications are in the code migrationdomain. We develop API2API, a tool to automatically learn the APImappings between Java and C# using a characteristic of the API2VECvectors for API elements in the two languages: semantic relationsamong API elements in their usages are observed in the two vectorspaces for the two languages as similar geometric arrangements amongtheir API2VEC vectors. Our empirical evaluation shows that API2APIrelatively improves 22.6% and 40.1% top-1 and top-5 accuracy over astate-of-the-art mining approach for API mappings. Finally, as anotherapplication in code migration, we are able to migrate equivalent APIusages from Java to C# with up to 90.6% recall and 87.2% precision.","conference":"IEEE","terms":"Java;C# languages;Tools;Semantics;Neural networks;Syntactics,application program interfaces;data mining;geometry;Java;neural nets;source code (software);text analysis;vectors,API embedding;API usages;API applications;Word2Vec vectors;neural network models;text corpus;linguistic words contexts;API2VEC;API elements;API sequences;source code;semantic relations;vector offsets;code migration domain;API mappings;Java;C#;geometric arrangements;code migration","keywords":"Word2Vec;API embedding;API usages;migration","startPage":"438","endPage":"449","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985683","citationCount":9,"referenceCount":45,"year":2017,"authors":"T. D. Nguyen; A. T. Nguyen; H. D. Phan; T. N. Nguyen","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3407"},"title":"Automated Reporting of Anti-Patterns and Decay in Continuous Integration","abstract":"Continuous Integration (CI) is a widely-used software engineering practice. The software is continuously built so that changes can be easily integrated and issues such as unmet quality goals or style inconsistencies get detected early. Unfortunately, it is not only hard to introduce CI into an existing project, but it is also challenging to live up to the CI principles when facing tough deadlines or business decisions. Previous work has identified common anti-patterns that reduce the promised benefits of CI. Typically, these anti-patterns slowly creep into a project over time before they are identified. We argue that automated detection can help with early identification and prevent such a process decay. In this work, we further analyze this assumption and survey 124 developers about CI anti-patterns. From the results, we build CI-Odor, a reporting tool for CI processes that detects the existence of four relevant anti-patterns by analyzing regular build logs and repository information. In a study on the 18,474 build logs of 36 popular JAVA projects, we reveal the presence of 3,823 high-severity warnings spread across projects. We validate our reports in a survey among 13 original developers of these projects and through general feedback from 42 developers that confirm the relevance of our reports.","conference":"IEEE","terms":"Tools;Pipelines;Detectors;Software;Best practices;Merging;Informatics,Java;program testing;software engineering,continuous integration;software engineering practice;CI principles;common anti-patterns;process decay;CI anti-patterns;Java projects;CI-Odor tool","keywords":"Continuous Integration, Anti-Pattern, Detection, CI-Smell, CI-Decay","startPage":"105","endPage":"115","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811921","citationCount":2,"referenceCount":35,"year":2019,"authors":"C. Vassallo; S. Proksch; H. C. Gall; M. Di Penta","affiliations":"University of Zurich; University of Zurich; University of Zurich; University of Sannio","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3408"},"title":"Going Farther Together: The Impact of Social Capital on Sustained Participation in Open Source","abstract":"Sustained participation by contributors in opensource software is critical to the survival of open-source projects and can provide career advancement benefits to individual contributors. However, not all contributors reap the benefits of open-source participation fully, with prior work showing that women are particularly underrepresented and at higher risk of disengagement. While many barriers to participation in open-source have been documented in the literature, relatively little is known about how the social networks that open-source contributors form impact their chances of long-term engagement. In this paper we report on a mixed-methods empirical study of the role of social capital (i.e., the resources people can gain from their social connections) for sustained participation by women and men in open-source GitHub projects. After combining survival analysis on a large, longitudinal data set with insights derived from a user survey, we confirm that while social capital is beneficial for prolonged engagement for both genders, women are at disadvantage in teams lacking diversity in expertise.","conference":"IEEE","terms":"Social networking (online);Open source software;Cultural differences;Bonding;Task analysis;Collaboration;Merging,gender issues;project management;public domain software;social aspects of automation;software engineering,open-source software;open-source GitHub projects;sustained participation;social capital;social networks;open-source participation","keywords":"social capital;open source software;gender","startPage":"688","endPage":"699","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812044","citationCount":5,"referenceCount":100,"year":2019,"authors":"H. S. Qiu; A. Nolte; A. Brown; A. Serebrenik; B. Vasilescu","affiliations":"Carnegie Mellon University, USA; University of Tartu, Estonia; Bryn Mawr College, USA; Eindhoven University of Technology, Netherlands; Carnegie Mellon University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3409"},"title":"Can Latent Topics in Source Code Predict Missing Architectural Tactics?","abstract":"Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the system's quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases.","conference":"IEEE","terms":"Recommender systems;Inference algorithms;Training;Machine learning algorithms;Security;Software reliability,public domain software;recommender systems;software architecture;software quality;software reliability;source code (software),source code;missing architectural tactics prediction;software system;system quality analysis;bottom-up approach;latent topics;recommender system;predictor models;open source systems;package-level tactic recommendation generation;Apache Hive;Hadoop","keywords":"Architectural design and implementation;tactic recommender;emergent design","startPage":"15","endPage":"26","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985646","citationCount":1,"referenceCount":58,"year":2017,"authors":"R. Gopalakrishnan; P. Sharma; M. Mirakhorli; M. Galster","affiliations":"Rochester Inst. of Technol., Rochester, NY, USA; Rochester Inst. of Technol., Rochester, NY, USA; Rochester Inst. of Technol., Rochester, NY, USA; Univ. of Canterbury, Christchurch, New Zealand","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d340a"},"title":"GreenBundle: An Empirical Study on the Energy Impact of Bundled Processing","abstract":"Energy consumption is a concern in the data-center and at the edge, on mobile devices such as smartphones. Software that consumes too much energy threatens the utility of the end-user's mobile device. Energy consumption is fundamentally a systemic kind of performance and hence it should be addressed at design time via a software architecture that supports it, rather than after release, via some form of refactoring. Unfortunately developers often lack knowledge of what kinds of designs and architectures can help address software energy consumption. In this paper we show that some simple design choices can have significant effects on energy consumption. In particular we examine the Model-View-Controller architectural pattern and demonstrate how converting to Model-View-Presenter with bundling can improve the energy performance of both benchmark systems and real world applications. We show the relationship between energy consumption and bundled and delayed view updates: bundling events in the presenter can often reduce energy consumption by 30%.","conference":"IEEE","terms":"Energy consumption;Benchmark testing;Computer architecture;Unified modeling language;Observers;Software;Mobile handsets,energy consumption;mobile computing;power aware computing;software architecture;software maintenance,energy impact;software energy consumption;energy performance;model-view-presenter;model-view-controller architectural pattern","keywords":"software energy consumption, MVP, MVC, software architecture","startPage":"1107","endPage":"1118","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811956","citationCount":0,"referenceCount":80,"year":2019,"authors":"S. A. Chowdhury; A. Hindle; R. Kazman; T. Shuto; K. Matsui; Y. Kamei","affiliations":"University of Alberta, Canada; University of Alberta, Canada; University of Hawaii, USA; Kyushu University, Japan; Kyushu University, Japan; Kyushu University, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d340b"},"title":"Clone Refactoring with Lambda Expressions","abstract":"Lambda expressions have been introduced in Java 8 to support functional programming and enable behavior parameterization by passing functions as parameters to methods. The majority of software clones (duplicated code) are known to have behavioral differences (i.e., Type-2 and Type-3 clones). However, to the best of our knowledge, there is no previous work to investigate the utility of Lambda expressions for parameterizing such behavioral differences in clones. In this paper, we propose a technique that examines the applicability of Lambda expressions for the refactoring of clones with behavioral differences. Moreover, we empirically investigate the applicability and characteristics of the Lambda expressions introduced to refactor a large dataset of clones. Our findings show that Lambda expressions enable the refactoring of a significant portion of clones that could not be refactored by any other means.","conference":"IEEE","terms":"Cloning;Data mining;Java;Europe;Testing;Open source software,functional programming;Java;lambda calculus;software maintenance,clone refactoring;lambda expressions;Java 8;functional programming;behavior parameterization;software clones;duplicated code;behavioral differences;Type-2 clones;Type-3 clones","keywords":"Refactoring;Code duplication;Lambda expressions","startPage":"60","endPage":"70","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985650","citationCount":9,"referenceCount":41,"year":2017,"authors":"N. Tsantalis; D. Mazinanian; S. Rostami","affiliations":"Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d340c"},"title":"Detection and Repair of Architectural Inconsistencies in Java","abstract":"Java is one of the most widely used programming languages. However, the absence of explicit support for architectural constructs, such as software components, in the programming language itself has prevented software developers from achieving the many benefits that come with architecture-based development. To address this issue, Java 9 has introduced the Java Platform Module System (JPMS), resulting in the first instance of encapsulation of modules with rich software architectural interfaces added to a mainstream programming language. The primary goal of JPMS is to construct and maintain large applications efficiently-as well as improve the encapsulation, security, and maintainability of Java applications in general and the JDK itself. A challenge, however, is that module declarations do not necessarily reflect actual usage of modules in an application, allowing developers to mistakenly specify inconsistent dependencies among the modules. In this paper, we formally define 8 inconsistent modular dependencies that may arise in Java-9 applications. We also present DARCY, an approach that leverages these definitions and static program analyses to automatically (1) detect the specified inconsistent dependencies within Java applications and (2) repair those identified inconsistencies. The results of our experiments, conducted over 38 open-source Java-9 applications, indicate that architectural inconsistencies are widespread and demonstrate the benefits of DARCY in automated detection and repair of these inconsistencies.","conference":"IEEE","terms":"Java;Computer architecture;Software;Bars;Encapsulation;Security;Maintenance engineering,Java;object-oriented programming;program diagnostics;software architecture;software maintenance;software prototyping;systems analysis,encapsulation;mainstream programming language;JPMS;Java applications;module declarations;static program analyses;identified inconsistencies;architectural inconsistencies;automated detection;repair;architectural constructs;software components;software developers;architecture-based development;Java Platform Module System;programming languages;open-source Java-9 applications;inconsistent modular dependencies;software architectural interfaces;DARCY","keywords":"Java Platform Module System;Architectural Inconsistencies;Static Program Analysis;Software Architecture;Module;Detection;Repair;Security;Maintainability;Encapsulation;Software Bloat;Java","startPage":"560","endPage":"571","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812079","citationCount":1,"referenceCount":74,"year":2019,"authors":"N. Ghorbani; J. Garcia; S. Malek","affiliations":"University of California, Irvine; University of California, Irvine; University of California, Irvine","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d340d"},"title":"Pattern-Based Mining of Opinions in Q\u0026A Websites","abstract":"Informal documentation contained in resources such as Q\u0026A websites (e.g., Stack Overflow) is a precious resource for developers, who can find there examples on how to use certain APIs, as well as opinions about pros and cons of such APIs. Automatically identifying and classifying such opinions can alleviate developers' burden in performing manual searches, and can be used to recommend APIs that are good from some points of view (e.g., performance), or highlight those less ideal from other perspectives (e.g., compatibility). We propose POME (Pattern-based Opinion MinEr), an approach that leverages natural language parsing and pattern-matching to classify Stack Overflow sentences referring to APIs according to seven aspects (e.g., performance, usability), and to determine their polarity (positive vs negative). The patterns have been inferred by manually analyzing 4,346 sentences from Stack Overflow linked to a total of 30 APIs. We evaluated POME by (i) comparing the pattern-matching approach with machine learners leveraging the patterns themselves as well as n-grams extracted from Stack Overflow posts; (ii) assessing the ability of POME to detect the polarity of sentences, as compared to sentiment-analysis tools; (iii) comparing POME with the state-of-the-art Stack Overflow opinion mining approach, Opiner, through a study involving 24 human evaluators. Our study shows that POME exhibits a higher precision than a state-of-the-art technique (Opiner), in terms of both opinion aspect identification and polarity assessment.","conference":"IEEE","terms":"Sentiment analysis;Tools;Documentation;Data mining;Software engineering;Software;Databases,application program interfaces;data mining;natural language processing;pattern classification;Web sites,POME;opinion aspect identification;polarity assessment;informal documentation;Q\u0026A websites;natural language parsing;Stack Overflow sentences;pattern-based opinion miner;stack overflow opinion mining approach;pattern-based mining;stack overflow posts;APIs;pattern-matching approach","keywords":"opinion mining;Stack Overflow;sentiment analysis;aspect detection","startPage":"548","endPage":"559","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811960","citationCount":0,"referenceCount":47,"year":2019,"authors":"B. Lin; F. Zampetti; G. Bavota; M. Di Penta; M. Lanza","affiliations":"Università della Svizzera italiana (USI); University of Sannio; Università della Svizzera italiana (USI); University of Sannio; Università della Svizzera italiana (USI)","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d340e"},"title":"Graph Embedding Based Familial Analysis of Android Malware using Unsupervised Learning","abstract":"The rapid growth of Android malware has posed severe security threats to smartphone users. On the basis of the familial trait of Android malware observed by previous work, the familial analysis is a promising way to help analysts better focus on the commonalities of malware samples within the same families, thus reducing the analytical workload and accelerating malware analysis. The majority of existing approaches rely on supervised learning and face three main challenges, i.e., low accuracy, low efficiency, and the lack of labeled dataset. To address these challenges, we first construct a fine-grained behavior model by abstracting the program semantics into a set of subgraphs. Then, we propose SRA, a novel feature that depicts the similarity relationships between the Structural Roles of sensitive API call nodes in subgraphs. An SRA is obtained based on graph embedding techniques and represented as a vector, thus we can effectively reduce the high complexity of graph matching. After that, instead of training a classifier with labeled samples, we construct malware link network based on SRAs and apply community detection algorithms on it to group the unlabeled samples into groups. We implement these ideas in a system called GefDroid that performs Graph embedding based familial analysis of AnDroid malware using unsupervised learning. Moreover, we conduct extensive experiments to evaluate GefDroid on three datasets with ground truth. The results show that GefDroid can achieve high agreements (0.707-0.883 in term of NMI) between the clustering results and the ground truth. Furthermore, GefDroid requires only linear run-time overhead and takes around 8.6s to analyze a sample on average, which is considerably faster than the previous work.","conference":"IEEE","terms":"Malware;Unsupervised learning;Semantics;Detection algorithms;Security;Feature extraction;Face,application program interfaces;graph theory;invasive software;learning (artificial intelligence);pattern classification;pattern clustering;smart phones;unsupervised learning,familial analysis;unsupervised learning;analytical workload;supervised learning;graph embedding techniques;malware link network;Android malware;SRA;similarity relationships;GefDroid","keywords":"Android malware;graph embedding;familial analysis;unsupervised learning","startPage":"771","endPage":"782","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812083","citationCount":0,"referenceCount":61,"year":2019,"authors":"M. Fan; X. Luo; J. Liu; M. Wang; C. Nong; Q. Zheng; T. Liu","affiliations":"Xi'an Jiaotong University, China; The Hong Kong Polytechnic University, China; Xi'an Jiaotong University; Southeast University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d340f"},"title":"SPAIN: Security Patch Analysis for Binaries towards Understanding the Pain and Pills","abstract":"Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities.","conference":"IEEE","terms":"Security;Semantics;Software;Tools;Scalability;Registers;Portable document format,security of data;software engineering,SPAIN;security patch analysis;software vulnerability;software security;scalable binary-level patch analysis framework;binary programs","keywords":"","startPage":"462","endPage":"472","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985685","citationCount":9,"referenceCount":44,"year":2017,"authors":"Z. Xu; B. Chen; M. Chandramohan; Y. Liu; F. Song","affiliations":"Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Inf. Sci. \u0026 Technol., Shanghai Tech Univ., China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3410"},"title":"Superion: Grammar-Aware Greybox Fuzzing","abstract":"In recent years, coverage-based greybox fuzzing has proven itself to be one of the most effective techniques for finding security bugs in practice. Particularly, American Fuzzy Lop (AFL for short) is deemed to be a great success in fuzzing relatively simple test inputs. Unfortunately, when it meets structured test inputs such as XML and JavaScript, those grammar-blind trimming and mutation strategies in AFL hinder the effectiveness and efficiency. To this end, we propose a grammar-aware coverage-based greybox fuzzing approach to fuzz programs that process structured inputs. Given the grammar (which is often publicly available) of test inputs, we introduce a grammar-aware trimming strategy to trim test inputs at the tree level using the abstract syntax trees (ASTs) of parsed test inputs. Further, we introduce two grammar-aware mutation strategies (i.e., enhanced dictionary-based mutation and tree-based mutation). Specifically, tree-based mutation works via replacing subtrees using the ASTs of parsed test inputs. Equipped with grammar-awareness, our approach can carry the fuzzing exploration into width and depth. We implemented our approach as an extension to AFL, named Superion; and evaluated the effectiveness of Superion using large- scale programs (i.e., an XML engine libplist and three JavaScript engines WebKit, Jerryscript and ChakraCore). Our results have demonstrated that Superion can improve the code coverage (i.e., 16.7% and 8.8% in line and function coverage) and bug-finding capability (i.e., 34 new bugs, among which we discovered 22 new vulnerabilities with 19 CVEs assigned and 3.2K USD bug bounty rewards received) over AFL and jsfunfuzz.","conference":"IEEE","terms":"Fuzzing;Grammar;Computer bugs;XML;Syntactics;Engines;Instruments,grammars;grey systems;program compilers;program debugging;program testing;security of data;trees (mathematics),abstract syntax trees;tree-based mutation;AFL;Superion;grammar-blind trimming mutation strategies;grammar-aware coverage-based greybox fuzzing approach;American fuzzy lop;security bugs;XML;JavaScript;ASTs;parsed test","keywords":"Greybox Fuzzing, Structured Inputs, ASTs","startPage":"724","endPage":"735","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811923","citationCount":1,"referenceCount":73,"year":2019,"authors":"J. Wang; B. Chen; L. Wei; Y. Liu","affiliations":"Nanyang Technological University, Singapore; Fudan University, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3411"},"title":"Do Developers Discover New Tools On The Toilet?","abstract":"Maintaining awareness of useful tools is a substantial challenge for developers. Physical newsletters are a simple technique to inform developers about tools. In this paper, we evaluate such a technique, called Testing on the Toilet, by performing a mixed-methods case study. We first quantitatively evaluate how effective this technique is by applying statistical causal inference over six years of data about tools used by thousands of developers. We then qualitatively contextualize these results by interviewing and surveying 382 developers, from authors to editors to readers. We found that the technique was generally effective at increasing software development tool use, although the increase varied depending on factors such as the breadth of applicability of the tool, the extent to which the tool has reached saturation, and the memorability of the tool name.","conference":"IEEE","terms":"Tools;Software;Google;Testing;Software engineering;Advertising;Productivity,program testing;software development management;software tools;statistical analysis,physical newsletters;statistical causal inference;tool name;software development tool;testing on the toilet","keywords":"software engineering;diffusion of innovations","startPage":"465","endPage":"475","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812046","citationCount":1,"referenceCount":43,"year":2019,"authors":"E. Murphy-Hill; E. K. Smith; C. Sadowski; C. Jaspan; C. Winter; M. Jorde; A. Knight; A. Trenk; S. Gross","affiliations":"Google, LLC; Bloomberg; Google, LLC; Google, LLC; Waymo; Google, LLC; Google, LLC; Google, LLC; Google, LLC","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3412"},"title":"How Practitioners Perceive Coding Proficiency","abstract":"Coding proficiency is essential to software practitioners. Unfortunately, our understanding on coding proficiency often translates to vague stereotypes, e.g., \"able to write good code\". The lack of specificity hinders employers from measuring a software engineer's coding proficiency, and software engineers from improving their coding proficiency skills. This raises an important question: what skills matter to improve one's coding proficiency. To answer this question, we perform an empirical study by surveying 340 software practitioners from 33 countries across 5 continents. We first identify 38 coding proficiency skills grouped into nine categories by interviewing 15 developers from three companies. We then ask our survey respondents to rate the level of importance for these skills, and provide rationales of their ratings. Our study highlights a total of 21 important skills that receive an average rating of 4.0 and above (important and very important), along with rationales given by proponents and dissenters. We discuss implications of our findings to researchers, educators, and practitioners.","conference":"IEEE","terms":"Encoding;Software;Interviews;Companies;Computer bugs;Computer languages;Programming,project management;software development management,software practitioners;coding proficiency skills;coding proficiency;software engineers","keywords":"practitioners;coding proficiency","startPage":"924","endPage":"935","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812050","citationCount":0,"referenceCount":49,"year":2019,"authors":"X. Xia; Z. Wan; P. S. Kochhar; D. Lo","affiliations":"Faculty of Information Technology, Monash University; College of Computer Science and Technology, Zhejiang University; Microsoft, Canada; School of Information Systems, Singapore Management University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3413"},"title":"An Unsupervised Approach for Discovering Relevant Tutorial Fragments for APIs","abstract":"Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely Fragment Recommender for APIs with PageRank and Topic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.","conference":"IEEE","terms":"Tutorials;Correlation;Ontologies;Programming;Manuals;Software;Filtering algorithms,application program interfaces;computer aided instruction;computer science education;grammars;recommender systems;unsupervised learning,API tutorials;unsupervised approach;Fragment Recommender for APIs with PageRank and Topic model;FRAPT;fragment parser;fragment filter;nonexplanatory fragment identification;variable resolution challenge;pronoun challenge;F-Measure;application programming interfaces","keywords":"Application Programming Interface;PageRank Algorithm;Topic Model;Unsupervised Approaches","startPage":"38","endPage":"48","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985648","citationCount":8,"referenceCount":41,"year":2017,"authors":"H. Jiang; J. Zhang; Z. Ren; T. Zhang","affiliations":"Sch. of Software, Dalian Univ. of Technol., Dalian, China; Sch. of Software, Dalian Univ. of Technol., Dalian, China; Sch. of Software, Dalian Univ. of Technol., Dalian, China; Coll. of Comput. Sci. \u0026 Technol., Harbin Eng. Univ., Harbin, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3414"},"title":"Automated Refactoring of Legacy Java Software to Default Methods","abstract":"Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.","conference":"IEEE","terms":"Java;Software;Semantics;Concrete;Face;Printing;Syntactics,Java;software maintenance,automated refactoring;legacy Java software;software design pattern;fully-automated type constraint-based refactoring approach;Eclipse plug-in;GitHub repositories","keywords":"refactoring;java;interfaces;default methods","startPage":"82","endPage":"93","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985652","citationCount":5,"referenceCount":42,"year":2017,"authors":"R. Khatchadourian; H. Masuhara","affiliations":"City Univ. of New York, New York, NY, USA; Tokyo Inst. of Technol., Tokyo, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3415"},"title":"Easy Modelling and Verification of Unpredictable and Preemptive Interrupt-Driven Systems","abstract":"The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difficult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts' essential features visually and concisely. We also propose an automata-based semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both real-world and existing literature were conducted, and the results demonstrate our approach's usability and effectiveness.","conference":"IEEE","terms":"Unified modeling language;Satellites;Task analysis;Semantics;Automata;Usability;Testing,automata theory;embedded systems;formal verification;interrupts;Unified Modeling Language,interrupt-driven systems;embedded systems;interrupt arrival;interrupt handling;model checking;behavioural state space;modelling language;interrupt sequence diagram;ISD;UML sequence diagram notations;hybrid automata","keywords":"Interrupt-driven systems, Sequence diagrams, System modelling, Model checking","startPage":"212","endPage":"222","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812085","citationCount":0,"referenceCount":40,"year":2019,"authors":"M. Pan; S. Chen; Y. Pei; T. Zhang; X. Li","affiliations":"Nanjing University, China; Nanjing University, China; The Hong Kong Polytechnic University, China; Nanjing University, China; Nanjing University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3416"},"title":"ProEva: Runtime Proactive Performance Evaluation Based on Continuous-Time Markov Chains","abstract":"Software systems, especially service-based software systems, need to guarantee runtime performance. If their performance is degraded, some reconfiguration countermeasures should be taken. However, there is usually some latency before the countermeasures take effect. It is thus important not only to monitor the current system status passively but also to predict its future performance proactively. Continuous-time Markov chains (CTMCs) are suitable models to analyze time-bounded performance metrics (e.g., how likely a performance degradation may occur within some future period). One challenge to harness CTMCs is the measurement of model parameters (i.e., transition rates) in CTMCs at runtime. As these parameters may be updated by the system or environment frequently, it is difficult for the model builder to provide precise parameter values. In this paper, we present a framework called ProEva, which extends the conventional technique of time-bounded CTMC model checking by admitting imprecise, interval-valued estimates for transition rates. The core method of ProEva computes asymptotic expressions and bounds for the imprecise model checking output. We also present an evaluation of accuracy and computational overhead for ProEva.","conference":"IEEE","terms":"Software engineering,formal verification;Markov processes;software performance evaluation,ProEva;runtime proactive performance evaluation;continuous-time Markov chains;service-based software systems;time-bounded CTMC model checking","keywords":"continuous-time Markov chain;imprecise parameters;performance;Quality-of-Service","startPage":"484","endPage":"495","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985687","citationCount":2,"referenceCount":37,"year":2017,"authors":"G. Su; T. Chen; Y. Feng; D. S. Rosenblum","affiliations":"Sch. of Comput. \u0026 Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia; Dept. of Comput. Sci., Middlesex Univ., London, UK; Fac. of Eng. \u0026 Inf. Technol, Univ. of Technol. Sydney, Sydney, NSW, Australia; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3417"},"title":"How Good Is a Security Policy against Real Breaches? A HIPAA Case Study","abstract":"Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for what. A breach corresponds to a norm violation. We develop a semantic similarity metric for pairwise comparison between the norm that represents a policy clause and the norm that has been violated by a reported breach. We use the US Health Insurance Portability and Accountability Act (HIPAA) as a case study. Our investigation of a subset of the breaches reported by the US Department of Health and Human Services (HHS) reveals the gaps between HIPAA and reported breaches, leading to a coverage of 65%. Additionally, our classification of the 1,577 HHS breaches shows that 44% of the breaches are accidental misuses and 56% are malicious misuses. We find that HIPAA's gaps regarding accidental misuses are significantly larger than its gaps regarding malicious misuses.","conference":"IEEE","terms":"Ontologies;Security;Semantics;Medical services;Cognition;Measurement;Taxonomy,inference mechanisms;security of data;software engineering,HIPAA;security policies;security breaches;semantic reasoning;SEMAVER;norm violation;semantic similarity metric;US Health Insurance Portability and Accountability Act;US Department of Health and Human Services;HHS;accidental misuses;malicious misuses;software development","keywords":"Security and privacy breaches;social norms;breach ontology;semantic similarity","startPage":"530","endPage":"540","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985691","citationCount":1,"referenceCount":41,"year":2017,"authors":"Ö. Kafali; J. Jones; M. Petruso; L. Williams; M. P. Singh","affiliations":"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Coll. of Arts \u0026 Sci., Elon Univ., Elon, NC, USA; Dept. of Comput. Sci., Appalachian State Univ., Boone, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3418"},"title":"Safe Automated Refactoring for Intelligent Parallelization of Java 8 Streams","abstract":"Streaming APIs are becoming more pervasive in mainstream Object-Oriented programming languages. For example, the Stream API introduced in Java 8 allows for functional-like, MapReduce-style operations in processing both finite and infinite data structures. However, using this API efficiently involves subtle considerations like determining when it is best for stream operations to run in parallel, when running operations in parallel can be less efficient, and when it is safe to run in parallel due to possible lambda expression side-effects. In this paper, we present an automated refactoring approach that assists developers in writing efficient stream code in a semantics-preserving fashion. The approach, based on a novel data ordering and typestate analysis, consists of preconditions for automatically determining when it is safe and possibly advantageous to convert sequential streams to parallel and unorder or de-parallelize already parallel streams. The approach was implemented as a plug-in to the Eclipse IDE, uses the WALA and SAFE analysis frameworks, and was evaluated on 11 Java projects consisting of ?642K lines of code. We found that 57 of 157 candidate streams (36.31%) were refactorable, and an average speedup of 3.49 on performance tests was observed. The results indicate that the approach is useful in optimizing stream code to their full potential.","conference":"IEEE","terms":"Java;Pipelines;Writing;Data structures;Semantics;Image color analysis;Instruction sets,application program interfaces;data structures;Java;software maintenance,semantics-preserving fashion;typestate analysis;sequential streams;parallel streams;optimizing stream code;safe automated refactoring;intelligent parallelization;Java 8;Stream API;MapReduce-style operations;finite data structures;infinite data structures;stream operations;running operations;automated refactoring approach;lambda expression side-effects;stream code;mainstream object-oriented programming languages","keywords":"refactoring;static analysis;automatic parallelization;typestate analysis;Java 8;streams","startPage":"619","endPage":"630","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811925","citationCount":0,"referenceCount":76,"year":2019,"authors":"R. Khatchadourian; Y. Tang; M. Bagherzadeh; S. Ahmed","affiliations":"City University of New York (CUNY) Hunter College; City University of New York (CUNY) Graduate Center; Oakland University; Oakland University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3419"},"title":"Scalable Approaches for Test Suite Reduction","abstract":"Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or specification models, that are not commonly available at large scale. We present a family of novel very efficient approaches for similarity-based test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for finding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input). We evaluate four approaches in a version that selects a fixed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some fixed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency. When applied to a suite of more than 500K real world test cases, the most efficient of the four approaches could select B test cases (for varying B values) in less than 10 seconds.","conference":"IEEE","terms":"Testing;Big Data;Software;Measurement;Fault detection;Scalability;Clustering algorithms,Big Data;program testing;set theory,similarity-based test suite reduction;evenly spread subset;test source code;B test cases;scalable approaches;test suite reduction approaches;large-size test suites;modern massive systems;code coverage metrics;big data domain;smart heuristics;command line input","keywords":"Clustering;Random projection;Similarity-based testing;Software testing;Test suite reduction","startPage":"419","endPage":"429","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812048","citationCount":0,"referenceCount":36,"year":2019,"authors":"E. Cruciani; B. Miranda; R. Verdecchia; A. Bertolino","affiliations":"Gran Sasso Science Institute; Federal University of Pernambuco: Recife, Pernambuco; Gran Sasso Science Institute \u0026 Vrije Universiteit Amsterdam; ISTI – Consiglio Nazionale delle Ricerche","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d341a"},"title":"NL2Type: Inferring JavaScript Function Types from Natural Language Information","abstract":"JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1% and a recall of 78.9% when considering only the top-most suggestion, and with a precision of 95.5% and a recall of 89.6% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.","conference":"IEEE","terms":"Natural languages;Data mining;Predictive models;Semantics;Deep learning;Manuals,digital signatures;Java;learning (artificial intelligence);natural language processing;neural nets;pattern classification;program diagnostics;source code (software);type theory,NL2Type;natural language information;type signatures;type inference algorithms;JavaScript function types;statical typed languages;type prediction;source code;LSTM;DeepTyper;deep learning","keywords":"JavaScript;deep learning;type inference;comments;identifiers","startPage":"304","endPage":"315","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811893","citationCount":2,"referenceCount":65,"year":2019,"authors":"R. S. Malik; J. Patra; M. Pradel","affiliations":"TU Darmstadt, Germany; TU Darmstadt, Germany; TU Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d341b"},"title":"Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews","abstract":"Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce CHANGEADVISOR, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44,683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of CHANGEADVISOR in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%).","conference":"IEEE","terms":"Computer bugs;Mobile communication;Joining processes;Tools;Maintenance engineering;Software;Feature extraction,mobile computing;natural language processing;source code (software),textual based heuristics;CHANGEADVISOR;user feedback;source code components;mobile apps","keywords":"Mobile Apps;Mining User Reviews;Natural Language Processing;Impact Analysis","startPage":"106","endPage":"117","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985654","citationCount":20,"referenceCount":63,"year":2017,"authors":"F. Palomba; P. Salza; A. Ciurumelea; S. Panichella; H. Gall; F. Ferrucci; A. De Lucia","affiliations":"Delft Univ. of Technol., Delft, Netherlands; Univ. of Salerno, Fisciano, Italy; Univ. of Zurich, Zurich, Switzerland; Univ. of Zurich, Zurich, Switzerland; Univ. of Zurich, Zurich, Switzerland; Univ. of Salerno, Fisciano, Italy; Univ. of Salerno, Fisciano, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d341c"},"title":"Active Inductive Logic Programming for Code Search","abstract":"Modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code. The key insight of our interactive code search technique ALICE is that user feedback can be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, ALICE automatically extracts a logic query from code features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). ALICE then infers a new logic query that separates positive examples from negative examples via active inductive logic programming. Our comprehensive simulation experiment shows that ALICE removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to user labeling mistakes. Our choice of leveraging both positive and negative examples and using nested program structure as an inductive bias is effective in refining search queries. Compared with an existing interactive code search technique, ALICE does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with much fewer search iterations. A case study with real developers shows that ALICE is easy to use and helps express complex code patterns.","conference":"IEEE","terms":"Labeling;Logic programming;Semantics;Tools;Database languages;Feature extraction;Computer bugs,inductive logic programming;query languages;query processing;search engines,active inductive logic programming;user feedback;user labeling mistakes;search query;search pattern;structural properties;semantic properties;interactive code search technique ALICE;query language;logic facts;user annotations;logic query;code features;search iterations;nested program structure;interactive code search technique","keywords":"Code Search, Active Learning, Inductive Logic Programming","startPage":"292","endPage":"303","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812091","citationCount":1,"referenceCount":66,"year":2019,"authors":"A. Sivaraman; T. Zhang; G. Van den Broeck; M. Kim","affiliations":"University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d341d"},"title":"Challenges for Static Analysis of Java Reflection - Literature Review and Empirical Study","abstract":"The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.","conference":"IEEE","terms":"Java;Tools;Bibliographies;Grammar;Software;Systematics;Semantics,application program interfaces;computational linguistics;Java;program diagnostics;public domain software;software tools;source code (software);trees (mathematics),static analysis tool;Java Reflection API;literature review;software behavior;real-world Java code analysis;Java projects;reflective Java code;Java systems;collected descriptive statistics;abstract syntax trees;source code;code idioms;nonexceptional exceptions;programmatic filtering meta objects;dynamic proxies;collections semantics;reflection code analysis","keywords":"Java;Reflection;Static Analysis;Systematic Literature Review;Empirical Study","startPage":"507","endPage":"518","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985689","citationCount":9,"referenceCount":80,"year":2017,"authors":"D. Landman; A. Serebrenik; J. J. Vinju","affiliations":"Centrum Wiskunde \u0026 Inf., Amsterdam, Netherlands; Centrum Wiskunde \u0026 Inf., Amsterdam, Netherlands; Centrum Wiskunde \u0026 Inf., Amsterdam, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d341e"},"title":"RADAR: A Lightweight Tool for Requirements and Architecture Decision Analysis","abstract":"Uncertainty and conflicting stakeholders' objectives make many requirements and architecture decisions particularly hard. Quantitative probabilistic models allow software architects to analyse such decisions using stochastic simulation and multi-objective optimisation, but the difficulty of elaborating the models is an obstacle to the wider adoption of such techniques. To reduce this obstacle, this paper presents a novel modelling language and analysis tool, called RADAR, intended to facilitate requirements and architecture decision analysis. The language has relations to quantitative AND/OR goal models used in requirements engineering and to feature models used in software product lines. However, it simplifies such models to a minimum set of language constructs essential for decision analysis. The paper presents RADAR's modelling language, automated support for decision analysis, and evaluates its application to four real-world examples.","conference":"IEEE","terms":"Radar;Computer architecture;Analytical models;Mathematical model;Optimization;Decision analysis;Software,software architecture;software product lines;systems analysis,modelling language;RADAR;architecture decision analysis;requirements decision analysis;quantitative AND/OR goal models;requirements engineering;software product lines;Requirements and Architecture Decision Analyser","keywords":"Decision Analysis;Requirements Engineering;Software Architecture;Goal Modelling;Monte-Carlo Simulation;Multi-Objective Optimisation;Search-Based Software Engineering;Expected Value of Information","startPage":"552","endPage":"562","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985693","citationCount":2,"referenceCount":46,"year":2017,"authors":"S. A. Busari; E. Letier","affiliations":"Dept. of Comput. Sci., Univ. Coll. London, London, UK; Dept. of Comput. Sci., Univ. Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d341f"},"title":"Towards Understanding and Reasoning About Android Interoperations","abstract":"Hybrid applications (apps) have become one of the most attractive options for mobile app developers thanks to its support for portability and device-specific features. Android hybrid apps, for example, support portability via JavaScript, device-specific features via Android Java, and seamless interactions between them. However, their interoperation semantics is often under-documented and unintuitive, which makes hybrid apps vulnerable to errors. While recent research has addressed such vulnerabilities, none of them are based on any formal grounds. In this paper, we present the first formal specification of Android interoperability to establish a firm ground for understanding and reasoning about the interoperations. We identify its semantics via extensive testing and thorough inspection of Android source code. We extend an existing multi-language semantics to formally express the key features of hybrid mechanisms, dynamic and indistinguishable interoperability. Based on the extensions, we incrementally define a formal interoperation semantics and disclose its numerous unintuitive and inconsistent behaviors. Moreover, on top of the formal semantics, we devise a lightweight type system that can detect bugs due to the unintuitive inter-language communication. We show that it detects more bugs more efficiently than HybriDroid, the state-of-the-art analyzer of Android hybrid apps, in real-world Android hybrid apps.","conference":"IEEE","terms":"Java;Semantics;Interoperability;Bridges;Computer bugs;Switches;Cognition,Android (operating system);formal specification;Java;mobile computing;open systems,Android interoperations;device-specific features;Android Java;formal specification;Android interoperability;Android source code;dynamic interoperability;formal interoperation semantics;real-world Android hybrid apps;multilanguage semantics","keywords":"Android hybrid applications, interoperability, multi-language systems, operational semantics, type system","startPage":"223","endPage":"233","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811927","citationCount":1,"referenceCount":25,"year":2019,"authors":"S. Bae; S. Lee; S. Ryu","affiliations":"KAIST; KAIST, South Korea; KAIST, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3420"},"title":"Software Documentation Issues Unveiled","abstract":"(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.","conference":"IEEE","terms":"Documentation;Software;Tools;Maintenance engineering;Taxonomy;Interviews;Information services,application program interfaces;data mining;document handling;software maintenance;system documentation;Web sites,issue repositories;software system;API reference guide;user manual;software documentation issues;documentation-related artifacts;mailing lists;Stack Overflow discussions;pull requests","keywords":"Documentation, Empirical Study","startPage":"1199","endPage":"1210","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811931","citationCount":0,"referenceCount":127,"year":2019,"authors":"E. Aghajani; C. Nagy; O. L. Vega-Márquez; M. Linares-Vásquez; L. Moreno; G. Bavota; M. Lanza","affiliations":"Software Institute, Università della Svizzera italiana (USI), Switzerland; Software Institute, Università della Svizzera italiana (USI), Switzerland; Universidad de los Andes, Colombia; Universidad de los Andes, Colombia; Colorado State University, USA; Software Institute, Università della Svizzera italiana (USI), Switzerland; Software Institute, Università della Svizzera italiana (USI), Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3421"},"title":"On Reliability of Patch Correctness Assessment","abstract":"Current state-of-the-art automatic software repair (ASR) techniques rely heavily on incomplete specifications, or test suites, to generate repairs. This, however, may cause ASR tools to generate repairs that are incorrect and hard to generalize. To assess patch correctness, researchers have been following two methods separately: (1) Automated annotation, wherein patches are automatically labeled by an independent test suite (ITS) - a patch passing the ITS is regarded as correct or generalizable, and incorrect otherwise, (2) Author annotation, wherein authors of ASR techniques manually annotate the correctness labels of patches generated by their and competing tools. While automated annotation cannot ascertain that a patch is actually correct, author annotation is prone to subjectivity. This concern has caused an on-going debate on the appropriate ways to assess the effectiveness of numerous ASR techniques proposed recently. In this work, we propose to assess reliability of author and automated annotations on patch correctness assessment. We do this by first constructing a gold set of correctness labels for 189 randomly selected patches generated by 8 state-of-the-art ASR techniques through a user study involving 35 professional developers as independent annotators. By measuring inter-rater agreement as a proxy for annotation quality - as commonly done in the literature - we demonstrate that our constructed gold set is on par with other high-quality gold sets. We then compare labels generated by author and automated annotations with this gold set to assess reliability of the patch assessment methodologies. We subsequently report several findings and highlight implications for future studies.","conference":"IEEE","terms":"Maintenance engineering;Tools;Gold;Reliability;Task analysis;Software;Best practices,program testing;software maintenance;software reliability,patch correctness assessment;ASR tools;independent test suite;correctness labels;automated annotations;annotation quality;patch assessment methodologies;automatic software repair techniques;randomly selected patches;ASR techniques;automated annotation;author annotation","keywords":"Automated program repair;empirical study;test case generationn","startPage":"524","endPage":"535","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812054","citationCount":0,"referenceCount":84,"year":2019,"authors":"X. D. Le; L. Bao; D. Lo; X. Xia; S. Li; C. Pasareanu","affiliations":"Carnegie Mellon University; Zhejiang University City College; Singapore Management University; Monash University; Zhejiang University; Carnegie Mellon University and NASA Ames Research Center","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3422"},"title":"How C++ Developers Use Immutability Declarations: An Empirical Study","abstract":"Best practices for developers, as encoded in recent programming language designs, recommend the use of immutability whenever practical. However, there is a lack of empirical evidence about the uptake of this advice. Our goal is to understand the usage of immutability by C++ developers in practice. This work investigates how C++ developers use immutability by analyzing their use of the C++ immutability qualifier, const, and by analyzing the code itself. We answer the following broad questions about const usage: 1) do developers actually write non-trivial (more than 3 methods) immutable classes and immutable methods? 2) do developers label their immutable classes and methods? We analyzed 7 medium-to-large open source projects and collected two sources of empirical data: 1) const annotations by developers, indicating an intent to write immutable code; and 2) the results of a simple static analysis which identified easily const-able methods---those that clearly did not mutate state. We estimate that 5% of non-trivial classes (median) are immutable. We found the vast majority of classes do carry immutability labels on methods: surprisingly, developers const-annotate 46% of methods, and we estimate that at least 51% of methods could be const-annotated. Furthermore, developers missed immutability labels on at least 6% of unannotated methods. We provide an in-depth discussion on how developers use const and the results of our analyses.","conference":"IEEE","terms":"Software engineering,C++ language;program diagnostics;software engineering,immutable code;C++ developers;immutability declarations;immutability qualifier;programming language designs;static analysis","keywords":"immutability;language design;empirical studies;static analysis","startPage":"362","endPage":"372","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812125","citationCount":0,"referenceCount":18,"year":2019,"authors":"J. Eyolfson; P. Lam","affiliations":"University of California, Los Angeles, USA; University of Waterloo, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3423"},"title":"Software Development Waste","abstract":"Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.","conference":"IEEE","terms":"Software;Interviews;Manufacturing;Taxonomy;Programming;Production systems,software engineering,software development waste;complex socio-technical activity;constructivist grounded theory;software development consultancy;Pivotal;lean software development","keywords":"Software engineering waste;Extreme Programming;Lean Software Development","startPage":"130","endPage":"140","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985656","citationCount":10,"referenceCount":30,"year":2017,"authors":"T. Sedano; P. Ralph; C. Péraire","affiliations":"Pivotal, Palo Alto, CA, USA; Univ. of Auckland, Auckland, New Zealand; Electr. \u0026 Comput. Eng., Carnegie Mellon Univ., Moffett Field, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3424"},"title":"Decoding the Representation of Code in the Brain: An fMRI Study of Code Review and Expertise","abstract":"Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%, p \u003c; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p \u003c; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level.","conference":"IEEE","terms":"Biomedical imaging;Software engineering;Brain;Software;Computer science;Natural languages;Tools,biomedical MRI;medical image processing;software engineering,fMRI study;software engineering tasks;medical imaging techniques;functional magnetic resonance imaging;code comprehension;code review;prose review;neural representations;programming languages;natural languages","keywords":"medical imaging;code comprehension;prose review","startPage":"175","endPage":"186","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985660","citationCount":11,"referenceCount":85,"year":2017,"authors":"B. Floyd; T. Santander; W. Weimer","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3425"},"title":"Redundant Loads: A Software Inefficiency Indicator","abstract":"Modern software packages have become increasingly complex with millions of lines of code and references to many external libraries. Redundant operations are a common performance limiter in these code bases. Missed compiler optimization opportunities, inappropriate data structure and algorithm choices, and developers' inattention to performance are some common reasons for the existence of redundant operations. Developers mainly depend on compilers to eliminate redundant operations. However, compilers' static analysis often misses optimization opportunities due to ambiguities and limited analysis scope; automatic optimizations to algorithmic and data structural problems are out of scope. We develop LoadSpy, a whole-program profiler to pinpoint redundant memory load operations, which are often a symptom of many redundant operations. The strength of LoadSpy exists in identifying and quantifying redundant load operations in programs and associating the redundancies with program execution contexts and scopes to focus developers' attention on problematic code. LoadSpy works on fully optimized binaries, adopts various optimization techniques to reduce its overhead, and provides a rich graphic user interface, which make it a complete developer tool. Applying LoadSpy showed that a large fraction of redundant loads is common in modern software packages despite highest levels of automatic compiler optimizations. Guided by LoadSpy, we optimize several well-known benchmarks and real-world applications, yielding significant speedups.","conference":"IEEE","terms":"Redundancy;Optimization;Loading;Software;Tools;Monitoring;Registers,application program interfaces;data structures;graphical user interfaces;optimisation;optimising compilers;program debugging;program diagnostics;software libraries,redundant loads;software inefficiency indicator;modern software packages;common performance limiter;missed compiler optimization opportunities;algorithm choices;algorithmic data structural problems;LoadSpy;redundant memory load operations;automatic compiler optimizations;data structure;redundant load operations;graphic user interface","keywords":"Whole-program profiling;Software optimization;Performance measurement;Tools","startPage":"982","endPage":"993","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811970","citationCount":1,"referenceCount":81,"year":2019,"authors":"P. Su; S. Wen; H. Yang; M. Chabbi; X. Liu","affiliations":"College of William \u0026 Mary, USA; College of William \u0026 Mary, USA; Beihang University, China; Scalable Machines Research, USA; College of William \u0026 Mary, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3426"},"title":"Global Optimization of Numerical Programs Via Prioritized Stochastic Algebraic Transformations","abstract":"Numerical code is often applied in the safety-critical, but resource-limited areas. Hence, it is crucial for it to be correct and efficient, both of which are difficult to ensure. On one hand, accumulated rounding errors in numerical programs can cause system failures. On the other hand, arbitrary/infinite-precision arithmetic, although accurate, is infeasible in practice and especially in resource-limited scenarios because it performs thousands of times slower than floating-point arithmetic. Thus, it has been a significant challenge to obtain high-precision, easy-to-maintain, and efficient numerical code. This paper introduces a novel global optimization framework to tackle this challenge. Using our framework, a developer simply writes the infinite-precision numerical program directly following the problem's mathematical requirement specification. The resulting code is correct and easy-to-maintain, but inefficient. Our framework then optimizes the program in a global fashion (i.e., considering the whole program, rather than individual expressions or statements as in prior work), the key technical difficulty this work solves. To this end, it analyzes the program's numerical value flows across different statements through a symbolic trace extraction algorithm, and generates optimized traces via stochastic algebraic transformations guided by effective rule selection. We first evaluate our technique on numerical benchmarks from the literature; results show that our global optimization achieves significantly higher worst-case accuracy than the state-of-the-art numerical optimization tool. Second, we show that our framework is also effective on benchmarks having complicated program structures, which are challenging for numerical optimization. Finally, we apply our framework on real-world code to successfully detect numerical bugs that have been confirmed by developers.","conference":"IEEE","terms":"Optimization;Software;Benchmark testing;Tools;Measurement;Computer science;Computer bugs,algebra;optimisation,resource-limited scenarios;floating-point arithmetic;infinite-precision numerical program;resulting code;global fashion;numerical value;optimized traces;numerical benchmarks;program structures;real-world code;numerical bugs;numerical programs;prioritized stochastic algebraic transformations;safety-critical;resource-limited areas;accumulated rounding errors;numerical code;global optimization framework;numerical optimization tool","keywords":"program optimization;numerical analysis;program transformation","startPage":"1131","endPage":"1141","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812093","citationCount":0,"referenceCount":41,"year":2019,"authors":"X. Wang; H. Wang; Z. Su; E. Tang; X. Chen; W. Shen; Z. Chen; L. Wang; X. Zhang; X. Li","affiliations":"Nanjing University, China; Nanjing University, China; ETH Zurich, Switzerland / UC Davis, United States; Nanjing University, China; Nanjing University, China; Nanjing University, China; Nanjing University, China; Nanjing University, China; Nanjing University, China; Nanjing University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3427"},"title":"Do Developers Read Compiler Error Messages?","abstract":"In integrated development environments, developers receive compiler error messages through a variety of textual and visual mechanisms, such as popups and wavy red underlines. Although error messages are the primary means of communicating defects to developers, researchers have a limited understanding on how developers actually use these messages to resolve defects. To understand how developers use error messages, we conducted an eye tracking study with 56 participants from undergraduate and graduate software engineering courses at our university. The participants attempted to resolve common, yet problematic defects in a Java code base within the Eclipse development environment. We found that: 1) participants read error messages and the difficulty of reading these messages is comparable to the difficulty of reading source code, 2) difficulty reading error messages significantly predicts participants' task performance, and 3) participants allocate a substantial portion of their total task to reading error messages (13%-25%). The results of our study offer empirical justification for the need to improve compiler error messages for developers.","conference":"IEEE","terms":"Gaze tracking;Libraries;Java;Software engineering;Visualization;Google;Navigation,Java;program compilers,compiler error messages;software engineering courses;Java code base;Eclipse development environment;source code","keywords":"compiler errors;eye tracking;integrated development environments;programmer comprehension;reading;visual attention","startPage":"575","endPage":"585","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985695","citationCount":6,"referenceCount":38,"year":2017,"authors":"T. Barik; J. Smith; K. Lubick; E. Holmes; J. Feng; E. Murphy-Hill; C. Parnin","affiliations":"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Psychol., Washington \u0026 Lee Univ., Lexington, VA, USA; Dept. of Psychol., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3428"},"title":"AdJust: Runtime Mitigation of Resource Abusing Third-Party Online Ads","abstract":"Online advertising is the most critical revenue stream for many Internet companies. However, showing ads on websites comes with a price tag. Since website contents and third-party ads are blended together, third-party ads may compete with the publisher contents, delaying or even breaking the rendering of first-party contents. In addition, dynamically including scripts from ad networks all over the world may introduce buggy scripts that slow down page loads and even freeze the browser. The resulting poor usability problems lead to bad user experience and lower profits. The problems caused by such resource abusing ads are originated from two root causes: First, content publishers have no control over third-party ads. Second, publishers cannot differentiate resource consumed by ads from that consumed by their own contents. To address these challenges, we propose an effective technique, AdJust, that allows publishers to specify constraints on events associated with third-party ads (e.g., URL requests, HTML element creations, and timers), so that they can mitigate user experience degradations and enforce consistent ads experience to all users. We report on a series of experiments over the Alexa top 200 news websites. The results point to the efficacy of our proposed techniques: AdJust effectively mitigated degradations that freeze web browsers (on 36 websites), reduced the load time of publisher contents (on 61 websites), prioritized publisher contents (on 166 websites) and ensured consistent rendering orders among top ads (on 68 websites).","conference":"IEEE","terms":"Servers;Runtime;Browsers;Delays;Web pages;Sports;Uniform resource locators,advertising data processing;rendering (computer graphics);Web sites,content publishers;third-party ads;prioritized publisher contents;resource abusing third-party online ads;website contents;first-party contents;AdJust technique;online advertising","keywords":"online ads;defective ads;resource abusing;performance degradation","startPage":"1005","endPage":"1015","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811929","citationCount":0,"referenceCount":30,"year":2019,"authors":"W. Wang; I. L. Kim; Y. Zheng","affiliations":"University at Buffalo; Purdue University; IBM T.J. Watson Research Center","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3429"},"title":"9.6 Million Links in Source Code Comments: Purpose, Evolution, and Decay","abstract":"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10% of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.","conference":"IEEE","terms":"C++ languages;Java;Licenses;Documentation;Python;Web sites,Internet;public domain software;software maintenance,source code repositories;link targets;source code comments;link-fixing pull requests;source software repositories;World Wide Web;mixed-methods approach","keywords":"code comment;link decay;knowledge sharing","startPage":"1211","endPage":"1221","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811933","citationCount":1,"referenceCount":45,"year":2019,"authors":"H. Hata; C. Treude; R. G. Kula; T. Ishio","affiliations":"Nara Institute of Science and Technology, Japan; University of Adelaide, Australia; Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d342a"},"title":"Symbolic Repairs for GR(1) Specifications","abstract":"Unrealizability is a major challenge for GR(1), an expressive assume-guarantee fragment of LTL that enables efficient synthesis. Some works attempt to help engineers deal with unrealizability by generating counter-strategies or computing an unrealizable core. Other works propose to repair the unrealizable specification by suggesting repairs in the form of automatically generated assumptions. In this work we present two novel symbolic algorithms for repairing unrealizable GR(1) specifications. The first algorithm infers new assumptions based on the recently introduced JVTS. The second algorithm infers new assumptions directly from the specification. Both algorithms are sound. The first is incomplete but can be used to suggest many different repairs. The second is complete but suggests a single repair. Both are symbolic and therefore efficient. We implemented our work, validated its correctness, and evaluated it on benchmarks from the literature. The evaluation shows the strength of our algorithms, in their ability to suggest repairs and in their performance and scalability compared to previous solutions.","conference":"IEEE","terms":"Maintenance engineering;Safety;Glass;Cascading style sheets;Benchmark testing;Scalability;Standards,formal specification;maintenance engineering;program debugging,symbolic repairs;counter-strategies;symbolic algorithms;GR1 specification;JVTS","keywords":"reactive synthesis;repair;GR(1)","startPage":"1016","endPage":"1026","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812056","citationCount":0,"referenceCount":37,"year":2019,"authors":"S. Maoz; J. O. Ringert; R. Shalom","affiliations":"Tel Aviv University; University of Leicester; Tel Aviv University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d342b"},"title":"The List is the Process: Reliable Pre-Integration Tracking of Commits on Mailing Lists","abstract":"A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history. We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth. Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements.","conference":"IEEE","terms":"Postal services;Tools;Linux;Kernel;Electronic mail;Software reliability,data mining;electronic mail;Linux;public domain software;software maintenance,OSS development processes;open development process;mailing lists;software evolution;software repositories;open source software projects;Linux kernel;reliable safety-critical industrial products;formal process requirements","keywords":"software engineering;mining software repositories;mailing lists;patches;commits","startPage":"807","endPage":"818","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812060","citationCount":0,"referenceCount":41,"year":2019,"authors":"R. Ramsauer; D. Lohmann; W. Mauerer","affiliations":"OTH Regensburg; University of Hanover; OTH Regensburg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d342c"},"title":"DockerizeMe: Automatic Inference of Environment Dependencies for Python Code Snippets","abstract":"Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50% of public Python gists fail due to an import error for a missing library. We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.","conference":"IEEE","terms":"Python;Libraries;Knowledge based systems;Wheels;Task analysis;Indexes;Inference algorithms,graph theory;inference mechanisms;knowledge acquisition;Python;software packages;source code (software),DockerizeMe;automatic inference;environment dependencies;Stack Overflow;programming techniques;programming language;missing library;Python code snippet;offline knowledge acquisition;graph-based inference procedure;GitHub gist system;Python package index","keywords":"Docker;Configuration Management;Environment Inference;Dependencies;Python","startPage":"328","endPage":"338","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811897","citationCount":1,"referenceCount":26,"year":2019,"authors":"E. Horton; C. Parnin","affiliations":"North Carolina State University; North Carolina State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d342d"},"title":"Harnessing Evolution for Multi-Hunk Program Repair","abstract":"Despite significant advances in automatic program repair (APR) techniques over the past decade, practical deployment remains an elusive goal. One of the important challenges in this regard is the general inability of current APR techniques to produce patches that require edits in multiple locations, i.e., multi-hunk patches. In this work, we present a novel APR technique that generalizes single-hunk repair techniques to include an important class of multi-hunk bugs, namely bugs that may require applying a substantially similar patch at a number of locations. We term such sets of repair locations as evolutionary siblings - similar looking code, instantiated in similar contexts, that are expected to undergo similar changes. At the heart of our proposed method is an analysis to accurately identify a set of evolutionary siblings, for a given bug. This analysis leverages three distinct sources of information, namely the test-suite spectrum, a novel code similarity analysis, and the revision history of the project. The discovered siblings are then simultaneously repaired in a similar fashion. We instantiate this technique in a tool called HERCULES and demonstrate that it is able to correctly fix 46 bugs in the Defects4J dataset, the highest of any individual APR technique to date. This includes 15 multi-hunk bugs and overall 11 bugs which have not been fixed by any other technique so far.","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;Cloning;Tools;History;Search problems;Syntactics,program debugging;program testing,multihunk program repair;automatic program repair techniques;multihunk patches;single-hunk repair techniques;repair locations;evolutionary siblings;code similarity analysis;APR technique;HERCULES tool;Defects4J dataset","keywords":"automatic program repair, multi-hunk patches, code similarity","startPage":"13","endPage":"24","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812131","citationCount":0,"referenceCount":47,"year":2019,"authors":"S. Saha; R. k. Saha; M. r. Prasad","affiliations":"University of California Santa Barbara; Fujitsu Laboratories of America, Inc.; Fujitsu Laboratories of America, Inc.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d342e"},"title":"From Diversity by Numbers to Diversity as Process: Supporting Inclusiveness in Software Development Teams with Brainstorming","abstract":"Negative experiences in diverse software development teams have the potential to turn off minority participants from future team-based software development activity. We examine the use of brainstorming as one concrete team processes that may be used to improve the satisfaction of minority developers when working in a group. Situating our study in time-intensive hackathon-like environments where engagement of all team members is particularly crucial, we use a combination of survey and interview data to test our propositions. We find that brainstorming strategies are particularly effective for team members who identify as minorities, and support satisfaction with both the process and outcomes of teamwork through different mechanisms.","conference":"IEEE","terms":"Software;Teamwork;Software engineering;Concrete;Cultural differences;Organizations,software engineering,software development;time-intensive hackathon-like environments;brainstorming","keywords":"Diversity;hackathons;teamwork;brainstorming;satisfaction;software engineering management","startPage":"152","endPage":"163","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985658","citationCount":4,"referenceCount":47,"year":2017,"authors":"A. Filippova; E. Trainer; J. D. Herbsleb","affiliations":"Inst. for Software Res., Carnegie Mellon Univ., Pittsburgh, PA, USA; Inst. for Software Res., Carnegie Mellon Univ., Pittsburgh, PA, USA; Inst. for Software Res., Carnegie Mellon Univ., Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d342f"},"title":"Socio-Technical Work-Rate Increase Associates With Changes in Work Patterns in Online Projects","abstract":"Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance. Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked.","conference":"IEEE","terms":"Stress;Task analysis;Software;Productivity;Data models;Multitasking;Switches,Internet;project management;regression analysis;software engineering,GitHub developers;technical activities;social activities;work patterns;technical dimensions;social dimensions;technical productivity;social productivity;software developers;socio-technical work-rate increase;online projects;regression models","keywords":"software engineering;work related stress;work patterns;multitasking;socio technical;work rate increase;survey;empirical analysis;regression modeling;GitHub;software developers;open source software;online projects;commits;comments;sentiment;social activities;technical activities;focus switching;recommendations;repositories;issues;pull requests;discussions;teams;collaboration;team members;social coding","startPage":"936","endPage":"947","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811968","citationCount":0,"referenceCount":58,"year":2019,"authors":"F. Sarker; B. Vasilescu; K. Blincoe; V. Filkov","affiliations":"University of California, Davis, United States; Carnegie Mellon University, United States; University of Auckland, New Zealand; University of California, Davis, United States","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3430"},"title":"Search-Driven String Constraint Solving for Vulnerability Detection","abstract":"Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-the-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one, this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support. We have implemented the proposed search-driven constraint solving technique in the ACO-Solver tool, which we have evaluated in the context of injection and XSS vulnerability detection for Java Web applications. We have assessed the benefits and costs of combining the proposed technique with two state-of-the-art constraint solvers (Z3-str2 and CVC4). The experimental results, based on a benchmark with 104 constraints derived from nine realistic Web applications, show that our approach, when combined in a state-of-the-art solver, significantly improves the number of detected vulnerabilities (from 4.7% to 71.9% for Z3-str2, from 85.9% to 100.0% for CVC4), and solves several cases on which the solver fails when used stand-alone (46 more solved cases for Z3-str2, and 11 more for CVC4), while still keeping the execution time affordable in practice.","conference":"IEEE","terms":"Java;Libraries;Security;Ant colony optimization;Automata;Search problems;Standards,ant colony optimisation;Internet;Java;program verification;search problems,input sanitization;program validation operations;complex string operations;search-driven string constraint solving technique;hybrid constraint solving procedure;ant colony optimization metaheuristic;fallback mechanism;ACO-solver tool;XSS vulnerability detection;Java Web applications","keywords":"vulnerability detection;string constraint solving;search-based software engineering","startPage":"198","endPage":"208","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985662","citationCount":1,"referenceCount":56,"year":2017,"authors":"J. Thomé; L. K. Shar; D. Bianculli; L. Briand","affiliations":"SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3431"},"title":"Towards Automating Precision Studies of Clone Detectors","abstract":"Current research in clone detection suffers from poor ecosystems for evaluating precision of clone detection tools. Corpora of labeled clones are scarce and incomplete, making evaluation labor intensive and idiosyncratic, and limiting intertool comparison. Precision-assessment tools are simply lacking. We present a semiautomated approach to facilitate precision studies of clone detection tools. The approach merges automatic mechanisms of clone classification with manual validation of clone pairs. We demonstrate that the proposed automatic approach has a very high precision and it significantly reduces the number of clone pairs that need human validation during precision experiments. Moreover, we aggregate the individual effort of multiple teams into a single evolving dataset of labeled clone pairs, creating an important asset for software clone research.","conference":"IEEE","terms":"Cloning;Tools;Detectors;Manuals;Inspection;Software;Aggregates,program diagnostics;software maintenance;software management;software tools,clone detection tools;precision-assessment tools;semiautomated approach;clone classification;precision experiments;labeled clone pairs;software clone research;clone detectors;labor intensive evaluation;human validation","keywords":"Precision Evaluation, Clone Detection, Machine learning, Open source labeled datasets","startPage":"49","endPage":"59","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811972","citationCount":0,"referenceCount":28,"year":2019,"authors":"V. Saini; F. Farmahinifarahani; Y. Lu; D. Yang; P. Martins; H. Sajnani; P. Baldi; C. V. Lopes","affiliations":"University of California Irvine; University of California Irvine; University of California Irvine; University of California Irvine; University of California Irvine; Microsoft, USA; University of California Irvine; University of California Irvine","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3432"},"title":"CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries","abstract":"Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies.","conference":"IEEE","terms":"Libraries;Computer bugs;Testing;Training;Atmospheric modeling;Task analysis;Deep learning,learning (artificial intelligence);neural nets;program debugging;software libraries,cross-backend validation;deep learning libraries;deep learning systems;DL software libraries;CRADLE;cross-implementation inconsistency checking;anomaly propagation tracking","keywords":"deep learning software testing;cross-implementation testing;bugs detection;software testing","startPage":"1027","endPage":"1038","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812095","citationCount":2,"referenceCount":75,"year":2019,"authors":"H. V. Pham; T. Lutellier; W. Qi; L. Tan","affiliations":"University of Waterloo; University of Waterloo; University of Science and Technology of China; Purdue University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3433"},"title":"AutoTap: Synthesizing and Repairing Trigger-Action Programs Using LTL Properties","abstract":"End-user programming, particularly trigger-action programming (TAP), is a popular method of letting users express their intent for how smart devices and cloud services interact. Unfortunately, sometimes it can be challenging for users to correctly express their desires through TAP. This paper presents AutoTap, a system that lets novice users easily specify desired properties for devices and services. AutoTap translates these properties to linear temporal logic (LTL) and both automatically synthesizes property-satisfying TAP rules from scratch and repairs existing TAP rules. We designed AutoTap based on a user study about properties users wish to express. Through a second user study, we show that novice users made significantly fewer mistakes when expressing desired behaviors using AutoTap than using TAP rules. Our experiments show that AutoTap is a simple and effective option for expressive end-user programming.","conference":"IEEE","terms":"Programming;Smart devices;Microsoft Windows;Rain;Maintenance engineering;Safety;Computer bugs,cloud computing;software maintenance;temporal logic,AutoTap;LTL properties;trigger-action programming;smart devices;property-satisfying TAP rules;cloud services;end-user programming;trigger-action programs;linear temporal logic;program repair;program synthesis","keywords":"End-user programming;Trigger-action programming;Program synthesis;Program repair","startPage":"281","endPage":"291","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811900","citationCount":0,"referenceCount":41,"year":2019,"authors":"L. Zhang; W. He; J. Martinez; N. Brackenbury; S. Lu; B. Ur","affiliations":"The University of Chicago; The University of Chicago; The University of Chicago; The University of Chicago; The University of Chicago; The University of Chicago","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3434"},"title":"An Empirical Study on Mutation, Statement and Branch Coverage Fault Revelation That Avoids the Unreliable Clean Program Assumption","abstract":"Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed ('clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings also revealed that fault revelation starts to increase significantly only once relatively high levels of coverage are attained.","conference":"IEEE","terms":"Testing;Java;Correlation;Robustness;Standards;Tools,program testing;software fault tolerance;software quality,branch coverage fault revelation;unreliable clean program assumption avoidance;test quality indicator;semantic deviations;mutation testing","keywords":"Mutation testing;test effectiveness;code coverage;real faults;test adequacy","startPage":"597","endPage":"608","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985697","citationCount":14,"referenceCount":57,"year":2017,"authors":"T. T. Chekam; M. Papadakis; Y. Le Traon; M. Harman","affiliations":"Interdiscipl. Centre for Security, Reliability \u0026 Trust, Univ. of Luxembourg, Luxembourg City, Luxembourg; Interdiscipl. Centre for Security, Reliability \u0026 Trust, Univ. of Luxembourg, Luxembourg City, Luxembourg; Interdiscipl. Centre for Security, Reliability \u0026 Trust, Univ. of Luxembourg, Luxembourg City, Luxembourg; Univ. Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3435"},"title":"2nd International Workshop on Rapid Continuous Software Engineering (RCoSE 2015)","abstract":"Continuous software engineering refers to the organizational  capability to develop, release and learn from software in very short  rapid cycles, typically hours, days or a very small numbers of  weeks.  This requires not only agile processes in teams but in the  complete research and development organization. Additionally, the  technology used in the different development phases, like  requirements engineering and system integration, must support the  quick development cycles. Finally, automatic live experimentation  for different system alternatives enables fast gathering of required  data for decision making. The workshop, the second in the series  after the first one at ICSE 2014, aims to bring the research  communities of the aforementioned areas together to exchange  challenges, ideas, and solutions to bring software engineering a  step further to being a holistic continuous process. The workshop  program is based on eight papers selected in the peer-review process  and supplemented by interaction and discussions at the workshop. The  topics range from agile methods, continuous software engineering  practices to specific techniques, like visualization and testing.","conference":"IEEE","terms":"Conferences;Software engineering;Software;Companies;Testing,,","keywords":"","startPage":"993","endPage":"994","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203144","citationCount":2,"referenceCount":8,"year":2015,"authors":"M. Tichy; J. Bosch; M. Goedicke; B. Fitzgerald","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3436"},"title":"8th International Workshop on Search-Based Software Testing (SBST 2015)","abstract":"This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.","conference":"IEEE","terms":"Conferences;Software engineering;Software;Software testing;Search problems;Measurement,,","keywords":"","startPage":"1001","endPage":"1002","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148","citationCount":1,"referenceCount":3,"year":2015,"authors":"G. Gay; G. Antoniol","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3437"},"title":"1st International Workshop on Software Protection (SPRO 2015)","abstract":"There are many reasons to protect software: your banking app needs to be protected to prevent fraud; software operating on critical infrastructures needs to be protected against vulnerability discovery; software vendors and service companies need it to protect their business; etc. In the past decade, many techniques to protect software have been presented and broken. Beyond making individual techniques better, the challenge includes to be able to deploy them in practice and be able to evaluate them. This is the objective of SPRO, the first International Workshop on Software Protection: to bring together researchers and industrial practitioners both from software protection and the wider software engineering community to share experience and provide directions for future research, in order to stimulate the use of software engineering techniques in novel aspects of software protection. This first edition of the workshop is held at ICSE 2015 in Florence (Italy) with the aim of creating a community working in this new growing area of security, and to highlight its synergies with different research fields of software engineering, like: formal models, program analysis, reverse engineering, code transformations, empirical evaluation, and software metrics. This paper presents the research themes and challenges of the workshop, describes the workshop organization, and summarizes the research papers.","conference":"IEEE","terms":"Software protection;Conferences;Software;Software engineering;Security;Analytical models;Reverse engineering,,","keywords":"Obfuscation;Virtualization;Information Hiding;security modelling;security metrics","startPage":"1013","endPage":"1014","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203154","citationCount":0,"referenceCount":9,"year":2015,"authors":"P. Falcarin; B. Wyseur","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3438"},"title":"5th International Workshop on the Twin Peaks of Requirements and Architecture (TwinPeaks 2015)","abstract":"The relationships and interdependencies between software requirements and the architectures of software-intensive systems are described in the Twin Peaks model. The fundamental idea of the Twin Peaks model is that Requirements Engineering and Software Architecture should not be treated in isolation. Instead, we need to progressively discover and specify requirements while concurrently exploring alternative architectural solutions. However, bridging the gap between Requirements Engineering and Software Architecture has mainly been discussed independently in the respective communities. Therefore, this ICSE workshop aims at bringing together researchers, practitioners and educators from the Requirements Engineering and Software Architecture fields to jointly explore the strong interdependencies between requirements and architecture. Based on the results from previous editions of the workshop, this edition focuses on agile software development contexts and on exploring lightweight techniques for integrating requirements and architectural thinking.","conference":"IEEE","terms":"Conferences;Computer architecture;Requirements engineering;Software architecture;Software;Context,,","keywords":"twin peaks;requirements engineering;software architecture;agile","startPage":"1017","endPage":"1018","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203156","citationCount":0,"referenceCount":4,"year":2015,"authors":"M. Galster; M. Mirakhorli","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d3439"},"title":"10th International Workshop on Automation of Software Test (AST 2015)","abstract":"This paper is a report on The 10th IEEE/ACMInternational Workshop on Automation of Software Test (AST2015) at the 37th International Conference on Software Engineering(ICSE 2015). It sets a special theme on testing oracles.Keynote speeches and charette discussions are organized aroundthis special theme. 16 full research papers and 2 keynotes willbe presented in the two-day workshop. The report will give thebackground of the workshop and the selection of the specialtheme, and report on the organization of the workshop. Theprovisional program will be presented with a list of the sessionsand papers to be presented at the workshop.","conference":"IEEE","terms":"Conferences;Software;Automation;Software testing;Graphical user interfaces;Speech,,","keywords":"Automation of Software Test;Software Testing;Software Tools;Test Oracle","startPage":"963","endPage":"964","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203129","citationCount":0,"referenceCount":0,"year":2015,"authors":"R. Subramanyan; L. Mariani; D. Hao","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d343a"},"title":"4th International Workshop on Games and Software Engineering (GAS 2015)","abstract":"We present a summary of the 4th ICSE Workshop on Games and Software Engineering. The full day workshop is planned to include a keynote speaker, game-jam demonstration session, and paper presentations on game software engineering topics related to software engineering education, frameworks for game development and infrastructure, quality assurance, and model-based game development. The accepted papers are overviewed here.","conference":"IEEE","terms":"Games;Software engineering;Conferences;Engines;Education;Quality assurance;Computer architecture,,","keywords":"Game engineering;software engineering","startPage":"979","endPage":"980","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203137","citationCount":0,"referenceCount":0,"year":2015,"authors":"J. Bishop; K. M. L. Cooper; W. Scacchi; J. Whitehead","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d343b"},"title":"4th International Workshop on Realizing AI Synergies in Software Engineering (RAISE 2015)","abstract":"This workshop is the fourth in the series and continued to build upon the work carried out at the previous iterations of the International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, which were held at ICSE in 2012, 2013 and 2014. RAISE 2015 brought together researchers and practitioners from the artificial intelligence (AI) and software engineering (SE) disciplines to build on the interdis- ciplinary synergies that exist and to stimulate further interaction across these disciplines. Mutually beneficial characteristics have appeared in the past few decades and are still evolving due to new challenges and technological advances. Hence, the question that motivates and drives the RAISE Workshop series is: \"Are SE and AI researchers ignoring important insights from AI and SE?\". To pursue this question, RAISE'15 explored not only the application of AI techniques to SE problems but also the application of SE techniques to AI problems. RAISE not only strengthens the AI- and-SE community but also continues to develop a roadmap of strategic research directions for AI and SE.","conference":"IEEE","terms":"Artificial intelligence;Conferences;Software engineering;Electronic mail;Software;Computer science;Robots,,","keywords":"","startPage":"991","endPage":"992","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203143","citationCount":0,"referenceCount":0,"year":2015,"authors":"B. Turhan; A. Bener; R. Harrison; A. Miransky; C. Mericli; L. Minku","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9116eee8435e8e7d343c"},"title":"3rd International Workshop on Software Engineering for Systems-of-Systems (SESoS 2015)","abstract":"Systems-of-Systems (SoS) refer to a new class of software-intensive systems, where their constituent systems work cooperatively in order to fulfill specific missions. Characterized by managerial and operational independence, geographic distribution, evolutionary development, and emergent behavior, SoS bring substantial challenges to the software engineering area. SESoS 2015, held in Florence, Italy, on May 17, 2015, as a joint workshop of the 37th International Conference on Software Engineering (ICSE), provided a forum to exchange ideas and experiences, analyze current research and development issues, discuss promising solutions, and to explore inspiring visions for the future of Software Engineering (SE) for SoS.","conference":"IEEE","terms":"Software engineering;Conferences;Electronic mail;Committees;Software;Research and development;Vehicle dynamics,,","keywords":"System-of-Systems;Software Engineering;Ultra-large Scale Systems","startPage":"1011","endPage":"1012","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203153","citationCount":0,"referenceCount":5,"year":2015,"authors":"F. Oquendo; P. Avgeriou; C. E. Cuesta; K. Drira; E. Y. Nakagawa; J. C. Maldonado; A. Zisman","affiliations":"NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d343d"},"title":"1st International Workshop on TEchnical and LEgal aspects of data pRIvacy and Security (TELERISE 2015)","abstract":"This paper is the report on the 1st International Workshop on TEchnical and LEgal aspects of data pRIvacy and SEcurity (TELERISE 2015) at the 37th International Conference on Software Engineering (ICSE 2015). TELERISE investigates privacy and security issues in data sharing from a technical and legal perspective. Keynote speech as well as selected papers presented at the event fit the topics of the workshop. This report gives the rationale of TELERISE and it provides a provisional program.","conference":"IEEE","terms":"Conferences;Law;Security;Data privacy;Privacy;Organizations,,","keywords":"","startPage":"1015","endPage":"1016","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203155","citationCount":0,"referenceCount":3,"year":2015,"authors":"I. Matteucci; P. Mori; M. Petrocchi","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d343e"},"title":"[Journal First] Model Comprehension for Security Risk Assessment: An Empirical Comparison of Tabular vs. Graphical Representations","abstract":"Context: Tabular and graphical representations are used to communicate security risk assessments for IT systems. However, there is no consensus on which type of representation better supports the comprehension of risks (such as the relationships between threats, vulnerabilities and security controls). Vessey's cognitive fit theory predicts that graphs should be better because they capture spatial relationships. Method: We report the results of two studies performed in two countries with 69 and 83 participants respectively, in which we assessed the effectiveness of tabular and graphical representations concerning the extraction of correct information about security risks. Results: Participants who applied tabular risk models gave more precise and complete answers to the comprehension questions when requested to find simple and complex information about threats, vulnerabilities, or other elements of the risk models. Conclusions: Our findings can be explained by Vessey's cognitive fit theory as tabular models implicitly capture elementary linear spatial relationships. Interest for ICSE: It is almost taken for granted in Software Engineering that graphical-, diagram-based models are \"the\" way to go (e.g., the SE Body of Knowledge). This paper provides some experimental-based doubts that this might not always be the case. It will provide an interesting debate that might ripple to traditional requirements and design notations outside security.","conference":"IEEE","terms":"Security;Task analysis;Unified modeling language;Software engineering;Risk management;Data mining;Complexity theory,risk analysis;risk management;security of data;software engineering;solid modelling,comprehension questions;Vessey's cognitive fit theory;elementary linear spatial relationships;diagram-based models;security risk assessment;graphical representations;security controls;security risks;tabular risk models;model comprehension;Software Engineering","keywords":"Empirical Study;Security Risk Assessment;Risk Modeling;Comprehensibility;Cognitive Fit","startPage":"395","endPage":"395","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453097","citationCount":0,"referenceCount":0,"year":2018,"authors":"K. Labunets; F. Massacci; F. Paci; S. Marczak; F. M. de Oliveira","affiliations":"Delft Univ. of Technol., Delft, Netherlands; Univ. of Trento, Trento, Italy; Univ. of Southampton, Southampton, UK; Pontificia Univ. Catolica do Rio Grande do Sul, Rio Grande, Brazil; Pontificia Univ. Catolica do Rio Grande do Sul, Rio Grande, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d343f"},"title":"Global-Aware Recommendations for Repairing Violations in Exception Handling","abstract":"This paper presents an extended abstract incorporated as a journalrst paper into the ICSE'18 program.","conference":"IEEE","terms":"Maintenance engineering;Software engineering;Software maintenance;Java;Tools;Recommender systems,exception handling;object-oriented programming;program diagnostics;recommender systems,global-aware recommendations;repairing violations;journalrst paper;exception handling","keywords":"Exception Handling;Recommender system;Software maintenance","startPage":"858","endPage":"858","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453159","citationCount":1,"referenceCount":0,"year":2018,"authors":"E. A. Barbosa; A. Garcia","affiliations":"Digital Metropolis Inst., UFRN, Natal, Brazil; Inf. Dept., PUC-Rio, Rio de Janeiro, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3440"},"title":"Poster: Static Analysis of Concurrent Higher-Order Programs","abstract":"Few static analyses support concurrent higher-order programs. Tools for detecting concurrency bugs such as deadlocks and race conditions are nonetheless invaluable to developers. Concurrency can be implemented using a variety of models, each supported by different synchronization primitives. Using this poster, we present an approach for analyzing concurrent higher-order programs in a precise manner through abstract interpretation. We instantiate the approach for two static analyses that are capable of detecting deadlocks and race conditions in programs that rely either on compare-and-swap (cas), or on conventional locks for synchronization. We observe few false positives and false negatives on a corpus of small concurrent programs, with better results for the lock-based analyses. We also observe that these programs lead to a smaller state space to be explored by the analyses. Our results show that the choice of synchronization primitives supported by an abstract interpreter has an important impact on the complexity of the static analyses performed with this abstract interpreter.","conference":"IEEE","terms":"System recovery;Concurrent computing;Synchronization;Model checking;Programming;Explosions;Adaptation models,concurrency (computers);program diagnostics;synchronisation,static concurrent higher-order program analysis;concurrency bug detection;synchronization primitives;race conditions;deadlock detection;compare-and-swap;lock-based analysis;abstract interpreter","keywords":"static analysis;abstract interpretation;concurrency;higher-order","startPage":"821","endPage":"822","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203086","citationCount":0,"referenceCount":9,"year":2015,"authors":"Q. Stievenart; J. Nicolay; W. De Meuter; C. De Roover","affiliations":"Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3441"},"title":"Poster: Model-based Run-time Variability Resolution for Robotic Applications","abstract":"In this paper we present our ongoing work on Robotics Run-time Adaptation (RRA). RRA is a model-driven approach that addresses robotics runtime adaptation by modeling and resolving run-time variability of robotic applications.","conference":"IEEE","terms":"Adaptation models;Robots;Context;Computer architecture;Context modeling;Engines;Software architecture,robots,model-based run-time variability resolution;robotic applications;robotics run-time adaptation;RRA;model-driven approach","keywords":"Variability Resolution;Runtime Adaptation;Model-based Approach","startPage":"829","endPage":"830","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203090","citationCount":1,"referenceCount":4,"year":2015,"authors":"L. Gherardi; N. Hochgeschwender","affiliations":"Inst. for Dynamic Syst. \u0026 Control, ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., Bonn-Rhein-Sieg Univ., St. Augustin, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3442"},"title":"Workshop on Applications of Human Error Research to Improve Software Engineering (WAHESE 2015)","abstract":"Advances in the psychological understanding of the origins and manifestations of human error have led to tremendous reductions in errors in fields such as medicine, aviation, and nuclear power plants. This workshop is intended to foster a better understanding of software engineering errors and how a psychological perspective can reduce them, improving software quality and reducing maintenance costs. The workshop goal is to develop a body of knowledge that can advance our understanding of the psychological processes (of human reasoning, planning, and problem solving) and how they fail during the software development. Applying human error research to software quality improvement will provide insights to the cognitive aspects of software development. The workshop will include interactive session to discuss common themes of errors in different fields, and structure software error information to detect and prevent software errors during the development.","conference":"IEEE","terms":"Conferences;Psychology;Software engineering;Software quality;Taxonomy;Accidents,,","keywords":"","startPage":"1019","endPage":"1020","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203157","citationCount":0,"referenceCount":15,"year":2015,"authors":"G. S. Walia; J. C. Caver; G. Bradshaw","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3443"},"title":"Analysis of Android Inter-App Security Vulnerabilities Using COVERT","abstract":"The state-of-the-art in securing mobile software systems are substantially intended to detect and mitigate vulnerabilities in a single app, but fail to identify vulnerabilities that arise due to the interaction of multiple apps, such as collusion attacks and privilege escalation chaining, shown to be quite common in the apps on the market. This paper demonstrates COVERT, a novel approach and accompanying tool-suite that relies on a hybrid static analysis and lightweight formal analysis technique to enable compositional security assessment of complex software. Through static analysis of Android application packages, it extracts relevant security specifications in an analyzable formal specification language, and checks them as a whole for inter-app vulnerabilities. To our knowledge, COVERT is the first formally-precise analysis tool for automated compositional analysis of Android apps. Our study of hundreds of Android apps revealed dozens of inter-app vulnerabilities, many of which were previously unknown.","conference":"IEEE","terms":"Androids;Humanoid robots;Security;Analytical models;Smart phones;Metals;Mobile communication,Android (operating system);formal specification;mobile computing;program diagnostics;security of data;specification languages,Android inter-app security vulnerability analysis;COVERT approach;mobile software systems;collusion attacks;privilege escalation chaining;hybrid static analysis;lightweight formal analysis technique;complex software compositional security assessment;Android application package static analysis;formal specification language;formally-precise analysis tool","keywords":"","startPage":"725","endPage":"728","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203053","citationCount":16,"referenceCount":14,"year":2015,"authors":"A. Sadeghi; H. Bagheri; S. Malek","affiliations":"Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA; Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA; Dept. of Comput. Sci., George Mason Univ., Fairfax, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3444"},"title":"Exploration, Analysis, and Manipulation of  Source Code Using srcML","abstract":"This technology briefing is intended for those interested in constructing custom software analysis and manipulation tools to support research or commercial applications. srcML (srcML.org) is an infrastructure consisting of an XML representation for C/C++/C#/Java source code along with efficient parsing technology to convert source code to-and-from the srcML format. The briefing describes srcML, the toolkit, and the application of XPath and XSLT to query and modify source code. Additionally, a hands-on tutorial of how to use srcML and XML tools to construct custom analysis and manipulation tools will be conducted.","conference":"IEEE","terms":"Software;XML;Conferences;Software engineering;Tutorials;Robustness,C++ language;Java;program compilers;program diagnostics;source code (software);XML,source code exploration;srcML;source code analysis;source code manipulation;custom software analysis;manipulation tools;XML representation;C-C++-C#-Java source code;parsing technology;XPath;XSLT;custom analysis","keywords":"srcML;static program analysis;program transformation;XML","startPage":"951","endPage":"952","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203124","citationCount":2,"referenceCount":4,"year":2015,"authors":"J. I. Maletic; M. L. Collard","affiliations":"Dept. of Comput. Sci., Kent State Univ., Kent, OH, USA; Dept. of Comput. Sci., Univ. of Akron, Akron, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3445"},"title":"Poster: MAPP: The Berkeley Model and Algorithm Prototyping Platform","abstract":"We describe the Berkeley Model and Algorithm Prototyping Platform (MAPP), designed to facilitate experimentation with numerical algorithms and models. MAPP is written entirely in MATLAB and is available as open source under the GNU GPL.","conference":"IEEE","terms":"Mathematical model;Algorithm design and analysis;Integrated circuit modeling;Object oriented modeling;Numerical models;MATLAB;SPICE,graphical user interfaces;public domain software;SPICE,model-and-algorithm prototyping platform;numerical algorithms;MAPP;Matlab;open source software;GNU GPL;SPICE","keywords":"","startPage":"825","endPage":"826","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203088","citationCount":1,"referenceCount":7,"year":2015,"authors":"T. Wang; K. Aadithya; B. Wu; J. Roychowdhury","affiliations":"EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3446"},"title":"Poster: Software Development Risk Management: Using Machine Learning for Generating Risk Prompts","abstract":"Software risk management is a critical component of software development management. Due to the magnitude of potential losses, risk identification and mitigation early on become paramount. Lists containing hundreds of possible risk prompts are available both in academic literature as well as in practice. Given the large number of risks documented, scanning the lists for risks and pinning down relevant risks, though comprehensive, becomes impractical. In this work, a machine learning algorithm is developed to generate risk prompts, based on software project characteristics and other factors. The work also explores the utility of post-classification tagging of risks.","conference":"IEEE","terms":"Software;Taxonomy;Tagging;Neural networks;Machine learning algorithms;Risk management;Distance measurement,identification technology;learning (artificial intelligence);risk management;software development management,software risk management;software development management;machine learning;risk identification;risk mitigation;post-classification tagging","keywords":"Software risk;software management;machine learning;risk prompts","startPage":"833","endPage":"834","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203092","citationCount":0,"referenceCount":10,"year":2015,"authors":"H. R. Joseph","affiliations":"Dept. of Electr. Eng., Indian Inst. of Technol. Madras, Chennai, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3447"},"title":"Correctness and Relative Correctness","abstract":"In the process of trying to define what is a software fault, we have found that to formally define software faults we need to introduce the concept of relative correctness, i.e. the property of a program to be more-correct than another with respect to a given specification. A feature of a program is a fault (for a given specification)only because there exists an alternative to it that would make the program more-correct with respect to the specification.In this paper, we explore applications of the concept of relative correctness in program testing, program repair, and program design.Specifically, we argue that in many situations of software testing, fault removal and program repair, testing for relative correctness rather than absolute correctness leads to clearer conclusions and better outcomes. Also, we find that designing programs by stepwise correctness-enhancing transformations rather than by stepwise correctness-preserving refinements leads to simpler programs and is more tolerant of designer mistakes.","conference":"IEEE","terms":"Maintenance engineering;Software;Semantics;Generators;Software engineering;Software testing,formal specification;program testing;software fault tolerance;software maintenance,relative correctness;software fault;formal specification;program testing;program repair;program design","keywords":"","startPage":"591","endPage":"594","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203020","citationCount":3,"referenceCount":13,"year":2015,"authors":"N. Diallo; W. Ghardallou; A. Mili","affiliations":"New Jersey Inst. of Technol., Newark, NJ, USA; Fac. of Sci. of Tunis, Univ. of Tunis El Manar, Tunis, Tunisia; New Jersey Inst. of Technol., Newark, NJ, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3448"},"title":"Load Testing Large-Scale Software Systems","abstract":"Large-scale software systems (e.g., Amazon and Dropbox) must be load tested to ensure that they can service thousands or millions of concurrent requests every day. In this technical briefing, we will describe the state of research and practices in the area of load testing. We will focus on the techniques used in the three phases of a load test: (1) designing a load test, (2) executing a load test, and (3) analyzing the results of a load test. This technical briefing is targeted at load testing practitioners and software engineering researchers interested in testing and analyzing the behavior of large-scale software systems.","conference":"IEEE","terms":"Testing;Software systems;Software engineering;Conferences;Computer science;Monitoring,program testing,Dropbox;Amazon;large-scale software system load testing","keywords":"load testing;software testing;performance;scalability","startPage":"955","endPage":"956","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203126","citationCount":0,"referenceCount":12,"year":2015,"authors":"Z. M. J. Jiang","affiliations":"Dept. of Electr. Eng. \u0026 Comput. Sci., York Univ., Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3449"},"title":"1st International Workshop on Big Data Software Engineering (BIGDSE 2015)","abstract":"Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. BIGDSE 2015 discusses the link between Big Data and software engineering and critically looks into issues such as cost-benefit of big data.","conference":"IEEE","terms":"Big data;Software engineering;Data mining;Conferences;Software systems;Market research,,","keywords":"","startPage":"965","endPage":"966","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203130","citationCount":0,"referenceCount":2,"year":2015,"authors":"L. Baresi; T. Menzies; A. Metzger; T. Zimmermann","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d344a"},"title":"Software Engineering in Ferrari F1","abstract":"Summary form only given. The software and hardware development in Ferrari F1 is characterized by a very short cycle time. Typically during the in-season development, the fixes and new developments need to be addressed in few days, in order to be ready for the following race. At the same time the hardware, like new electronic control units or new devices need to be developed from one year to the other. In this scenario the validation procedures are very critical, because of the need to achieve the same results in a shorter time.","conference":"IEEE","terms":"Software;Hardware;Software engineering;Conferences;Biographies;Inverters;Batteries,software engineering,software engineering;Ferrari F1;software development;hardware development;in-season development;electronic control units","keywords":"","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194555","citationCount":1,"referenceCount":0,"year":2015,"authors":"C. Silenzi","affiliations":"Scuderia, Ferrari, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d344b"},"title":"JRebel.Android: Runtime Class- and Resource Reloading for Android","abstract":"Developers writing Android applications suffer from a dreadful redeploy time every time they need to test changes to the source code. While runtime class reloading systems are widely used for the underlying programming language, Java, there is currently no support for reloading code on the Android platform. This paper presents a new tool, JRebel.Android that enables automatic runtime class- and resource reloading capabilities for Android. The target of this paper is the Android developer as well as the researcher for which dynamic updating capabilities on mobile devices can serve as a basic building block within areas such as runtime maintenance or self-adaptive systems. JRebel.Android is able to reload classes in much less than 1 second, saving more than 91% of the total redeploy time for small apps, more than 95% for medium size apps, and even more for larger apps.","conference":"IEEE","terms":"Androids;Humanoid robots;Java;Runtime;Standards;Google;Instruments,Android (operating system);Java;software tools;source code (software),JRebel.Android tool;runtime class-resource reloading system;source code;programming language;Java;dynamic updating capability;mobile devices;self-adaptive systems;runtime maintenance","keywords":"Class Reloading;Android;Dynamic Software Updating;Resource Reloading","startPage":"741","endPage":"744","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203057","citationCount":0,"referenceCount":11,"year":2015,"authors":"R. Raudjärv; A. R. Gregersen","affiliations":"R\u0026D Dept., Zeroturnaround OU, Tartu, Estonia; R\u0026D Dept., Zeroturnaround OU, Tartu, Estonia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d344c"},"title":"Fast and Precise Statistical Code Completion","abstract":"The main problem we try to solve is API code completion which is both precise and works in real-time. We describe an efficient implementation of an N-gram language model combined with several smoothing methods and a completion algorithm based on beam search. We show that our system is both fast and precise using a thorough experimental evaluation. With optimal parameters we are able to find completions in milliseconds and the desired completion is in the top 3 suggestions in 89% of the time.","conference":"IEEE","terms":"Smoothing methods;Runtime;Computational modeling;Real-time systems;Libraries;Probability;Semantics,application program interfaces;programming languages;statistical analysis,statistical code completion;API code completion;N-gram language model;smoothing methods;completion algorithm;beam search;experimental evaluation","keywords":"Statistical Language Models;Code Completion;Synthesis;APIs","startPage":"757","endPage":"759","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203061","citationCount":0,"referenceCount":16,"year":2015,"authors":"P. Roos","affiliations":"ETH Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d344d"},"title":"The Art and Science of Analyzing Software Data; Quantitative Methods","abstract":"Using the tools of quantitative data science, software engineers that can predict useful information on new projects based on past projects. This tutorial reflects on the state-of-the-art in quantitative reasoning in this important field. This tutorial discusses the following: (a) when local data is scarce, we show how to adapt data from other organizations to local problems; (b) when working with data of dubious quality, we show how to prune spurious information; (c) when data or models seem too complex, we show how to simplify data mining results; (d) when the world changes, and old models need to be updated, we show how to handle those updates; (e) when the effect is too complex for one model, we show to how reason over ensembles.","conference":"IEEE","terms":"Software;Software engineering;Computer science;Tutorials;Data mining;Data models;Art,data analysis;data mining;software quality,software data analysis;quantitative methods;quantitative data science tools;software engineers;data mining","keywords":"","startPage":"959","endPage":"960","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203128","citationCount":2,"referenceCount":4,"year":2015,"authors":"T. Menzies; L. Minku; F. Peters","affiliations":"Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Comptuer Sci., Univ. of Birmingham, Birmingham, UK; Lero, Univ. of Limerick, Limerick, Ireland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d344e"},"title":"8th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE 2015)","abstract":"Software is created for and with a wide range of stakeholders, from customers to management, from value-added providers to customer service personnel. These stakeholders work with teams of software engineers to develop and evolve software systems that support their activities. All of these people and their interactions are central to software development. Thus, it is crucial to investigate the dynamic and frequently changing Cooperative and Human Aspects of Software Engineering (CHASE), both before and after deployment, in order to understand current software practices, processes, and tools. In turn, this enables us to design tools and support mechanisms that improve software creation, software maintenance, and customer communication.Researchers and practitioners have long recognized the need to investigate these aspects, however, their articles are scattered across conferences and communities. This workshop will provide a unified forum for discussing high quality research studies, models, methods, and tools for human and cooperative aspects of software engineering. This will be the 8th in a series of workshops, which continue to be a meeting place for the academic, industrial, and practitioner communities interested in this area, and will give opportunities to present and discuss works-in-progress.","conference":"IEEE","terms":"Software;Software engineering;Conferences;Stakeholders;Teamwork;Committees,,","keywords":"Management;Performance;Human Factors","startPage":"969","endPage":"970","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203132","citationCount":0,"referenceCount":0,"year":2015,"authors":"A. Begel; R. Prikladnicki; Y. Dittrich; C. R. B. d. Souza; A. Sarma; S. Athavale","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d344f"},"title":"Smart Programming Playgrounds","abstract":"Modern IDEs contain sophisticated components for inferring missing types, correcting bad syntax and completing partial expressions in code, but they are limited to the context that is explicitly defined in a project's configuration. These tools are ill-suited for quick prototyping of incomplete code snippets, such as those found on the Web in Q\u0026A forums or walk-through tutorials, since such code snippets often assume the availability of external dependencies and may even contain implicit references to an execution environment that provides data or compute services. We propose an architecture for smart programming playgrounds that can facilitate rapid prototyping of incomplete code snippets through a semi-automatic context resolution that involves identifying static dependencies, provisioning external resources on the cloud and injecting resource bindings to handles in the original code fragment. Such a system could be potentially useful in a range of different scenarios, from sharing code snippets on the Web to experimenting with new ideas during traditional software development.","conference":"IEEE","terms":"Context;Databases;Programming;IEEE catalogs;Libraries;Graphical user interfaces;Engines,Internet;programming environments;software architecture;software prototyping,smart programming playgrounds;IDEs;incomplete code snippet prototyping;semiautomatic context resolution;cloud computing;code fragment;software development;integrated development environments","keywords":"playgrounds;code snippets;dependency injection;cloud computing","startPage":"607","endPage":"610","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203024","citationCount":0,"referenceCount":12,"year":2015,"authors":"R. Padhye; P. Dhoolia; S. Mani; V. S. Sinha","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3450"},"title":"Mining Temporal Properties of Data Invariants","abstract":"System specifications are important in maintaining program correctness, detecting bugs, understanding systems and guiding test case generation. Often, these specifications are not explicitly written by developers. If we want to use them for analysis, we need to obtain them through other methods; for example, by mining them out of program behavior. Several tools exist to mine data invariants and temporal properties from program traces, but few examine the temporal relationships between data invariants. An example of this kind of relationship would be \"the return value of the method isFull? is false until the field size reaches the value capacity\". We propose a data-temporal property miner, Quarry, which mines Linear Temporal Logic (LTL) relations of arbitrary length and complexity between Daikon-style data invariants. We infer data invariants from systems using Daikon, recompose these data invariants into sequences, and mine temporal properties over these sequences. Our preliminary results suggest that this method may recover important system properties.","conference":"IEEE","terms":"Data mining;Software engineering;Conferences;Context;Software;Hardware;Instruments,computational complexity;data mining;program debugging;program verification;temporal logic,temporal property mining;data invariants;system specifications;program correctness;bug detection;test case generation;program behavior;program traces;isFull method;value capacity;data-temporal property miner;Quarry;linear temporal logic relation mining;LTL relation mining;Daikon-style data invariants;Daikon","keywords":"","startPage":"751","endPage":"753","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203059","citationCount":2,"referenceCount":21,"year":2015,"authors":"C. Lemieux","affiliations":"Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3451"},"title":"Enabling Testing of Android Apps","abstract":"Existing approaches for automated testing of An- droid apps are designed to achieve different goals and exhibit some pros and cons that should be carefully considered by developers and testers. For instance, random testing (RT) provides a high ratio of infeasible inputs or events, and test cases generated with RT and systematic exploration-based testing (SEBT) are not representative of natural (i.e., real) application usage scenarios. In addition, collecting test scripts for automated testing is expensive. We address limitations of existing tools for GUI-based testing of Android apps in a novel hybrid approach called T+. Our approach is based on a novel framework, which is aimed at generating actionable test cases for different testing goals. The framework also enables GUI-based testing without expensive test scripts collection for the stakeholders.","conference":"IEEE","terms":"Testing;Androids;Humanoid robots;Graphical user interfaces;Vocabulary;Software engineering;Systematics,Android (operating system);graphical user interfaces;program testing,testing goal;actionable test case generation;T+ approach;GUI-based testing;automated testing;test script collection;natural application usage scenario;SEBT;systematic exploration-based testing;random testing;Android apps testing","keywords":"","startPage":"763","endPage":"765","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203063","citationCount":6,"referenceCount":14,"year":2015,"authors":"M. Linares-Vásquez","affiliations":"Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3452"},"title":"2nd International Workshop on Context for Software Development (CSD 2015)","abstract":"The goal of this one-day workshop is to bring together researchers interested in techniques and tools that leverage context information that accumulates around development activities. Developers continuously make use of context to make decisions, coordinate their work, understand the purpose behind their tasks, and understand how their tasks fit with the rest of the project. However, there is little research on defining what context is, how we can model it, and how we can use those models to better support software development at large. This workshop brings together scholars interested in identifying, gathering and modelling context information in software development, as well as discussing its applications.","conference":"IEEE","terms":"Context;Software;Conferences;Software engineering;Context modeling;Computer science;Committees,,","keywords":"","startPage":"973","endPage":"974","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203134","citationCount":0,"referenceCount":3,"year":2015,"authors":"K. Blincoe; D. Damian; G. Valetto; J. D. Herbsleb","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3453"},"title":"Code Repurposing as an Assessment Tool","abstract":"Code repurposing is often used for system development and to learn both APIs and techniques. Repurposing code typically requires that you understand the code first. This makes it an excellent candidate as an assessment tool in computer science and software engineering education. This technique might have a special application in combatting plagiarism. This paper discusses experiences using code repurposing as an assessment tool in different courses and with different sections.","conference":"IEEE","terms":"Plagiarism;Standards;Software;Programming profession;Education;Joints,computer science education;programming,code repurposing;system development;API;application program interface;code understanding;computer science education;software engineering education;plagiarism","keywords":"Assessment;Code Repurposing;Plagiarism Softwaredevelopment;Cloze Testing","startPage":"295","endPage":"298","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202975","citationCount":2,"referenceCount":7,"year":2015,"authors":"J. Sant","affiliations":"Fac. of Appl. Sci. \u0026 Technol., Sheridan Coll., Oakville, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3454"},"title":"Qualitative Analysis of Knowledge Transfer in Pair Programming","abstract":"Knowledge transfer in the context of pair programming is both a desired effect and a necessary precondition. There is no detailed understanding yet of how effective and efficient knowledge transfer in this particular context actually works. My qualitative research is concerned with the analysis of professional software developer's sessions to capture their specific knowledge transfer skill in the form of comprehensible, relevant, and practical patterns.","conference":"IEEE","terms":"Programming profession;Knowledge transfer;Software;Software engineering;Context;Propulsion,knowledge management;professional aspects;software prototyping,qualitative analysis;pair programming;professional software developer session analysis;knowledge transfer skill","keywords":"","startPage":"855","endPage":"858","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203098","citationCount":1,"referenceCount":16,"year":2015,"authors":"F. Zieris","affiliations":"Freie Univ. Berlin, Berlin, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3455"},"title":"Evolution-Aware Monitoring-Oriented Programming","abstract":"Monitoring-Oriented Programming (MOP) helps develop more reliable software by means of monitoring against formal specifications. While MOP showed promising results, all prior research has focused on checking a single version of software. We propose to extend MOP to support multiple software versions and thus be more relevant in the context of rapid software evolution. Our approach, called eMOP, is inspired by regression test selection -- a well studied, evolution-centered technique. The key idea in eMOP is to monitor only the parts of code that changed between versions. We illustrate eMOP by means of a running example, and show the results of preliminary experiments. eMOP opens up a new line of research on MOP -- it can significantly improve usability and performance when applied across multiple versions of software and is complementary to algorithmic MOP advances on a single version.","conference":"IEEE","terms":"Monitoring;Runtime;Testing;Open source software;Java;Programming,configuration management;formal specification;program testing;program verification;software maintenance;software prototyping;software reliability,evolution-aware monitoring-oriented programming;software reliability;formal specification;software checking;software versions;rapid software evolution;eMOP;regression test selection;evolution-centered technique;code monitoring","keywords":"Runtime Verification;Regression Testing;Runtime Monitoring;Monitoring-Oriented Programming","startPage":"615","endPage":"618","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203026","citationCount":4,"referenceCount":23,"year":2015,"authors":"O. Legunsen; D. Marinov; G. Rosu","affiliations":"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3456"},"title":"Commit Bubbles","abstract":"Developers who use version control are expected to produce systematic commit histories that show well-defined steps with logical forward progress. Existing version control tools assume that developers also write code systematically. Unfortunately, the process by which developers write source code is often evolutionary, or as-needed, rather than systematic. Our contribution is a fragment-oriented concept called Commit Bubbles that will allow developers to construct systematic commit histories that adhere to version control best practices with less cognitive effort, and in a way that integrates with their as-needed coding workflows.","conference":"IEEE","terms":"History;Systematics;Context;Encoding;Switches;Best practices;Software,configuration management;software tools;source code (software),commit bubbles;systematic commit histories;version control tools;source code;fragment-oriented concept;as-needed coding workflows","keywords":"version control;integrated development environments;software maintenance;software tools","startPage":"631","endPage":"634","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203030","citationCount":3,"referenceCount":22,"year":2015,"authors":"T. Barik; K. Lubick; E. Murphy-Hill","affiliations":"ABB Corp. Res., Raleigh, NC, USA; North Carolina State Univ., Raleigh, NC, USA; North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3457"},"title":"Towards Model Driven Architecture and Analysis of System of Systems Access Control","abstract":"Nowadays there is growing awareness of the importance of Systems of Systems (SoS) which are large-scale systems composed of complex systems. SoS possess specific properties when compared with monolithic complex systems, in particular: operational independence, managerial independence, evolutionary development, emergent behavior and geographic distribution. One of the current main challenges is the impact of these properties on SoS security modeling and analysis. In this research proposal, we introduce a new method incorporating a process, a language and a software architectural tool to model, analyze and predict security architectural alternatives of SoS. Thus security will be taken into account as soon as possible in the life cycle of the SoS, making it less expensive.","conference":"IEEE","terms":"Security;Systems of systems;Unified modeling language;Computer architecture;Medical services;Conferences;Analytical models,authorisation;marine safety;safety-critical software;software architecture;systems engineering,model driven architecture;system-of-systems access control analysis;large-scale systems;complex systems;operational independence;managerial independence;evolutionary development;emergent behavior;geographic distribution;SoS security modeling;SoS security analysis;software architectural tool;SoS life cycle","keywords":"Model Driven Engineering;Maritime Security;Architectural Alternatives;Simulation.","startPage":"867","endPage":"870","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203101","citationCount":3,"referenceCount":19,"year":2015,"authors":"J. El Hachem","affiliations":"LIUPPA Lab., Univ. of PAU \u0026 Pays Adour, Mont-de-Marsan, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3458"},"title":"Textual Analysis for Code Smell Detection","abstract":"The negative impact of smells on the quality of a software systems has been empirical investigated in several studies. This has recalled the need to have approaches for the identification and the removal of smells. While approaches to remove smells have investigated the use of both structural and conceptual information extracted from source code, approaches to identify smells are based on structural information only. In this paper, we bridge the gap analyzing to what extent conceptual information, extracted using textual analysis techniques, can be used to identify smells in source code. The proposed textual-based approach for detecting smells in source code, coined as TACO (Textual Analysis for Code smell detectiOn), has been instantiated for detecting the Long Method smell and has been evaluated on three Java open source projects. The results indicate that TACO is able to detect between 50% and 77% of the smell instances with a precision ranging between 63% and 67%. In addition, the results show that TACO identifies smells that are not identified by approaches based on solely structural information.","conference":"IEEE","terms":"Software engineering;Conferences;Accuracy;Data mining;Software systems;Societies,information retrieval;Java,software systems;structural information;textual analysis techniques;TACO;textual analysis for code smell detection;long method smell;Java open source projects","keywords":"","startPage":"769","endPage":"771","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203065","citationCount":7,"referenceCount":32,"year":2015,"authors":"F. Palomba","affiliations":"Dept. of Manage. \u0026 Inf. Technol., Univ. of Salerno, Fisciano, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3459"},"title":"7th International Workshop on Modeling in Software Engineering (MiSE 2015)","abstract":"Models are an important tool in conquering the increasing complexity of modern software systems. Key industries are strategically directing their development environments towards more extensive use of modeling techniques. MiSE 2015 aimed to understand, through critical analysis, the current and future uses of models in the engineering of software-intensive systems. The MiSE workshop series has proven to be an effective forum for discussing modeling techniques from both the MDE and software engineering perspectives. An important goal of this workshop is to foster exchange between these two communities. In 2015 the focus was on considering the current state of tool support and the challenges that need to be addressed to improve the maturity of tools. There was also analysis of successful applications of modeling techniques in specific application domains, with attempts to determine how the participants' experiences can be carried over to other domains.","conference":"IEEE","terms":"Conferences;Analytical models;Software;Adaptation models;Software engineering;Electronic mail,,","keywords":"modeling;quality;software tools","startPage":"985","endPage":"986","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203140","citationCount":0,"referenceCount":0,"year":2015,"authors":"J. Gray; M. Chechik; V. Kulkarni; R. F. Paige","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d345a"},"title":"Optimising Energy Consumption of Design Patterns","abstract":"Software design patterns are widely used in software engineering to enhance productivity and maintainability.However, recent empirical studies revealed the high energy overhead in these patterns. Our vision is to automatically detect and transform design patterns during compilation for better energy efficiency without impacting existing coding practices. In this paper, we propose compiler transformations for two design patterns, Observer and Decorator, and perform an initial evaluation of their energy efficiency.","conference":"IEEE","terms":"Observers;Energy consumption;Optimization;Software;Computers;Transforms;Software engineering,power aware computing;program compilers;software maintenance,energy consumption optimisation;software design patterns;software engineering;maintainability;compilation;energy efficiency;coding practices;compiler transformations","keywords":"","startPage":"623","endPage":"626","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203028","citationCount":15,"referenceCount":20,"year":2015,"authors":"A. Noureddine; A. Rajan","affiliations":"Sch. of Inf., Univ. of Edinburgh, Edinburgh, UK; Sch. of Inf., Univ. of Edinburgh, Edinburgh, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d345b"},"title":"Safe Evolution Patterns for Software Product Lines","abstract":"Despite a global recognition of the problem, and massive investment from researchers and practitioners, the evolution of complex software systems is still a major challenge for today's architects and developers. In the context of product lines, or highly configurable systems, variability in the implementation and design makes many of the pre-existing challenges even more difficult to tackle. Many approaches and tools have been designed, but developers still miss the tools and methods enabling safe evolution of complex, variable systems. In this paper, we present our research plans toward this goal: making the evolution of software product lines safer. We show, by use of two concrete examples of changes that occurred in Linux, that simple heuristics can be applied to facilitate change comprehension and avoid common mistakes, without relying on heavy tooling. Based on those observations, we present the steps we intend to take to build a framework to regroup and classify changes, run simple checks, and eventually increase the quality of code deliveries affecting the variability model, mapping and implementation of software product lines.","conference":"IEEE","terms":"Linux;Kernel;Frequency modulation;Software product lines;Context;Conferences;Feature extraction,software maintenance;software product lines;software quality,evolution patterns;software product lines;software systems evolution;Linux;change comprehension;code delivery quality;variability model","keywords":"Product line;evolution;variability","startPage":"875","endPage":"878","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203103","citationCount":0,"referenceCount":17,"year":2015,"authors":"N. Dintzner","affiliations":"Software Eng. Res. Group, Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d345c"},"title":"Understanding Conflicts Arising from Collaborative Development","abstract":"When working in a collaborative development environment, developers implement tasks separately. Consequently, during the integration process, one might have to deal with conflicting changes. Previous studies indicate that conflicts occur frequently and impair developers' productivity. Such evidence motivates the development of tools that try to tackle this problem. However, despite the existing evidence, there are still many unanswered questions. The goal of this research is to investigate conflict characteristics in practice through empirical studies and use this body of knowledge to improve strategies that support software developers working collaboratively.","conference":"IEEE","terms":"Collaboration;Software engineering;Syntactics;Crystals;Semantics;Conferences;Productivity,groupware;software engineering,collaborative development;conflict characteristics;software developers","keywords":"Collaborative Software Development;Conflicts;Empirical Studies","startPage":"775","endPage":"777","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203067","citationCount":1,"referenceCount":8,"year":2015,"authors":"P. Accioly","affiliations":"Inf. Center, Fed. Univ. of Pernambuco, Recife, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d345d"},"title":"Post-Dominator Analysis for Precisely Handling Implicit Flows","abstract":"Most web applications today use JavaScript for including third-party scripts, advertisements etc., which pose a major security threat in the form of confidentiality and integrity violations. Dynamic information flow control helps address this issue of information stealing. Most of the approaches over-approximate when unstructured control flow comes into picture, thereby raising a lot of false alarms. We utilize the post-dominator analysis technique to determine the context of the program at a given point and prove that this approach is the most precise technique to handle implicit flows.","conference":"IEEE","terms":"Security;Context;Conferences;Computer languages;Software engineering;Lattices;Programmable logic arrays,authoring languages;Java;program diagnostics;security of data,JavaScript;security threat;Web applications;integrity violations;confidentiality violations;dynamic information flow control;unstructured control flow;post-dominator analysis technique;implicit flow handling","keywords":"","startPage":"787","endPage":"789","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203071","citationCount":0,"referenceCount":13,"year":2015,"authors":"A. Bichhawat","affiliations":"Saarland Univ., Saarbrucken, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d345e"},"title":"4th International Workshop on Green and Sustainable Software (GREENS 2015)","abstract":"Engineering green software-intensive systems is critical in our drive towards a sustainable, smarter planet. The goal of green software engineering is to apply green principles to the design and operation of software-intensive systems. Green and self-greening software systems have tremendous potential to decrease energy consumption. Moreover, enterprise software can and should be re-thought to address sustainability issues using innovative business models, processes, and incentives. Monitoring and measuring the greenness of software is critical towards the notion of sustainable and green software. Demonstrating improvement is paramount for users to achieve and affect change. Thus, the theme of GREENS 2015 is Towards a Green Software Body of Knowledge. The GREENS workshop series brings together researchers and practitioners to discuss both the state-of-the-art and state-of-the-practice in green software, including novel ideas, research challenges, methods, experiences, and tools to support the engineering of sustainable and energy efficient software systems.","conference":"IEEE","terms":"Green products;Software engineering;Conferences;Industries;Software systems;Energy consumption,,","keywords":"Green software engineering;green design;key green indicators (KGIs);green monitoring;green adaptation;smart green sensors and actuators;self-greening;energy efficiency;sustainability;green scheduling;green computing;green IT","startPage":"981","endPage":"982","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203138","citationCount":0,"referenceCount":0,"year":2015,"authors":"M. Morisio; P. Lago; N. Meyer; H. A. Müller; G. Scanniello","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d345f"},"title":"5th International Workshop on Product LinE Approaches in Software Engineering PLE for a Sustainable Society (PLEASE 2015)","abstract":"This paper summarizes the motivation, objectives, and format of the 5th International Workshop on Product LinE Approaches in Software Engineering (PLEASE15). The main goal of the PLEASE workshop series is to encourage and promote the adoption of Software Product Line Engineering. This year's edition focuses on the link between software product line engineering (SPLE) and new challenges posed by emerging societal trends. Towards this end, we invited reports on (1) opportunities posed by societal challenges for SPLE research and practice and (2) concrete solutions exemplifying application of SPLE techniques to societal challenges.","conference":"IEEE","terms":"Conferences;Software product lines;Software;Committees;Electronic mail;Concrete,,","keywords":"","startPage":"989","endPage":"990","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203142","citationCount":0,"referenceCount":5,"year":2015,"authors":"J. Rubin; G. Botterweck; A. Pleuss; D. Weiss","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3460"},"title":"Towards a Practical Security Analysis Methodology","abstract":"The research community has proposed numerous techniques to perform security-oriented analyses based on a software design model. Such a formal analysis can provide precise security guarantees to the software designer, and facilitate the discovery of subtle flaws. Nevertheless, using such techniques in practice poses a big challenge for the average software designer, due to the narrow scope of each technique, the heterogeneous set of modelling languages that are required, and the analysis results that are often hard to interpret. Within the course of our research, we intend to provide practitioners with an integrated, easy-to-use modelling and analysis environment that enables them to work on a broad range of common security concerns without leaving the software design's level of abstraction.","conference":"IEEE","terms":"Unified modeling language;Analytical models;Cryptography;Vocabulary;Software design,security of data;software engineering,security analysis methodology;security-oriented analysis;software design model;formal analysis;security guarantee;modelling languages;software designabstraction level","keywords":"","startPage":"883","endPage":"886","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203105","citationCount":0,"referenceCount":24,"year":2015,"authors":"A. v. d. Berghe","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3461"},"title":"Code Reviews Do Not Find Bugs. How the Current Code Review Best Practice Slows Us Down","abstract":"Because of its many uses and benefits, code reviews are a standard part of the modern software engineering workflow. Since they require involvement of people, code reviewing is often the longest part of the code integration activities. Using experience gained at Microsoft and with support of data, we posit (1) that code reviews often do not find functionality issues that should block a code submission; (2) that effective code reviews should be performed by people with specific set of skills; and (3) that the social aspect of code reviews cannot be ignored. We find that we need to be more sophisticated with our guidelines for the code review workflow. We show how our findings from code reviewing practice influence our code review tools at Microsoft. Finally, we assert that, due to its costs, code reviewing practice is a topic deserving to be better understood, systematized and applied to software engineering workflow with more precision than the best practice currently prescribes.","conference":"IEEE","terms":"Software engineering;Software;Inspection;Best practices;Standards;Guidelines;Switches,program compilers;program diagnostics;software engineering,code review tools;code review workflow;code submission;Microsoft;code integration;software engineering workflow;code review best practice;bug finding","keywords":"Software engineering workflow;code reviews;code integration","startPage":"27","endPage":"28","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202946","citationCount":15,"referenceCount":6,"year":2015,"authors":"J. Czerwonka; M. Greiler; J. Tilford","affiliations":"Microsoft Corp., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3462"},"title":"Deep Representations for Software Engineering","abstract":"Deep learning subsumes algorithms that automatically learn compositional representations. The ability of these models to generalize well has ushered in tremendous advances in many fields. We propose that software engineering (SE) research is a unique opportunity to use these transformative approaches. Our research examines applications of deep architectures such as recurrent neural networks and stacked restricted Boltzmann machines to SE tasks.","conference":"IEEE","terms":"Software;Computational modeling;Machine learning;Conferences;Software engineering;Computer architecture;Context,Boltzmann machines;learning (artificial intelligence);recurrent neural nets;software engineering,deep representation;software engineering;deep learning subsumes algorithm;compositional representation learning;transformative approach;deep architecture;recurrent neural networks;stacked restricted Boltzmann machine","keywords":"","startPage":"781","endPage":"783","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203069","citationCount":1,"referenceCount":45,"year":2015,"authors":"M. White","affiliations":"Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3463"},"title":"Poster: Static Detection of Configuration-Dependent Bugs in Configurable Software","abstract":"Configurable software systems enable developers to configure at compile time a single variant of the system to tailor it towards specific environments and features. Although traditional static analysis tools can assist developers in software development and maintenance, they can only run on a concrete configuration of a configurable software system. Thus, it is necessary to derive many configurations so that the configuration-specific parts of the source code can be checked. To avoid this tedious and error-prone process, we propose an approach to automatically derive a set of configurations that cover as many combinations of configuration-specific blocks of code or source files as possible. We represent a C program with CPP directives (e.g., #ifdef) with a CPP control-flow graph (CPP-CFG) in which CPP expressions are condition nodes and #ifdef blocks are statement nodes. We then explore possible paths on CPP-CFG with dynamic symbolic execution and depth-first search algorithms, and correspondingly, producing possible combinations of concrete blocks of C code, on which an existing static analysis tool can run. Our preliminary evaluation on a benchmark of configuration-dependent bugs on Linux shows that our approach can detect more bugs than a state-of-the-art tool.","conference":"IEEE","terms":"Computer bugs;Concrete;Linux;Heuristic algorithms;Kernel;Software systems,graph theory;program diagnostics;software maintenance;tree searching,static detection;configuration-dependent bugs;configurable software systems;static analysis tools;software development;software maintenance;configuration-specific blocks;C program;CPP directives;CPP control-flow graph;CPP-CFG;dynamic symbolic execution;depth-first search algorithms;Linux","keywords":"configuration-dependent bugs;configurable software;dynamic symbolic execution","startPage":"795","endPage":"796","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203073","citationCount":0,"referenceCount":4,"year":2015,"authors":"J. Al-Kofahi; L. Guo; H. V. Nguyen; H. A. Nguyen; T. N. Nguyen","affiliations":"Electr. \u0026 Comput. Eng. Dept., Iowa State Univ., Ames, IA, USA; UPMC Univ. Paris 06, Paris, France; Electr. \u0026 Comput. Eng. Dept., Iowa State Univ., Ames, IA, USA; Electr. \u0026 Comput. Eng. Dept., Iowa State Univ., Ames, IA, USA; Electr. \u0026 Comput. Eng. Dept., Iowa State Univ., Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3464"},"title":"Bootstrapping Mobile App Development","abstract":"Modern IDEs provide limited support for developers when starting a new data-driven mobile app. App developers are currently required to write copious amounts of boilerplate code, scripts, organise complex directories, and author actual functionality. Although this scenario is ripe for automation, current tools are yet to address it adequately. In this paper we present RAPPT, a tool that generates the scaffolding of a mobile app based on a high level description specified in a Domain Specific Language (DSL). We demonstrate the feasibility of our approach by an example case study and feedback from a professional development team. Demo at: https://www.youtube.com/watch?v=ffquVgBYpLM.","conference":"IEEE","terms":"Androids;Humanoid robots;Mobile communication;DSL;Motion pictures;Software engineering;Productivity,application program interfaces;mobile computing;specification languages,DSL;domain specific language;RAPPT tool;data-driven mobile app;integrated development environment;IDE;mobile application development","keywords":"Model Driven Development;Code Generation;Mobile App Prototyping","startPage":"657","endPage":"660","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203036","citationCount":10,"referenceCount":11,"year":2015,"authors":"S. Barnett; R. Vasa; J. Grundy","affiliations":"Centre for Comput. \u0026 Eng. Software Syst. \u0026 Software Innovation Lab., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Centre for Comput. \u0026 Eng. Software Syst. \u0026 Software Innovation Lab., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Centre for Comput. \u0026 Eng. Software Syst. \u0026 Software Innovation Lab., Swinburne Univ. of Technol., Melbourne, VIC, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3465"},"title":"scvRipper: Video Scraping Tool for Modeling Developers' Behavior Using Interaction Data","abstract":"Screen-capture tool can record a user's interaction with software and application content as a stream of screenshots which is usually stored in certain video format. Researchers have used screen-captured videos to study the programming activities that the developers carry out. In these studies, screen-captured videos had to be manually transcribed to extract software usage and application content data for the study purpose. This paper presents a computer-vision based video scraping tool (called scvRipper) that can automatically transcribe a screen-captured video into time-series interaction data according to the analyst's need. This tool can address the increasing need for automatic behavioral data collection methods in the studies of human aspects of software engineering.","conference":"IEEE","terms":"Software;Visualization;Data mining;Graphical user interfaces;Computational modeling;Instruments,computer vision;user interfaces;video signal processing,scvRipper;developer behavior modelling;interaction data;screen-capture tool;user interaction;screen-captured videos;computer-vision based video scraping tool;time-series interaction data;automatic behavioral data collection methods;software engineering;application content data","keywords":"Video Scraping;Interaction Data","startPage":"673","endPage":"676","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203040","citationCount":3,"referenceCount":16,"year":2015,"authors":"L. Bao; J. Li; Z. Xing; X. Wang; B. Zhou","affiliations":"Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3466"},"title":"Mining Patterns of Sensitive Data Usage","abstract":"When a user downloads an Android application from a market, she does not know much about its actual behavior. A brief description, a set of screenshots, and the list of permissions, which give a high level intuition of what the application might be doing, are all the user sees before installing and running the application on his device. These elements are not enough to decide whether the application is secure, and for sure they do not indicate whether it might violate the user's privacy by leaking some sensitive data. The goal of my thesis is to employ both static and dynamic taint analyses to gather information on how Android applications use sensitive data. The main hypothesis of this work is that malicious and benign mobile applications differ in how they use sensitive data, and consequently information flow can be used effectively to identify malware.","conference":"IEEE","terms":"Androids;Humanoid robots;Data mining;Malware;Software engineering;Medical services,Android (operating system);data mining;data privacy;invasive software;mobile computing;program diagnostics,pattern mining;sensitive data usage pattern;Android application;application security;user privacy;static taint analysis;dynamic taint analysis;mobile applications;malware identification","keywords":"","startPage":"891","endPage":"894","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203107","citationCount":1,"referenceCount":14,"year":2015,"authors":"V. Avdiienko","affiliations":"Software Eng. Dept., Saarland Univ., Saarbrucken, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3467"},"title":"A Declarative Foundation for Comprehensive History Querying","abstract":"Researchers in the field of Mining Software Repositories perform studies about the evolution of software projects. To this end, they use the version control system storing the changes made to a single software project. Such studies are concerned with the source code characteristics in one particular revision, the commit data for that revision, how the code evolves over time and what concrete, fine-grained changes were applied to the source code between two revisions. Although tools exist to analyse an individual concern, scripts and manual work is required to combine these tools to perform a single experiment. We present a general-purpose history querying tool named QwalKeko that enables expressing these concerns in a single uniform language, and having them detected in a git repository. We have validated our work by means of replication studies as well as through MSR studies of our own.","conference":"IEEE","terms":"Software;History;Database languages;Libraries;Programming;Medical services;Java,configuration management;data mining;project management;software maintenance;source code (software),declarative foundation;comprehensive history querying;software repository mining;software project evolution;version control system;source code characteristics;general-purpose history querying tool;QwalKeko;single uniform language;git repository","keywords":"declarative programming;program querying;history querying;mining software repositories","startPage":"907","endPage":"910","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203111","citationCount":3,"referenceCount":19,"year":2015,"authors":"R. Stevens","affiliations":"Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3468"},"title":"Poster: Interactive and Collaborative Source Code Annotation","abstract":"Software documentation plays an important role in sharing the knowledge behind source code between distributed programmers. Good documentation makes source code easier to understand; on the other hand, developers have to constantly update the documentation whenever the source code changes. Developers will benefit from an automated tool that simplifies keeping documentation up-to-date and facilitates collaborative editing. In this paper, we explore the concept of collaborative code annotation by combining the idea from crowdsourcing. We introduce Cumiki, a web-based collaborative annotation tool that makes it easier for crowds of developers to collaboratively create the up-to-date documentation. This paper describes the user interface, the mechanism, and its implementation, and discusses the possible usage scenarios.","conference":"IEEE","terms":"Documentation;Collaboration;Crowdsourcing;Software;Software engineering;User interfaces;Programming,distributed programming;Internet;software tools;source code (software);system documentation,collaborative source code annotation;interactive source code annotation;software documentation;distributed programmers;collaborative editing;crowdsourcing;Web-based collaborative annotation tool;Cumiki tool;user interface","keywords":"","startPage":"799","endPage":"800","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203075","citationCount":2,"referenceCount":7,"year":2015,"authors":"R. Suzuki","affiliations":"Univ. of Tokyo, Tokyo, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3469"},"title":"2nd International Workshop on Requirements Engineering and Testing (RET 2015)","abstract":"The RET (Requirements Engineering and Testing) workshop provides a meeting point for researchers and practitioners from the two separate fields of Requirements Engineering (RE) and Testing. The goal is to improve the connection and alignment of these two areas through an exchange of ideas, challenges, practices, experiences and results. The long term aim is to build a community and a body of knowledge within the intersection of RE and Testing. One of the main outputs of the 1st workshop was a collaboratively constructed map of the area of RET showing the topics relevant to RET for these. The 2nd workshop will continue in the same interactive vein and include a keynote, paper presentations with ample time for discussions, and a group exercise. For true impact and relevance this cross-cutting area requires contribution from both RE and Testing, and from both researchers and practitioners. For that reason we welcome a range of paper contributions from short experience papers to full research papers that both clearly cover connections between the two fields.","conference":"IEEE","terms":"Testing;Conferences;Requirements engineering;Software engineering;Software;Industries;Joining processes,,","keywords":"requirements engineering;testing;alignment","startPage":"997","endPage":"998","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203146","citationCount":1,"referenceCount":4,"year":2015,"authors":"E. Bjarnason; M. Morandini; M. Borg; M. Unterkalmsteiner; M. Felderer; M. Staats","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d346a"},"title":"2nd International Workshop on Software Engineering Methods in Spreadsheets (SEMS 2015)","abstract":"Spreadsheets are heavily used in industry, becausethey are easily written and adjusted, using an intuitive visual interface. They often start out as simple tools; however, over time spreadsheets can become increasingly complex, up to the point where they become complicated and inflexible. In many ways, spreadsheet are similar to software: both concern the storage and manipulation of data and the presentation of results to the user. Because of this similarity, many methods and techniques from software engineering can be applied to spreadsheets. The role of SEMS, the International Workshop on Software Engineering Methods in Spreadsheets is to explore the possibilities of applying successful methods from software engineering to spreadsheets. Some, like testing and visualization, have been tried before and can be built upon. For methods that have not yet been tried on spreadsheets, SEMS will serve as a platform for early feedback. The SEMS program included an industrial keynote, \"spreadsheet stories\" (success or failure), short and long research papers,a good mix of industrial and academic researchers, as well as lively discussion and debate.","conference":"IEEE","terms":"Conferences;Software engineering;Industries;Committees;Electronic mail;Software;Testing,,","keywords":"","startPage":"1005","endPage":"1006","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203150","citationCount":0,"referenceCount":0,"year":2015,"authors":"F. Hermans; R. F. Paige; P. Sestoft","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d346b"},"title":"The ECCO Tool: Extraction and Composition for Clone-and-Own","abstract":"Software reuse has become mandatory for companies to compete and a wide range of reuse techniques are available today. However, ad hoc practices such as copying existing systems and customizing them to meet customer-specific needs are still pervasive, and are generically called clone-and-own. We have developed a conceptual framework to support this practice named ECCO that stands for Extraction and Composition for Clone-and-Own. In this paper we present our Eclipse-based tool to support this approach. Our tool can automatically locate reusable parts from previously developed products and subsequently compose a new product from a selection of desired features. The tools demonstration video can be found here: http://youtu.be/N6gPekuxU6o.","conference":"IEEE","terms":"Software;Feature extraction;Data mining;Conferences;Software engineering;Systematics;Distance measurement,software product lines,ECCO tool;software reuse;customer-specific needs;extraction-and-composition-for-clone-and-own;Eclipse-based tool;feature selection;software product lines","keywords":"product variants;clone-and-own;reuse;features;feature interactions","startPage":"665","endPage":"668","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203038","citationCount":12,"referenceCount":9,"year":2015,"authors":"S. Fischer; L. Linsbauer; R. E. Lopez-Herrejon; A. Egyed","affiliations":"Johannes Kepler Univ. Linz, Linz, Austria; Johannes Kepler Univ. Linz, Linz, Austria; Johannes Kepler Univ. Linz, Linz, Austria; Johannes Kepler Univ. Linz, Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d346c"},"title":"Automated Program Repair in an Integrated Development Environment","abstract":"We present the integration of the AutoFix automated program repair technique into the EiffelStudio Development Environment. AutoFix presents itself like a recommendation system capable of automatically finding bugs and suggesting fixes in the form of source-code patches. Its performance suggests usage scenarios where it runs in the background or during work interruptions, displaying fix suggestions as they become available. This is a contribution towards the vision of semantic Integrated Development Environments, which offer powerful automated functionality within interfaces familiar to developers. A screencast highlighting the main features of AutoFix can be found at: http://youtu.be/Ff2ULiyL-80.","conference":"IEEE","terms":"Maintenance engineering;Contracts;Heuristic algorithms;Computer bugs;Testing;Algorithm design and analysis;Semantics,program diagnostics;software maintenance;source code (software),AutoFix automated program repair technique;EiffelStudio development environment;recommendation system;source-code patches;semantic integrated development environments","keywords":"","startPage":"681","endPage":"684","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203042","citationCount":5,"referenceCount":12,"year":2015,"authors":"Y. Pei; C. A. Furia; M. Nordio; B. Meyer","affiliations":"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d346d"},"title":"Towards Generation of Software Development Tasks","abstract":"The presence of well defined fine-grained sub-tasks is important to the development process: having a fine-grained task context has been shown to allow developers to more efficiently resume work. However, determining how to break a high level task down into sub-tasks is not always straightforward. Sometimes developers lack experience, and at other times, the task definition is not clear enough to afford confident decomposition. In my research I intend to show that by using syntactic mining of past task descriptions and their decomposition, I can provide automatically derived sub-task suggestions to afford more confident task decomposition by developers.","conference":"IEEE","terms":"Software;Data mining;Programming;Logic gates;Electronic mail;Stress;Medical services,data mining;software engineering,software development tasks;fine-grained sub-tasks;syntactic mining","keywords":"","startPage":"915","endPage":"918","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203113","citationCount":0,"referenceCount":16,"year":2015,"authors":"C. A. Thompson","affiliations":"Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d346e"},"title":"Poster: Filtering Code Smells Detection Results","abstract":"Many tools for code smell detection have been developed, providing often different results. This is due to the informal definition of code smells and to the subjective interpretation of them. Usually, aspects related to the domain, size, and design of the system are not taken into account when detecting and analyzing smells. These aspects can be used to filter out the noise and achieve more relevant results. In this paper, we propose different filters that we have identified for five code smells. We provide two kind of filters, Strong and Weak Filters, that can be integrated as part of a detection approach.","conference":"IEEE","terms":"Libraries;Matched filters;Surgery;Couplings;Accuracy;Information filters,software reliability,code smell detection;subjective code smell interpretation;code smell filtering;strong filters;weak filters","keywords":"","startPage":"803","endPage":"804","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203077","citationCount":8,"referenceCount":3,"year":2015,"authors":"F. Arcelli Fontana; V. Ferme; M. Zanoni","affiliations":"Dept. of Inf., Syst. \u0026 Commun., Univ. of Milano-Bicocca, Milan, Italy; Fac. of Inf., Univ. of Lugano (USI), Lugano, Switzerland; Dept. of Inf., Syst. \u0026 Commun., Univ. of Milano-Bicocca, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d346f"},"title":"Poster: Automatically Fixing Real-World JavaScript Performance Bugs","abstract":"Programs often suffer from poor performance that can be fixed by relatively simple changes. Currently, developers either manually identify and fix such performance problems, or they rely on compilers to optimize their code. Unfortunately, manually fixing performance bugs is non-trivial, and compilers are limited to a predefined set of optimizations. This paper presents an approach for automatically finding and fixing performance bugs in JavaScript programs. To focus our work on relevant problems, we study 37 real-world performance bug fixes from eleven popular JavaScript projects and identify several recurring fix patterns. Based on the results of the study, we present a static analysis that identifies occurrences of common fix patterns and a fix generation technique that proposes to transform a given program into a more efficient program. Applying the fix generation technique to three libraries with known performance bugs yields fixes that are equal or equivalent to those proposed by the developers, and that lead to speedups between 10% and 25%.","conference":"IEEE","terms":"Computer bugs;Reactive power;Optimization;Maintenance engineering;Semantics;Transforms;Libraries,Java;program debugging;program diagnostics,compilers;JavaScript programs;static analysis;common fix patterns;fix generation technique;real-world performance bugs","keywords":"","startPage":"811","endPage":"812","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203081","citationCount":2,"referenceCount":6,"year":2015,"authors":"M. Selakovic; M. Pradel","affiliations":"Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany; Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3470"},"title":"1st International Workshop on Software Engineering for Smart Cyber-Physical Systems (SEsCPS 2015)","abstract":"Cyber-physical system (CPS) have been recognized as a top-priority in research and development. The innovations sought for CPS demand them to deal effectively with dynamicity of their environment, to be scalable, adaptive, tolerant to threats, etc. -- i.e. they have to be smart. Although approaches in software engineering (SE) exist that individually meet these demands, their synergy to address the challenges of smart CPS (sCPS) in a holistic manner remains an open challenge. The workshop focuses on software engineering challenges for sCPS. The goals are to increase the understanding of problems of SE for sCPS, study foundational principles for engineering sCPS, and identify promising SE solutions for sCPS. Based on these goals, the workshop aims to formulate a research agenda for SE of sCPS.","conference":"IEEE","terms":"Conferences;Software engineering;Cyber-physical systems;Software;Adaptation models;Committees;Technological innovation,,","keywords":"","startPage":"1009","endPage":"1010","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203152","citationCount":3,"referenceCount":8,"year":2015,"authors":"T. Bures; D. Weyns; M. Klein; R. E. Haber","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3471"},"title":"Supporting Scientific SE Process Improvement","abstract":"The increasing complexity of scientific software can result in significant impacts on the research itself. In traditional software development projects, teams adopt historical best practices into their development processes to mitigate the risk of such problems. In contrast, the gap that has formed between the traditional and scientific software communities leaves scientists to rely on only their own experience when facing software process improvement (SPI) decisions. Rather than expect scientists to become software engineering (SE) experts or the SE community to learn all of the intricacies involved in scientific software development projects, we seek a middle ground. The Scientific Software Process Improvement Framework (SciSPIF) will allow scientists to self-drive their own SPI efforts while leveraging the collective experiences of their peers and linking their concerns to established SE best practices. This proposal outlines the known challenges of scientific software development, relevant concepts from traditional SE research, and our planned methodology for collecting the data required to construct SciSPIF while staying grounded in the actual goals and concerns of the scientists.","conference":"IEEE","terms":"Software;Software engineering;Best practices;Conferences;Interviews;Scientific computing;Encoding,natural sciences computing;project management;risk management;software development management;software process improvement,SciSPIF;Scientific Software Process Improvement Framework;scientific software development project;software engineering experts;SPI decision;software process improvement;scientific software community;risk mitigation;software development process;scientific software complexity;scientific SE process improvement","keywords":"software engineering;process improvement;scientific software","startPage":"923","endPage":"926","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203115","citationCount":1,"referenceCount":28,"year":2015,"authors":"E. S. Mesh","affiliations":"Dept. of Comput. \u0026 Inf. Sci., Rochester Inst. of Technol., Rochester, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3472"},"title":"Poster: Segmentation Based Online Performance Problem Diagnosis","abstract":"Currently, the performance problems of software systems gets more and more attentions. Among various diagnosis methods based on system traces, principal component analysis (PCA) based methods are widely used due to the high accuracy of the diagnosis results and requiring no specific domain knowledge. However, according to our experiments, we have validated several shortcomings existed in PCA-based methods, including requiring traces with a same call sequence, inefficiency when the traces are long, and missing performance problems. To cope with these issues, we introduce a segmentation based online diagnosis method in this poster.","conference":"IEEE","terms":"Principal component analysis;Software systems;Accuracy;Measurement;Monitoring;Computer architecture;Conferences,principal component analysis;program diagnostics;software performance evaluation,online performance problem diagnosis;software systems performance problems;diagnosis methods;system traces;principal component analysis;PCA based methods;segmentation based online diagnosis method","keywords":"performance problem diagnosis;principal component analysis;system trace;software reliability","startPage":"807","endPage":"808","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203079","citationCount":1,"referenceCount":6,"year":2015,"authors":"J. Zhou; Z. Chen; J. Wang","affiliations":"Sci. \u0026 Technol. on Parallel \u0026 Distrib. Process. Lab., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3473"},"title":"Poster: Is Carmen Better than George? Testing the Exploratory Tester Using HCI Techniques","abstract":"Exploratory software testing is an activity which can be carried out by both untrained and formally trained testers. In this paper, we propose using Human Computer Interaction (HCI) techniques to carry out a study of exploratory testing strategies used by the two groups of testers. This data will be used to make recommendations to companies with regards to the mix of skills and training required for testing teams.","conference":"IEEE","terms":"Human computer interaction;Software testing;Data collection;Training;Computer bugs;Software,human computer interaction;program testing,HCI techniques;human computer interaction;exploratory software testing","keywords":"Exploratory Testing Strategies;Human-Computer Interaction","startPage":"815","endPage":"816","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203083","citationCount":2,"referenceCount":7,"year":2015,"authors":"A. Borg; C. Porter; M. Micallef","affiliations":"Univ. of Malta, Msida, Malta; Univ. of Malta, Msida, Malta; Univ. of Malta, Msida, Malta","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3474"},"title":"Free Hugs -- Praising Developers for Their Actions","abstract":"Developing software is a complex, intrinsically intellectual, and therefore ephemeral activity, also due to the intangible nature of the end product, the source code. There is a thin red line between a productive development session, where a developer actually does something useful and productive, and a session where the developer essentially produces \"fried air\", pieces of code whose quality and usefulness are doubtful at best. We believe that well-thought mechanisms of gamification built on fine-grained interaction information mined from the IDE can crystallize and reward good coding behavior. We present our preliminary experience with the design and implementation of a micro-gamification layer built into an object-oriented IDE, which at the end of each development session not only helps the developer to understand what he actually produced, but also praises him in case the development session was productive. Building on this, we envision an environment where the IDE reflects on the deeds of the developers and by providing a historical view also helps to track and reward long-term growth in terms of development skills, not dissimilar from the mechanics of role-playing games.","conference":"IEEE","terms":"Software;Games;Navigation;Software engineering;Productivity;Programming;User interfaces,object-oriented programming;software engineering;source code (software),software development;source code;gamification mechanisms;fine-grained interaction information;coding behavior;microgamification layer;object-oriented IDE;role-playing game mechanics;integrated development environment","keywords":"gamification;ide;interaction data;doctor;window plague","startPage":"555","endPage":"558","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203011","citationCount":2,"referenceCount":17,"year":2015,"authors":"R. Minelli; A. Mocci; M. Lanza","affiliations":"Fac. of Inf., Univ. of Lugano, Lugano, Switzerland; Fac. of Inf., Univ. of Lugano, Lugano, Switzerland; Fac. of Inf., Univ. of Lugano, Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3475"},"title":"MU-MMINT: An IDE for Model Uncertainty","abstract":"Developers have to work with ever-present design-time uncertainty, i.e., Uncertainty about selecting among alternative design decisions. However, existing tools do not support working in the presence of uncertainty, forcing developers to either make provisional, premature decisions, or to avoid using the tools altogether until uncertainty is resolved. In this paper, we present a tool, called MU-MMINT, that allows developers to express their uncertainty within software artifacts and perform a variety of model management tasks such as reasoning, transformation and refinement in an interactive environment. In turn, this allows developers to defer the resolution of uncertainty, thus avoiding having to undo provisional decisions. See the companion video: http://youtu.be/kAWUm-iFatM.","conference":"IEEE","terms":"Uncertainty;Unified modeling language;Visualization;Adaptation models;Software;Cognition;Analytical models,software engineering;software tools,MU-MMINT tool;IDE;integrated development environment;model uncertainty;design-time uncertainty;design decisions;software artifacts;model management tasks;reasoning task;transformation task;refinement task","keywords":"","startPage":"697","endPage":"700","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203046","citationCount":7,"referenceCount":17,"year":2015,"authors":"M. Famelis; N. Ben-David; A. Di Sandro; R. Salay; M. Chechik","affiliations":"Univ. of Toronto, Toronto, ON, Canada; Univ. of Toronto, Toronto, ON, Canada; Univ. of Toronto, Toronto, ON, Canada; Univ. of Toronto, Toronto, ON, Canada; Univ. of Toronto, Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3476"},"title":"Ekstazi: Lightweight Test Selection","abstract":"Regression testing is a crucial, but potentially time-consuming, part of software development. Regression test selection (RTS), which runs only a subset of tests, was proposed over three decades ago as a promising way to speed up regression testing. However, RTS has not been widely adopted in practice. We propose EKSTAZI , a lightweight RTS tool, that can integrate well with testing frameworks and build systems, increasing the chance for adoption. EKSTAZI tracks dynamic dependencies of tests on files and requires no integration with version-control systems. We implemented EKSTAZI for Java+JUnit and Scala+ScalaTest, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC). The results show that EKSTAZI reduced the end-to-end testing time by 32% on average compared to executing all tests. EKSTAZI has been adopted for day-to-day use by several Apache developers. The demo video for EKSTAZI can be found at http://www.youtube.com/watch?v=jE8K5_UCP28.","conference":"IEEE","terms":"Testing;Java;Instruments;Open source software;Monitoring;Google,Java;program testing;regression analysis;software tools,EKSTAZI;lightweight test selection;regression testing;software development;regression test selection;lightweight RTS tool;dynamic dependency tracking;Java+JUnit;Scala+ScalaTest;open-source projects;Apache developers","keywords":"","startPage":"713","endPage":"716","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203050","citationCount":15,"referenceCount":20,"year":2015,"authors":"M. Gligoric; L. Eloussi; D. Marinov","affiliations":"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3477"},"title":"Verification of Android Applications","abstract":"This study investigates an alternative approach to analyze Android applications using model checking. We develop an extension to Java Path Finder (JPF) called JPF-Android to verify Android applications outside of the Android platform. JPF is a powerful Java model checker and analysis engine that is very effective at detecting corner-case and hard-to-find errors using its fine-grained analysis capabilities. JPF-Android provides a simplified model of the Android application framework on which an Android application can run and it can generate input events or parse an input script containing sequences of input events to drive the execution of the application. JPF-Android traverses all execution paths of the application by simulating these input events and can detect common property violations such as deadlocks and runtime exceptions in Android applications. It also introduces user defined execution specifications called Checklists to verify the flow of application execution.","conference":"IEEE","terms":"Androids;Humanoid robots;Java;Libraries;Analytical models;Model checking,Android (operating system);Java;program verification,verification;Android applications;model checking;Java path finder;JPF-Android;Android platform;Java model checker;analysis engine;corner-case errors detection;hard-to-find errors detection;execution paths;property violations;deadlocks;runtime exceptions;Checklists;application execution","keywords":"","startPage":"931","endPage":"934","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203117","citationCount":0,"referenceCount":12,"year":2015,"authors":"H. v. d. Merwe","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3478"},"title":"Agile Project Management: From Self-Managing Teams to Large-Scale Development","abstract":"Agile software development represents a new approach for planning and managing software projects. It puts less emphasis on up-front plans and strict control and relies more on informal collaboration, coordination, and learning. This briefing provides a characterization and definition of agile project management based on extensive studies of large-scale industrial projects. It explains the circumstances behind the change from traditional management with its focus on direct supervision and standardization of work processes, to the newer, agile focus on self-managing teams, including its opportunities and benefits, but also its complexity and challenges. The main focus of the briefing is the four principles of agile project management: minimum critical specification, autonomous teams, redundancy, and feedback and learning. The briefing is intended for researchers, practitioners and educators in software engineering, especially project managers. For researchers, an updated state of the art will be uncovered, and the presentation will be based on current best evidence. For practitioners, principles, processes, and key success factors will be outlined and a successful large-scale case study of agile project management will be presented. For educators, the briefing will provide the basis for developing course material.","conference":"IEEE","terms":"Software;Project management;Planning;Complexity theory;Uncertainty;Redundancy,project management;software development management;software prototyping,agile project management;self-managing teams;agile software development;software project planning;software projects management;large-scale industrial project development;software engineering","keywords":"Software Engineering;Large-Scale;Project Management;Agile Development;Self-Management;Portfolio Management","startPage":"945","endPage":"946","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203121","citationCount":2,"referenceCount":1,"year":2015,"authors":"T. Dybå; T. Dingsøyr","affiliations":"SINTEF, Trondheim, Norway; SINTEF, Trondheim, Norway","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d3479"},"title":"Poster: ProNat: An Agent-Based System Design for Programming in Spoken Natural Language","abstract":"The emergence of natural language interfaces has led to first attempts of programming in natural language. We present ProNat, a tool for script-like programming in spoken natural language (SNL). Its agent-based architecture unifies deep natural language understanding (NLU) with modular software design. ProNat focuses on the extraction of processing flows and control structures from spoken utterances. For evaluation we have begun to build a speech corpus. First experiments are conducted in the domain of domestic robotics, but ProNat's architecture makes domain acquisition easy. Test results with spoken utterances in ProNat seem promising, but much work has to be done to achieve deep NLU.","conference":"IEEE","terms":"Natural languages;Programming;Speech;Robots;Ontologies;Unified modeling language;Speech recognition,authoring languages;natural language interfaces;programming languages;software agents;software architecture;software tools,agent-based system design;spoken natural language;natural language interfaces;SNL;script-like programming;deep natural language understanding;NLU;modular software design;agent-based architecture;control structures;processing flow extraction;speech corpus;domestic robotics;ProNat architecture;spoken utterances","keywords":"programming in natural language;natural language processing;end user programming;ontology;agent-based software design;natural lanuage understanding;robotics","startPage":"819","endPage":"820","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203085","citationCount":7,"referenceCount":12,"year":2015,"authors":"S. Weigelt; W. F. Tichy","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d347a"},"title":"CACHECA: A Cache Language Model Based Code Suggestion Tool","abstract":"Nearly every Integrated Development Environment includes a form of code completion. The suggested completions (\"suggestions\") are typically based on information available at compile time, such as type signatures and variables in scope. A statistical approach, based on estimated models of code patterns in large code corpora, has been demonstrated to be effective at predicting tokens given a context. In this demo, we present CACHECA, an Eclipse plug in that combines the native suggestions with a statistical suggestion regime. We demonstrate that a combination of the two approaches more than doubles Eclipse's suggestion accuracy. A video demonstration is available at https://www.youtube.com/watch?v=3INk0N3JNtc.","conference":"IEEE","terms":"Accuracy;Engines;Context;Context modeling;Java;Computational modeling;Predictive models,programming languages;software tools;source code (software),CACHECA;Eclipse plugin;cache language model;code suggestion tool;integrated development environment;code completion;statistical approach;code pattern model estimation","keywords":"","startPage":"705","endPage":"708","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203048","citationCount":12,"referenceCount":8,"year":2015,"authors":"C. Franks; Z. Tu; P. Devanbu; V. Hellendoorn","affiliations":"Dept. of Comput. Sci., Univ. of California at Davis, Davis, CA, USA; Dept. of Comput. Sci., Univ. of California at Davis, Davis, CA, USA; Dept. of Comput. Sci., Univ. of California at Davis, Davis, CA, USA; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d347b"},"title":"StressCloud: A Tool for Analysing Performance and Energy Consumption of Cloud Applications","abstract":"Finding the best deployment configuration that maximises energy efficiency while guaranteeing system performance of cloud applications is an extremely challenging task. It requires the evaluation of system performance and energy consumption under a wide variety of realistic workloads and deployment configurations. This paper demonstrates StressCloud, an automatic performance and energy consumption analysis tool for cloud applications in real-world cloud environments. StressCloud supports 1) the modelling of realistic cloud application workloads, 2) the automatic generation and running of load tests, and 3) the profiling of system performance and energy consumption.","conference":"IEEE","terms":"Computational modeling;Load modeling;Energy consumption;Cloud computing;System performance;Data models;Servers,cloud computing;energy conservation;energy consumption;power aware computing;software performance evaluation;software tools,StressCloud tool;cloud performance analysis;energy efficiency;system performance evaluation;energy consumption analysis tool;real-world cloud environments;load tests","keywords":"","startPage":"721","endPage":"724","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203052","citationCount":5,"referenceCount":10,"year":2015,"authors":"F. Chen; J. Grundy; J. Schneider; Y. Yang; Q. He","affiliations":"Sch. of Software \u0026 Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Software \u0026 Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Software \u0026 Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Software \u0026 Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia; Sch. of Software \u0026 Electr. Eng., Swinburne Univ. of Technol., Melbourne, VIC, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d347c"},"title":"The Green Lab: Experimentation in Software Energy Efficiency","abstract":"Software energy efficiency is a research topic where experimentation is widely adopted. Nevertheless, current studies and research approaches struggle to find generalizable findings that can be used to build a consistent knowledge base for energy-efficient software. To this end, we will discuss how to combine the traditional hypothesis-driven (top-down) approach with a bottom-up discovery approach. In this technical briefing, participants will learn the challenges that characterize the research in software energy efficiency. They will experience the complexity in this field and its implications for experimentation.","conference":"IEEE","terms":"Software;Energy consumption;Software engineering;Energy measurement;Complexity theory;Software measurement;Conferences,energy conservation;green computing;software engineering,green laboratory;software energy efficiency;hypothesis-driven approach;bottom-up discovery approach","keywords":"Energy Efficiency;Software Engineering;Empirical Methods","startPage":"941","endPage":"942","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203119","citationCount":13,"referenceCount":8,"year":2015,"authors":"G. Procaccianti; P. Lago; A. Vetrò; D. M. Fernández; R. Wieringa","affiliations":"Dept. of Comput. Sci., VU Univ., Amsterdam, Netherlands; Dept. of Comput. Sci., VU Univ., Amsterdam, Netherlands; Tech. Univ. Munchen, Munich, Germany; Tech. Univ. Munchen, Munich, Germany; Univ. of Twente, Enschede, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9117eee8435e8e7d347d"},"title":"The Use of Text Retrieval and Natural Language Processing in Software Engineering","abstract":"This technical briefing presents the state of the art Text Retrieval and Natural Language Processing techniques used in Software Engineering and discusses their applications in the field.","conference":"IEEE","terms":"Software;Software engineering;Natural language processing;Committees;Conferences;Tutorials;Information retrieval,information retrieval;natural language processing;software engineering;text analysis,text retrieval;software engineering;natural language processing techniques","keywords":"Text retrieval;natural language processing","startPage":"949","endPage":"950","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203123","citationCount":2,"referenceCount":0,"year":2015,"authors":"V. Arnaoudova; S. Haiduc; A. Marcus; G. Antoniol","affiliations":"Univ. of Texas at Dallas, Richardson, TX, USA; Florida State Univ., Tallahassee, FL, USA; Univ. of Texas at Dallas, Richardson, TX, USA; Polytech. Montreal, Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d347e"},"title":"Poster: Conquering Uncertainty in Java Programming","abstract":"Uncertainty in programming is one of the challenging issues to be tackled, because it is error-prone for many programmers to temporally avoid uncertain concerns only using simple language constructs such as comments and conditional statements. This paper proposes ucJava, a new Java programming environment for conquering uncertainty. Our environment provides a modular programming style for uncertainty and supports test-driven development taking uncertainty into consideration.","conference":"IEEE","terms":"Uncertainty;Java;Programming;Testing;Connectors;Programming environments,Java,Java programming uncertainty;language constructs;conditional statements;comments;ucJava;Java programming environment;modular programming style;test-driven development","keywords":"","startPage":"823","endPage":"824","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203087","citationCount":2,"referenceCount":7,"year":2015,"authors":"T. Fukamachi; N. Ubayashi; S. Hosoai; Y. Kamei","affiliations":"Kyushu Univ., Fukuoka, Japan; Kyushu Univ., Fukuoka, Japan; Kyushu Univ., Fukuoka, Japan; Kyushu Univ., Fukuoka, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d347f"},"title":"Poster: Tierless Programming in JavaScript","abstract":"Whereas \"responsive\" web applications already offered a more desktop-like experience, there is an increasing user demand for \"rich\" web applications (RIAs) that offer collaborative and even off-line functionality. Realizing these qualities requires distributing previously centralized application logic and state vertically from the server to a client tier (e.g., for desktop-like and off-line client functionality), and horizontally between instances of the same tier (e.g., for collaborative client functionality and for scaling of resource-starved services). Both bring about the essential complexity of distributing application assets and maintaining their consistency, along with the accidental complexity of reconciling a myriad of heterogenous tier-specific technology. Tierless programming languages enable developing web applications as a single artefact that is automatically split in tier-specific code - resulting in a development process akin to that of a desktop application. This relieves developers of distribution and consistency concerns, as well as the need to align different tier-specific technologies. However, programmers still have to adopt a new and perhaps esoteric language. We therefore advocate developing tierless programs in a general-purpose language instead. In this poster, we introduce our approach to tierless programming in JavaScript. We expand upon our previous work by identifying development challenges arising from this approach that could be resolved through tool support.","conference":"IEEE","terms":"Servers;Reactive power;Programming;Prototypes;Libraries;Collaboration;Complexity theory,Internet;Java;programming languages,JavaScript;responsive Web applications;RIA;accidental complexity;tierless programming languages;general-purpose language","keywords":"","startPage":"831","endPage":"832","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203091","citationCount":1,"referenceCount":8,"year":2015,"authors":"L. Philips; W. De Meuter; C. De Roover","affiliations":"Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3480"},"title":"Incorporating Human Intention into Self-Adaptive Systems","abstract":"Self-adaptive systems are fed with contextual information from the environments in which the systems operate,from within themselves, and from the users. Traditional self-adaptive systems research has focused on inputs of systems performance, resources, exception, and error recovery that drive systems' reaction to their environments. The intelligent ability ofthese self-adaptive systems is impoverished without knowledge ofa user's covert attention (thoughts, emotions, feelings). As a result, it is difficult to build effective systems that anticipate and react to users' needs as projected by covert behavior. This paperpresents the preliminary research results on capturing users'intention through neural input, and in reaction, commanding actions from software systems (e.g., load an application) based on human intention. Further, systems can self-adapt and refine their behaviors driven by such human covert behavior. The long-term research goal is to incorporate and synergize human neural input.Thus establishing software systems with a self-adaptive capability to \"feel\" and \"anticipate\" users intentions and put the human in the loop.","conference":"IEEE","terms":"Electroencephalography;Mice;Computers;Digital signal processing;Software systems;Software engineering;Adaptive systems,human computer interaction;software engineering,self-adaptive systems;human intention;contextual information;software systems;human covert behavior;human neural input;human computer interface","keywords":"Brain computer interface (BCI);human computer interface (HCI);neural input;self-adaptive systems;overt and covert behavior;human in the loop","startPage":"571","endPage":"574","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203015","citationCount":2,"referenceCount":21,"year":2015,"authors":"S. Huang; P. Miranda","affiliations":"Comput. \u0026 Electr. Eng. \u0026 Comput. Sci., Florida Atlantic Univ., Boca Raton, FL, USA; Comput. \u0026 Electr. Eng. \u0026 Comput. Sci., Florida Atlantic Univ., Boca Raton, FL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3481"},"title":"6th International Workshop on Emerging Trends in Software Metrics (WETSoM 2015)","abstract":"WETSoM is a gathering of researchers and practitioners to discuss the progress on software metrics knowledge. Motivations for this workshop include the low impact that software metrics have on current software development and the increased interest in research. The goals of this workshop include critically examining the evidence for the effectiveness of existing metrics and identifying new directions for metrics. Evidence for existing metrics includes how the metrics have been used in practice and studies showing their effectiveness. Identifying new directions includes use of new theories, such as complex network theory, on which to base metrics.","conference":"IEEE","terms":"Software;Software metrics;Conferences;Software engineering;Market research;Complex networks,,","keywords":"Software Metrics;Software Quality","startPage":"1021","endPage":"1022","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203158","citationCount":0,"referenceCount":0,"year":2015,"authors":"S. Counsell; A. Visaggio; R. Tonelli; E. Tempero","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3482"},"title":"Ariadne: Topology Aware Adaptive Security for Cyber-Physical Systems","abstract":"This paper presents Ariadne, a tool for engineering topology aware adaptive security for cyber-physical systems. It allows security software engineers to model security requirements together with the topology of the operational environment. This model is then used at runtime to perform speculative threat analysis to reason about the consequences that topological changes arising from the movement of agents and assets can have on the satisfaction of security requirements. Our tool also identifies an adaptation strategy that applies security controls when necessary to prevent potential security requirements violations.","conference":"IEEE","terms":"Security;Topology;Servers;Adaptation models;Runtime;Mobile handsets;Ports (Computers),security of data;software tools,Ariadne tool;cyber-physical systems;engineering topology aware adaptive security;security software engineers;speculative threat analysis;adaptation strategy","keywords":"Adaptive Systems;Security;Verification","startPage":"729","endPage":"732","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203054","citationCount":11,"referenceCount":5,"year":2015,"authors":"C. Tsigkanos; L. Pasquale; C. Ghezzi; B. Nuseibeh","affiliations":"Politec. di Milano, Milan, Italy; Lero - the Irish Software Res. Centre, Limerick, Ireland; Politec. di Milano, Milan, Italy; Lero - the Irish Software Res. Centre, Limerick, Ireland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3483"},"title":"Reactive Programming: A Walkthrough","abstract":"Over the last few years, Reactive Programming has emerged as the trend to support the development of reactive software through dedicated programming abstractions. Reactive Programming has been increasingly investigated in the programming languages community and it is now gaining the interest of practitioners. Conversely, it has received so far less attention from the software engineering community. This technical briefing bridges this gap through an accurate overview of Reactive Programming, discussing the available frameworks and outlining open research challenges with an emphasis on cross-field research opportunities.","conference":"IEEE","terms":"Software engineering;Software;Programming profession;Computer languages;Observers;Graphical user interfaces,programming languages;software engineering,reactive programming;reactive software development;programming abstraction;programming language","keywords":"","startPage":"953","endPage":"954","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203125","citationCount":5,"referenceCount":11,"year":2015,"authors":"G. Salvaneschi; A. Margara; G. Tamburrelli","affiliations":"Tech. Univ. Darmstadt, Darmstadt, Germany; Univ. della Svizzera italiana, Lugano, Switzerland; Vrije Univ., Amsterdam, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3484"},"title":"Poster: An Efficient Equivalence Checking Method for Petri Net Based Models of Programs","abstract":"The initial behavioural specification of any software programs goes through significant optimizing and parallelizing transformations, automated and also human guided, before being mapped to an architecture. Establishing validity of these transformations is crucial to ensure that they preserve the original behaviour. PRES+ model (Petri net based Representation of Embedded Systems) encompassing data processing is used to model parallel behaviours. Being value based with inherent scope of capturing parallelism, PRES+ models depict such data dependencies more directly; accordingly, they are likely to be more convenient as the intermediate representations (IRs) of both the source and the transformed codes for translation validation than strictly sequential variable-based IRs like Finite State Machines with Data path (FSMDs) (which are essentially sequential control flow graphs (CFGs)). In this work, a path based equivalence checking method for PRES+ models is presented.","conference":"IEEE","terms":"Computational modeling;Petri nets;Data models;Sparks;Benchmark testing;Embedded systems,embedded systems;formal specification;parallel programming;Petri nets;program interpreters;program verification;software architecture;source code (software),Petri net based models;behavioural specification;software programs;optimizing transformations;parallelizing transformations;architecture;PRES+ model;Petri net based representation of embedded systems;data processing;parallel behaviours;parallelism;data dependencies;intermediate representations;source codes;transformed codes;translation validation;sequential variable-based IR;finite state machines with data path;FSMD;sequential control flow graphs;CFG;path based equivalence checking method","keywords":"Equivalence checking;PRES+ model;FSMD model","startPage":"827","endPage":"828","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203089","citationCount":5,"referenceCount":7,"year":2015,"authors":"S. Bandyopadhyay; D. Sarkar; C. Mandal","affiliations":"Indian Inst. of Technol., Kharagpur, Kharagpur, India; Indian Inst. of Technol., Kharagpur, Kharagpur, India; Indian Inst. of Technol., Kharagpur, Kharagpur, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3485"},"title":"Poster: Reasoning Based on Imperfect Context Data in Adaptive Security","abstract":"Enabling software systems to adjust their protection in continuously changing environments with imperfect context information is a grand challenging problem. The issue of uncertain reasoning based on imperfect information has been overlooked in traditional logic programming with classical negation when applied to dynamic systems. This paper sketches a non-monotonic approach based on Answer Set Programming to reason with imperfect context data in adaptive security where there is little or no knowledge about certainty of the actions and events.","conference":"IEEE","terms":"Context;Security;Cognition;Adaptation models;Monitoring;Context modeling;Patents,data protection;logic programming;security of data,imperfect context data;adaptive security;software systems;logic programming;dynamic systems;nonmonotonic approach;answer set programming","keywords":"Adaptive security;imperfect data;non-monotonic logic","startPage":"835","endPage":"836","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203093","citationCount":4,"referenceCount":5,"year":2015,"authors":"S. Sartoli; A. S. Namin","affiliations":"Comput. Sci. Dept., Texas Tech Univ., Lubbock, TX, USA; Comput. Sci. Dept., Texas Tech Univ., Lubbock, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3486"},"title":"VERMEER: A Tool for Tracing and Explaining Faulty C Programs","abstract":"We present VERMEER, a new automated debugging tool for C. VERMEER combines two functionalities: (1) a dynamic tracer that produces a linearized trace from a faulty C program and a given test input; and (2) a static analyzer that explains why the trace fails. The tool works in phases that simplify the input program to a linear trace, which is then analyzed using an automated theorem prover to produce the explanation. The output of each phase is a valid C program. VERMEER is able to produce useful explanations of non trivial traces for real C programs within a few seconds. The tool demo can be found at http://youtu.be/E5lKHNJVerU.","conference":"IEEE","terms":"Concrete;Benchmark testing;Debugging;Interpolation;Libraries;Software;Algorithm design and analysis,C language;program debugging;program diagnostics;software tools;theorem proving,VERMEER;faulty C program tracing;automated debugging tool;dynamic tracer;linearized trace;static analyzer;automated theorem prover","keywords":"","startPage":"737","endPage":"740","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203056","citationCount":1,"referenceCount":16,"year":2015,"authors":"D. Schwartz-Narbonne; C. Oh; M. Schäf; T. Wies","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3487"},"title":"Profiling Kernels Behavior to Improve CPU / GPU Interactions","abstract":"Most modern computer and mobile devices have a graphical processing unit (GPU) available for any general purpose computation. GPU supports a programming model that is radically different from traditional sequential programming. As such, programming GPU is known to be hard and error prone, despite the large number of available APIs and dedicated programming languages. In this paper we describe a profiling technique that reports on the interaction between a CPU and GPUs. The resulting execution profile may then reveal anomalies and suboptimal situations, in particular due to an improper memory configuration. Our profiler has been effective at identifying suboptimal memory allocation usage in one image processing application.","conference":"IEEE","terms":"Graphics processing units;Kernel;Programming;Visualization;Central Processing Unit;Computers;Measurement,graphics processing units;storage management,kernel behavior profiling technique;CPU-GPU interactions;mobile devices;graphical processing unit;programming model;improper memory configuration;suboptimal memory allocation usage;image processing application","keywords":"gpgpu;opencl;memory profiling;profiling","startPage":"754","endPage":"756","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203060","citationCount":0,"referenceCount":9,"year":2015,"authors":"R. Salgado","affiliations":"Pleiad Lab., Univ. of Chile, Santiago, Chile","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3488"},"title":"Big(ger) Data in Software Engineering","abstract":"\"Big Data\" analytics has become the next hot topic for most companies - from financial institutions to technology companies to service providers. Likewise in software engineering, data collected about the development of software, the operation of the software in the field, and the users feedback on software have been used before. However, collecting and analyzing this information across hundreds of thousands or millions of software projects gives us the unique ability to reason about the ecosystem at large, and software in general. At no time in history has there been easier access to extremely powerful computational resources as it is today, thanks to the advances in cloud computing, both from the technology and business perspectives. In this technical briefing, we will present the state-of-the-art with respect to the research carried out in the area of big data analytics in software engineering research.","conference":"IEEE","terms":"Software;Software engineering;Big data;Data mining;Companies;Conferences;Ecosystems,Big Data;data analysis;software engineering,Big Data analytics;software engineering;software project","keywords":"Big Data;Software Engineering","startPage":"957","endPage":"958","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203127","citationCount":0,"referenceCount":2,"year":2015,"authors":"M. Nagappan; M. Mirakhorli","affiliations":"Dept. of Software Eng., Rochester Inst. of Technol., Rochester, NY, USA; Dept. of Software Eng., Rochester Inst. of Technol., Rochester, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3489"},"title":"3rd International Workshop on Conducting Empirical Studies in Industry (CESI 2015)","abstract":"Few would deny today the importance of empirical studies in the field of Software Engineering (SE) and, indeed, an increasing number of studies are being conducted involving the software industry. While literature abounds on empirical procedures, relatively little is known about the dynamics and complexity of conducting empirical studies in the software industry. What are the impediments and how to best handle them? This driver underlies the organisation of the third in a series of workshops, CESI 2015. Apart from structured presentations and discussions from academic and industry participants, this workshop (like predecessor workshops) includes a \"wall of ideas\" session where all participants asynchronously post their ideas on the wall, literally, which are then analysed. As a tangible output, the workshop's discussions will be summarised in a post-workshop report.","conference":"IEEE","terms":"Conferences;Industries;Software;Software engineering;Context;Committees;Australia,,","keywords":"Empirical studies;software industry","startPage":"967","endPage":"968","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203131","citationCount":0,"referenceCount":0,"year":2015,"authors":"X. Franch; N. H. Madhavji; C. H. C. Duarte","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d348a"},"title":"How and When to Transfer Software Engineering Research via Extensions","abstract":"It is often reported that there is a large gap between software engineering research and practice, with little transfer from research to practice. While this is true in general, one transfer technique is increasingly breaking down this barrier: extensions to integrated development environments (IDEs). With the proliferation of app stores for IDEs and increasing transfer effort from researchers several research-based extensions have seen significant adoption. In this talk we'll discuss our experience transferring code search research, which currently is in the top 5% of Visual Studio extensions with over 13,000 downloads, as well as other research techniques transferred via extensions such as NCrunch, FindBugs, Code Recommenders, Mylyn, and Instasearch. We'll use the lessons learned from our transfer experience to provide case study evidence as to best practices for successful transfer, supplementing it with the quantitative evidence offered by app store and usage data across the broader set of extensions. The goal of this 30 minute talk is to provide researchers with a realistic view on which research techniques can be transferred to practice as well as concrete steps to execute such a transfer.","conference":"IEEE","terms":"Software engineering;Computer bugs;Electronic mail;Visualization;Testing;Prototypes;Software,software engineering,software engineering research;integrated development environments;IDE;research-based extensions;Visual Studio extensions","keywords":"integrated development environment;technology transfer;plugins;case study","startPage":"239","endPage":"240","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202968","citationCount":2,"referenceCount":14,"year":2015,"authors":"D. Shepherd; K. Damevski; L. Pollock","affiliations":"ABB Corp. Res., Raleigh, NC, USA; Virginia State Univ., Petersburg, VA, USA; Univ. of Delaware, Newark, DE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d348b"},"title":"Industry/University Collaboration in Software Engineering Education: Refreshing and Retuning Our Strategies","abstract":"This panel session will explore strategies for industry/university collaboration in software engineering education. Specific discussion topics will include new strategies for successful industry/university collaboration, exploration of reasons why some of the old strategies no longer work, and regional/geographical differences noted by the international set of panelists. The panel hopes to identify new promising strategies for such collaborations. Specific industry representatives will be invited to attend and participate in the discussion.","conference":"IEEE","terms":"Software engineering;Industries;Software;Collaboration;Training;Conferences,computer based training;computer science education;educational institutions;organisational aspects,industry-university collaboration;software engineering education;regional differences;geographical differences","keywords":"","startPage":"273","endPage":"275","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202972","citationCount":2,"referenceCount":10,"year":2015,"authors":"N. R. Mead","affiliations":"Software Eng. Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d348c"},"title":"Understanding the Software Fault Introduction Process","abstract":"Testing and debugging research revolves around faults, yet we have a limited understanding of the processes by which faults are introduced and removed. Previous work in this area has focused on describing faults rather than explaining the introduction and removal processes, meaning that a great deal of testing and debugging research depends on assumptions that have not been empirically validated. We propose a three-phase project to develop an explanatory theory of the fault introduction process and describe how the project will be completed.","conference":"IEEE","terms":"Testing;Debugging;Software engineering;Software;Fault diagnosis;Computer crashes;Medical services,program debugging;program testing;research and development;software fault tolerance,software fault introduction process;debugging research;removal processes;testing research;three-phase project","keywords":"software faults;testing;debugging","startPage":"843","endPage":"846","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203095","citationCount":0,"referenceCount":16,"year":2015,"authors":"L. Inozemtseva","affiliations":"Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d348d"},"title":"Dynamic Safety Cases for Through-Life Safety Assurance","abstract":"We describe dynamic safety cases, a novel operationalization of the concept of through-life safety assurance, whose goal is to enable proactive safety management. Using an example from the aviation systems domain, we motivate our approach, its underlying principles, and a lifecycle. We then identify the key elements required to move towards a formalization of the associated framework.","conference":"IEEE","terms":"Monitoring;Biomedical monitoring;Safety management;Runtime;Cognition;Temperature sensors,safety-critical software,through-life safety assurance;dynamic safety cases;through-life safety assurance concept;proactive safety management;aviation systems domain;safety principles;safety lifecycle","keywords":"Dynamic safety case;Safety assurance;Lifecycle processes;Safety management","startPage":"587","endPage":"590","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203019","citationCount":16,"referenceCount":17,"year":2015,"authors":"E. Denney; G. Pai; I. Habli","affiliations":"SGT / NASA Ames Res. Center, Moffett Field, CA, USA; SGT / NASA Ames Res. Center, Moffett Field, CA, USA; Dept. of Comput. Sci., Univ. of York, York, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d348e"},"title":"A Unified Framework for the Comprehension of Software's Time","abstract":"The dimension of time in software appears in both program execution and software evolution. Much research has been devoted to the understanding of either program execution or software evolution, but these two research communities have developed tools and solutions exclusively in their respective context. In this paper, we claim that a common comprehension framework should apply to the time dimension of software. We formalize this as a meta-model that we instantiate and apply to the two different comprehension problems.","conference":"IEEE","terms":"Software;Heating;Visualization;Collaboration;History;Measurement;Context,software maintenance,software time dimension;program execution;software evolution;meta-model;comprehension framework;software maintenance","keywords":"","startPage":"603","endPage":"606","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203023","citationCount":0,"referenceCount":19,"year":2015,"authors":"O. Benomar; H. Sahraoui; P. Poulin","affiliations":"Dept. I.R.O., Univ. de Montreal, Montreal, QC, Canada; Dept. I.R.O., Univ. de Montreal, Montreal, QC, Canada; Dept. I.R.O., Univ. de Montreal, Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d348f"},"title":"Mining the Metadata -- and Its Consequences","abstract":"Traditionally metadata, the who, when, where of a phone call, the IP address, time, date of an Internet connection, has been viewed as deserving of less privacy than the contents of the communication. But ubiquitous computing and communication has changed that equation, and such transactional information has become increasingly revelatory. In this talk, I will discuss how metadata is used in all sorts of investigations, from malware to malfeasance. I will also discuss how the ubiquity of metadata must mean a change in our approaches to it.","conference":"IEEE","terms":"Metadata;Privacy;Security;Surveillance;Data privacy;Law,data mining;data privacy;Internet;invasive software;meta data;ubiquitous computing,metadata mining;phone call;IP address;Internet connection;ubiquitous computing;transactional information;malware;malfeasance;metadata ubiquity","keywords":"","startPage":"4","endPage":"5","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194556","citationCount":1,"referenceCount":13,"year":2015,"authors":"S. Landau","affiliations":"Worcester Polytech. Inst., Worcester, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3490"},"title":"FormTester: Effective Integration of Model-Based and Manually Specified Test Cases","abstract":"Whilst Model Based Testing (MBT) is an improvement over manual test specification, the leap from it to MBT can be hard. Only recently MBT tools for Web applications have emerged that can recover models from existing manually specified test cases. However, there are further requirements for supporting both MBT and manually specified tests. First, we need support for the generation of test initialization procedures. Also, we want to identify areas of the system that are not testable due to defects. We present Form Tester, a new MBT tool addressing these limitations. An evaluation with real Web applications shows that Form Tester helps to reduce the time spent on developing test cases.","conference":"IEEE","terms":"Manuals;Testing;Adaptation models;Automation;Software engineering;Computational modeling;Writing,formal specification;program testing,FormTester;model-based test case;manually-specified test case;model-based testing;Web applications;MBT tools;test initialization procedure generation","keywords":"","startPage":"745","endPage":"748","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203058","citationCount":0,"referenceCount":9,"year":2015,"authors":"R. Dixit; C. Lutteroth; G. Weber","affiliations":"Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand; Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand; Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3491"},"title":"A Combined Technique of GUI Ripping and Input Perturbation Testing for Android Apps","abstract":"Mobile applications have become an integral part of the daily lives of millions of users, thus making necessary to ensure their security and reliability. Moreover the increasing number of mobile applications with rich Graphical User Interfaces (GUI) creates a growing need for automated techniques of GUI Testing for mobile applications. In this paper, the GUI Ripping Technique is combined with the Input Perturbation Testing to improve the quality of Android Application Testing. The proposed technique, based on a systematic and automatic exploration of the behavior of Android applications, creates a model of the explored GUI and then uses it to generate the perturbed text inputs. The technique was evaluated on many Android apps and its results were compared with random input tests.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Androids;Humanoid robots;Mobile applications;Computer bugs;Software engineering,Android (operating system);graphical user interfaces;mobile computing;program testing;security of data;software quality;software reliability,GUI ripping technique;input perturbation testing;graphical user interfaces;GUI testing automated techniques;mobile applications;Android application testing;perturbed text input generation;random input tests","keywords":"Android Application Testing;GUI Ripping;Testing Automation;Input Perturbation Testing","startPage":"760","endPage":"762","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203062","citationCount":5,"referenceCount":16,"year":2015,"authors":"G. Imparato","affiliations":"Univ. of Naples “Federico II”, Naples, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3492"},"title":"1st International Workshop on Complex faUlts and Failures in LargE Software Systems (COUFLESS 2015)","abstract":"COUFLESS is a one-day workshop that starts with keynote speaker, Prof. Kishor S. Trivedi from Duke University, North Carolina, USA whose talk's title is titled: \"Why Does Software Fail and What Should be Done About It?\"   A total of 15 papers were submitted to COUFLESS with 53 authors from nine countries and each paper received at least three reviews by the 26 members of the program committee from 11 countries, making it a truly International Workshop. After a long discussion, 11 papers were accepted with the acceptance rate of 73%. Accepted papers address the issues of localizing and debugging complex faults in large-scale software applications.","conference":"IEEE","terms":"Conferences;Databases;Debugging;Software systems;Committees,,","keywords":"Workshop;Mandelbug;failure;debugging","startPage":"971","endPage":"972","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203133","citationCount":0,"referenceCount":0,"year":2015,"authors":"M. Grechanik; J. Alonso; A. P. Nikora","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3493"},"title":"Scalability Studies on Selective Mutation Testing","abstract":"Mutation testing is a test method which is designed to evaluate a test suite's quality. Due to the expensive cost of mutation testing, selective mutation testing was first proposed in 1991 by Mathur, in which a subset of mutants are selected aiming to achieve the same effectiveness as the whole set of mutants in evaluating the quality of test suites. Though selective mutation testing has been widely investigated in recent years, many people still doubt if it can suit well for large programs. Realizing that none of the existing work has systematically studied the scalability of selective mutation testing, I plan to work on the scalability of selective mutation testing through several studies.","conference":"IEEE","terms":"Testing;Scalability;Software;Medical services;Predictive models;Java;Measurement,program testing,scalability studies;selective mutation testing","keywords":"","startPage":"851","endPage":"854","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203097","citationCount":1,"referenceCount":20,"year":2015,"authors":"J. Zhang","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3494"},"title":"Capsule-Oriented Programming","abstract":"“Explicit concurrency should be abolished from all higher-level programming languages (i.e. everything except - perhaps- plain machine code.).” Dijkstra [1] (paraphrased). A promising class of concurrency abstractions replaces explicit concurrency mechanisms with a single linguistic mechanism that combines state and control and uses asynchronous messages for communications, e.g. active objects or actors, but that doesn't remove the hurdle of understanding non-local control transfer. What if the programming model enabled programmers to simply do what they do best, that is, to describe a system in terms of its modular structure and write sequential code to implement the operations of those modules and handles details of concurrency? In a recently sponsored NSF project we are developing such a model that we call capsule-oriented programming and its realization in the Panini project. This model favors modularity over explicit concurrency, encourages concurrency correctness by construction, and exploits modular structure of programs to expose implicit concurrency.","conference":"IEEE","terms":"Concurrent computing;Programming;Java;Synchronization;Global Positioning System;Program processors;Message systems,concurrency (computers);high level languages;software engineering,capsule-oriented programming;higher-level programming languages;concurrency abstractions;explicit concurrency mechanisms;single linguistic mechanism;nonlocal control transfer;sequential code;NSF project;Panini project","keywords":"Modularity;implicit concurrency;programming model;modular structure;concurrency abstraction;capsules","startPage":"611","endPage":"614","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203025","citationCount":6,"referenceCount":33,"year":2015,"authors":"H. Rajan","affiliations":"Dept. of Comput. Sci., Iowa State Univ. of Sci. \u0026 Technol., Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3495"},"title":"Statistical Learning and Software Mining for Agent Based Simulation of Software Evolution","abstract":"In the process of software development it is of high interest for a project manager to gain insights about the ongoing process and possible development trends at several points in time. Substantial factors influencing this process are, e.g., the constellation of the development team, the growth and complexity of the system, and the error-proneness of software entities. For this purpose we build an agent based simulation tool which predicts the future of a project under given circumstances, stored in parameters, which control the simulation process. We estimate these parameters with the help of software mining. Our work exposed the need for a more fine-grained model for the developer behavior. Due to this we create a learning model, which helps us to understand the contribution behavior of developers and, thereby, to determine simulation parameters close to reality. In this paper we present our agent based simulation model for software evolution and describe how methods from statistical learning and data mining serves us to estimate suitable simulation parameters.","conference":"IEEE","terms":"Hidden Markov models;Software;Object oriented modeling;Data models;Data mining;Predictive models;Adaptation models,data mining;digital simulation;learning (artificial intelligence);parameter estimation;software agents;software engineering,statistical learning;software mining;agent based simulation tool;software evolution;software development process;parameter estimation","keywords":"Software Process Simulation;Hidden Markov Model;Agent Based Modeling;Developer Behavior","startPage":"863","endPage":"866","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203100","citationCount":4,"referenceCount":17,"year":2015,"authors":"V. Honsel","affiliations":"Inst. of Comput. Sci., Univ. of Gottingen, Gottingen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3496"},"title":"An Approach to Detect Android Antipatterns","abstract":"Mobile applications are becoming complex software systems that must be developed quickly and evolve regularly to fit new user requirements and execution contexts. However, addressing these constraints may result in poor design choices, known as antipatterns, which may degrade software quality and performance. Thus, the automatic detection of antipatterns is an important activity that eases the future maintenance and evolution tasks. Moreover, it helps developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in their infancy. In this paper, we presents a tooled approach, called Paprika, to analyze Android applications and to detect object-oriented and Android-specific antipatterns from binaries of applications.","conference":"IEEE","terms":"Androids;Humanoid robots;Mobile communication;Java;Software;Measurement;Mobile applications,Android (operating system);mobile computing;software maintenance;software quality,Android antipattern detection;mobile applications;complex software systems;software quality degradation;software performance degradation;antipattern automatic detection;object-oriented applications;Paprika tooled approach;object-oriented detection","keywords":"","startPage":"766","endPage":"768","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203064","citationCount":7,"referenceCount":22,"year":2015,"authors":"G. Hecht","affiliations":"Inria, Univ. of Lille 1, Lille, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3497"},"title":"2nd International Workshop on Crowd Sourcing in Software Engineering (CSI-SE 2015)","abstract":"Crowdsourcing is increasingly revolutionizing the ways in which software is engineered. Programmers increasingly crowdsource answering their questions through Q\u0026A sites. Non-programmers may contribute human-intelligence to development projects, by, for example, usability testing software or even play games with a purpose to implicitly construct formal specifications. Crowdfunding helps to democratize decisions about what software to build. Software engineering researchers may even benefit from new opportunities to evaluate their work with real developers by recruiting developers from the crowd. CSI- SE will inform the software engineering community of current techniques and trends in crowdsourcing, discuss the application of crowdsourcing to software engineering to date, and identify new opportunities to apply crowdsourcing to solve software engineering problems.","conference":"IEEE","terms":"Crowdsourcing;Software engineering;Conferences;Usability;Testing;Games,,","keywords":"","startPage":"975","endPage":"976","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203135","citationCount":0,"referenceCount":0,"year":2015,"authors":"G. Fraser; T. D. LaToza; L. Mariani","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3498"},"title":"DIETs: Recommender Systems for Mobile API Developers","abstract":"The increasing number of posts related to mobile app development indicates unaddressed problems in the usage of mobile APIs. Arguing that these problems result from in- adequate documentation and shortcomings in the design and implementation of the APIs, the goal of this research is to develop and evaluate two developers' issues elimination tools (DIETs) for mobile API developers to diminish the problems of mobile applications (apps) development.After categorizing the problems, we investigate their causes, by exploring the relationships between the topics and trends of posts on Stack Overflow, the app developers' experience, the API and test code, and its changes. The results of these studies will be used to develop two DIETs that support API developers to improve the documentation, design, and implementation of their APIs.","conference":"IEEE","terms":"Mobile communication;Documentation;Smart phones;Androids;Humanoid robots;Software engineering;Data mining,application program interfaces;mobile computing;recommender systems;system documentation,DIET;recommender systems;mobile API developers;mobile app development;developer issues elimination tool;mobile application development;Stack Overflow;app developer experience;test code;system documentation","keywords":"recommender systems;api;mobile api developers","startPage":"859","endPage":"862","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203099","citationCount":0,"referenceCount":20,"year":2015,"authors":"S. Beyer","affiliations":"Univ. of Klagenfurt, Klagenfurt, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d3499"},"title":"Combining Mastery Learning with Project-Based Learning in a First Programming Course: An Experience Report","abstract":"One of the challenges in teaching a first programming course is that in the same course, the students must learn basic programming techniques and high level abstraction abilities, and the application of those techniques and concepts in problem solving and (engineering) design. To confront this challenge, in previous years, we have included a project-based learning phase at the end of our course to encourage the acquisition of high level design and creativity. To address some of the shortcomings of our previous editions, we have recently included a mastery phase to the course. While project-based learning is suitable for teaching high-level skills that require design and creativity and prepare the students for the study of software engineering, mastery-based learning is suitable for concrete skills such as basic programming tasks. Our particular innovation is to allow students into the project phase only if they have demonstrated a minimum predefined competency level in programming. The combination of the two approaches seems to address most of the requirements of a first programming course. We present our motivation for combining the two pedagogical techniques and our experience with the course.","conference":"IEEE","terms":"Programming profession;Education;Software engineering;Concrete;Software;Java,computer science education;educational courses;programming;teaching,mastery learning;project-based learning;programming course;basic programming techniques;high level abstraction abilities;high-level skills teaching;programming competency level","keywords":"first programming course;mastery learning;project-based learning;CS1/CS2;introductory programming;software engineering education","startPage":"315","endPage":"318","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202980","citationCount":4,"referenceCount":10,"year":2015,"authors":"M. Jazayeri","affiliations":"Univ. of Lugano, Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d349a"},"title":"A Unified Approach to Automatic Testing of Architectural Constraints","abstract":"Architectural decisions are often encoded in the form of constraints and guidelines. Non-functional requirements can be ensured by checking the conformance of the implementation against this kind of invariant. Conformance checking is often a costly and error-prone process that involves the use of multiple tools, differing in effectiveness, complexity and scope of applicability. To reduce the overall effort entailed by this activity, we propose a novel approach that supports verification of human-readable declarative rules through the use of adapted off-the-shelf tools. Our approach consists of a rule specification DSL, called Dicto, and a tool coordination framework, called Probo. The approach has been implemented in a soon to be evaluated prototype.","conference":"IEEE","terms":"Testing;Software architecture;Software;DSL;Guidelines;Stakeholders;Computer architecture,automatic test software;formal specification;program testing;software architecture,unified approach;automatic architectural constraint testing;architectural decisions;nonfunctional requirements;conformance checking;error-prone process;human-readable declarative rule verification;rule specification DSL;Dicto;tool coordination framework;Probo","keywords":"software architecture;conformance checking;architectural constraints","startPage":"871","endPage":"874","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203102","citationCount":0,"referenceCount":16,"year":2015,"authors":"A. Caracciolo","affiliations":"Software Composition Group, Univ. of Bern, Bern, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d349b"},"title":"The Future of Software Engineering (SEIP Keynote)","abstract":"Summary form only given. No matter what future we may envision, it relies on software that has not yet been written. Even now, software-intensive systems have woven themselves into the interstitial spaces of civilization, and we as individuals and as a species have slowly surrendered ourselves to computing. Looking back, we can identify several major and distinct styles whereby we have built such systems. We have come a long way, and even today, we certainly can name a number of best practices for software development that yield systems of quality. However, by no means can we stand still: the nature of the systems we build continues to change, and as they collectively weave themselves into our live, we must attend not only to the technical elements of software development, we must also attend to human needs. In this presentation we will look at the history of software engineering and offer some grand challenges for the future.","conference":"IEEE","terms":"Software engineering;Software;Weaving;Computer architecture;Conferences;Best practices;History,software engineering,software engineering;software-intensive systems;interstitial civilization spaces;software development","keywords":"software engineering;history;future","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202943","citationCount":0,"referenceCount":0,"year":2015,"authors":"G. Booch","affiliations":"IBM Res., Austin, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d349c"},"title":"A Large Scale Study of License Usage on GitHub","abstract":"The open source community relies upon licensing in order to govern the distribution, modification, and reuse of existing code. These licenses evolve to better suit the requirements of the development communities and to cope with unaddressed or new legal issues. In this paper, we report the results of a large empirical study conducted over the change history of 16,221 open source Java projects mined from Git Hub. Our study investigates how licensing usage and adoption changes over a period of ten years. We consider both the distribution of license usage within projects of a rapidly growing forge and the extent that new versions of licenses are introduced in these projects.","conference":"IEEE","terms":"Licenses;Software;Conferences;Software engineering;Data mining;Law,Java;law;public domain software;software reusability,license usage;GitHub;open source community;licensing;code distribution;code modification;code reuse;development communities;legal issues;open source Java projects;license distribution","keywords":"Software Licenses;Mining Software Repositories;Empirical Studies","startPage":"772","endPage":"774","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203066","citationCount":10,"referenceCount":16,"year":2015,"authors":"C. Vendome","affiliations":"Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d349d"},"title":"Automatic Categorization of Software Libraries Using Bytecode","abstract":"Automatic software categorization is the task of assigning categories or tags to software libraries in order to summarize their functionality. Correctly assigning these categories is essential to ensure that relevant libraries can be easily retrieved by developers from large repositories. Current categorization approaches rely on the semantics reflected in the source code, or use supervised machine learning techniques, which require a set of labeled software as a training data. These approaches fail when such information is not available. We propose a novel unsupervised approach for the automatic categorization of Java libraries, which uses the bytecode of a library in order to determine its category. We show that the approach is able to successfully categorize libraries from the Apache Foundation Repository.","conference":"IEEE","terms":"Software;Software libraries;Data mining;Accuracy;Conferences;Semantics,Java;software libraries;source code (software);unsupervised learning,automatic software library categorization;bytecode;source code;unsupervised approach;Java libraries;Apache Foundation Repository","keywords":"software categorization;bytecode;clustering;dirichlet process;automatic labeling","startPage":"784","endPage":"786","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203070","citationCount":1,"referenceCount":11,"year":2015,"authors":"J. Escobar-Avila","affiliations":"Dept. of Comput. Sci., Florida State Univ., Tallahassee, FL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d349e"},"title":"7th International Workshop on Principles of Engineering Service-Oriented and Cloud Systems (PESOS 2015)","abstract":"PESOS has established itself as a forum that brings together software engineering researchers and practitioners working in the areas of service-oriented systems to discuss research challenges, new developments and applications, as well as methods, techniques, experiences, and tools to support engineering, evolution and adaptation of service-oriented systems. The technical advances and growing adoption of Cloud computing is creating new challenges for the PESOS the software services community to explore the approaches to better engineer software systems that are designed, developed, operated and governed in the context of the Cloud. We again attracted high-quality submissions on a diverse set of relevant topics such as better approaches to engineering service-based collaborative systems, Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) models of cloud computing and associated software quality attributes. PESOS 2015 will continue to be the key forum for collecting case studies and artifacts for educators and researchers in this area.","conference":"IEEE","terms":"Conferences;Collaboration;Software as a service;Cloud computing;Service-oriented architecture,,","keywords":"Software engineering;software services;collaborative services;cloud computing;cloud services;SOA;service-oriented architecture;service-oriented systems","startPage":"987","endPage":"988","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203141","citationCount":0,"referenceCount":0,"year":2015,"authors":"M. A. Babar; H. Paik; M. Chetlur; M. Bauer; A. M. Sharifloo","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d349f"},"title":"Mining Software Repositories for Social Norms","abstract":"Social norms facilitate coordination and cooperation among individuals, thus enable smoother functioning of social groups such as the highly distributed and diverse open source software development (OSSD) communities. In these communities, norms are mostly implicit and hidden in huge records of human-interaction information such as emails, discussions threads, bug reports, commit messages and even source code. This paper aims to introduce a new line of research on extracting social norms from the rich data available in software repositories. Initial results include a study of coding convention violations in JEdit, Argo UML and Glassfish projects. It also presents a new life-cycle model for norms in OSSD communities and demonstrates how a number of norms extracted from the Python development community follow this life-cycle model.","conference":"IEEE","terms":"Encoding;Software;Proposals;Electronic mail;Java;Data mining;Monitoring,data mining;public domain software;software engineering;source code (software),software repository mining;social norm extraction;open source software development community;OSSD community;human-interaction information;source code;JEdit;Argo UML;Glassfish projects;life-cycle model;Python development community","keywords":"","startPage":"627","endPage":"630","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203029","citationCount":4,"referenceCount":8,"year":2015,"authors":"H. K. Dam; B. T. R. Savarimuthu; D. Avery; A. Ghose","affiliations":"Univ. of Wollongong, Wollongong, NSW, Australia; Univ. of Otago, Dunedin, New Zealand; Univ. of Wollongong, Wollongong, NSW, Australia; Univ. of Wollongong, Wollongong, NSW, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a0"},"title":"Bixie: Finding and Understanding Inconsistent Code","abstract":"We present Bixie, a tool to detect inconsistencies in Java code. Bixie detectsinconsistent code at a higher precision than previous tools and provides novelfault localization techniques to explain why code is inconsistent. Wedemonstrate the usefulness of Bixie on over one million lines of code, showthat it can detect inconsistencies at a low false alarm rate, and fix a numberof inconsistencies in popular open-source projects. Watch our Demo at http://youtu.be/QpsoUBJMxhk.","conference":"IEEE","terms":"Java;Arrays;Benchmark testing;Runtime;Unsolicited electronic mail;Computer bugs;Open source software,Java;program compilers;program diagnostics;software fault tolerance,Bixie;inconsistent code finding;inconsistent code understanding;Java code;fault localization technique;open-source projects","keywords":"","startPage":"645","endPage":"648","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203033","citationCount":1,"referenceCount":12,"year":2015,"authors":"T. McCarthy; P. Rümmer; M. Schäf","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a1"},"title":"Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches","abstract":"Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.","conference":"IEEE","terms":"Testing;Context;Unified modeling language;Software;Context modeling;Software engineering;Conferences,program testing,test case generation;model-based testing approach;software testing;MBT;test case prioritization problem;prioritization technique","keywords":"","startPage":"879","endPage":"882","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203104","citationCount":2,"referenceCount":27,"year":2015,"authors":"J. F. S. Ouriques","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a2"},"title":"Casper: Using Ghosts to Debug Null Deferences with Dynamic Causality Traces","abstract":"Fixing software errors requires understanding their root cause. In this paper, we introduce \"causality traces'', they are specially crafted execution traces augmented with the information needed to reconstruct a causal chain from a root cause to an execution error. We propose an approach and a tool, called Casper, for dynamically constructing causality traces for null dereference errors. The core idea of Casper is to inject special values, called \"ghosts\", into the execution stream to construct the causality trace at runtime. We evaluate our contribution by providing and assessing the causality traces of 14 real null dereference bugs collected over six large, popular open-source projects.","conference":"IEEE","terms":"Null value;Computer bugs;Debugging;History;Runtime;Open source software;Java,program debugging;software tools,Casper;null deference error debug;dynamic causality traces;software error fixing;execution error;ghosts;open-source projects","keywords":"","startPage":"790","endPage":"791","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203072","citationCount":0,"referenceCount":3,"year":2015,"authors":"B. Cornu","affiliations":"Univ. of Lille, Lille, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a3"},"title":"4th SEMAT Workshop on General Theory of Software Engineering (GTSE 2015)","abstract":"General theories explain the fundamental phenomena that constitute a research domain. They apply across a domain and often integrate many theories and concepts into a single cohesive view. While general theories are extremely important for education and research coordination, and common in many disciplines (e.g. sociology, criminology, electrical engineering, biology, physics), software engineering lacks a well-accepted general theory. The General Theory of Software Engineering workshop seeks to rectify this situation by promoting theory development in software engineering. The fourth workshop in this series, held in conjunction with the International Conference on Software Engineering, displayed a promising trend toward more theory development papers.","conference":"IEEE","terms":"Software engineering;Conferences;Software;Jacobian matrices;Concrete;Complexity theory;Education,software engineering,SEMAT;Software Engineering Method and Theory;General Theory of Software Engineering;GTSE workshop","keywords":"Research methodology;process theory;questionnaire;case study;field study","startPage":"983","endPage":"984","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203139","citationCount":0,"referenceCount":16,"year":2015,"authors":"P. Ralph; G. Engels; I. Jacobson; M. Goedicke","affiliations":"Univ. of Auckland, Auckland, New Zealand; Univ. of Paderborn, Paderborn, Germany; Ivar Jacobson Int., Verbier, Switzerland; Uni Duisburg-Essen, Paluno, Duisburg-Essen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a4"},"title":"ViDI: The Visual Design Inspector","abstract":"We present ViDI (Visual Design Inspector), a novel code review tool which focuses on quality concerns and design inspection as its cornerstones. It leverages visualization techniques to represent the reviewed software and augments the visualization with the results of quality analysis tools. To effectively understand the contribution of a reviewer in terms of the impact of her changes on the overall system quality, ViDI supports the recording and further inspection of reviewing sessions. ViDI is an advanced prototype which we will soon release to the Pharo open-source community.","conference":"IEEE","terms":"Visualization;Inspection;Quality assessment;Software systems;Software engineering;Birds,data visualisation;software quality;software reviews;software tools,ViDI;visual design inspector;code review tool;visualization technique;system quality","keywords":"Lugano","startPage":"653","endPage":"656","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203035","citationCount":2,"referenceCount":11,"year":2015,"authors":"Y. Tymchuk; A. Mocci; M. Lanza","affiliations":"REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland; REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland; REVEAL @ Fac. of Inf., Univ. of Lugano, Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a5"},"title":"Measuring Software Developers' Perceived Difficulty with Biometric Sensors","abstract":"As a developer works on a change task, he or she might perceive some parts of the task as easy and other parts as being very difficult. Currently, little is known about when a developer experiences different difficulty levels, although being able to assess these difficulty levels would be helpful for many reasons. For instance, a developer's perceived difficulty might be used to determine the likelihood of a bug being introduced into the code or the quality of the code a developer is working with. In psychology, biometric measurements, such as electro-dermal activity or heart rate, have already been extensively used to assess a person's mental state and emotions, but only little research has been conducted to investigate how these sensors can be used in the context of software engineering. In our research we want to take advantage of the insights gained in these psychological studies and investigate whether such biometric sensors can be used to measure developers' perceived difficulty while working on a change task and support them in their work.","conference":"IEEE","terms":"Biosensors;Software;Software engineering;Software measurement;Psychology,sensors;software quality,software developer measurement;biometric sensors;code quality;biometric measurements;psychology;electro-dermal activity;heart rate;person mental state assessment;emotion assessment;software engineering","keywords":"","startPage":"887","endPage":"890","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203106","citationCount":2,"referenceCount":33,"year":2015,"authors":"S. C. Müller","affiliations":"Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a6"},"title":"Poster: Improving Cloud-Based Continuous Integration Environments","abstract":"We propose a novel technique for improving the efficiency of cloud-based continuous integration development environments. Our technique identifies repetitive, expensive and time-consuming setup activities that are required to run integration and system tests in the cloud, and consolidates them into preconfigured testing virtual machines such that the overall costs of test execution are minimized. We create such testing machines by reconfiguring and opportunistically snapshotting the virtual machines already registered in the cloud.","conference":"IEEE","terms":"Virtual machining;Software;Servers;Standards;Load modeling;Software testing,cloud computing;virtual machines,of cloud-based continuous integration development environments;preconfigured testing virtual machines;test execution cost;testing machines","keywords":"integration test;test-driven development;snapshot;system tests;linear programming;cost flow;flow constraints","startPage":"797","endPage":"798","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203074","citationCount":3,"referenceCount":6,"year":2015,"authors":"A. Gambi; Z. Rostyslav; S. Dustdar","affiliations":"Distrib. Syst. Group, Vienna Univ. of Technol., Vienna, Austria; Distrib. Syst. Group, Vienna Univ. of Technol., Vienna, Austria; Distrib. Syst. Group, Vienna Univ. of Technol., Vienna, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a7"},"title":"3rd International Workshop on Release Engineering (RELENG 2015)","abstract":"Release engineering deals with all activities inbetween regular development and actual usage of asoftware product by the end user, i.e., integration, build, testexecution, packaging and delivery of software. Although re-search on this topic goes back for decades, the increasing heterogeneity and variability of software products along withthe recent trend to reduce the release cycle to days or even hoursstarts to question some of the common beliefs and practicesof the field. For example, a project like Mozilla Firefox releasesevery 6 weeks, generating updates for dozens of existing Fire-fox versions on 5 desktop, 2 mobile and 3 mobile desktopplatforms, each of which for more than 80 locales. In this con-text, the International Workshop on Release Engineering(RELENG) aims to provide a highly interactive forum for re-searchers and practitioners to address the challenges of, findsolutions for and share experiences with release engineering, and to build connections between the various communities.","conference":"IEEE","terms":"Software;Conferences;Software engineering;Google;Packaging;Testing;Maintenance engineering,,","keywords":"release engineering;integration;build system;test execution;packaging;deployment;continuous delivery","startPage":"995","endPage":"996","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203145","citationCount":0,"referenceCount":11,"year":2015,"authors":"B. Adams; S. Bellomo; C. Bird; F. Khomh; K. Moir","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a8"},"title":"Source Code Curation on StackOverflow: The Vesperin System","abstract":"The past few years have witnessed the rise of software question and answer sites like StackOverflow, where developers can pose detailed coding questions and receive quality answers. Developers using these sites engage in a complex code foraging process of understanding and adapting the code snippets they encounter. We introduce the notion of source code curation to cover the act of discovering some source code of interest, cleaning and transforming (refining) it, and then presenting it in a meaningful and organized way. In this paper, we present Vesperin, a source code curation system geared towards curating Java code examples on StackOverflow.","conference":"IEEE","terms":"Java;Software;Aerospace electronics;Context;Programming;Software engineering,Java;software quality;source code (software),Java code;complex code foraging process;Vesperin system;StackOverflow;source code curation system","keywords":"","startPage":"661","endPage":"664","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203037","citationCount":1,"referenceCount":12,"year":2015,"authors":"H. Sanchez; J. Whitehead","affiliations":"Comput. Sci. Dept., UC Santa Cruz, Santa Cruz, CA, USA; Comput. Media Dept., UC Santa Cruz, Santa Cruz, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34a9"},"title":"Search-Based Migration of Model Variants to Software Product Line Architectures","abstract":"Software Product Lines (SPLs) are families of related software systems developed for specific market segments or domains. Commonly, SPLs emerge from sets of existing variants when their individual maintenance becomes infeasible. However, current approaches for SPL migration do not support design models, are partially automated, or do not reflect constraints from SPL domains. To tackle these limitations, the goal of this doctoral research plan is to propose an automated approach to the SPL migration process at the design level. This approach consists of three phases: detection, analysis and transformation. It uses as input the class diagrams and lists of features for each system variant, and relies on search-based algorithms to create a product line architecture that best captures the variability present in the variants. Our expected contribution is to support the adoption of SPL practices in companies that face the scenario of migrating variants to SPLs.","conference":"IEEE","terms":"Software;Feature extraction;Programmable logic arrays;Unified modeling language;Software product lines;Medical services,software architecture;software product lines,product line architecture;search-based algorithm;SPL migration process;SPL architecture;software product line architecture;search-based migration algorithm","keywords":"Reuse;Migration;Re-engineering;Software Product Line;Search-Based Software Engineering","startPage":"895","endPage":"898","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203108","citationCount":0,"referenceCount":24,"year":2015,"authors":"W. K. G. Assunção","affiliations":"DINF, Fed. Univ. of Parana, Curitiba, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34aa"},"title":"An Integrated Multi-Agent-Based Simulation Approach to Support Software Project Management","abstract":"Software projects often do not accomplish what is expected. They fail to comply with the planned schedule, cost more than predicted, or are simply not completed at all owing to issues such as bad planning, a poorly chosen team or an incorrect definition of the tasks to be performed. Although simulation methods and tools have been introduced to alleviate these problems, there is a lack of simulation approaches that integrate software project knowledge, software development processes, project-related situation-awareness, and learning techniques to help project managers to make more informed decisions and hence reach successful conclusions with software projects. In addition, in order to be more proactive, such approaches need to provide simulations based on both static and dynamic situation-awareness data, support (self-)adaptive project planning and execution, and recommend remedial courses of action when real-time project anomalies occur. In this context, this PhD research aims to create an integrated multi-agent-based simulation to support software project management in a more comprehensive way.","conference":"IEEE","terms":"Software;Data models;Project management;Adaptation models;Planning;Real-time systems;Decision making,learning (artificial intelligence);multi-agent systems;planning;project management;software development management,integrated multiagent-based simulation approach;software project management;software development processes;project-related situation-awareness;learning techniques;static situation-awareness data;dynamic situation-awareness data;self-adaptive project planning","keywords":"software project management;multi-agent-based simulation","startPage":"911","endPage":"914","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203112","citationCount":2,"referenceCount":11,"year":2015,"authors":"D. d. M. Baia","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34ab"},"title":"Poster: Discovering Code Dependencies by Harnessing Developer's Activity","abstract":"Monitoring software developer's interactions in an integrated development environment is sought for revealing new information about developers and developed software. In this paper we present an approach for identifying potential source code dependencies solely from interaction data. We identify three kinds of potential dependencies and additionally assign them to developer's activity as well, to reveal detailed task-related connections in the source code. Interaction data as a source allow us to identify these candidates for dependencies even for dynamically typed programming languages, or across multiple languages in the source code. After first evaluations and positive results we continue with collecting data in professional environment of Web developers, and evaluating our approach.","conference":"IEEE","terms":"Navigation;Software;Syntactics;Context;Debugging;Maintenance engineering;Computer languages,programming environments;programming languages;software engineering;source code (software);system monitoring,source code dependency discovery;software developer interaction monitoring;integrated development environment;interaction data;programming languages;Web developers","keywords":"Source code dependency;interaction data;task context;implicit feedback;dynamic typing","startPage":"801","endPage":"802","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203076","citationCount":4,"referenceCount":7,"year":2015,"authors":"M. Konopka; P. Navrat; M. Bielikova","affiliations":"Fac. of Inf. \u0026 Inf. Technol., Slovak Univ. of Technol. in Bratislava, Bratislava, Slovakia; Fac. of Inf. \u0026 Inf. Technol., Slovak Univ. of Technol. in Bratislava, Bratislava, Slovakia; Fac. of Inf. \u0026 Inf. Technol., Slovak Univ. of Technol. in Bratislava, Bratislava, Slovakia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34ac"},"title":"Poster: Symbolic Execution of MPI Programs","abstract":"MPI is widely used in high performance computing. In this extended abstract, we report our current status of analyzing MPI programs. Our method can provide coverage of both input and non-determinism for MPI programs with mixed blocking and non-blocking operations. In addition, to improve the scalability further, a deadlock-oriented guiding method for symbolic execution is proposed. We have implemented our methods, and the preliminary experimental results are promising.","conference":"IEEE","terms":"System recovery;Scalability;High performance computing;Runtime;Message passing;Standards;Space exploration,application program interfaces;message passing;parallel processing;program diagnostics;software reliability,MPI program symbolic execution;high performance computing;MPI program analysis;deadlock-oriented guiding method","keywords":"MPI;Symbolic Execution;Synchronous;Asynchronous;Deadlock","startPage":"809","endPage":"810","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203080","citationCount":1,"referenceCount":7,"year":2015,"authors":"X. Fu; Z. Chen; H. Yu; C. Huang; W. Dong; J. Wang","affiliations":"State Key Lab. of High Performance Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; State Key Lab. of High Performance Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; State Key Lab. of High Performance Comput., Nat. Univ. of Defense Technol., Changsha, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34ad"},"title":"Second International Workshop on Software Architecture and Metrics (SAM 2015)","abstract":"Software engineers and architects of complex software systems need to balance hard quality attribute requirements while at the same time manage risks and make decisions with a system-wide and long-lasting impact. To achieve these tasks efficiently, they need quantitative information about design-time and run-time system aspects through usable and quick tools. While there is body of work focusing on code quality and metrics, their applicability at the design and architecture level and at scale are inconsistent and not proven. We are interested in exploring whether architecture can assist with better contextualizing existing system and code quality and metrics approaches. Furthermore, we ask whether we need additional architecture-level metrics to make progress and whether something as complex and subtle as software architecture can be quantified. The goal of this workshop is to discuss progress, gather empirical evidence, and identify priorities for a research agenda on architecture and metrics in the software engineering field.","conference":"IEEE","terms":"Computer architecture;Conferences;Software architecture;Software;Software measurement,,","keywords":"Software architecture; metrics; software analytics; technical debt; software quality; software maintenance and evolution; empirical software engineering; qualitative methods","startPage":"999","endPage":"1000","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203147","citationCount":2,"referenceCount":6,"year":2015,"authors":"I. Ozkaya; R. L. Nord; H. Koziolek; P. Avgeriou","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34ae"},"title":"2nd International Workshop on Software Engineering Research and Industrial Practice (SER\u0026IP 2015)","abstract":"Differing perceptions and expectations are obstaclesto collaboration between software engineering (SE) researchersand practitioners: Researchers often have a view thatpractitioners are reluctant to share real data. Practitionersbelieve that researchers are mostly working on topics which aredivorced from real industrial needs. Researchers believe thatpractitioners are looking for quick fixes. Practitioners have aview that case studies in research do not represent thecomplexities of real projects. Researchers may expect a few yearsto do research on a problem whereas practitioners expect a quicksolution that pays off immediately.Researchers and practitioners need to identify the gaps and todiscover the ways to collaborate to strengthen SE research andindustrial practice (IP). The main purpose of this workshop is tobring together researchers and practitioners to discuss thecurrent state of SE research and IP and to enhance collaborationbetween them. The SER\u0026IP 2015 workshop provided a platformto share success stories of SE research-practice partnerships aswell as to discuss the challenges, through a day-long agenda ofkeynotes, paper presentations and round table discussions.","conference":"IEEE","terms":"Conferences;Industries;Software engineering;Collaboration;Software;Business;Australia,,","keywords":"Software engineering research;industrial practice;researchers;practitioners;collaboration;challenges","startPage":"1007","endPage":"1008","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203151","citationCount":0,"referenceCount":5,"year":2015,"authors":"J. Bishop; R. Shukla; F. Shull; S. Sen","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34af"},"title":"Extract Package Refactoring in ARIES","abstract":"Software evolution often leads to the degradation of software design quality. In Object-Oriented (OO) systems, this often results in packages that are hard to understand and maintain, as they group together heterogeneous classes with unrelated responsibilities. In such cases, state-of-the-art re-modularization tools solve the problem by proposing a new organization of the existing classes into packages. However, as indicated by recent empirical studies, such approaches require changing thousands of lines of code to implement the new recommended modularization. In this demo, we present the implementation of an Extract Package refactoring approach in ARIES (Automated Refactoring In EclipSe), a tool supporting refactoring operations in Eclipse. Unlike state-of-the-art approaches, ARIES automatically identifies and removes single low-cohesive packages from software systems, which represent localized design flaws in the package organization, with the aim to incrementally improve the overall quality of the software modularisation.","conference":"IEEE","terms":"Couplings;Measurement;Iterative closest point algorithm;Software engineering;Software systems;Organizations,object-oriented programming;software maintenance;software quality;software tools,extract package refactoring;ARIES tool;software evolution;software design quality degradation;OO systems;object-oriented systems;re-modularization tools;automated refactoring in EclipSe;single low-cohesive packages;software systems;localized design flaws;software modularisation","keywords":"","startPage":"669","endPage":"672","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203039","citationCount":2,"referenceCount":7,"year":2015,"authors":"F. Palomba; M. Tufano; G. Bavota; R. Oliveto; A. Marcus; D. Poshyvanyk; A. De Lucia","affiliations":"Dept. of Manage. \u0026 Inf. Technol., Univ. of Salerno, Salerno, Italy; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Center for Appl. Software Eng., Free Univ. of Bolzano-Bozen, Bolzano, Italy; Dept. of Biosci. \u0026 Territory, Univ. of Molise, Campobasso, Italy; Dept. of Comput. Sci., Univ. of Texas, Austin, TX, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Manage. \u0026 Inf. Technol., Univ. of Salerno, Salerno, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b0"},"title":"FLEXISKETCH TEAM: Collaborative Sketching and Notation Creation on the Fly","abstract":"When software engineers collaborate, they frequently use whiteboards or paper for sketching diagrams. This is fast and flexible, but the resulting diagrams cannot be interpreted by software modeling tools. We present FLEXISKETCH TEAM, a tool solution consisting of a significantly extended version of our previous, single-user FLEXISKETCH tool for Android devices and a new desktop tool. Our solution for collaborative, model-based sketching of free-form diagrams allows users to define and re-use diagramming notations on the fly. Several users can work simultaneously on the same model sketch with multiple tablets. The desktop tool provides a shared view of the drawing canvas which can be projected onto an electronic whiteboard. Preliminary results from an exploratory study show that our tool motivates meeting participants to actively take part in sketching as well as defining ad-hoc notations.","conference":"IEEE","terms":"Metamodeling;Libraries;Collaboration;Object oriented modeling;Software;Conferences;Unified modeling language,mobile computing;software engineering;software tools,FLEXISKETCH team;collaborative sketching;software modeling tools;Android devices;desktop tool;single-user FLEXISKETCH tool;collaborative model-based sketching;free-form diagrams;diagramming notation creation on the fly;electronic whiteboard;multiple tablets","keywords":"","startPage":"685","endPage":"688","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203043","citationCount":7,"referenceCount":19,"year":2015,"authors":"D. Wüest; N. Seyff; M. Glinz","affiliations":"Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b1"},"title":"Contributor's Performance, Participation Intentions, Its Influencers and Project Performance","abstract":"Software project performance largely depends on the software development team. Studies have shown that interest and activity levels of contributors at any time significantly affect project success measures. This dissertation provides suggestions to enhance contributors' performance and participation intentions to help improve project performance. To do so, we mine historical data in software repositories from a two-pronged approach: 1) To assess contributors' performance to identify strengths and areas of improvement. 2) To measure the influence of factors on contributors' participation and performance, and provide suggestions that help advance contributor's engagement. The methodology used in this study leverage empirical techniques, both quantitative and qualitative, to conduct the analysis. We believe that the insights presented here will help contributors improve their performance. Also, we expect managers and business analysts to benefit from the suggestions to revise factors that negatively influence contributors' engagement and hence improve project performance.","conference":"IEEE","terms":"Atmospheric measurements;Particle measurements;Open source software;Data mining;Software measurement;Software engineering,data mining;project management;software development management,participation intentions;project performance;contributor performance;software project performance;software development team;historical data mining;software repository;contributor engagement","keywords":"Mining Software Repositories;Performance;Participation Intentions;Software Projects","startPage":"919","endPage":"922","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203114","citationCount":0,"referenceCount":35,"year":2015,"authors":"A. Rastogi","affiliations":"Indraprastha Inst. of Inf. Technol., Delhi, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b2"},"title":"Poster: Enhancing Partition Testing through Output Variation","abstract":"A major test case generation approach is to divide the input domain into disjoint partitions, from which test cases can be selected. However, we observe that in some traditional approaches to partition testing, the same partition may be associated with different output scenarios. Such an observation implies that the partitioning of the input domain may not be precise enough for effective software fault detection. To solve this problem, partition testing should be fine-tuned to additionally use the information of output scenarios in test case generation, such that these test cases are more fine-grained not only with respect to the input partitions but also from the perspective of output scenarios.","conference":"IEEE","terms":"Testing;Software;Concrete;Conferences;Software engineering;Fault detection;Indexes,fault diagnosis;program testing;software fault tolerance,partition testing;test case generation approach;disjoint partitions;output variation;software fault detection;input domain partitioning","keywords":"partition testing;choice relation framework;output scenario","startPage":"805","endPage":"806","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203078","citationCount":0,"referenceCount":5,"year":2015,"authors":"H. Liu; P. Poon; T. Y. Chen","affiliations":"RMIT Univ., Melbourne, VIC, Australia; Hong Kong Polytech. Univ., Hong Kong, China; Swinburne Univ. of Technol., Hawthorn, VIC, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b3"},"title":"Poster: Dynamic Analysis Using JavaScript Proxies","abstract":"JavaScript has become a popular programming language. However, its highly dynamic nature encumbers static analysis for quality assurance purposes. Only dynamic techniques such as concolic testing seem to cope. Often, these involve an instrumentation phase in which source code is extended with analysis-specific concerns. The corresponding implementations represent a duplication of engineering efforts. To facilitate developing dynamic analyses for JavaScript, we introduce Aran; a general-purpose JavaScript instrumenter that takes advantage of proxies, a recent addition to the JavaScript reflection APIs.","conference":"IEEE","terms":"Instruments;Performance analysis;Testing;Shadow mapping;Runtime;Browsers,authoring languages;Java;program diagnostics;program testing;quality assurance;software quality;source code (software),JavaScript proxies;dynamic program analysis;programming language;quality assurance;static program analysis;concolic testing;source code;instrumentation phase;Aran general-purpose JavaScript instrumenter;JavaScript reflection APIs","keywords":"Dynamic Analysis;JavaScript;Harmony Proxy;Instrumentation;ECMAScript6","startPage":"813","endPage":"814","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203082","citationCount":1,"referenceCount":9,"year":2015,"authors":"L. Christophe; C. De Roover; W. De Meuter","affiliations":"Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium; Software Languages Lab., Vrije Univ. Brussel, Brussels, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b4"},"title":"SE4HPCS'15: The 2015 International Workshop on Software Engineering for High Performance Computing in Science","abstract":"HPC software is developed and used in a wide variety of scientific domains including nuclear physics, computational chemistry, crash simulation, satellite data processing, fluid dynamics, climate modeling, bioinformatics, and vehicle development. The increase in the importance of this software motivates the need to identify and understand appropriate software engineering (SE) practices for HPC architectures. Because of the variety of the scientific domains addressed using HPC, existing SE tools and techniques developed for the business/IT community are often not efficient or effective. Appropriate SE solutions must account for the salient characteristics of the HPC, research oriented development environment. This situation creates a need for members of the SE community to interact with members of the scientific and HPC communities to address this need. This workshop facilitates that collaboration by bringing together members of the SE, the scientific, and the HPC communities to share perspectives and present findings relevant to research, practice, and education. A significant portion of the workshop is devoted to focused interaction among the participants with the goal of generating a research agenda to improve tools, techniques, and experimental methods regarding SE for HPC science.","conference":"IEEE","terms":"Software;Conferences;Software engineering;Computer architecture;Scientific computing;Computational modeling;High performance computing,,","keywords":"Software Engineering;Computational Science;Computational Engineering;High Performance Computing","startPage":"1003","endPage":"1004","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203149","citationCount":0,"referenceCount":7,"year":2015,"authors":"J. C. Carver; N. Chue Hong; P. Ciancarini","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b5"},"title":"Varis: IDE Support for Embedded Client Code in PHP Web Applications","abstract":"In software development, IDE services are used to assist developers in programming tasks. In dynamic web applications, however, since the client-side code is embedded in the server-side program as string literals, providing IDE services for such embedded code is challenging. We introduce Varis, a tool that provides services on the embedded client-side code. We perform symbolic execution on a PHP program to approximate its output and parse it into a VarDOM that compactly represents all its DOM variations. Using the VarDOM, we implement various types of IDE services for embedded client code including syntax highlighting, code completion, and 'find declaration'.","conference":"IEEE","terms":"HTML;Syntactics;Cascading style sheets;Servers;Presses;Approximation methods,embedded systems;Internet;programming environments,Varis tool;IDE support services;embedded client-side code;PHP Web applications;software development;programming tasks;server-side program;symbolic execution;VarDOM;editor services","keywords":"IDE services;embedded code;web applications","startPage":"693","endPage":"696","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203045","citationCount":3,"referenceCount":10,"year":2015,"authors":"H. V. Nguyen; C. Kästner; T. N. Nguyen","affiliations":"ECpE Dept., Iowa State Univ., Ames, IA, USA; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; ECpE Dept., Iowa State Univ., Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b6"},"title":"Software Requirements Patterns - A State of the Art and the Practice","abstract":"Software requirement patterns are an increasingly popular approach to knowledge reuse in the requirements engineering phase. Several research proposals have been formulated in the last years, and this technical briefing presents them. Beyond that, a report on the current adoption of these proposals (or any other ad-hoc approach) in industry is presented. This state of the practice will show that the need to pave the road to successful adoption still persists.","conference":"IEEE","terms":"Software;Requirements engineering;Software engineering;Bibliographies;Systematics;Proposals;Industries,formal specification;formal verification;object-oriented programming;systems analysis,software requirement patterns;knowledge reuse;requirements engineering","keywords":"Software Requirements Patterns;Requirements Engineering;Patterns;Requirements Reuse;Knowledge Engineering;Empirical Study;Literature Review;Survey","startPage":"943","endPage":"944","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203120","citationCount":1,"referenceCount":16,"year":2015,"authors":"X. Franch","affiliations":"Group of Software \u0026 Service Eng. (GESSI), Univ. Politec. de Catalunya (UPC-BarcelonaTech), Barcelona, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b7"},"title":"Poster: VIBeS, Transition System Mutation Made Easy","abstract":"Mutation testing is an established technique used to evaluate the quality of a set of test cases. As model-based testing took momentum, mutation techniques were lifted to the model level. However, as for code mutation analysis, assessing test cases on a large set of mutants can be costly. In this paper, we introduce the Variability-Intensive Behavioural teSting (VIBeS) framework. Relying on Featured Transition Systems (FTSs), we represent all possible mutants in a single model constrained by a feature model for mutant (in)activation. This allow to assess all mutants in a single test case execution. We present VIBeS implementation steps and the DSL we defined to ease model-based mutation analysis.","conference":"IEEE","terms":"Testing;Analytical models;Computational modeling;DSL;Java;Adaptation models;Load modeling,program testing;software quality,VIBeS;transition system mutation techniques;mutation testing;model-based testing;variability-intensive behavioural testing framework;featured transition systems;FTSs;single test case execution;model-based mutation analysis;DSL;code mutation analysis","keywords":"Model-Based Mutation Testing;Featured Transition Systems;VIBeS","startPage":"817","endPage":"818","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203084","citationCount":4,"referenceCount":8,"year":2015,"authors":"X. Devroey; G. Perrouin; P. Schobbens; P. Heymans","affiliations":"PReCISE Res. Center, Univ. of Namur, Namur, Belgium; PReCISE Res. Center, Univ. of Namur, Namur, Belgium; PReCISE Res. Center, Univ. of Namur, Namur, Belgium; PReCISE Res. Center, Univ. of Namur, Namur, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b8"},"title":"StriSynth: Synthesis for Live Programming","abstract":"Motivated by applications in automating repetitive file manipulations, we present a tool called StriSynth, which allows end-users to perform transformations over data using examples. Based on provided examples, our tool automatically generates scripts for non-trivial file manipulations. Although the current focus of StriSynth are file manipulations, it implements a more general string transformation framework. This framework builds on and further extends the functionality of Flash Fill -- a Microsoft Excel extension for string transformations. An accompanying video to this paper is available at the following website http://youtu.be/kkDZphqIdFM.","conference":"IEEE","terms":"Partitioning algorithms;Radiation detectors;Writing;Programming;Electronic mail;Computers;Benchmark testing,authoring languages;programming,StriSynth;live programming synthesis;automating repetitive file manipulations;script generation;nontrivial file manipulations;string transformation framework;Flash Fill;Microsoft Excel extension;string transformations","keywords":"Synthesis;Live Programming;Scripting;File Manipulation;Programming by Example","startPage":"701","endPage":"704","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203047","citationCount":4,"referenceCount":11,"year":2015,"authors":"S. Gulwani; M. Mayer; F. Niksic; R. Piskac","affiliations":"Microsoft Res., Redmond, WA, USA; EPFL, Lausanne, Switzerland; MPI-SWS, Germany; Yale Univ., New Haven, CT, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34b9"},"title":"TesMa and CATG: Automated Test Generation Tools for Models of Enterprise Applications","abstract":"We present CATG, an open-source concolic test generation tool for Java and its integration with TesMa, a model-based testing tool which automatically generates test cases from formal design documents. TesMa takes as input a set of design documents of an application under test. The design documents are provided in the form of database table definitions, process-flow diagrams, and screen definitions. From these design documents, TesMa creates Java programs for the feasible execution scenarios of the application. CATG performs concolic testing on these Java programs to generate suitable databases and test inputs required to test the application under test. A demo video of the tool is available at https://www.youtube.com/watch?v=9lEvPwR7g-Q.","conference":"IEEE","terms":"Java;Testing;Databases;Concrete;Open source software;Business;Libraries,business data processing;data flow analysis;Java;program testing;public domain software,TesMa program;CATG;automated test generation tool;enterprise application model;open-source concolic test generation tool;Java program;design document;database table definition;process-flow diagram;screen definition","keywords":"","startPage":"717","endPage":"720","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203051","citationCount":8,"referenceCount":8,"year":2015,"authors":"H. Tanno; X. Zhang; T. Hoshino; K. Sen","affiliations":"NTT Labs., Japan; NTT Labs., Japan; NTT Labs., Japan; EECS Dept., UC Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34ba"},"title":"A Security Practices Evaluation Framework","abstract":"Software development teams need guidance on choosing security practices so they can develop code securely. The academic and practitioner literature on software development security practices is large, and expanding. However, published empirical evidence for security practice use in software development is limited and fragmented, making choosing appropriate practices difficult. Measurement frameworks offer a tool for collecting and comparing software engineering data. The goal of this work is to aid software practitioners in evaluating security practice use in the development process by defining and validating a measurement framework for software development security practice use and outcomes. We define the Security Practices Evaluation Framework (SP-EF), a measurement framework for software development security practices. SP-EF supports evidence-based practice selection. To enable comparison of practices across publications and projects, we define an ontology of software development security practices. We evaluate the framework and ontology on historical data and industrial projects.","conference":"IEEE","terms":"Security;Software;Software measurement;Ontologies;Context;Size measurement;Process control,security of data;software development management;software metrics;software quality,security practice evaluation framework;software development teams;software development security practices;software engineering data;software development process;SP-EF;evidence-based practice selection;software development security practice ontology;industrial projects","keywords":"Security;Quality;Measurement Frameworks;Software Development Lifecycle.","startPage":"935","endPage":"938","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203118","citationCount":1,"referenceCount":13,"year":2015,"authors":"P. Morrison","affiliations":"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34bb"},"title":"Software Engineering for Privacy in-the-Large","abstract":"There will be an estimated 35 zettabytes (35 × 1021) of digital records worldwide by the year 2020. This effectively amounts to privacy management on an ultra-large-scale. In this briefing, we discuss the privacy challenges posed by such an ultra-large-scale ecosystem - we term this “Privacy in the Large”. We will contrast existing approaches to privacy management, reflect on their strengths and limitations in this regard and outline key software engineering research and practice challenges to be addressed in the future.","conference":"IEEE","terms":"Privacy;Software engineering;Data privacy;Security;Biological system modeling;Usability,data privacy;software engineering,software engineering;privacy management;ultra-large-scale ecosystem;privacy-in-the-large","keywords":"","startPage":"947","endPage":"948","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203122","citationCount":1,"referenceCount":15,"year":2015,"authors":"P. Anthonysamy; A. Rashid","affiliations":"Security-Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Google, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34bc"},"title":"[Journal First] Augmenting and Structuring User Queries to Support Efficient Free-Form Code Search","abstract":"Source code terms such as method names and variable types are often different from conceptual words mentioned in a search query. This vocabulary mismatch problem can make code search inefficient. In this paper, we present Code voCaBulary (CoCaBu), an approach to resolving the vocabulary mismatch problem when dealing with free-form code search queries. Our approach leverages common developer questions and the associated expert answers to augment user queries with the relevant, but missing, structural code entities in order to improve the performance of matching relevant code examples within large code repositories. To instantiate this approach, we build GitSearch, a code search engine, on top of GitHub and Stack Overflow Q\u0026A data. We evaluate GitSearch in several dimensions to demonstrate that (1) its code search results are correct with respect to user-accepted answers; (2) the results are qualitatively better than those of existing Internet-scale code search engines; (3) our engine is competitive against web search engines, such as Google, in helping users solve programming tasks; and (4) GitSearch provides code examples that are acceptable or interesting to the community as answers for Stack Overflow questions.","conference":"IEEE","terms":"Search engines;Software engineering;Vocabulary;Indexes;Programming;Engines;Natural languages,Internet;query processing;question answering (information retrieval);search engines,structuring user queries;source code terms;search query;vocabulary mismatch problem;free-form code search queries;structural code entities;code repositories;GitSearch;user-accepted answers;web search engines;Internet-scale code search engines;expert answers;code vocabulary;CoCaBu;developer questions;stack overflow Q and A data;Google;programming tasks;user queries augmentation","keywords":"Code search;GitHub;Free form search;Query augmentation;StackOverflow;Vocabulary mismatch","startPage":"945","endPage":"945","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453173","citationCount":1,"referenceCount":0,"year":2018,"authors":"R. Sirres; T. F. Bissyandé; D. Kim; D. Lo; J. Klein; K. Kim; Y. Le Traon","affiliations":"Nat. Libr. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; Singapore Manage. Univ. - Singapore, Singapore, Singapore; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34bd"},"title":"[Journal First] Effect Sizes and their Variance for AB/BA Crossover Design Studies","abstract":"We addressed the issues related to repeated measures experimental design such as an AB/BA crossover design (where each participant uses each method) that have been neither discussed nor addressed in the software engineering literature. Firstly, there are potentially two different standardized mean difference effect sizes that can be calculated, depending on whether the mean difference is standardized by the pooled within groups variance or the within-participants variance. Hence, we provided equations for non-standardized and standardized effect sizes and explained the need for two different types of standardized effect size, one for the repeated measures and one that would be equivalent to an independent groups design. Secondly, as for any estimated parameters and also for the purposes of undertaking meta-analysis, it is necessary to calculate the variance of the standardized mean difference effect sizes (which is not the same as the variance of the study). Hence, we provided formulas for the small sample size effect size variance and the medium sample size approximation to the effect size variance, for both types of standardized effect size. We also presented the model underlying the AB/BA crossover design and provided two examples (an empirical analysis of the real data set by Scanniello, as well as simulated data) to demonstrate how to construct the two standardized mean difference effect sizes and their variances, both from standard descriptive statistics and from the outputs provided by the linear mixed model package lme4 in R. A conclusion is that crossover designs should be considered (instead of between groups design) only if: · previous research has suggested that ρ is greater than zero and preferably greater than 0.25; · there is either strong theoretical argument, or empirical evidence from a well-powered study, that the period by technique interaction is negligible. Summarizing, our journal first paper [3]: (1) Presents the formulas needed to calculate both non-standardized and standardized mean difference effect sizes for AB/BA crossover designs (see Section 4 and 5 of our paper [3]). (2) Presents the formulas needed to estimate the variances of the non-standardized and standardized effect sizes which in the later cases need to be appropriate for the small to medium sample sizes commonly used in software engineering crossover designs (see Section 5 of our paper [3]). (3) Explains how to calculate the effect sizes and their variances both from the descriptive statistics that should be reported and from the raw data (see Section 6 of our paper [3]). It is worth mentioning that we based our formulas on our own corrections to the formulas presented earlier by Curtin et al. [1]. Our corrections for the variances of standardized weighted mean difference of an AB/BA cross-over trial were accepted by the author of the original formulas (Curtin), submitted jointly as a letter to Editor of Statistics in Medicine to assure the widespread (also beyond the software engineering domain) adoption of the corrected formulas, and accepted [2]. We proposed an alternative formulation of the standardized effect size for individual difference effects that is comparable with the standardized effect size commonly used for pretest/posttest studies. We also corrected the small sample size and moderate sample size variances reported by Curtin et al. for both the individual difference effect size and the standardized effect size comparable to independent groups trials, showing the derivation of the formulas from the variance of at-variable. Using these results, researchers can now correctly calculate standardized effect size variances, allowing the calculation of confidence intervals for AB/BA cross-over trials, which in turn provides a direct link to null hypothesis testing and supports meta-analysis. Meta-analysts can now validly aggregate together results from independent groups, pretest/posttest and AB/BA cross-over trials. Last but not least, the presented contributions allow corrections of previously reported results.","conference":"IEEE","terms":"Software engineering;Size measurement;Analytical models;Data models;Standards;Clinical trials;Computer science,medical computing;software engineering;statistical analysis,standardized mean difference effect sizes;standardized weighted mean difference;sample size effect size variance;medium sample size approximation;difference effect size;AB-BA crossover design;software engineering literature;meta-analysis;standard descriptive statistics;linear mixed model package;null hypothesis testing;posttest studies;pretest studies;software engineering crossover designs","keywords":"Empirical software engineering;Meta-analysis;Effect size","startPage":"420","endPage":"420","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453101","citationCount":0,"referenceCount":0,"year":2018,"authors":"L. Madeyski; B. Kitchenham","affiliations":"Fac. of Comput. Sci. \u0026 Manage., Wroclaw Univ. of Sci. \u0026 Technol., Wroclaw, Poland; Sch. of Comput. \u0026 Math., Keele Univ., Keele, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9118eee8435e8e7d34be"},"title":"Enlightened Debugging","abstract":"Numerous automated techniques have been proposed to reduce the cost of software debugging, a notoriously time-consuming and human-intensive activity. Among these techniques, Statistical Fault Localization (SFL) is particularly popular. One issue with SFL is that it is based on strong, often unrealistic assumptions on how developers behave when debugging. To address this problem, we propose Enlighten, an interactive, feedback-driven fault localization technique. Given a failing test, Enlighten (1) leverages SFL and dynamic dependence analysis to identify suspicious method invocations and corresponding data values, (2) presents the developer with a query about the most suspicious invocation expressed in terms of inputs and outputs, (3) encodes the developer feedback on the correctness of individual data values as extra program specifications, and (4) repeats these steps until the fault is found. We evaluated Enlighten in two ways. First, we applied Enlighten to 1,807 real and seeded faults in 3 open source programs using an automated oracle as a simulated user; for over 96% of these faults, Enlighten required less than 10 interactions with the simulated user to localize the fault, and a sensitivity analysis showed that the results were robust to erroneous responses. Second, we performed an actual user study on 4 faults with 24 participants and found that participants who used Enlighten performed significantly better than those not using our tool, in terms of both number of faults localized and time needed to localize the faults.","conference":"IEEE","terms":"Debugging;Performance analysis;Tools;Generators;Calculators;Software debugging;Task analysis,fault diagnosis;program debugging;program testing;software fault tolerance,enlightened debugging;numerous automated techniques;software debugging;notoriously time-consuming;human-intensive activity;unrealistic assumptions;feedback-driven fault localization technique;Enlighten leverages SFL;dynamic dependence analysis;developer feedback;individual data values;extra program specifications;automated oracle;sensitivity analysis;open source programs;statistical fault localization","keywords":"debugging;fault localization;dynamic analysis","startPage":"82","endPage":"92","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453065","citationCount":2,"referenceCount":0,"year":2018,"authors":"X. Li; S. Zhu; M. d’Amorim; A. Orso","affiliations":"Georgia Inst. of Technol., Atlanta, GA, USA; Georgia Inst. of Technol., Atlanta, GA, USA; Fed. Univ. of Pernambuco, Recife, Brazil; Georgia Inst. of Technol., Atlanta, GA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34bf"},"title":"[Journal First] What Makes a Great Manager of Software Engineers?","abstract":"Having great managers is as critical to success as having a good team or organization. A great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skill), it has overlooked the software engineering manager. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups.","conference":"IEEE","terms":"Software;Software engineering;Birds;Productivity;Organizations;Tools;Knowledge engineering,DP management;project management;software development management;statistical analysis;team working,great manager;software engineers;good team;software engineering manager;software teams;manager attributes;software engineering research studies;statistical analysis;productivity","keywords":"Manager;Software Engineering;Knowledge Work;Empirical Study;Conceptual Framework;Manager Attributes;Technical Skills","startPage":"701","endPage":"701","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453140","citationCount":0,"referenceCount":0,"year":2018,"authors":"E. Kalliamvakou; C. Bird; T. Zimmermann; A. Begel; R. DeLine; D. M. German","affiliations":"Univ. of Victoria, Victoria, BC, Canada; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Univ. of Victoria, Victoria, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c0"},"title":"[Journal First] An Empirical Study on the Interplay Between Semantic Coupling and Co-change of Software Classes","abstract":"The evolution of software systems is an inevitable process which has to be managed effectively to enhance software quality. Change impact analysis (CIA) is a technique that identifies impact sets, i.e., the set of classes that require correction as a result of a change made to a class or artefact. These sets can also be considered as ripple effects and typically non-local: changes propagate to different parts of a system. Two classes are considered logically coupled if they have co-changed in the past; past research has shown that the precision of CIA techniques increases if logical and semantic coupling (i.e., the extent to which the lexical content of two classes is related) are both considered. However, the relationship between semantic and logical coupling of software artefacts has not been extensively studied and no dependencies established between these two types of coupling. Are two often co-changed artefacts also strongly connected from a semantic point of view? Are two semantically similar artefacts bound to co-change in the future? Answering those questions would help increase the precision of CIA. It would also help software maintainers to focus on a smaller subset of artefacts more likely to co-evolve in the future. This study investigated the relationship between semantic and logical coupling. Using Chi-squared statistical tests, we identified similarities in semantic coupling using class corpora and class identifiers. We then computed Spearman's rank correlation between semantic and logical coupling metrics for class pairs to detect whether semantic and logical relationships co-varied in OO software. Finally, we investigated the overlap between semantic and logical relationships by identifying the proportion of classes linked through both coupling types. Our empirical study and results were based on seventy-nine open-source software projects. Results showed that: (a) measuring the semantic similarity of classes by using their identifiers is computationally efficient; (b) using identifier-based coupling can be used interchangeably with semantic similarity based on their corpora, albeit not always; (c) no correlation between the strengths of semantic and change coupling was found. Finally, (d) a directional relationship between the two was identified; 70% of semantic dependencies are linked through change coupling but not vice versa. Based on our findings, we conclude that identifying more efficient methods of semantic coupling computation as well as a directional relationship between semantic and change dependencies could help to improve CIA methods that integrate semantic coupling information. This may also help to reveal implicit dependencies not captured by static source code analysis.","conference":"IEEE","terms":"Semantics;Couplings;Software;Computer science;Measurement;Software engineering;Correlation,object-oriented programming;software maintenance;software metrics;software quality;statistical analysis;statistical testing,directional relationship;semantic dependencies;semantic coupling computation;semantic coupling information;software classes;software systems;software quality;change impact analysis;CIA;software artefacts;semantically similar artefacts;software maintainers;class corpora;class identifiers;semantic coupling metrics;logical coupling metrics;semantic relationships;logical relationships;OO software;coupling types;open-source software projects;semantic similarity;identifier-based coupling","keywords":"Semantic coupling;co-change;Java software;metrics","startPage":"432","endPage":"432","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453103","citationCount":0,"referenceCount":0,"year":2018,"authors":"N. Ajienka; A. Capiluppi; S. Counsell","affiliations":"Dept. of Comput. Sci., Edge Hill Univ., Ormskirk, UK; Dept. of Comput. Sci., Brunel Univ. London, Uxbridge, UK; Dept. of Comput. Sci., Brunel Univ. London, Uxbridge, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c1"},"title":"[Journal First] MAHAKIL: Diversity Based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction","abstract":"This study presents MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with five other sampling approaches using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant pf values than the other oversampling approaches, based on robust statistical tests.","conference":"IEEE","terms":"Software;Software engineering;Software measurement;Urban areas;Biological system modeling;Predictive models;Robustness,learning (artificial intelligence);pattern classification;sampling methods;software fault tolerance;software metrics;software quality;statistical testing,sampling approaches;prediction models;prediction performance;oversampling approaches;class imbalance issue;software defect prediction;software defect datasets;chromosomal theory;synthetic oversampling approach;MAHAKIL approach;robust statistical tests","keywords":"Software defect prediction;Class imbalance learning;Synthetic sample generation;Data sampling methods;Classification problems","startPage":"699","endPage":"699","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453138","citationCount":1,"referenceCount":0,"year":2018,"authors":"K. E. Bennin; J. Keung; P. Phannachitta; A. Monden; S. Mensah","affiliations":"Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China; Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China; Coll. of Arts, Chiang Mai Univ., Chiang Mai, Thailand; Grad. Sch. of Natural Sci. \u0026 Technol., Okayama Univ., Okayama, Japan; Dept. of Comput. Sci., City Univ. of Hong Kong, Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c2"},"title":"[Journal First] Does Syntax Highlighting Help Programming Novices?","abstract":"Background: Program comprehension is an important skill for programmers - extending and debugging existing source code is part of the daily routine. Syntax highlighting is one of the most common tools used to support developers in understanding algorithms. However, most research in this area originates from a time when programmers used a completely different tool chain. Objective: We examined the influence of syntax highlighting on novices' ability to comprehend source code. Additional analyses cover the influence of task type and programming experience on the code comprehension ability itself and its relation to syntax highlighting. Method: We conducted a controlled experiment with 390 undergraduate students in an introductory Java programming course. We measured the correctness with which they solved small coding tasks. Each test subject received some tasks with syntax highlighting and some without. Results: The data provided no evidence that syntax highlighting improves novices' ability to comprehend source code. Limitations: There are very few similar experiments and it is unclear as of yet which factors impact the effectiveness of syntax highlighting. One major limitation may be the types of tasks chosen for this experiment. Conclusion: The results suggest that syntax highlighting squanders a feedback channel from the IDE to the programmer that can be used more effectively.","conference":"IEEE","terms":"Syntactics;Task analysis;Programming;Software engineering;Visualization;Human computer interaction;Tools,computer science education;educational courses;feedback;Java;programming;programming environments,source code;syntax highlighting squanders;programming novices;code comprehension;undergraduate students;program comprehension;Java programming course;feedback channel;IDE","keywords":"Syntax Highlighting;Source Code Typography;Code Colouring;IDE Interface;Program Comprehension","startPage":"704","endPage":"704","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453142","citationCount":0,"referenceCount":0,"year":2018,"authors":"C. Hannebauer; M. Hesenius; V. Gruhn","affiliations":"NA; Univ. of Duisburg-Essen, Essen, Germany; Univ. of Duisburg-Essen, Essen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c3"},"title":"DetReduce: Minimizing Android GUI Test Suites for Regression Testing","abstract":"In recent years, several automated GUI testing techniques for Android apps have been proposed. These tools have been shown to be effective in achieving good test coverage and in finding bugs without human intervention. Being automated, these tools typically run for a long time (say, for several hours), either until they saturate test coverage or until a testing time budget expires. Thus, these automated tools are not good at generating concise regression test suites that could be used for testing in incremental development of the apps and in regression testing. We propose a heuristic technique that helps create a small regression test suite for an Android app from a large test suite generated by an automated Android GUI testing tool. The key insight behind our technique is that if we can identify and remove some common forms of redundancies introduced by existing automated GUI testing tools, then we can drastically lower the time required to minimize a GUI test suite. We have implemented our algorithm in a prototype tool called DetReduce. We applied DetReduce to several Android apps and found that DetReduce reduces a test-suite by an average factor of16.9× in size and14.7× in running time. We also found that for a test suite generated by running SwiftHand and a randomized test generation algorithm for 8 hours, DetReduce minimizes the test suite in an average of 14.6 hours.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Tools;Androids;Humanoid robots;Redundancy;Computer bugs,Android (operating system);graphical user interfaces;program testing,Android GUI test suites;randomized test generation algorithm;GUI test suite;testing tools;automated Android GUI testing tool;regression test suite;concise regression test suites;automated tools;testing time budget;Android app;automated GUI testing techniques;regression testing;DetReduce","keywords":"Android;GUI;Test minimization","startPage":"445","endPage":"455","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453105","citationCount":2,"referenceCount":0,"year":2018,"authors":"W. Choi; K. Sen; G. Necul; W. Wang","affiliations":"Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of Illinois, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c4"},"title":"Roles and Impacts of Hands-on Software Architects in Five Industrial Case Studies","abstract":"Whether software architects should also code is an enduring question. In order to satisfy performance, security, reliability and other quality concerns, architects need to compare and carefully choose a combination of architectural patterns, styles or tactics. Then later in the development cycle, these architectural choices must be implemented completely and correctly so there will not be any drift from envisioned design. In this paper, we use data analytics-based techniques to study five large-scale software systems, examining the impact and the role of software architects who write code on software quality. Our quantitative study is augmented with a follow-up interview of architects. This paper provides empirical evidence for supporting the pragmatic opinions that architects should write code. Our analysis shows that implementing architectural tactics is more complex than delivering functionality, tactics are more error prone than software functionalities, and the architects tend to introduce fewer bugs into the implementation of architectural tactics compared to the developers.","conference":"IEEE","terms":"Software engineering,data analysis;software architecture;software quality,architectural choices;large-scale software systems;software architects;software quality;quantitative study;architectural tactics;software functionalities","keywords":"Architect;coding;tactics;architecture savvy","startPage":"117","endPage":"127","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453069","citationCount":0,"referenceCount":0,"year":2018,"authors":"I. Rehman; M. Mirakhorli; M. Nagappan; A. Aralbay Uulu; M. Thornton","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c5"},"title":"Open Source Barriers to Entry, Revisited: A Sociotechnical Perspective","abstract":"Research has revealed that significant barriers exist when entering Open-Source Software (OSS) communities and that women disproportionately experience such barriers. However, this research has focused mainly on social/cultural factors, ignoring the environment itself - the tools and infrastructure. To shed some light onto how tools and infrastructure might somehow factor into OSS barriers to entry, we conducted a field study with five teams of software professionals, who worked through five use-cases to analyze the tools and infrastructure used in their OSS projects. These software professionals found tool/infrastructure barriers in 7% to 71% of the use-case steps that they analyzed, most of which are tied to newcomer barriers that have been established in the literature. Further, over 80% of the barrier types they found include attributes that are biased against women.","conference":"IEEE","terms":"Tools;Databases;Problem-solving;Open source software;Cultural differences;Face,cultural aspects;public domain software,sociotechnical perspective;OSS barriers;software professionals;OSS projects;newcomer barriers;open source barriers;open-source software communities;social-cultural factors;tool-infrastructure barriers;use-case steps","keywords":"open source software;newcomer;gender","startPage":"1004","endPage":"1015","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453179","citationCount":8,"referenceCount":0,"year":2018,"authors":"C. Mendez; H. S. Padala; Z. Steine-Hanson; C. Hildebrand; A. Horvath; C. Hill; L. Simpson; N. Patil; A. Sarma; M. Burnett","affiliations":"Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA; Oregon State Univ., Corvallis, OR, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c6"},"title":"Prioritizing Browser Environments for Web Application Test Execution","abstract":"When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from -12.24% to 39.05% for no ordering, and from -0.04% to 45.85% for random ordering.","conference":"IEEE","terms":"Browsers;Testing;History;Schedules;Optimal scheduling;Operating systems;Production,Internet;online front-ends;program testing,test case prioritization;application failures;testing client-side Web application;open-source Web applications;Web application test execution;Web-browser environments;regression testing Web applications;Web-browser types;feedback delay","keywords":"Web application testing;Regression testing;Browser environments","startPage":"468","endPage":"479","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453107","citationCount":0,"referenceCount":0,"year":2018,"authors":"J. Kwon; I. Ko; G. Rothermel","affiliations":"Sch. of Comput., KAIST, Daejeon, South Korea; Sch. of Comput., KAIST, Daejeon, South Korea; Univ. of Nebraska-Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c7"},"title":"[Journal First] Studying the Dialogue Between Users and Developers of Free Apps in the Google Play Store","abstract":"The popularity of mobile apps continues to grow over the past few years. Mobile app stores, such as the Google Play Store and Apple's App Store provide a unique user feedback mechanism to app developers through app reviews. In the Google Play Store (and most recently in the Apple App Store), developers are able to respond to such user feedback. Over the past years, mobile app reviews have been studied excessively by researchers. However, much of prior work (including our own prior work) incorrectly assumes that reviews are static in nature and that users never update their reviews. In a recent study, we started analyzing the dynamic nature of the review-response mechanism. Our previous study showed that responding to a review often has a positive effect on the rating that is given by the user to an app. In this paper [1], we revisit our prior finding in more depth by studying 4.5 million reviews with 126,686 responses of 2,328 top free-to-download apps in the Google Play Store. One of the major findings of our paper is that the assumption that reviews are static is incorrect. In particular, we find that developers and users in some cases use this response mechanism as a rudimentary user support tool, where dialogues emerge between users and developers through updated reviews and responses. Even though the messages are often simple, we find instances of as many as ten user-developer back-and-forth messages that occur via the response mechanism. Using a mixed-effect model, we identify that the likelihood of a developer responding to a review increases as the review rating gets lower or as the review content gets longer. In addition, we identify four patterns of developers: 1) developers who primarily respond to only negative reviews, 2) developers who primarily respond to negative reviews or to reviews based on their content, 3) developers who primarily respond to reviews which are posted shortly after the latest release of their app, and 4) developers who primarily respond to reviews which are posted long after the latest release of their app. We perform a qualitative analysis of developer responses to understand what drives developers to respond to a review. We manually analyzed a statistically representative random sample of 347 reviews with responses of the top ten apps with the highest number of developer responses. We identify seven drivers that make a developer respond to a review, of which the most important ones are to thank the users for using the app and to ask the user for more details about the reported issue. Our findings show that it can be worthwhile for app owners to respond to reviews, as responding may lead to an increase in the given rating. In addition, our findings show that studying the dialogue between users and developers provides valuable insights that can lead to improvements in the app store and the user support process. The main contributions of this paper are as follows: (1) Our paper is the first work to demonstrate the dynamic nature of reviews. (2) Furthermore, we are the first to demonstrate a peculiar use of the app-review platforms as a user support medium. (3) In addition, our work is the first work to deeply explore developer responses in a systematic manner. (4) Finally, our classification of developer-responses highlights the value of providing canned or even automated responses in next generation app-review platforms.","conference":"IEEE","terms":"Google;Software engineering;Software;Computer science;Australia;Tools;Systematics,consumer behaviour;mobile computing,free-to-download apps;review-response mechanism;mobile app reviews;Apple App Store;app developers;unique user feedback mechanism;mobile app stores;Google Play Store;free apps;developer-responses;app-review platforms;app owners;developer responses;negative reviews;review content;review rating;review increases;developer responding;user-developer;updated reviews;rudimentary user support tool","keywords":"Google Play Store;User-developer dialogue;Developer response;Mixed-effect model;Android mobile apps","startPage":"164","endPage":"164","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453075","citationCount":0,"referenceCount":0,"year":2018,"authors":"S. Hassan; C. Tantithamthavorn; C. Bezemer; A. E. Hassan","affiliations":"Software Anal. \u0026 Intell. Lab., Queen's Univ., Kingston, ON, Canada; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Software Anal. \u0026 Intell. Lab., Queen's Univ., Kingston, ON, Canada; Software Anal. \u0026 Intell. Lab., Queen's Univ., Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c8"},"title":"[Journal First] The Scent of a Smell: An Extensive Comparison Between Textual and Structural Smells","abstract":"Code smells are symptoms of poor design or implementation choices that have a negative effect on several aspects of software maintenance and evolution, such as program comprehension or change-and fault-proneness. This is why researchers have spent a lot of effort on devising methods that help developers to automatically detect them in source code. Almost all the techniques presented in literature are based on the analysis of structural properties extracted from source code, although alternative sources of information (e.g., textual analysis) for code smell detection have also been recently investigated. Nevertheless, some studies have indicated that code smells detected by existing tools based on the analysis of structural properties are generally ignored (and thus not refactored) by the developers. In this paper, we aim at understanding whether code smells detected using textual analysis are perceived and refactored by developers in the same or different way than code smells detected through structural analysis. To this aim, we set up two different experiments. We have first carried out a software repository mining study to analyze how developers act on textually or structurally detected code smells. Subsequently, we have conducted a user study with industrial developers and quality experts in order to qualitatively analyze how they perceive code smells identified using the two different sources of information. Results indicate that textually detected code smells are easier to identify and for this reason they are considered easier to refactor with respect to code smells detected using structural properties. On the other hand, the latter are often perceived as more severe, but more difficult to exactly identify and remove.","conference":"IEEE","terms":"Tools;Software engineering;Maintenance engineering;Data mining;Detectors;Software systems;Software quality,data mining;software maintenance;source code (software),structural smells;source code;textual analysis;code smell detection;textually detected code smells;textual smells;software maintenance;software evolution;software repository mining study","keywords":"code smells;empirical study;mining software repositories","startPage":"740","endPage":"740","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453146","citationCount":0,"referenceCount":0,"year":2018,"authors":"F. Palomba; A. Panichella; A. Zaidman; R. Oliveto; A. De Lucia","affiliations":"Univ. of Zurich, Zurich, Switzerland; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Univ. of Molise, Pesche, Italy; Univ. of Salerno, Salerno, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34c9"},"title":"Inferring Hierarchical Motifs from Execution Traces","abstract":"Program comprehension is a necessary step for performing many software engineering tasks. Dynamic analysis is effective in producing execution traces that assist comprehension. Traces are rich sources of information regarding the behaviour of a program. However, it is challenging to gain insight from traces due to their overwhelming amount of data and complexity. We propose a generic technique for facilitating comprehension by inferring recurring execution motifs. Inspired by bioinformatics, motifs are patterns in traces that are flexible to small changes in execution, and are captured in a hierarchical model. The hierarchical nature of the model provides an overview of the behaviour at a high-level, while preserving the execution details and intermediate levels in a structured manner. We design a visualization that allows developers to observe and interact with the model. We implement our approach in an open-source tool, called Sabalan, and evaluate it through a user experiment. The results show that using Sabalan improves developers' accuracy in performing comprehension tasks by 54%.","conference":"IEEE","terms":"Tools;Task analysis;Electronic mail;Biological system modeling;Software engineering;Bioinformatics;Data visualization,bioinformatics;object-oriented programming;program visualisation;software engineering;software maintenance,execution traces;program comprehension;software engineering tasks;dynamic analysis;assist comprehension;execution motifs;hierarchical model;hierarchical nature;execution details;comprehension tasks;inferring hierarchical motifs","keywords":"Program comprehension;behavioural model;hierarchical motifs","startPage":"776","endPage":"787","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453150","citationCount":3,"referenceCount":0,"year":2018,"authors":"S. Alimadadi; A. Mesbah; K. Pattabiraman","affiliations":"Northeastern Univ., Boston, MA, USA; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ca"},"title":"[Journal First] A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches","abstract":"Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.","conference":"IEEE","terms":"Benchmark testing;Software;Measurement;Software engineering;Computer science;Proposals;Ranking (statistics),quality assurance;software metrics;software quality,experiment setups;performance metrics;benchmark cross-project defect prediction approaches;data standardization;software products;software projects;quality assurance;CPDP","keywords":"cross project defect prediction;benchmark;comparison;replication","startPage":"1063","endPage":"1063","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453185","citationCount":0,"referenceCount":4,"year":2018,"authors":"S. Herbold; A. Trautsch; J. Grabowski","affiliations":"Insititute of Comput. Sci., Univ. of Goettingen, Gottingen, Germany; Insititute of Comput. Sci., Univ. of Goettingen, Gottingen, Germany; Insititute of Comput. Sci., Univ. of Goettingen, Gottingen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34cb"},"title":"[Journal First] Correctness Attraction: A Study of Stability of Software Behavior Under Runtime Perturbation","abstract":"Can the execution of software be perturbed without breaking the correctness of the output? In this paper, we devise a protocol to answer this question from a novel perspective. In an experimental study, we observe that many perturbations do not break the correctness in ten subject programs. We call this phenomenon \"correctness attraction\". The uniqueness of this protocol is that it considers a systematic exploration of the perturbation space as well as perfect oracles to determine the correctness of the output. To this extent, our findings on the stability of software under execution perturbations have a level of validity that has never been reported before in the scarce related work. A qualitative manual analysis enables us to set up the first taxonomy ever of the reasons behind correctness attraction.","conference":"IEEE","terms":"Perturbation methods;Software;Protocols;Stability analysis;Runtime;Systematics,program diagnostics;program testing,execution perturbations;perturbation space;phenomenoncorrectness attraction;subject programs;runtime perturbation;software behavior","keywords":"perturbation analysis;software correctness;empirical study","startPage":"481","endPage":"481","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453109","citationCount":0,"referenceCount":0,"year":2018,"authors":"B. Danglot; P. Preux; B. Baudry; M. Monperrus","affiliations":"Nord Eur., Inria, Villeneuve-d'Ascq, France; Univ. Lille, Villeneuve-d'Ascq, France; Bretage Atlantique, Inria Rennes, Rennes, France; KTH R. Inst. of Technol., Stockholm, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34cc"},"title":"The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game","abstract":"Motivation: The security of any system is a direct consequence of stakeholders' decisions regarding security requirements and their relative prioritisation. Such decisions are taken with varying degrees of expertise in security. In some organisations - particularly those with resources - these are the preserve of computer (or information) security teams. In others - typically smaller organisations - the computing services team may be charged with the responsibility. Often managers have a role to play as guardians of business targets and goals. Be it common workplace practices or strategic decision making, security decisions underpin not only the initial security requirements and their prioritisation but also the adaptation and evolution of these requirements as new business or security contexts arise. However, little is currently understood about how these various demographics approach cyber security decisions and the strategies and approaches that underpin those decisions. What are the typical decision patterns, if any, the consequences of such patterns and their impact (positive or negative) on the security of the system in question? Nor is there any substantial understanding of how the strategies and decision patterns of these different groups contrast. Is security expertise necessarily an advantage when making security decisions in a given context? Answers to these questions are key to understanding the \"how\" and \"why\" behind security decision processes. The Game: In this talk [1], we present a tabletop game - Decisions and Disruptions (D-D) [2] - as a means to investigate these very questions. The game tasks a group of players with managing the security of a small utility company while facing a variety of threats. The game provides a requirements sandbox in which players can experiment with threats, learn about decision making and its consequences, and reflect on their own perception of risk. The game is intentionally kept short - 2 hours - and simple enough to be played without prior training. A cyber-physical infrastructure, depicted through a Lego(R) board, makes the game easy to understand and accessible to players from varying backgrounds and security expertise, without being too trivial a setting for security experts. Key insights: We played D-D with 43 players divided into homogeneous groups (group sizes of 2-6 players): 4 groups of security experts, 4 groups of non-technical managers and 4 groups of general computer scientists. Such observations should, of course, not be generalised, however, the substantial sample size enables in-depth qualitative analysis. Our analysis reveals a number of novel insights regarding security decisions of our three demographics: - Strategies: Security experts had a strong interest in advanced technological solutions and tended to neglect intelligence gathering, to their own detriment: some security expert teams achieved poor results in the game. Managers, too, were technology-driven and focused on data protection while neglecting human factors more than other groups. Computer scientists tended to balance human factors and intelligence gathering with technical solutions, and achieved the best results of the three demographics. - Decision Processes: Technical experience significantly changes the way players think. Teams with little technical experience had shallow, intuition-driven discussions with few concrete arguments. Technical teams, and the most experienced in particular, had much richer debates, driven by concrete scenarios, anecdotes from experience, and procedural thinking. Security experts showed a high confidence in their decisions - despite some of them having bad consequences - while the other groups tended to doubt their own skills - even when they were playing good games. - Patterns: A number of characteristic plays could be identified, some good (balance between priorities, open-mindedness, and adapting strategies based on inputs that challenge one's pre-conceptions), some bad (excessive focus on particular issues, confidence in charismatic leaders), some ugly (\"tunnel vision\" syndrome by over-confident players). These patterns are documented and discussed in the full paper - showing the virtue of the positive ones, discouraging the negative ones, and inviting the readers to do their own introspection. Conclusion: D-D complements existing work on gamification as a means to improve security awareness, education, and training. Beyond the analysis of the security decisions of the three demographics, there is a definite educational and awareness-raising aspect to D-D (as noted consistently by players in all our subject groups). Game boxes will be brought to the conference for demonstration purposes, and the audience will be invited to experiment with D-D themselves, make their own decisions, and reflect on their own perception of security.","conference":"IEEE","terms":"Games;Cyber-physical systems;Computer security;Software engineering;Privacy;Google,computer games;cyber-physical systems;decision making;game theory;human factors;organisational aspects;security of data,decision making;tabletop game;stakeholders decisions;organisations;computer security teams;demographics approach;decisions and disruptions;D-D;sandbox requirements;qualitative analysis;human factors;gamification;security expert teams;players;cyber security decisions;cyber-physical systems game","keywords":"security decisions;security requirements;game;decision patterns","startPage":"496","endPage":"496","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453113","citationCount":1,"referenceCount":0,"year":2018,"authors":"S. Frey; A. Rashid; P. Anthonysamy; M. Pinto-Albuquerque; S. A. Naqvi","affiliations":"Univ. of Southampton, Southampton, UK; Univ. of Bristol, Bristol, UK; Google, Switzerland; Inst. Univ. de Lisboa, Lisbon, Portugal; Lancaster Univ., Lancaster, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34cd"},"title":"Leveraging Program Analysis to Reduce User-Perceived Latency in Mobile Applications","abstract":"Reducing network latency in mobile applications is an effective way of improving the mobile user experience and has tangible economic benefits. This paper presents PALOMA, a novel client-centric technique for reducing the network latency by prefetching HTTP requests in Android apps. Our work leverages string analysis and callback control-flow analysis to automatically instrument apps using PALOMA's rigorous formulation of scenarios that address \"what\" and \"when\" to prefetch. PALOMA has been shown to incur significant runtime savings (several hundred milliseconds per prefetchable HTTP request), both when applied on a reusable evaluation benchmark we have developed and on real applications.","conference":"IEEE","terms":"Prefetching;Uniform resource locators;Androids;Humanoid robots;Servers;Runtime;Mobile applications,mobile computing;program diagnostics;storage management,automatically instrument apps;prefetchable HTTP request;program analysis;user-perceived latency;mobile applications;mobile user experience;tangible economic benefits;novel client-centric technique;HTTP requests;Android apps","keywords":"program analysis;prefetch;network latency;mobile applications","startPage":"176","endPage":"186","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453077","citationCount":4,"referenceCount":0,"year":2018,"authors":"Y. Zhao; M. Schmitt Laser; Y. Lyu; N. Medvidovic","affiliations":"Univ. of Southern California, Los Angeles, CA, USA; Pontifical Catholic Univ. of Rio Grande do Sul, Porto Alegre, Brazil; Univ. of Southern California, Los Angeles, CA, USA; Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ce"},"title":"[Journal First] Journal First Presentation of an Experience Report on Applying Software Testing Academic Results in Industry: We Need Usable Automated Test Generation","abstract":"What is the impact of software engineering research on current practices in industry? In this paper, I report on my direct experience as a PhD/post-doc working in software engineering research projects, and then spending the following five years as an engineer in two different companies (the first one being the same I worked in collaboration with during my post-doc). Given a background in software engineering research, what cutting-edge techniques and tools from academia did I use in my daily work when developing and testing the systems of these companies? Regarding validation and verification (my main area of research), the answer is rather short: as far as I can tell, only FindBugs. In this paper, I report on why this was the case, and discuss all the challenging, complex open problems we face in industry and which somehow are \"neglected\" in the academic circles. In particular, I will first discuss what actual tools I could use in my daily work, such as JaCoCo and Selenium. Then, I will discuss the main open problems I faced, particularly related to environment simulators, unit and web testing. After that, popular topics in academia are presented, such as UML, regression and mutation testing. Their lack of impact on the type of projects I worked on in industry is then discussed. Finally, from this industrial experience, I provide my opinions about how this situation can be improved, in particular related to how academics are evaluated, and advocate for a greater involvement into open-source projects.","conference":"IEEE","terms":"Software engineering;Industries;Software testing;Test pattern generators;Companies;Tools,automatic test pattern generation;automatic test software;program testing;public domain software;software engineering,cutting-edge techniques;daily work;complex open problems;main open problems;web testing;regression;mutation testing;industrial experience;experience report;usable automated test generation;direct experience;PhD/post-doc;software engineering research projects;time 5.0 year","keywords":"Industry;Practice;Technology Transfer;Impact;Applied Research","startPage":"1065","endPage":"1065","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453187","citationCount":0,"referenceCount":0,"year":2018,"authors":"A. Arcuri","affiliations":"Fac. of Technol., Westerdals Oslo ACT, Oslo, Norway","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34cf"},"title":"[Journal First] Are Vulnerabilities Discovered and Resolved Like Other Defects?","abstract":"Software defect data has long been used to drive software development process improvement. If security defects (i.e., vulnerabilities) are discovered and resolved by different software development practices than non-security defects, the knowledge of that distinction could be applied to drive process improvement. The goal of this research is to support technical leaders in making security-specific software development process improvements by analyzing the differences between the discovery and resolution of defects versus that of vulnerabilities. We extend Orthogonal Defect Classification (ODC), a scheme for classifying software defects to support software development process improvement, to study process-related differences between vulnerabilities and defects, creating ODC + Vulnerabilities (ODC+V). We applied ODC+V to classify 583 vulnerabilities and 583 defects across 133 releases of three open-source projects (Firefox, phpMyAdmin, and Chrome). Compared with defects, vulnerabilities are found later in the development cycle and are more likely to be resolved through changes to conditional logic. In Firefox, vulnerabilities are resolved 33% more quickly than defects. From a process improvement perspective, these results indicate opportunities may exist for more efficient vulnerability detection and resolution. We found ODC+V's property of associating vulnerability and defect discovery and resolution events with their software development process contexts helpful for gaining insight into three open source software projects. The addition of the SecurityImpact attribute, in particular, brought visibility into when threat types are discovered during the development process. We would expect use of ODC+V (and of base ODC) periodically over time to be helpful for steering software development projects toward their quality assurance goals.","conference":"IEEE","terms":"Random access memory;Software engineering;Software measurement;Open source software;Security;Drives,pattern classification;project management;public domain software;quality assurance;security of data;software development management,software development projects;orthogonal defect classification;ODC;vulnerability detection;security-specific software development process;Firefox;phpMyAdmin;Chrome;open source software projects;defect discovery;nonsecurity defects;security defects","keywords":"metrics;security","startPage":"498","endPage":"498","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453115","citationCount":0,"referenceCount":0,"year":2018,"authors":"P. Morrison; R. Pandita; X. Xiao; R. Chillarege; L. Williams","affiliations":"North Carolina State Univ., Raleigh, NC, USA; Phase Change Software, Golden, CO, USA; Case Western Reserve Univ., Cleveland, OH, USA; Chillarege Inc., Raleigh, NC, USA; North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d0"},"title":"Hybrid Regression Test Selection","abstract":"Regression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time.","conference":"IEEE","terms":"Testing;Tools;Java;Runtime;Open source software,program testing;regression analysis,hybrid RTS approach;granularity RTS;selected test ratio;offline testing time;method-level changes;instance method additions/deletions;file-level changes;online testing time;code changes;RTS techniques;hybrid regression test selection;regression testing cost reduction;HyRTS technique;FRTS","keywords":"Test selection;Regression testing;Dynamic analysis;Empirical study","startPage":"199","endPage":"209","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453079","citationCount":9,"referenceCount":0,"year":2018,"authors":"L. Zhang","affiliations":"Univ. of Texas at Dallas, Dallas, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d1"},"title":"Inheritance Usage Patterns in Open-Source Systems","abstract":"This research investigates how object-oriented inheritance is actually used in practice. The aim is to close the gap between inheritance guidance and inheritance practice. It is based on detailed analyses of 2440 inheritance hierarchies drawn from 14 open-source systems. The original contributions made by this paper concern pragmatic assessment of inheritance hierarchy design quality. The findings show that inheritance is very widely used but that most of the usage patterns that occur in practice are simple in structure. They are so simple that they may not require much inheritance-specific design consideration. On the other hand, the majority of classes defined using inheritance actually appear within a relatively small number of large, complex hierarchies. While some of these large hierarchies appear to have a consistent structure, often based on a problem domain model or a design pattern, others do not. Another contribution is that the quality of hierarchies, especially the large problematic ones, may be assessed in practice based on size, shape, and the definition and invocation of novel methods - all properties that can be detected automatically.","conference":"IEEE","terms":"Software engineering;Gold,inheritance;object-oriented methods;public domain software,inheritance hierarchies;complex hierarchies;inheritance-specific design consideration;inheritance hierarchy design quality;14 open-source systems;inheritance guidance;object-oriented inheritance;inheritance usage patterns","keywords":"Object-oriented;inheritance;open source;empirical;design guidance","startPage":"245","endPage":"255","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453083","citationCount":0,"referenceCount":0,"year":2018,"authors":"J. Stevenson; M. Wood","affiliations":"Dept. of Comput. \u0026 Inf. Sci., Univ. of Strathclyde, Glasgow, UK; Dept. of Comput. \u0026 Inf. Sci., Univ. of Strathclyde, Glasgow, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d2"},"title":"[Journal First] Analyzing a Decade of Linux System Calls","abstract":"Over the past 25 years, thousands of developers have contributed more than 18 million lines of code (LOC) to the Linux kernel. As the Linux kernel forms the central part of various operating systems that are used by millions of users, the kernel must be continuously adapted to the changing demands and expectations of these users. The Linux kernel provides its services to an application through system calls. The combined set of all system calls forms the essential Application Programming Interface (API) through which an application interacts with the kernel. In this paper, we conduct an empirical study of 8,770 changes that were made to Linux system calls during the last decade (i.e., from April 2005 to December 2014). In particular, we study the size of the changes, and we manually identify the type of changes and bug fixes that were made. Our analysis provides an overview of the evolution of the Linux system calls over the last decade. We find that there was a considerable amount of technical debt in the kernel, that was addressed by adding a number of sibling calls (i.e., 26% of all system calls). In addition, we find that by far, the ptraceand signal handling system calls are the most challenging to maintain. Our study can be used by developers who want to improve the design and ensure the successful evolution of their own kernel APIs.","conference":"IEEE","terms":"Linux;Kernel;Computer bugs;Software engineering;Maintenance engineering;Testing,application program interfaces;Linux;operating system kernels;program debugging,essential application programming interface;kernel API;sibling calls;operating systems;Linux kernel forms;Linux system calls","keywords":"Linux kernel;System calls;API evolution;Software evolution","startPage":"267","endPage":"267","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453085","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. Bagherzadeh; N. Kahani; C. Bezemer; A. E. Hassan; J. Dingel; J. R. Cordy","affiliations":"Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d3"},"title":"The Evolution of Requirements Practices in Software Startups","abstract":"We use Grounded Theory to study the evolution of requirements practices of 16 software startups as they grow and introduce new products and services. These startups operate in a dynamic environment, with significant time and market pressure, and rarely have time for systematic requirements analysis. Our theory describes the evolution of practice along six dimensions that emerged as relevant to their requirements activities: requirements artefacts, knowledge management, requirements-related roles, planning, technical debt and product quality. Beyond the relationships among the dimensions, our theory also explains the turning points that drove the evolution along these dimensions. These changes are reactive, rather than planned, suggesting an overall pragmatic lightness, i.e., flexibility, in the startups' evolution towards engineering practices for requirements. Our theory organises knowledge about evolving requirements practice in maturing startups, and provides practical insights for startups' assessing their own evolution as they face challenges to their growth. Our research also suggests that a startup's evolution along the six dimensions is not fundamental to its success, but has significant effects on their product, their employees and the company.","conference":"IEEE","terms":"Companies;Interviews;Software;Biological system modeling;Turning;Software engineering,organisational aspects;software development management,requirements artefacts;engineering practices;maturing startups;practical insights;software startups;systematic requirement analysis","keywords":"requirements engineering;startups;evolution;grounded theory","startPage":"823","endPage":"833","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453156","citationCount":1,"referenceCount":0,"year":2018,"authors":"C. Gralha; D. Damian; A. Wasserman; M. Goulão; J. Araújo","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d4"},"title":"Collaborative Model-Driven Software Engineering: A Classification Framework and a Research Map [Extended Abstract]","abstract":"This proposal is about a study we recently published in the IEEE Transaction of Software Engineering journal [4]. Context: Collaborative software engineering (CoSE) deals with methods, processes and tools for enhancing collaboration, communication, and co-ordination (3C) among team members. CoSE can be employed to conceive different kinds of artifacts during the development and evolution of software systems. For instance, when focusing on software design, multiple stakeholders with different expertise and responsibility collaborate on the system design. Model-Driven Software Engineering (MDSE) provides suitable techniques and tools for specifying, manipulating, and analyzing modeling artifacts including metamodels, models, and transformations. Collaborative MDSE consists of methods or techniques in which multiple stakeholders manage, collaborate, and are aware of each others' work on a set of shared models. A collaborative MDSE approach is composed of three main complementary dimensions: (i) a model management infrastructure for managing the life cycle of the models, (ii) a set of collaboration means for allowing involved stakeholders to work on the modelling artifacts collaboratively, and (iii) a set of communication means for allowing involved stakeholders to exchange, share, and communicate information within the team. Collaborative MDSE is attracting several research efforts from different research areas (e.g., model-driven engineering, global software engineering, etc.), resulting in a variegated scientific body of knowledge on the topic. Objective: In this study we aim at identifying, classifying, and understanding existing collaborative MDSE approaches. More specifically, our goal is to assess (i) the key characteristics of collaborative MDSE approaches (e.g., model editing environments, model versioning mechanisms, model repositories, support for communication and decision making), (ii) their faced challenges and limitations, and (iii) the interest of researchers in collaborative MDSE approaches over time and their focus on the three dimensions of collaborative MDSE. Method: In order to achieve this, we designed and conducted a systematic mapping study on collaborative MDSE. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 106 selected papers, further clustered into 48 primary studies, along a time span of nineteen years. A suitable classification framework has been empirically defined and rigorously applied for extracting key information from each selected study. We collated, summarized, and analyzed extracted data by applying scientifically sound data synthesis techniques. Results: In addition to a number of specific insights, our analysis revealed the following key findings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration. Conclusion: This study gives a solid foundation for a thorough identification and comparison of existing and future approaches for collaborative MDSE. Those results can be used by both researchers and practitioners for identifying existing research/technical gaps to attack, better scoping their own contributions to the field, or better understanding or refining existing ones.","conference":"IEEE","terms":"Software engineering;Analytical models;Unified modeling language;Stakeholders;Collaborative software;Tools,decision making;groupware;software engineering;software maintenance;Unified Modeling Language,collaborative model-driven software engineering;CoSE;UML-based approach;decision making;understanding existing collaborative MDSE approaches;model-driven engineering","keywords":"Collaborative MDSE;CoMDSE;C MDSE;model driven engineering;collaborative software engineering;CoSE;systematic mapping study","startPage":"535","endPage":"535","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453119","citationCount":0,"referenceCount":0,"year":2018,"authors":"D. Di Ruscio; M. Franzago; I. Malavolta; H. Muccini","affiliations":"DISIM Dept., Univ. of L'Aquila, L'Aquila, Italy; DISIM Dept., Univ. of L'Aquila, L'Aquila, Italy; DISIM Dept., Univ. of L'Aquila, L'Aquila, Italy; Vrije Univ. Amsterdam, Amsterdam, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d5"},"title":"[Journal First] Are Fix-Inducing Changes a Moving Target?: A Longitudinal Case Study of Just-in-Time Defect Prediction","abstract":"Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.","conference":"IEEE","terms":"Software engineering;Predictive models;History;Fluctuations;Training;Data models;Software,data mining;just-in-time;learning (artificial intelligence);public domain software;software fault tolerance;software management,fix-inducing changes;JIT models;just-in-time models;just-in-time defect prediction;Qt systems;OpenStack systems;fix-inducing code changes;code change properties","keywords":"Just In Time prediction;Defect prediction;Mining software repositories","startPage":"560","endPage":"560","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453123","citationCount":1,"referenceCount":0,"year":2018,"authors":"S. McIntosh; Y. Kamei","affiliations":"McGill Univ., Montréal, QC, Canada; Kyushu Univ., Fukuoka, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d6"},"title":"Metamorphic Testing of RESTful Web APIs","abstract":"Web Application Programming Interfaces (APIs) specify how to access services and data over the network, typically using Web services. Web APIs are rapidly proliferating as a key element to foster reusability, integration, and innovation, enabling new consumption models such as mobile or smart TV apps. Companies such as Facebook, Twitter, Google, eBay or Netflix receive billions of API calls every day from thousands of different third-party applications and devices, which constitutes more than half of their total traffic. As Web APIs are progressively becoming the cornerstone of software integration, their validation is getting more critical. In this context, the fast detection of bugs is of utmost importance to increase the quality of internal products and third-party applications. However, testing Web APIs is challenging mainly due to the difficulty to assess whether the output of an API call is correct, i.e., the oracle problem. For instance, consider the Web API of the popular music streaming service Spotify. Suppose a search for albums with the query \"redhouse\" returning 21 total matches: Is this output correct? Do all the albums in the result set contain the keyword? Are there any albums containing the keyword not included in the result set? Answering these questions is difficult, even with small result sets, and often infeasible when the results are counted by thousands or millions. Metamorphic testing alleviates the oracle problem by providing an alternative when the expected output of a test execution is complex or unknown. Rather than checking the output of an individual program execution, metamorphic testing checks whether multiple executions of the program under test fulfil certain necessary properties called metamorphic relations. For instance, consider the following metamorphic relation in Spotify: two searches for albums with the same query should return the same number of total results regardless of the size of pagination. Suppose that a new Spotify search is performed using the exact same query as before and increasing the maximum number of results per page from 20 (default value) to 50: This search returns 27 total albums (6 more matches than in the previous search), which reveals a bug. This is an example of a real and reproducible fault detected using the approach presented in this paper and reported to Spotify. According to Spotify developers, it was a regression fault caused by a fix with undesired side effects. In this paper [1], we present a metamorphic testing approach for the automated detection of faults in RESTful Web APIs (henceforth also referred to as simply Web APIs). We introduce the concept of metamorphic relation output patterns. A Metamorphic Relation Output Pattern (MROP) defines an abstract output relation typically identified in Web APIs, regardless of their application domain. Each MROP is defined in terms of set operations among test outputs such as equality, union, subset, or intersection. MROPs provide a helpful guide for the identification of metamorphic relations, broadening the scope of our work beyond a particular Web API. Based on the notion of MROP, a methodology is proposed for the application of the approach to any Web API following the REST architectural pattern. The approach was evaluated in several steps. First, we used the proposed methodology to identify 33 metamorphic relations in four Web APIs developed by undergraduate students. All the relations are instances of the proposed MROPs. Then, we assessed the effectiveness of the identified relations at revealing 317 automatically seeded faults (i.e., mutants) in the APIs under test. As a result, 302 seeded faults were detected, achieving a mutation score of 95.3%. Second, we evaluated the approach using real Web APIs and faults. In particular, we identified 20 metamorphic relations in the Web API of Spotify and 40 metamorphic relations in the Web API of YouTube. Each metamorphic relation was implemented and automatically executed using both random and manual test data. In total, 469K metamorphic tests were generated. As a result, 21 metamorphic relations were violated, and 11 issues revealed and reported (3 issues in Spotify and 8 issues in YouTube). To date, 10 of the reported issues have been either confirmed by the API developers or reproduced by other users supporting the effectiveness of our approach.","conference":"IEEE","terms":"Testing;Software engineering;Software;Computer bugs;YouTube;Computer languages;Technological innovation,application program interfaces;program testing;Web services,metamorphic relation output pattern;RESTful Web API;MROP;metamorphic testing;REST architectural pattern","keywords":"Metamorphic testing;REST;RESTful Web services;Web API","startPage":"882","endPage":"882","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453162","citationCount":0,"referenceCount":0,"year":2018,"authors":"S. Segura; J. A. Parejo; J. Troya; A. Ruiz-Cortés","affiliations":"Dept. of Comput. Languages \u0026 Syst., Univ. de Sevilla, Sevilla, Spain; Dept. of Comput. Languages \u0026 Syst., Univ. de Sevilla, Sevilla, Spain; Dept. of Comput. Languages \u0026 Syst., Univ. de Sevilla, Sevilla, Spain; Dept. of Comput. Languages \u0026 Syst., Univ. de Sevilla, Sevilla, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d7"},"title":"On the Dichotomy of Debugging Behavior Among Programmers","abstract":"Debugging is an inevitable activity in most software projects, often difficult and more time-consuming than expected, giving it the nickname the \"dirty little secret of computer science.\" Surprisingly, we have little knowledge on how software engineers debug software problems in the real world, whether they use dedicated debugging tools, and how knowledgeable they are about debugging. This study aims to shed light on these aspects by following a mixed-methods research approach. We conduct an online survey capturing how 176 developers reflect on debugging. We augment this subjective survey data with objective observations on how 458 developers use the debugger included in their integrated development environments (IDEs) by instrumenting the popular Eclipse and IntelliJ IDEs with the purpose-built plugin WatchDog 2.0. To clarify the insights and discrepancies observed in the previous steps, we followed up by conducting interviews with debugging experts and regular debugging users. Our results indicate that IDE-provided debuggers are not used as often as expected, as \"printf debugging\" remains a feasible choice for many programmers. Furthermore, both knowledge and use of advanced debugging features are low. These results call to strengthen hands-on debugging experience in computer science curricula and have already refined the implementation of modern IDE debuggers.","conference":"IEEE","terms":"Debugging;Software;Tools;Instruments;Interviews;Computer bugs;Software engineering,computer debugging;program debugging;software engineering;software maintenance,integrated development environments;debugging experts;regular debugging users;printf debugging;advanced debugging features;software projects;software engineers debug software problems;debugging tools;mixed-methods research approach","keywords":"Debugging;Testing;WatchDog;IntelliJ;Eclipse","startPage":"572","endPage":"583","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453125","citationCount":2,"referenceCount":0,"year":2018,"authors":"M. Beller; N. Spruit; D. Spinellis; A. Zaidman","affiliations":"Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Athens Univ. of Econ. \u0026 Bus., Athens, Greece; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d8"},"title":"[Journal First] Challenges and Pitfalls on Surveying Evidence in the Software Engineering Technical Literature: An Exploratory Study with Novices","abstract":"The evidence-based software engineering approach advocates the use of evidence from empirical studies to support the decisions on the adoption of software technologies by practitioners in the software industry. To this end, many guidelines have been proposed to contribute to the execution and repeatability of literature reviews, and to the confidence of their results, especially regarding systematic literature reviews (SLR). To investigate similarities and differences, and to characterize the challenges and pitfalls of the planning and generated results of SLR research protocols dealing with the same research question and performed by similar teams of novice researchers in the context of the software engineering field. We qualitatively compared (using Jaccard and Kappa coefficients) and evaluated (using DARE) same goal SLR research protocols and outcomes undertaken by similar research teams. Seven similar SLR protocols regarding quality attributes for use cases executed in 2010 and 2012 enabled us to observe unexpected differences in their planning and execution. Even when the participants reached some agreement in the planning, the outcomes were different. The research protocols and reports allowed us to observe six challenges contributing to the divergences in the results: researchers' inexperience in the topic, researchers' inexperience in the method, lack of clearness and completeness of the papers, lack of a common terminology regarding the problem domain, lack of research verification procedures, and lack of commitment to the SLR. According to our findings, it is not possible to rely on results of SLRs performed by novices. Also, similarities at a starting or intermediate step during different SLR executions may not directly translate to the next steps, since non-explicit information might entail differences in the outcomes, hampering the repeatability and confidence of the SLR process and results. Although we do have expectations that the presence and follow-up of a senior researcher can contribute to increasing SLRs' repeatability, this conclusion can only be drawn upon the existence of additional studies on this topic. Yet, systematic planning, transparency of decisions and verification procedures are key factors to guarantee the reliability of SLRs.","conference":"IEEE","terms":"Software engineering;Software;Planning;Protocols;Knowledge engineering;Systematics;Terminology,DP industry;project management;software engineering,unexpected differences;research verification procedures;similar SLR protocols;software engineering field;SLR research protocols;systematic literature reviews;software industry;software technologies;evidence-based software engineering approach;software engineering technical literature;systematic planning;additional studies;senior researcher;SLR process","keywords":"Novice researchers;Systematic literature review;Evidence based software engineering;Exploratory study","startPage":"1194","endPage":"1194","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453200","citationCount":0,"referenceCount":0,"year":2018,"authors":"T. Vieira Ribeiro; J. Massollar; G. Horta Travassos","affiliations":"PESC/COPPE, Fed. Univ. of Rio de Janeiro, Rio de Janeiro, Brazil; PESC/COPPE, Fed. Univ. of Rio de Janeiro, Rio de Janeiro, Brazil; PESC/COPPE, Fed. Univ. of Rio de Janeiro, Rio de Janeiro, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34d9"},"title":"Understanding the Factors for Fast Answers in Technical Q\u0026A Websites: An Empirical Study of Four Stack Exchange Websites","abstract":"Technical questions and answers (Q\u0026A) websites accumulate a significant amount of knowledge from users. Developers are especially active on these Q\u0026A websites, since developers are constantly facing new development challenges that require help from other experts. Over the years, Q\u0026A website designers have derived several incentive systems (e.g., gamification) to encourage users to answer questions that are posted by others. However, the current incentive systems primarily focus on the quantity and quality of the answers instead of encouraging the rapid answering of questions. Improving the speed of getting an answer can significantly improve the user experience and increase user engagement on such Q\u0026A websites. In this paper [1], we study the factors for fast answers on such Q\u0026A websites. Our goal is to explore how one may improve the current incentive systems to motivate fast answering of questions. We use a logistic regression model to analyze 46 factors along four dimensions (i.e., question, asker, answer, and answerer dimension) in order to understand the relationship between the studied factors and the needed time to get an accepted answer. The question dimension calculates various textual and readability features of a question, as well as the popularity and difficulty of the question's tags. The asker dimension calculates the reputation of an asker and his/her historical tendency to get answers. The answer dimension computes textual features from the text of the accepted answer. The answerer dimension computes the historical activity level of the answerer who answered the question. We conduct our study on the four most popular (i.e., with the most questions) Q\u0026A Stack Exchange websites: Stack Overflow, Mathematics, Ask Ubuntu, and Superuser. We find that (i) factors in the answerer dimension have the strongest effect on the needed time to get an accepted answer, after controlling for other factors; (ii) the current incentive system does not recognize non-frequent answerers who often answer questions which frequent answerers are not able to answer well. Such questions that are answered by non-frequent answerers are as important as those that are answered by frequent answerers; (iii) the current incentive system motivates frequent answerers well, but such frequent answerers tend to answer short questions. Our findings suggest that the designers of Q\u0026A website should improve their incentive systems to motivate non-frequent answerers to be more active and to answer questions faster, in order to shorten the waiting time for an answer (especially for questions that require specific knowledge that frequent answerers might not possess). In addition, the question answering incentive system needs to factor in the value and difficulty of answering the questions (e.g., by providing more rewards to harder questions or questions that remain unanswered for a long period of time).","conference":"IEEE","terms":"Software engineering;Software;Knowledge engineering;Computer science;Logistics;Analytical models;Mathematics,human factors;question answering (information retrieval);regression analysis;software engineering;Web sites,technical questions;incentive systems;answer questions;rapid answering;Q\u0026A website;fast answering;answerer dimension;accepted answer;question dimension;answer dimension;nonfrequent answerers;current incentive system motivates frequent answerers;answer short questions;question answering incentive system;stack exchange Websites;answer Websites","keywords":"Q\u0026A website;Factor importance analysis;Response time","startPage":"884","endPage":"884","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453164","citationCount":3,"referenceCount":0,"year":2018,"authors":"S. Wang; T. Chen; A. E. Hassan","affiliations":"Software Anal. \u0026 Intell. Lab., Queen's Univ., Kingston, ON, Canada; Dept. of Comput. Sci. \u0026 Eng., Concordia Univ., Montreal, QC, Canada; Software Anal. \u0026 Intell. Lab., Queen's Univ., Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34da"},"title":"Towards Practical Program Repair with On-demand Candidate Generation","abstract":"Effective program repair techniques, which modify faulty programs to fix them with respect to given test suites, can substantially reduce the cost of manual debugging. A common repair approach is to iteratively first generate candidate programs with possible bug fixes and then validate them against the given tests until a candidate that passes all the tests is found. While this approach is conceptually simple, due to the potentially high number of candidates that need to first be generated and then be compiled and tested, existing repair techniques that embody this approach have relatively low effectiveness, especially for faults at a fine granularity. To tackle this limitation, we introduce a novel repair technique, SketchFix, which generates candidate fixes on demand (as needed) during the test execution. Instead of iteratively re-compiling and re-executing each actual candidate program, SketchFix translates faulty programs to sketches, i.e., partial programs with \"holes\", and compiles each sketch once which may represent thousands of concrete candidates. With the insight that the space of candidates can be reduced substantially by utilizing the runtime behaviors of the tests, SketchFix lazily initializes the candidates of the sketches while validating them against the test execution. We experimentally evaluate SketchFix on the Defects4J benchmark and the experimental results show that SketchFix works particularly well in repairing bugs with expression manipulation at the AST node-level granularity compared to other program repair techniques. Specifically, SketchFix correctly fixes 19 out of 357 defects in 23 minutes on average using the default setting. In addition, SketchFix finds the first repair with 1.6% of re-compilations (#compiled sketches/#candidates) and 3.0% of re-executions out of all repair candidates.","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;Runtime;Syntactics;Debugging;Java,program compilers;program debugging;program diagnostics;program testing;software fault tolerance;software maintenance,effective program repair techniques;faulty programs;manual debugging;common repair approach;candidate programs;possible bug fixes;relatively low effectiveness;novel repair technique;SketchFix;candidate fixes;test execution;iteratively re-compiling;re-executing each actual candidate program;partial programs;concrete candidates;re-compilations;repair candidates;practical program repair;on-demand candidate generation;#compiled sketches-#candidates","keywords":"debugging;program repair;program synthesis;lazy initialization;execution driven pruning","startPage":"12","endPage":"23","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453056","citationCount":13,"referenceCount":0,"year":2018,"authors":"J. Hua; M. Zhang; K. Wang; S. Khurshid","affiliations":"Univ. of Texas at Austin, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34db"},"title":"Reducer-Based Construction of Conditional Verifiers","abstract":"Despite recent advances, software verification remains challenging. To solve hard verification tasks, we need to leverage not just one but several different verifiers employing different technologies. To this end, we need to exchange information between verifiers. Conditional model checking was proposed as a solution to exactly this problem: The idea is to let the first verifier output a condition which describes the state space that it successfully verified and to instruct the second verifier to verify the yet unverified state space using this condition. However, most verifiers do not understand conditions as input. In this paper, we propose the usage of an off-the-shelf construction of a conditional verifier from a given traditional verifier and a reducer. The reducer takes as input the program to be verified and the condition, and outputs a residual program whose paths cover the unverified state space described by the condition. As a proof of concept, we designed and implemented one particular reducer and composed three conditional model checkers from the three best verifiers at SV-COMP 2017. We defined a set of claims and experimentally evaluated their validity. All experimental data and results are available for replication.","conference":"IEEE","terms":"Automata;Software;Model checking;Tools;Software engineering;Task analysis,formal verification;program diagnostics;program verification,conditional model checking;verifier output;unverified state space;conditional model checkers;hard verification tasks;software verification;conditional verifier;reducer-based construction","keywords":"Conditional Model Checking;Formal Verification;Testing;Program Analysis;Software Verification;Sequential Combination","startPage":"1182","endPage":"1193","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453199","citationCount":0,"referenceCount":0,"year":2018,"authors":"D. Beyer; M. Jakobs; T. Lemberger; H. Wehrheim","affiliations":"LMU Munich, Munich, Germany; LMU Munich, Munich, Germany; LMU Munich, Munich, Germany; Paderborn Univ., Paderborn, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34dc"},"title":"Data Scientists in Software Teams: State of the Art and Challenges","abstract":"The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams. For example, Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.","conference":"IEEE","terms":"Software;Tools;Data science;Software engineering;Face;Best practices;Instruments,DP industry;social networking (online),software teams;data scientists;software industry;Facebook;LinkedIn;Microsoft","keywords":"Software productivity;data science","startPage":"585","endPage":"585","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453127","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. Kim; T. Zimmermann; R. DeLine; A. Begel","affiliations":"UCLA, Los Angeles, CA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34dd"},"title":"Secure Coding Practices in Java: Challenges and Vulnerabilities","abstract":"The Java platform and its third-party libraries provide useful features to facilitate secure coding. However, misusing them can cost developers time and effort, as well as introduce security vulnerabilities in software. We conducted an empirical study on StackOverflow posts, aiming to understand developers' concerns on Java secure coding, their programming obstacles, and insecure coding practices. We observed a wide adoption of the authentication and authorization features provided by Spring Security - a third-party framework designed to secure enterprise applications. We found that programming challenges are usually related to APIs or libraries, including the complicated cross-language data handling of cryptography APIs, and the complex Java-based or XML-based approaches to configure Spring Security. In addition, we reported multiple security vulnerabilities in the suggested code of accepted answers on the StackOverflow forum. The vulnerabilities included disabling the default protection against Cross-Site Request Forgery (CSRF) attacks, breaking SSL/TLS security through bypassing certificate validation, and using insecure cryptographic hash functions. Our findings reveal the insufficiency of secure coding assistance and documentation, as well as the huge gap between security theory and coding practices.","conference":"IEEE","terms":"Java;Cryptography;Encoding;Libraries;Programming;Authentication,application program interfaces;authorisation;computer crime;cryptography;Java;XML,secure coding practices;Java platform;third-party libraries;developers time;empirical study;StackOverflow posts;Java secure coding;programming obstacles;insecure coding practices;authorization features;third-party framework;programming challenges;complicated cross-language data handling;complex Java-based;XML-based approaches;cross-site request forgery attacks;Spring security;cryptography API;SSL-TLS security;secure coding assistance;StackOverflow forum;multiple security vulnerabilities","keywords":"Secure coding;Spring security;CSRF;SSL/TLS;certificate validation;cryptographic hash functions;authentication;authorization;StackOverflow;cryptography","startPage":"372","endPage":"383","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453095","citationCount":6,"referenceCount":0,"year":2018,"authors":"N. Meng; S. Nagy; D. Yao; W. Zhuang; G. Arango-Argoty","affiliations":"Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34de"},"title":"Identifying Design Problems in the Source Code: A Grounded Theory","abstract":"The prevalence of design problems may cause re-engineering or even discontinuation of the system. Due to missing, informal or outdated design documentation, developers often have to rely on the source code to identify design problems. Therefore, developers have to analyze different symptoms that manifest in several code elements, which may quickly turn into a complex task. Although researchers have been investigating techniques to help developers in identifying design problems, there is little knowledge on how developers actually proceed to identify design problems. In order to tackle this problem, we conducted a multi-trial industrial experiment with professionals from 5 software companies to build a grounded theory. The resulting theory offers explanations on how developers identify design problems in practice. For instance, it reveals the characteristics of symptoms that developers consider helpful. Moreover, developers often combine different types of symptoms to identify a single design problem. This knowledge serves as a basis to further understand the phenomena and advance towards more effective identification techniques.","conference":"IEEE","terms":"Software systems;Task analysis;Software design;Fats;Documentation;Companies,DP industry;software engineering;source code (software);system documentation,source code;grounded theory;design documentation;design problems identification;code elements;software companies","keywords":"design problem;grounded theory;software design;symptoms","startPage":"921","endPage":"931","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453170","citationCount":3,"referenceCount":0,"year":2018,"authors":"L. Sousa; A. Oliveira; W. Oizumi; S. Barbosa; A. Garcia; J. Lee; M. Kalinowski; R. de Mello; B. Fonseca; R. Oliveira; C. Lucena; R. Paes","affiliations":"NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34df"},"title":"[Journal First] Do Automated Program Repair Techniques Repair Hard and Important Bugs?","abstract":"The full version of this article is: Manish Motwani, Sandhya Sankaranarayanan, Rene Just, and Yuriy Brun, \"Do Automated Program Repair Techniques Repair Hard and Important Bugs?\" in Empirical Software Engineering, http://dx.doi.org/10.1007/s10664-017-9550-0.","conference":"IEEE","terms":"Maintenance engineering;Java;Computer bugs;Correlation;Software engineering;Complexity theory;Benchmark testing,program debugging;software maintenance,repair techniques;important bugs;automated program","keywords":"Automated program repair;Repairability","startPage":"25","endPage":"25","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453058","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. Motwani; S. Sankaranarayanan; R. Just; Y. Brun","affiliations":"Univ. of Massachusetts, Amherst, MA, USA; Univ. of Massachusetts, Amherst, MA, USA; Univ. of Massachusetts, Amherst, MA, USA; Univ. of Massachusetts, Amherst, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e0"},"title":"EARMO: An Energy-Aware Refactoring Approach for Mobile Apps","abstract":"With millions of smartphones sold every year, the development of mobile apps has grown substantially. The battery power limitation of mobile devices has push developers and researchers to search for methods to improve the energy efficiency of mobile apps. We propose a multiobjective refactoring approach to automatically improve the architecture of mobile apps, while controlling for energy efficiency. In this extended abstract we briefly summarize our work.","conference":"IEEE","terms":"Energy efficiency;Software engineering;Batteries;Energy measurement;Mobile handsets;Energy consumption;Software,C language;energy conservation;smart phones;software maintenance,multiobjective refactoring approach;mobile apps;energy efficiency;energy-aware refactoring approach;mobile devices;push developers","keywords":"Refactoring;Anti-patterns;Mobile apps;Energy consumption","startPage":"59","endPage":"59","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453062","citationCount":0,"referenceCount":0,"year":2018,"authors":"R. Morales; R. Saborido; F. Khomh; F. Chicano; G. Antoniol","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e1"},"title":"When Not to Comment: Questions and Tradeoffs with API Documentation for C++ Projects","abstract":"Without usable and accurate documentation of how to use an API, developers can find themselves deterred from reusing relevant code. In C++, one place developers can find documentation is in a header file. When information is missing, they may look at the corresponding implementation code. To understand what's missing from C++ API documentation and the factors influencing whether it will be fixed, we conducted a mixed-methods study involving two experience sampling surveys with hundreds of developers at the moment they visited implementation code, interviews with 18 of those developers, and interviews with 8 API maintainers. In many cases, updating documentation may provide only limited value for developers, while requiring effort maintainers don't want to invest. We identify a set of questions maintainers and tool developers should consider when improving API-level documentation.","conference":"IEEE","terms":"Documentation;Interviews;Tools;Task analysis;C++ languages;Google;Software engineering,application program interfaces;C++ language;system documentation,API-level documentation improvement;tool developers;questions maintainers;updating documentation;experience sampling surveys;mixed-methods study;C++ API documentation;corresponding implementation code;header file;place developers;relevant code;accurate documentation;usable documentation","keywords":"documentation;human factors;developer workflow;APIs;usability","startPage":"643","endPage":"653","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453133","citationCount":1,"referenceCount":0,"year":2018,"authors":"A. Head; C. Sadowski; E. Murphy-Hill; A. Knight","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e2"},"title":"Search-Based Test Data Generation for SQL Queries","abstract":"Database-centric systems strongly rely on SQL queries to manage and manipulate their data. These SQL commands can range from very simple selections to queries that involve several tables, subqueries, and grouping operations. And, as with any important piece of code, developers should properly test SQL queries. In order to completely test a SQL query, developers need to create test data that exercise all possible coverage targets in a query, e.g., JOINs and WHERE predicates. And indeed, this task can be challenging and time-consuming for complex queries. Previous studies have modeled the problem of generating test data as a constraint satisfaction problem and, with the help of SAT solvers, generate the required data. However, such approaches have strong limitations, such as partial support for queries with JOINs, subqueries, and strings (which are commonly used in SQL queries). In this paper, we model test data generation for SQL queries as a search-based problem. Then, we devise and evaluate three different approaches based on random search, biased random search, and genetic algorithms (GAs). The GA, in particular, uses a fitness function based on information extracted from the physical query plan of a database engine as search guidance. We then evaluate each approach in 2,135 queries extracted from three open source software and one industrial software system. Our results show that GA is able to completely cover 98.6% of all queries in the dataset, requiring only a few seconds per query. Moreover, it does not suffer from the limitations affecting state-of-the art techniques.","conference":"IEEE","terms":"Databases;Genetic algorithms;Search problems;Structured Query Language;Toy manufacturing industry;Software systems;Engines,database management systems;genetic algorithms;program testing;query processing;search problems;SQL,SQL query;complex queries;model test data generation;physical query plan;Search-Based Test Data Generation;database-centric systems;SQL queries;SQL commands;constraint satisfaction problem;search-based problem;biased random search;genetic algorithms;database engine;search guidance;open source software;industrial software system","keywords":"search based software engineering;automated test data generation;SQL;databases","startPage":"1220","endPage":"1230","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453204","citationCount":2,"referenceCount":0,"year":2018,"authors":"J. Castelein; M. Aniche; M. Soltani; A. Panichella; A. van Deursen","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e3"},"title":"Large-Scale Analysis of Framework-Specific Exceptions in Android Apps","abstract":"Mobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist - this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community.","conference":"IEEE","terms":"Testing;Androids;Humanoid robots;Computer bugs;Tools;Google,Android (operating system);dynamic testing;mobile computing;program debugging;program testing;public domain software,unique exception;open-source Android apps;framework-specific exceptions;bug detection techniques;real-world apps;identified exceptions;confirmed/fixed crashes;exception localization tool;dynamic testing tool;mobile apps;four-month long effort;analysis tools;real-world app crashes;frequent crashes;app developers","keywords":"Empirical study;mobile app bugs;testing;static analysis","startPage":"408","endPage":"419","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453100","citationCount":12,"referenceCount":0,"year":2018,"authors":"L. Fan; T. Su; S. Chen; G. Meng; Y. Liu; L. Xu; G. Pu; Z. Su","affiliations":"Sch. of Comput. Sci. \u0026 Software Eng., East China Normal Univ., Shanghai, China; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Software Eng., East China Normal Univ., Shanghai, China; SKLOIS, Inst. of Inf. Eng., Beijing, China; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Software Eng., East China Normal Univ., Shanghai, China; Shanghai Key Lab. of Trustworthy Comput., East China Normal Univ., Shanghai, China; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e4"},"title":"From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation","abstract":"A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.","conference":"IEEE","terms":"Graphical user interfaces;Skeleton;Layout;Visualization;Feature extraction;Tools;Task analysis,computer bootstrapping;computer vision;graphical user interfaces;language translation;mobile computing;neural nets,large-scale UI data;GUI exploration method;GUI components;computer vision;GUI generation rules;heuristically-designed visual understanding;bootstrap mobile GUI implementation;neural machine translator;GUI skeleton;UI design image","keywords":"User interface;reverse engineering;deep learning","startPage":"665","endPage":"676","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453135","citationCount":8,"referenceCount":0,"year":2018,"authors":"C. Chen; T. Su; G. Meng; Z. Xing; Y. Liu","affiliations":"Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; Res. Sch. of Comput. Sciecne, Australian Nat. Univ., Canberra, ACT, Australia; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e5"},"title":"Does the Propagation of Artifact Changes Across Tasks Reflect Work Dependencies?","abstract":"Developers commonly define tasks to help coordinate software development efforts--whether they be feature implementation, refactoring, or bug fixes. Developers establish links between tasks to express implicit dependencies that needs explicit handling--dependencies that often require the developers responsible for a given task to assess how changes in a linked task affect their own work and vice versa (i.e., change propagation). While seemingly useful, it is unknown if change propagation indeed coincides with task links. No study has investigated to what extent change propagation actually occurs between task pairs and whether it is able to serve as a metric for characterizing the underlying task dependency. In this paper, we study the temporal relationship between developer reading and changing of source code in relationship to task links. We identify seven situations that explain the varying correlation of change propagation with linked task pairs and find six motifs describing when change propagation occurs between non-linked task pairs. Our paper demonstrates that task links are indeed useful for recommending which artifacts to monitor for changes, which developers to involve in a task, or which tasks to inspect.","conference":"IEEE","terms":"Licenses,project management;software engineering;software maintenance,implicit dependencies;task links;extent change propagation;underlying task dependency;linked task pairs;nonlinked task pairs;software development efforts;explicit handling","keywords":"task links;change propagation;bugzilla;mylyn;empirical study","startPage":"397","endPage":"407","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453099","citationCount":2,"referenceCount":0,"year":2018,"authors":"C. Mayr-Dorn; A. Egyed","affiliations":"Johannes Kepler Univ., Linz, Austria; Johannes Kepler Univ., Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e6"},"title":"Automated Refactoring of OCL Constraints with Search","abstract":"Object Constraint Language (OCL) constraints are typically used for providing precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve in a regular basis, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL. To tackle such a challenge, we propose an automated search-based OCL constraint refactoring approach (SBORA) by defining and applying three OCL quality metrics (Complexity, Coupling, and Cohesion) and four semantics-preserving refactoring operators (i.e., Context Change, Swap, Split and Merge) which are encoded as potential solutions for search algorithms. A solution is therefore an optimal sequence of refactoring operators, which are sequentially applied to the original set of OCL constraints to automatically obtain a semantically equivalent set of OCL constraints with better understandability and maintainability in terms of Complexity, Coupling, and Cohesion. We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil\u0026Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25% Complexity and 39% Coupling and improve 47.75% Cohesion, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained encouraging results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment.","conference":"IEEE","terms":"Cancer;Unified modeling language;Complexity theory;Couplings;Software engineering;Semantics;Measurement,evolutionary computation;object-oriented programming;search problems;software maintenance;Unified Modeling Language,cancer registries;automated search-based OCL constraint refactoring approach;OCL quality metrics;semantics-preserving refactoring operators;search algorithms;Cancer Registry;original OCL constraint;original constraint set;object constraint language constraints","keywords":"Refactoring;Search;Object Constraint Language","startPage":"1243","endPage":"1243","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453206","citationCount":0,"referenceCount":0,"year":2018,"authors":"H. Lu; S. Wang; T. Yue; S. Ali; J. Nygard","affiliations":"Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway; Cancer Registry of Norway, Oslo, Norway","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e7"},"title":"[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration","abstract":"Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.","conference":"IEEE","terms":"Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software,Big Data;cloud computing;data analysis;data models;formal verification;NoSQL databases;program testing;quality of service;software quality,Hegira4Cloud;fault-tolerant data extraction;software design;verification tools;Data Intensive system;data loss;Mediation Data Model;software engineering;QoS analysis;big data frameworks;NoSQL databases;data migration;Quality of Service;developing- testing-reengineering;Database as a Service","keywords":"Data intensive applications;Experiment driven action research;Big data;Data migration","startPage":"93","endPage":"93","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453066","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. Scavuzzo; E. Di Nitto; D. Ardagna","affiliations":"Dipt. di Elettron. Inf. e Bioingegneria Milano, Politec. di MilanoDipartimento di Elettronica Informazione e Bioingegneria Milano, Milan, Italy; Dipt. di Elettron. Inf. e Bioingegneria Milano, Politec. di MilanoDipartimento di Elettronica Informazione e Bioingegneria Milano, Milan, Italy; Dipt. di Elettron. Inf. e Bioingegneria Milano, Politec. di MilanoDipartimento di Elettronica Informazione e Bioingegneria Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e8"},"title":"[Journal First] Sentiment Polarity Detection for Software Development","abstract":"The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within software repositories and information sources. With a few notable exceptions, empirical software engineering studies have exploited off-the-shelf sentiment analysis tools. However, such tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. In particular, Jongeling et al. show how the choice of the sentiment analysis tool may impact the conclusion validity of empirical studies because not only these tools do not agree with human annotation of developers' communication channels, but they also disagree among themselves. Our goal is to move beyond the limitations of off-the-shelf sentiment analysis tools when applied in the software engineering domain. Accordingly, we present Senti4SD, a sentiment polarity classifier for software developers' communication channels. Senti4SD exploits a suite of lexicon-based, keyword-based, and semantic features for appropriately dealing with the domain-dependent use of a lexicon. We built a Distributional Semantic Model (DSM) to derive the semantic features exploited by Senti4SD. Specifically, we ran word2vec on a collection of over 20 million documents from Stack Overflow, thus obtaining word vectors that are representative of developers' communication style. The classifier is trained and validated using a gold standard of 4,423 Stack Overflow posts, including questions, answers, and comments, which were manually annotated for sentiment polarity. We release the full lab package, which includes both the gold standard and the emotion annotation guidelines, to ease the execution of replications as well as new studies on emotion awareness in software engineering. To inform future research on word embedding for text categorization and information retrieval in software engineering, the replication kit also includes the DSM. Results. The contribution of the lexicon-based, keyword-based, and semantic features is assessed by our empirical evaluation leveraging different feature settings. With respect to SentiStrength, a mainstream off-the-shelf tool that we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. Furthermore, we provide empirical evidence of better performance also in presence of a minimal set of training documents.","conference":"IEEE","terms":"Software;Software engineering;Collaboration;Programming,data mining;information retrieval;pattern classification;sentiment analysis;social networking (online);software engineering,keyword-based features;sentiment polarity detection;empirical software engineering;distributional semantic model;DSM;stack overflow;emotion annotation guidelines;text categorization;information retrieval;replication kit;sentistrength;word embedding;software developers emotions;crowd-generated content mining;lexicon-based features;software developers communication channels;word vectors;sentiment polarity classifier;software engineering domain;problem reports;technical jargon;general-purpose social media;nontechnical domains;off-the-shelf sentiment analysis tools;information sources;software repositories;software development;Senti4SD;semantic features","keywords":"Sentiment Analysis;Communication Channels;Stack Overflow;Word Embedding;Social Software Engineering","startPage":"128","endPage":"128","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453070","citationCount":0,"referenceCount":0,"year":2018,"authors":"F. Calefato; F. Lanubile; F. Maiorano; N. Novielli","affiliations":"Univ. of Bari Aldo Moro, Bari, Italy; Univ. of Bari Aldo Moro, Bari, Italy; Univ. of Bari Aldo Moro, Bari, Italy; Univ. of Bari Aldo Moro, Bari, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34e9"},"title":"Redefining Prioritization: Continuous Prioritization for Continuous Integration","abstract":"Continuous integration (CI) development environments allow soft-ware engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach \"continuously\" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our approach on three non-trivial CI data sets. Our results show that our approach can be more effective than prior techniques.","conference":"IEEE","terms":"Software engineering,program testing;software engineering,continuous prioritization;continuous integration development environments;CI environments;nontrivial amounts;test case prioritization;test suites;test suite failure;nontrivial CI data sets;software engineers","keywords":"continuous integration;regression testing;large scale testing","startPage":"688","endPage":"698","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453137","citationCount":1,"referenceCount":0,"year":2018,"authors":"J. Liang; S. Elbaum; G. Rothermel","affiliations":"Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Univ. of Nebraska-Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ea"},"title":"[Journal First] Older Adults and Hackathons: A Qualitative Study","abstract":"Globally observed trends in aging indicate that older adults constitute a growing share of the population and an increasing demographic in the modern technologies marketplace. Therefore, it has become important to address the issue of participation of older adults in the process of developing solutions suitable for their group. In this study, we approached this topic by organizing a hackathon involving teams of young programmers and older adult participants. In our paper we describe a case study of that hackathon, in which our objective was to motivate older adults to participate in software engineering processes. Based on our results from an array of qualitative methods, we propose a set of good practices that may lead to improved older adult participation in similar events and an improved process of developing apps that target older adults.","conference":"IEEE","terms":"Information technology;Software engineering;Software;Collaboration;Human computer interaction;Programming profession,age issues;geriatrics;human factors;software engineering,young programmers;software engineering processes;older adult participants;hackathon","keywords":"older adults;elderly;participatory design;co-design;user-centered design;user experience;hackathons;qualitative methods;intergenerational interaction;intergenerational cooperation","startPage":"702","endPage":"703","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453141","citationCount":0,"referenceCount":0,"year":2018,"authors":"W. Kopec; B. Balcerzak; R. Nielek; G. Kowalik; A. Wierzbicki; F. Casati","affiliations":"Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Univ. of Trento, Povo, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34eb"},"title":"DeFlaker: Automatically Detecting Flaky Tests","abstract":"Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1,874 flaky tests from 4,846 failures, with a low false alarm rate (1.5%). DeFlaker had a higher recall (95.5% vs. 23%) of confirmed flaky tests than Maven's default flaky test detector.","conference":"IEEE","terms":"Monitoring;Testing;Java;Software;Tools;Syntactics;Detectors,Java;program testing;public domain software;regression analysis,DeFlaker;test failure;unknown flaky tests","keywords":"software testing;flaky tests;code coverage","startPage":"433","endPage":"444","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453104","citationCount":7,"referenceCount":0,"year":2018,"authors":"J. Bell; O. Legunsen; M. Hilton; L. Eloussi; T. Yung; D. Marinov","affiliations":"George Mason Univ., Fairfax, VA, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ec"},"title":"Identifying Features in Forks","abstract":"Fork-based development has been widely used both in open source communities and in industry, because it gives developers flexibility to modify their own fork without affecting others. Unfortunately, this mechanism has downsides: When the number of forks becomes large, it is difficult for developers to get or maintain an overview of activities in the forks. Current tools provide little help. We introduce INFOX, an approach to automatically identify non-merged features in forks and to generate an overview of active forks in a project. The approach clusters cohesive code fragments using code and network-analysis techniques and uses information-retrieval techniques to label clusters with keywords. The clustering is effective, with 90% accuracy on a set of known features. In addition, a human-subject evaluation shows that INFOX can provide actionable insight for developers of forks.","conference":"IEEE","terms":"Feature extraction;Computer bugs;Navigation;History;Tools;Visualization;Google,feature extraction;information retrieval;pattern clustering,identifying features;fork-based development;open source communities;INFOX;nonmerged features;clusters cohesive code fragments;network-analysis techniques;information-retrieval techniques;human-subject evaluation","keywords":"Fork-based development;Github;Community detection;Information retrieval;overview of forks;transparency","startPage":"105","endPage":"116","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453068","citationCount":3,"referenceCount":0,"year":2018,"authors":"S. Zhou; S. Stanciulescu; O. Leßenich; Y. Xiong; A. Wasowski; C. Kästner","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ed"},"title":"Automated Repair of Mobile Friendly Problems in Web Pages","abstract":"Mobile devices have become a primary means of accessing the Internet. Unfortunately, many websites are not designed to be mobile friendly. This results in problems such as unreadable text, cluttered navigation, and content over owing a device's viewport; all of which can lead to a frustrating and poor user experience. Existing techniques are limited in helping developers repair these mobile friendly problems. To address this limitation of prior work, we designed a novel automated approach for repairing mobile friendly problems in web pages. Our empirical evaluation showed that our approach was able to successfully resolve mobile friendly problems in 95% of the evaluation subjects. In a user study, participants preferred our repaired versions of the subjects and also considered the repaired pages to be more readable than the originals.","conference":"IEEE","terms":"Web pages;Maintenance engineering;Mobile handsets;Cascading style sheets;Layout;Navigation;Organizations,computer displays;Internet;mobile computing;Web sites,mobile devices;mobile friendly problems;Web pages;Internet;Websites;cluttered navigation;user experience","keywords":"Mobile Friendly Problems;automated repair;web apps","startPage":"140","endPage":"150","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453072","citationCount":3,"referenceCount":0,"year":2018,"authors":"S. Mahajan; N. Abolhassani; P. McMinn; W. G. J. Halfond","affiliations":"Univ. of Southern California, Los Angeles, CA, USA; Univ. of Southern California, Los Angeles, CA, USA; Univ. of Sheffield, Sheffield, UK; Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ee"},"title":"[Journal First] On the Use of Hidden Markov Model to Predict the Time to Fix Bugs","abstract":"A significant amount of time is spent by software developers in investigating bug reports. It is useful to indicate when a bug report will be closed, since it would help software teams to prioritise their work. Several studies have been conducted to address this problem in the past decade. Most of these studies have used the frequency of occurrence of certain developer activities as input attributes in building their prediction models. However, these approaches tend to ignore the temporal nature of the occurrence of these activities. In this paper, a novel approach using Hidden Markov models (HMMs) and temporal sequences of developer activities is proposed. The approach is empirically demonstrated in a case study using eight years of bug reports collected from the Firefox project.","conference":"IEEE","terms":"Computer bugs;Hidden Markov models;Software;Software engineering;Data science;Predictive models;Industrial engineering,hidden Markov models;program debugging,software developers;bug report;software teams;prediction models;hidden Markov model;bug fixing;Firefox project;HMM","keywords":"Bug repositories;Temporal activities;Time to fix a bug;Hidden Markov model","startPage":"700","endPage":"700","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453139","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. Habayeb; S. S. Murtaza; A. Miranskyy; A. B. Bener","affiliations":"Dept. of Mech. \u0026 Ind. Eng., Ryerson Univ., Toronto, ON, Canada; Dept. of Mech. \u0026 Ind. Eng., Ryerson Univ., Toronto, ON, Canada; Dept. of Comput. Sci., Ryerson Univ., Toronto, ON, Canada; Dept. of Mech. \u0026 Ind. Eng., Ryerson Univ., Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ef"},"title":"Do Programmers Work at Night or During the Weekend?","abstract":"Abnormal working hours can reduce work health, general well-being, and productivity, independent from a profession. To inform future approaches for automatic stress and overload detection, this paper establishes empirically collected measures of the work patterns of software engineers. To this aim, we perform the first largescale study of software engineers' working hours by investigating the time stamps of commit activities of 86 large open source software projects, both containing hired and volunteer developers. We find that two thirds of software engineers mainly follow typical office hours, empirically established to be from 10h to 18h, and do not usually work during nights and weekends. Large variations between projects and individuals exist. Surprisingly, we found no support that project maturation would decrease abnormal working hours. In the Firefox case study, we found that hired developers work more during office hours while seniority, either in terms of number of commits or job status, did not impact working hours. We conclude that the use of working hours or timestamps of work products for stress detection requires establishing baselines at the level of individuals.","conference":"IEEE","terms":"Companies;Software;Computer bugs;Software engineering;Productivity;Rhythm;Stress,health care;occupational health;project management;public domain software;software development management,abnormal working hours;work health;work patterns;software engineers;work products;open source software projects;overload detection;time stamps;stress detection;Firefox","keywords":"software repository mining;overtime;overwork;open source;apache;mozilla;weekend;night","startPage":"705","endPage":"715","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453143","citationCount":2,"referenceCount":0,"year":2018,"authors":"M. Claes; M. V. Mäntylä; M. Kuutila; B. Adams","affiliations":"ITEE, Univ. of Oulu, Oulu, Finland; ITEE, Univ. of Oulu, Oulu, Finland; ITEE, Univ. of Oulu, Oulu, Finland; MCIS, Polytech. Montreal, Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f0"},"title":"Context-Aware Conversational Developer Assistants","abstract":"Building and maintaining modern software systems requires developers to perform a variety of tasks that span various tools and information sources. The crosscutting nature of these development tasks requires developers to maintain complex mental models and forces them (a) to manually split their high-level tasks into low-level commands that are supported by the various tools, and (b) to (re) establish their current context in each tool. In this paper we present Devy, a Conversational Developer Assistant (CDA) that enables developers to focus on their high-level development tasks. Devy reduces the number of manual, often complex, low-level commands that developers need to perform, freeing them to focus on their high-level tasks. Specifically, Devy infers high-level intent from developer's voice commands and combines this with an automatically-generated context model to determine appropriate workflows for invoking low-level tool actions; where needed, Devy can also prompt the developer for additional information. Through a mixed methods evaluation with 21 industrial developers, we found that Devy provided an intuitive interface that was able to support many development tasks while helping developers stay focused within their development environment. While industrial developers were largely supportive of the automation Devy enabled, they also provided insights into several other tasks and workflows CDAs could support to enable them to better focus on the important parts of their development tasks.","conference":"IEEE","terms":"Task analysis;Tools;Context;Context modeling;Software;Switches,interactive systems;software engineering;ubiquitous computing,high-level tasks;low-level commands;Devy;high-level development tasks;high-level intent;automatically-generated context model;low-level tool actions;development environment;context-aware conversational developer assistants;modern software systems","keywords":"Conversational Development Assistants;Natural User Interfaces","startPage":"993","endPage":"1003","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453178","citationCount":4,"referenceCount":0,"year":2018,"authors":"N. Bradley; T. Fritz; R. Holmes","affiliations":"Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f1"},"title":"[Journal First] On the Diffuseness and the Impact on Maintainability of Code Smells: A Large Scale Empirical Investigation","abstract":"Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell types. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.","conference":"IEEE","terms":"Software engineering;History;Detectors;Maintenance engineering;Software systems;Correlation,public domain software;software maintenance,software maintainability;code fault-proneness;code change-proneness;open source projects;code comprehensibility;scale empirical investigation;code smells;smell-free classes;complex code","keywords":"code smells;empirical studies;mining software repositories","startPage":"482","endPage":"482","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453110","citationCount":0,"referenceCount":0,"year":2018,"authors":"F. Palomba; G. Bavota; M. Di Penta; F. Fasano; R. Oliveto; A. De Lucia","affiliations":"Univ. of Zurich, Zurich, Switzerland; Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. of Sannio, Benevento, Italy; Univ. of Molise, Pesche, Italy; Univ. of Molise, Pesche, Italy; Univ. of Salerno, Salerno, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f2"},"title":"[Journal First] Overfitting in Semantics-Based Automated Program Repair","abstract":"The primary goal of Automated Program Repair (APR) is to automatically fix buggy software, to reduce the manual bug-fix burden that presently rests on human developers. Existing APR techniques can be generally divided into two families: semantics-vs. heuristics-based. Semantics-based APR uses symbolic execution and test suites to extract semantic constraints, and uses program synthesis to synthesize repairs that satisfy the extracted constraints. Heuristic-based APR generates large populations of repair candidates via source manipulation, and searches for the best among them. Both families largely rely on a primary assumption that a program is correctly patched if the generated patch leads the program to pass all provided test cases. Patch correctness is thus an especially pressing concern. A repair technique may generate overfitting patches, which lead a program to pass all existing test cases, but fails to generalize beyond them. In this work, we revisit the overfitting problem with a focus on semantics-based APR techniques, complementing previous studies of the overfitting problem in heuristics-based APR. We perform our study using IntroClass and Codeflaws benchmarks, two datasets well-suited for assessing repair quality, to systematically characterize and understand the nature of overfitting in semantics-based APR. We find that similar to heuristics-based APR, overfitting also occurs in semantics-based APR in various different ways.","conference":"IEEE","terms":"Maintenance engineering;Semantics;Benchmark testing;Software engineering;Tools;Engines;Sociology,program debugging;program diagnostics;program testing;software fault tolerance;software maintenance,symbolic execution;test suites;program synthesis;overfitting patches;semantics-based APR techniques;semantics-based Automated Program Repair;buggy software;patch correctness;IntroClass;repair quality assessment;Codeflaws","keywords":"Automated Program Repair;Program Synthesis;Symbolic Execution;Patch Overfitting","startPage":"163","endPage":"163","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453074","citationCount":2,"referenceCount":0,"year":2018,"authors":"X. D. Le; F. Thung; D. Lo; C. Le Goues","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f3"},"title":"Self-Hiding Behavior in Android Apps: Detection and Characterization","abstract":"Applications (apps) that conceal their activities are fundamentally deceptive; app marketplaces and end-users should treat such apps as suspicious. However, due to its nature and intent, activity concealing is not disclosed up-front, which puts users at risk. In this paper, we focus on characterization and detection of such techniques, e.g., hiding the app or removing traces, which we call \"self hiding behavior\" (SHB). SHB has not been studied per se - rather it has been reported on only as a byproduct of malware investigations. We address this gap via a study and suite of static analyses targeted at SH in Android apps. Specifically, we present (1) a detailed characterization of SHB, (2) a suite of static analyses to detect such behavior, and (3) a set of detectors that employ SHB to distinguish between benign and malicious apps. We show that SHB ranges from hiding the app's presence or activity to covering an app's traces, e.g., by blocking phone calls/text messages or removing calls and messages from logs. Using our static analysis tools on a large dataset of 9,452 Android apps (benign as well as malicious) we expose the frequency of 12 such SH behaviors. Our approach is effective: it has revealed that malicious apps employ 1.5 SHBs per app on average. Surprisingly, SH behavior is also employed by legitimate (\"benign\") apps, which can affect users negatively in multiple ways. When using our approach for separating malicious from benign apps, our approach has high precision and recall (combined F-measure = 87.19%). Our approach is also efficient, with analysis typically taking just 37 seconds per app. We believe that our findings and analysis tool are beneficial to both app marketplaces and end-users.","conference":"IEEE","terms":"Malware;Smart phones;Tools;Static analysis;Security;Google;Software engineering,Android (operating system);invasive software;mobile computing;program diagnostics,Android apps;trace removal;self hiding behavior detection;activity concealing;app marketplaces;benign apps;legitimate apps;malicious apps;static analyses","keywords":"Android;static analysis;malware;mobile security","startPage":"728","endPage":"739","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453145","citationCount":2,"referenceCount":0,"year":2018,"authors":"Z. Shan; I. Neamtiu; R. Samuel","affiliations":"Wichita State Univ., Wichita, KS, USA; New Jersey Inst. of Technol., Newark, NJ, USA; New Jersey Inst. of Technol., Newark, NJ, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f4"},"title":"[Journal First] Analyzing the Effects of Test Driven Development in GitHub","abstract":"Testing is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models recommend Test Driven Development (TDD) as a key practice for reducing costs and improving code quality. The objective of this work is to perform a cost-benefit analysis of this practice. Previous work by Fucci et al. engaged in laboratory studies of developers actively engaged in test-driven development practices. Fucci et al. found little difference between test-first behaviour of TDD and test-later behaviour. To that end, we opted to conduct a study about TDD behaviours in the \"wild\" rather than in the laboratory. Thus we have conducted a comparative analysis of GitHub repositories that adopts TDD to a lesser or greater extent, in order to determine how TDD affects software development productivity and software quality. We classified GitHub repositories archived in 2015 in terms of how rigorously they practiced TDD, thus creating a TDD spectrum. We then matched and compared various subsets of these repositories on this TDD spectrum with control sets of equal size. The control sets were samples from all GitHub repositories that matched certain characteristics, and that contained at least one test file. We compared how the TDD sets differed from the control sets on the following characteristics: number of test files, average commit velocity, number of bug-referencing commits, number of issues recorded, usage of continuous integration, number of pull requests, and distribution of commits per author. We found that Java TDD projects were relatively rare. In addition, there were very few significant differences in any of the metrics we used to compare TDD-like and non-TDD projects; therefore, our results do not reveal any observable benefits from using TDD.","conference":"IEEE","terms":"Software engineering;Software;Testing;Computational modeling;Java;Blogs;Analytical models,cost-benefit analysis;Java;program testing;software engineering;software maintenance;software quality;software reliability,GitHub repositories;TDD spectrum;control sets;test file;TDD sets;Java TDD projects;nonTDD projects;software development lifecycle;agile process models;improving code quality;cost-benefit analysis;test-driven development practices;test-first behaviour;test-later behaviour;software development productivity;software quality;test driven development","keywords":"Test Driven Development;Human Factors in Software Development;GitHub Repositories;Continuous Integration","startPage":"1062","endPage":"1062","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453184","citationCount":0,"referenceCount":0,"year":2018,"authors":"N. Borle; M. Feghhi; E. Stroulia; R. Grenier; A. Hindle","affiliations":"Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada; Dept. of Comput. Sci., Univ. of Alberta, Edmonton, AB, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f5"},"title":"[Journal First] An Empirical Study of Early Access Games on the Steam Platform","abstract":"\"Early access\" is a release strategy for software that allows consumers to purchase an unfinished version of the software. In turn, consumers can influence the software development process by giving developers early feedback. This early access model has become increasingly popular through digital distribution platforms, such as Steam which is the most popular distribution platform for games. The plethora of options offered by Steam to communicate between developers and game players contribute to the popularity of the early access model. The early access model made a name for itself through several successful games, such as the DayZ game. The multiplayer survival-based game reached 400,000 sales during its first week as an early access game. However, the benefits of the early access model have been questioned as well. For instance, the Spacebase DF-9 game abandoned the early access stage unexpectedly, disappointing many players of the game. Shortly after abandoning the early access stage and terminating the development, twelve employees were laid off including the programmer and project lead. In this paper, we conduct an empirical study on 1,182 Early Access Games (EAGs) on the Steam platform to understand the characteristics, advantages and limitations of the early access model. We find that 15% of the games on Steam make use of the early access model, with the most popular EAG having as many as 29 million owners. 88% of the EAGs are classified by their developers as so-called \"indie\" games, indicating that most EAGs are developed by individual developers or small studios. We study the interaction between players and developers of EAGs and the Steam platform. We observe that on the one hand, developers update their games more frequently in the early access stage. On the other hand, the percentage of players that review a game during its early access stage is lower than the percentage of players that review the game after it leaves the early access stage. However, the average rating of the reviews is much higher during the early access stage, suggesting that players are more tolerant of imperfections in the early access stage. The positive review rate does not correlate with the length or the game update frequency of the early access stage. In addition, we discuss several learned lessons from the failure of an early access game. The main learned lesson from this failure is that the communication between the game developer and the players of the EAG is crucial. Players enjoy getting involved in the development of an early access game and they get emotionally involved in the decision-making about the game. Based on our findings, we suggest game developers to use the early access model as a method for eliciting early feedback and more positive reviews to attract additional new players. In addition, our findings suggest that developers can determine their release schedule without worrying about the length of the early access stage and the game update frequency during the early access stage.","conference":"IEEE","terms":"Games;Software;Software engineering;Decision making;Schedules;Lead;Computational modeling,computer games;sales management;software engineering,game developer;game players;Steam platform;software development process;consumers;digital distribution platforms;DayZ game;multiplayer survival-based game;Spacebase DF-9 game;employees;programmer;project lead;decision-making;EAG;early access stage;early access game;early access model","keywords":"early access games;computer games;Steam","startPage":"480","endPage":"480","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453108","citationCount":0,"referenceCount":0,"year":2018,"authors":"D. Lin; C. Bezemer; A. E. Hassan","affiliations":"Software Anal. \u0026 Intell. Lab. (SAIL), Queen's Univ. Kingston, Kingston, ON, Canada; Software Anal. \u0026 Intell. Lab. (SAIL), Queen's Univ. Kingston, Kingston, ON, Canada; Software Anal. \u0026 Intell. Lab. (SAIL), Queen's Univ. Kingston, Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f6"},"title":"ENTRUST: Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases","abstract":"Software systems are increasingly expected to cope with variable workloads, component failures and other uncertainties through self-adaptation. As such, self-adaptive software has been the subject of intense research over the past decade. Our work focuses on the use of self-adaptive software in applications with strict functional and non-functional requirements. These applications need compelling assurances that the software continues to meet its requirements while it reconfigures its architecture and parameters at runtime. To address this need, we introduce an end-to-end methodology for the ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST).","conference":"IEEE","terms":"Computer science;Runtime;Software systems;Standards;Safety,formal specification;software architecture;trusted computing,engineering trustworthy self-adaptive software;dynamic assurance cases;software systems;nonfunctional requirements;self-adaptive software","keywords":"self adaptive software systems;software engineering methodology;assurance evidence;assurance cases","startPage":"495","endPage":"495","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453112","citationCount":0,"referenceCount":0,"year":2018,"authors":"R. Calinescu; D. Weyns; S. Gerasimou; M. U. Iftikhar; I. Habli; T. Kelly","affiliations":"Dept. of Comput. Sci., Univ. of York, York, UK; Dept. of Comput. Sci., Katholieke Univ. Leuven, Leuven, Belgium; Dept. of Comput. Sci., Univ. of York, York, UK; Dept. of Comput. Sci., Linnaeus Univ., Vaxjo, Sweden; Dept. of Comput. Sci., Univ. of York, York, UK; Dept. of Comput. Sci., Univ. of York, York, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f7"},"title":"[Journal First] A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers","abstract":"Programmers who are blind use a screen reader to speak source code one word at a time, as though the code were text. This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. In this paper, we present an empirical study comparing the program comprehension of blind and sighted programmers. We found that both blind and sighted programmers prioritize reading method signatures over other areas of code. Both groups obtained an equal and high degree of comprehension, despite the different reading processes.","conference":"IEEE","terms":"Software engineering;Tools;Java;Measurement;Software maintenance;Visualization,handicapped aids;programming environments;public domain software;software engineering,program comprehension strategies;sighted programmers;source code;reading processes","keywords":"Program comprehension;accessibility technology;blindness","startPage":"788","endPage":"788","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453151","citationCount":0,"referenceCount":0,"year":2018,"authors":"A. Armaly; P. Rodeghero; C. McMillan","affiliations":"Univ. of Notre Dame, Notre Dame, IN, USA; Univ. of Notre Dame, Notre Dame, IN, USA; Univ. of Notre Dame, Notre Dame, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f8"},"title":"[Journal First] MSeer – An Advanced Technique for Locating Multiple Bugs in Parallel","abstract":"In practice, a program may contain multiple bugs. The simultaneous presence of these bugs may deteriorate the effectiveness of existing fault-localization techniques to locate program bugs. While it is acceptable to use all failed and successful tests to identify suspicious code for programs with exactly one bug, it is not appropriate to use the same approach for programs with multiple bugs because the due-to relationship between failed tests and underlying bugs cannot be easily identified. One solution is to generate fault-focused clusters by grouping failed tests caused by the same bug into the same clusters. We propose MSeer - an advanced fault localization technique for locating multiple bugs in parallel. Our major contributions include the use of (1) a revised Kendall tau distance to measure the distance between two failed tests, (2) an innovative approach to simultaneously estimate the number of clusters and assign initial medoids to these clusters, and (3) an improved K-medoids clustering algorithm to better identify the due-to relationship between failed tests and their corresponding bugs. Case studies on 840 multiple-bug versions of seven programs suggest that MSeer performs better in terms of effectiveness and efficiency than two other techniques for locating multiple bugs in parallel.","conference":"IEEE","terms":"Computer bugs;Software;Fault diagnosis;Computer science;Clustering algorithms;Measurement;Software engineering,parallel processing;pattern clustering;program debugging;program testing;software fault tolerance,fault-focused clusters;failed tests;advanced fault localization technique;program bugs;bug locating;fault-localization techniques;K-medoids clustering;MSeer","keywords":"Software fault localization;parallel debugging;multiple bugs;clustering;distance metrics","startPage":"1064","endPage":"1064","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453186","citationCount":0,"referenceCount":0,"year":2018,"authors":"R. Gao; W. E. Wong","affiliations":"Dept. of Comput. Sci., Univ. of Texas at Dallas, Richardson, TX, USA; Dept. of Comput. Sci., Univ. of Texas at Dallas, Richardson, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34f9"},"title":"[Journal First] Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware","abstract":"The number of malicious Android apps has been and continues to increase rapidly. These malware can damage or alter other files or settings, install additional applications, obfuscate their behaviors, propagate quickly, and so on. To identify and handle such malware, a security analyst can significantly benefit from identifying the family to which a malicious app belongs rather than only detecting if an app is malicious. To address these challenges, we present a novel machine learning-based Android malware detection and family-identification approach, RevealDroid, that operates without the need to perform complex program analyses or extract large sets of features. RevealDroid's selected features leverage categorized Android API usage, reflection-based features, and features from native binaries of apps. We assess RevealDroid for accuracy, efficiency, and obfuscation resilience using a large dataset consisting of more than 54,000 malicious and benign apps. Our experiments show that RevealDroid achieves an accuracy of 98% in detection of malware and an accuracy of 95% in determination of their families. We further demonstrate RevealDroid's superiority against state-of-the-art approaches.","conference":"IEEE","terms":"Malware;Feature extraction;Androids;Humanoid robots;Resilience;Machine learning;Reflection,Android (operating system);application program interfaces;invasive software;learning (artificial intelligence);mobile computing;program diagnostics,obfuscation-resilient detection;malicious Android apps;propagate quickly;security analyst;malicious app;Android malware detection;family-identification approach;Android API usage;reflection-based features;obfuscation resilience;complex program analysis;RevealDroid selected feature","keywords":"obfuscation;machine learning;lightweight;native code;reflection;Android malware","startPage":"497","endPage":"497","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453114","citationCount":0,"referenceCount":0,"year":2018,"authors":"J. Garcia; M. Hammad; S. Malek","affiliations":"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34fa"},"title":"Do You Remember This Source Code?","abstract":"Being familiar with the source code of a program comprises knowledge about its purpose, structure, and details. Consequently, familiarity is an important factor in many contexts of software development, especially for maintenance and program comprehension. As a result, familiarity is considered to some extent in many different approaches, for example, to model costs or to identify experts. Still, all approaches we are aware of require a manual assessment of familiarity and empirical analyses of forgetting in software development are missing. In this paper, we address this issue with an empirical study that we conducted with 60 open-source developers. We used a survey to receive information on the developers' familiarity and analyze the responses based on data we extract from their used version control systems. The results show that forgetting is an important factor when considering familiarity and program comprehension of developers. We find that a forgetting curve is partly applicable for software development, investigate three factors - the number of edits, ratio of owned code, and tracking behavior - that can impact familiarity with code, and derive a general memory strength for our participants. Our findings can be used to scope approaches that have to consider familiarity and they provide insights into forgetting in the context of software development.","conference":"IEEE","terms":"Software;Software engineering;Task analysis;Maintenance engineering;Psychology;Software reliability,configuration management;software development management;software maintenance,source code;software development;owned code;open-source developers","keywords":"familiarity;forgetting;empirical study;maintenance;program comprehension;expert identification;knowledge management","startPage":"764","endPage":"775","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453149","citationCount":1,"referenceCount":0,"year":2018,"authors":"J. Krüger; J. Wiemann; W. Fenske; G. Saake; T. Leich","affiliations":"Harz Univ. of Appl. Sci., Magdeburg, Germany; Otto-von-Guericke-Univ., Magdeburg, Germany; Otto-von-Guericke-Univ., Magdeburg, Germany; Otto-von-Guericke-Univ., Magdeburg, Germany; METOP GmbH, Harz Univ. of Appl. Sci., Magdeburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34fb"},"title":"How not to Structure Your Database-Backed Web Applications: A Study of Performance Bugs in the Wild","abstract":"Many web applications use databases for persistent data storage, and using Object Relational Mapping (ORM) frameworks is a common way to develop such database-backed web applications. Unfortunately, developing efficient ORM applications is challenging, as the ORM framework hides the underlying database query generation and execution. This problem is becoming more severe as these applications need to process an increasingly large amount of persistent data. Recent research has targeted specific aspects of performance problems in ORM applications. However, there has not been any systematic study to identify common performance anti-patterns in real-world such applications, how they affect resulting application performance, and remedies for them. In this paper, we try to answer these questions through a comprehensive study of 12 representative real-world ORM applications. We generalize 9 ORM performance anti-patterns from more than 200 performance issues that we obtain by studying their bug-tracking systems and profiling their latest versions. To prove our point, we manually fix 64 performance issues in their latest versions and obtain a median speedup of 2× (and up to 39× max) with fewer than 5 lines of code change in most cases. Many of the issues we found have been confirmed by developers, and we have implemented ways to identify other code fragments with similar issues as well.","conference":"IEEE","terms":"Rails;Scalability;Servers;Indexes;Computer bugs;Semantics,Internet;program debugging;public domain software;query processing;relational databases,persistent data storage;efficient ORM applications;ORM framework hides;underlying database query generation;performance problems;common performance anti-patterns;resulting application performance;real-world ORM applications;200 performance issues;performance bugs;object relational mapping frameworks;database-backed Web applications;ORM performance anti-patterns","keywords":"performance anti-patterns;Object-Relational Mapping frameworks;database-backed applications;bug study","startPage":"800","endPage":"810","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453153","citationCount":2,"referenceCount":0,"year":2018,"authors":"J. Yang; C. Yan; P. Subramaniam; S. Lu; A. Cheung","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34fc"},"title":"[Journal First] ChangeLocator: Locate Crash-Inducing Changes Based on Crash Reports","abstract":"Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crashinducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.","conference":"IEEE","terms":"Computer bugs;Software;Software engineering;Data models;Computer science;Maintenance engineering,program debugging;software maintenance,historical fixed crashes;crashinducing changes;bug fix location;bug inducing changes;crashing bug;software bugs;software crashes;crash reports;crash-inducing changes","keywords":"crash-inducing change;software crash;crash stack;bug localization","startPage":"536","endPage":"536","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453120","citationCount":0,"referenceCount":0,"year":2018,"authors":"R. Wu; M. Wen; S. Cheung; H. Zhang","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Sch. of Electr. Eng. \u0026 Comput., Univ. of Newcastle, Newcastle, NSW, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34fd"},"title":"[Journal First] Empirical Study on the Discrepancy Between Performance Testing Results from Virtual and Physical Environments","abstract":"Large software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving eld requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide exible and less costly environments for performance tests. However, the use of VMs may introduce confounding overhead (e.g., a higher than expected memory utilization with unstable I/O tra c) to the testing environment and lead to unre-alistic performance testing results. Yet, little research has studied the impact on test results of using VMs in performance testing activities. To evaluate the discrepancy between the performance testing results from virtual and physical environments, we perform a case study on two open source systems - namely Dell DVD Store (DS2) and CloudStore. We conduct the same performance tests in both virtual and physical environments and compare the performance testing results based on the three aspects that are typically examined for performance testing results: 1) single performance metric (e.g. CPU Time from virtual environment vs. CPU Time from physical environment), 2) the relationship among performance metrics (e.g. correlation between CPU and I/O) and 3) performance models that are built to predict system performance. Our results show that 1) A single metric from virtual and physical environments do not follow the same distribution, hence practitioners cannot simply use a scaling factor to compare the performance between environments, 2) correlations among performance metrics in virtual environments are different from those in physical environments 3) statistical models built based on the performance metrics from virtual environments are different from the models built from physical environments suggesting that practitioners cannot use the performance testing results across virtual and physical environments. In order to assist the practitioners leverage performance testing results in both environments, we investigate ways to reduce the discrepancy. We find that such discrepancy can be reduced by normalizing performance metrics based on deviance. Overall, we suggest that practitioners should not use the performance testing results from virtual environment with the simple assumption of straightforward performance overhead. Instead, practitioners should consider leveraging normalization techniques to reduce the discrepancy before examining performance testing results from virtual and physical environments.","conference":"IEEE","terms":"Testing;Measurement;Software engineering;Virtual environments;Correlation;Software performance,program testing;public domain software;software metrics;statistical analysis;virtual machines,performance metrics;performance testing environment;unre-alistic performance testing results;performance testing activities;physical environments;software systems;open source systems;Dell DVD store;CloudStore;practitioners leverage performance testing;virtual environments;scaling factor;statistical models","keywords":"Software performance engineering;Software performance analysis;Testing on virtual environments","startPage":"822","endPage":"822","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453155","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. M. Arif; W. Shang; E. Shihab","affiliations":"Dept. of Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. of Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. of Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34fe"},"title":"A Posteriori Typing for Model-Driven Engineering: Concepts, Analysis, and Applications","abstract":"Model-Driven Engineering (MDE) is a software engineering paradigm where models are actively used to specify, test, simulate, analyse and maintain the systems to be built, among other activities. Models can be defined using general-purpose modelling languages like the UML, but for particular domains, the use of domain-specific languages is pervasive. Either way, models must conform to a meta-model which defines their abstract syntax. In MDE, the definition of model management operations - often typed over project-specific meta-models - is recurrent. However, even if two operations are similar, they must be developed from scratch whenever they are applied to instances of different meta-models. This is so as operations defined (i.e., typed) over a meta-model cannot be directly reused for another. Part of this difficulty of reuse is because classes in meta-models are used in two ways: as templates to create objects and as static classifiers for them. These two aspects are inherently tied in most meta-modelling approaches, which results in unnecessarily rigid systems and hinders reusability of MDE artefacts. To enhance flexibility and reuse in MDE, we propose an approach to decouple object creation from typing [1]. The approach relies on standard mechanisms for object creation, and proposes the notion of a posteriori typing as a means to retype objects and enable multiple, partial, dynamic typings. A posteriori typing enhances flexibility because it allows models to be retyped with respect to other meta-models. Hence, we distinguish between creation meta-models used to construct models, and role meta-models into which models are retyped. This permits unanticipated reuse, as a model management operation defined for a role meta-model can be reused as-is with models built using a different creation meta-model, once such models are reclassified. Moreover, our approach permits expressing some types of bidirectional model transformations by reclassification. The transformations defined as reclassifications have better performance than the equivalent ones defined with traditional transformation languages, because reclassification does not require creating new objects. In [1], we propose two mechanisms to define a posteriori typings: type-level (mappings between meta-models) and instance-level (set of model queries). The paper presents the underlying theory and type correctness criteria of both mechanisms, defines some analysis methods, identifies practical restrictions for retyping specifications, and demonstrates the feasibility of the approach by an implementation atop our meta-modelling tool MetaDepth. We also explore application scenarios of a posteriori typing (to define transformations, for model transformation reuse, and to improve transformation expressiveness by dynamic type change), and present an experiment showing the potential performance gains when expressing transformations as retypings.","conference":"IEEE","terms":"Unified modeling language;Software engineering;Model driven engineering;Analytical models;Tools;Domain specific languages;Syntactics,formal specification;software engineering;software reusability;specification languages;Unified Modeling Language,posteriori typing;model transformation reuse;general-purpose modelling languages;model management operation;project-specific meta-models;meta-modelling approaches;creation meta-models;role meta-model;bidirectional model transformations;model queries;model-driven engineering;MetaDepth;meta-modelling tool","keywords":"Model-driven engineering;reuse;model typing;partial typing;dynamic typing;model transformations;bidirectionality;MetaDepth","startPage":"1136","endPage":"1136","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453194","citationCount":0,"referenceCount":0,"year":2018,"authors":"J. de Lara; E. Guerra","affiliations":"Modelling \u0026 Software Eng. Group, Univ. Autonoma de Madrid, Cantoblanco, Spain; Modelling \u0026 Software Eng. Group, Univ. Autonoma de Madrid, Cantoblanco, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d34ff"},"title":"Efficient Sampling of SAT Solutions for Testing","abstract":"In software and hardware testing, generating multiple inputs which satisfy a given set of constraints is an important problem with applications in fuzz testing and stimulus generation. However, it is a challenge to perform the sampling efficiently, while generating a diverse set of inputs which satisfy the constraints. We developed a new algorithm QuickSampler which requires a small number of solver calls to produce millions of samples which satisfy the constraints with high probability. We evaluate QuickSampler on large real-world benchmarks and show that it can produce unique valid solutions orders of magnitude faster than other state-of-the-art sampling tools, with a distribution which is reasonably close to uniform in practice.","conference":"IEEE","terms":"Hardware;Software;Fuzzing;Benchmark testing;Software engineering;Tools,computability;probability;program testing,quicksampler;sampling tools;unique valid solutions;fuzz testing;multiple inputs;hardware testing;SAT solutions;solver calls;stimulus generation","keywords":"sampling;stimulus generation;constraint-based testing;constrained random verification","startPage":"549","endPage":"559","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453122","citationCount":2,"referenceCount":0,"year":2018,"authors":"R. Dutra; K. Laeufer; J. Bachrach; K. Sen","affiliations":"EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA; EECS Dept., Univ. of California, Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d3500"},"title":"Precise Concolic Unit Testing of C Programs Using Extended Units and Symbolic Alarm Filtering","abstract":"Automated unit testing reduces manual effort to write unit test drivers/stubs and generate unit test inputs. However, automatically generated unit test drivers/stubs raise false alarms because they often over-approximate real contexts of a target function f and allow infeasible executions off. To solve this problem, we have developed a concolic unit testing technique CONBRIO. To provide realistic context to f, it constructs an extended unit of f that consists of f and closely relevant functions to f. Also, CONBRIO filters out a false alarm by checking feasibility of a corresponding symbolic execution path with regard to f's symbolic calling contexts obtained by combining symbolic execution paths of f's closely related predecessor functions. In the experiments on the crash bugs of 15 real-world C programs, CONBRIO shows both high bug detection ability (i.e. 91.0% of the target bugs detected) and high precision (i.e. a true to false alarm ratio is 1:4.5). Also, CONBRIO detects 14 new bugs in 9 target C programs studied in papers on crash bug detection techniques.","conference":"IEEE","terms":"Testing;Computer bugs;Filtering;Manuals;Software engineering;Space exploration,C language;program debugging;program diagnostics;program verification,symbolic alarm filtering;automated unit testing;manual effort;unit test inputs;automatically generated unit test drivers/stubs;over-approximate real contexts;infeasible executions;technique CONBRIO;closely relevant functions;corresponding symbolic execution path;symbolic calling contexts;symbolic execution paths;predecessor functions;false alarm ratio;crash bug detection techniques;C program precise concolic unit testing","keywords":"automated unit testing;unit test driver/stub generation;dynamic symbolic execution;false alarm reduction","startPage":"315","endPage":"326","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453090","citationCount":1,"referenceCount":0,"year":2018,"authors":"Y. Kim; Y. Choi; M. Kim","affiliations":"Sch. of Comput., KAIST, Daejeon, South Korea; Sch. of Comput. Sci. \u0026 Eng., Kyungpook Nat. Univ., Daegu, South Korea; Sch. of Comput., KAIST, Daejeon, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d3501"},"title":"Integrating Technical Debt Management and Software Quality Management Processes: A Framework and Field Tests","abstract":"Technical debt, defined as the maintenance obligations arising from shortcuts taken during the design, development, and deployment of software systems, has been shown to significantly impact the reliability and long-term evolution of software systems [1], [2]. Although academic research has moved beyond using technical debt only as a metaphor, and has begun compiling strong empirical evidence on the economic implications of technical debt, industry practitioners continue to find managing technical debt a challenging balancing act [3]. Despite the increasing awareness of the importance of managing technical debt in software product development, systematic processes for implementing technical debt management in software production have not been readily available. To address this gap, we developed and field tested a normative process framework that systematically incorporates steps for managing technical debt in commercial software production. The framework integrates processes required for technical debt management with existing software quality management processes prescribed by the project management body of knowledge (PMBOK) [4], and organizes the different processes for technical debt management under three steps: (1) make technical debt visible, (2) perform cost-benefit analysis, and (3) control technical debt. To implement the processes, we introduce a new artifact, called the technical debt register, which stores, for each software asset, the outstanding principal and the associated interest estimated for the technical debt embedded in the asset. The technical debt register also stores the desired control target for each software asset's technical debt, which is populated and used during the cost-benefit analysis and control target calculations. There are three main benefits from this integrated approach. First, it enables the uncovering of hidden technical debt embedded in systems. Established quality assurance and control practices can be utilized to effectively associate software defects with specific design and deployment decisions made by programmers. Such associations make technical debt visible to the team and thereby facilitate the quantification of debt-related principal and interest. Second, it helps to bridge the gaps that exist between the technical and economic assessments of technical debt, and aid in formulating actionable policies related to technical debt management. Finally, integrating technical debt management processes with established quality frameworks aids the wider adoption of emerging prescriptions for managing technical debt. We partnered with three commercial software product development organizations to implement the framework in real-world software production settings. All three organizations, irrespective of their varying software process maturity levels, were able to adopt the proposed framework and integrate the prescribed technical debt management processes with their existing software quality management processes. Our longitudinal data and case-study interviews indicate that the organizations were able to accrue economic benefits from the adoption and use of the integrated framework. And, based on our field study observations, we also identified a set of best practices that support the implementation and use of our framework: facilitating engagement between business and engineering stakeholders, adoption of policies based on a probabilistic analysis framework, and limiting process overheads.","conference":"IEEE","terms":"Software quality;Economics;Software engineering;Software systems;Product development;Production,product development;project management;quality assurance;quality management;software development management;software maintenance;software quality,software systems;managing technical debt;existing software quality management processes;technical debt register;software asset;hidden technical debt;prescribed technical debt management processes;integrating technical debt management;control technical debt","keywords":"Technical debt;software quality;software maintenance;software engineering economics;cost of quality;software product development;software process;case study","startPage":"883","endPage":"883","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453163","citationCount":0,"referenceCount":0,"year":2018,"authors":"N. Ramasubbu; C. Kemerer","affiliations":"Univ. of Pittsburgh, Pittsburgh, PA, USA; Univ. of Pittsburgh, Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9119eee8435e8e7d3502"},"title":"Debugging with Intelligence via Probabilistic Inference","abstract":"We aim to debug a single failing execution without the assistance from other passing/failing runs. In our context, debugging is a process with substantial uncertainty - lots of decisions have to be made such as what variables shall be inspected first. To deal with such uncertainty, we propose to equip machines with human-like intelligence. Specifically, we develop a highly automated debugging technique that aims to couple human-like reasoning (e.g., dealing with uncertainty and fusing knowledge) with program semantics based analysis, to achieve benefits from the two and mitigate their limitations. We model debugging as a probabilistic inference problem, in which the likelihood of each executed statement instance and variable being correct/faulty is modeled by a random variable. Human knowledge, human-like reasoning rules and program semantics are modeled as conditional probability distributions, also called probabilistic constraints. Solving these constraints identifies the most likely faulty statements. Our results show that the technique is highly effective. It can precisely identify root causes for a set of real-world bugs in a very small number of interactions with developers, much smaller than a recent proposal that does not encode human intelligence. Our user study also confirms that it substantially improves human productivity.","conference":"IEEE","terms":"Debugging;Probabilistic logic;Uncertainty;Cognition;Semantics;Python;Tools,inference mechanisms;program debugging,reasoning rules;program semantics;conditional probability distributions;faulty statements;human intelligence;human productivity;single failing execution;substantial uncertainty;highly automated debugging technique;couple human-like reasoning;fusing knowledge;model debugging;probabilistic inference problem;executed statement instance;human knowledge;passing-failing runs","keywords":"Debugging;Probabilistic Inference;Python","startPage":"1171","endPage":"1181","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453198","citationCount":0,"referenceCount":0,"year":2018,"authors":"Z. Xu; S. Ma; X. Zhang; S. Zhu; B. Xu","affiliations":"State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3503"},"title":"Measuring Program Comprehension: A Large-Scale Field Study with Professionals","abstract":"This paper is published in IEEE Transaction on Software Engineering (DOI: 10.1109/TSE.2017.2734091). Comparing with previous programming comprehension studies that are usually in controlled settings or have a small number of participants, we perform a more realistic investigation of program comprehension activities. To do this, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We collect 3,148 working hour data from 78 professional developers in a field study. We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. Then we measure comprehension time by calculating the time that developers spend on program comprehension. We find that on average developers spend ~58% of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension.","conference":"IEEE","terms":"Computer science;Software;Software engineering;Australia;Conferences;Software measurement;Information technology,online front-ends;reverse engineering;software engineering;software maintenance,large-scale field study;program comprehension activities;measure comprehension time;professional developers;program comprehension measurement","keywords":"Program Comprehension;Field Study;Inference Model","startPage":"584","endPage":"584","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453126","citationCount":2,"referenceCount":0,"year":2018,"authors":"X. Xia; L. Bao; D. Lo; Z. Xing; A. E. Hassan; S. Li","affiliations":"Fac. of Inf. Technol., Monash Univ., Clayton, VIC, Australia; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Res. Sch. of Comput. Sci., Australian Nat. Univ., Canberra, ACT, Australia; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3504"},"title":"Towards Reusing Hints from Past Fixes: An Exploratory Study on Thousands of Real Samples","abstract":"Researchers have recently proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta dependency graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlapped with each other in terms of the content, code structure, and identifier names of fixes. Our results show that if an APR approach composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach composes new fixes by fully or partially reusing the code structure of past fixes, up to 41.3% and 29.7% new fixes can be created.","conference":"IEEE","terms":"Computer bugs;Maintenance engineering;Syntactics;Software engineering;Computer science;Software;Measurement,program debugging;software maintenance,automatic program repair;APR approach;delta dependency graphs","keywords":"reusing past fix;empirical study;program repair","startPage":"885","endPage":"885","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453165","citationCount":0,"referenceCount":0,"year":2018,"authors":"H. Zhong; N. Meng","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3505"},"title":"[Journal First] A Correlation Study Between Automated Program Repair and Test-Suite Metrics","abstract":"Automated program repair has attracted attention due to its potential to reduce debugging cost. Prior works show the feasibility of automated repair, and the research focus is gradually shifting towards the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used. In this paper, we investigate the question: \"\"Can traditional test-suite metrics used in software testing be used for automated program repair?\"\". We empirically investigate the effectiveness of test-suite metrics (statement / branch coverage and mutation score) in controlling the reliability of repairs (the likelihood that repairs cause regressions). We conduct the largest-scale experiments to date with real-world software, and perform the first correlation study between test-suite metrics and the reliability of generated repairs. Our results show that by increasing test-suite metrics, the reliability of repairs tend to increase. Particularly, such trend is most strongly observed in statement coverage. This implies that traditional test-suite metrics used in software testing can also be used to improve the reliability of repairs in program repair.","conference":"IEEE","terms":"Maintenance engineering;Measurement;Correlation;Software reliability;Software;Software testing,program debugging;program testing;software maintenance,regressions;test-suite metrics;automated repair;generated repairs;correlation study;automated program repair;software testing;test-suites;generated patches","keywords":"Code Coverage;Program Repair;Mutation Testing","startPage":"24","endPage":"24","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453057","citationCount":0,"referenceCount":0,"year":2018,"authors":"J. Yi; S. H. Tan; S. Mechtaev; M. Böhme; A. Roychoudhury","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3506"},"title":"Online App Review Analysis for Identifying Emerging Issues","abstract":"Detecting emerging issues (e.g., new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (e.g., misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the identified issues can facilitate app development in practice. Moreover, we have successfully applied IDEA to several products of Tencent, which serve hundreds of millions of users.","conference":"IEEE","terms":"Computer bugs;Meteorology;Facebook;Google;Semantics;Software engineering,program debugging;program testing;smart phones,noise words;emerging app issues;IDEA;popular apps;official app changelogs;app development;online App review analysis;App reviews;Apple App Store","keywords":"app reviews;online analysis;emerging issues","startPage":"48","endPage":"58","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453061","citationCount":7,"referenceCount":0,"year":2018,"authors":"C. Gao; J. Zeng; M. R. Lyu; I. King","affiliations":"Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3507"},"title":"Automatic Software Repair: A Survey","abstract":"Despite their growing complexity and increasing size, modern software applications must satisfy strict release requirements that impose short bug fixing and maintenance cycles, putting significant pressure on developers who are responsible for timely producing high-quality software. To reduce developers workload, repairing and healing techniques have been extensively investigated as solutions for efficiently repairing and maintaining software in the last few years. In particular, repairing solutions have been able to automatically produce useful fixes for several classes of bugs that might be present in software programs. A range of algorithms, techniques, and heuristics have been integrated, experimented, and studied, producing a heterogeneous and articulated research framework where automatic repair techniques are proliferating. This paper organizes the knowledge in the area by surveying a body of 108 papers about automatic software repair techniques, illustrating the algorithms and the approaches, comparing them on representative examples, and discussing the open challenges and the empirical evidence reported so far.","conference":"IEEE","terms":"Maintenance engineering;Debugging;Software engineering;Fault diagnosis;Automation;Software systems,program debugging;software maintenance;software quality,articulated research framework;automatic repair techniques;automatic software repair techniques;modern software applications;strict release requirements;short bug;maintenance cycles;significant pressure;software programs;heterogeneous research framework;high-quality software","keywords":"Automatic Program Repair;Generate and Validate;Search Based;Semantics driven repair;Correct by Construction;Program Synthesis;Self Repairing","startPage":"1219","endPage":"1219","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453203","citationCount":0,"referenceCount":0,"year":2018,"authors":"L. Gazzola; L. Mariani; D. Micucci","affiliations":"Univ. degli Studi di Milano-Bicocca, Milan, Italy; Univ. degli Studi di Milano-Bicocca, Milan, Italy; Univ. degli Studi di Milano-Bicocca, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3508"},"title":"[Journal First] Inference of Development Activities from Interaction with Uninstrumented Applications","abstract":"This paper is published in Journal of Empirical Software Engineering (DOI: 10.1007/s10664-017-9547-8). Studying developers' behavior is crucial for designing effective techniques and tools to support developers' daily work. However, there are two challenges in collecting and analyzing developers' behavior data. First, instrumenting many software tools commonly used in real work settings (e.g., IDEs, web browsers) is difficult and requires significant resources. Second, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. To address these two challenges, we first use our ActivitySpace framework to improve the generalizability of the data collection. Then, we propose a Condition Random Field (CRF) based approach to segment and label the developers' low-level actions into a set of basic, yet meaningful development activities. To evaluate our proposed approach, we deploy the ActivitySpace framework in an industry partner's company and collect the real working data from ten professional developers' one-week work. We conduct an experiment with the collected data and a small number of initial human-labeled training data using the CRF model and the other three baselines (i.e., a heuristic-rules based method, a SVM classifier, and a random weighted classifier). The proposed CRF model achieves better performance (i.e., 0.728 accuracy and 0.672 macro-averaged F1-score) than the other three baselines.","conference":"IEEE","terms":"Software engineering;Australia;Tools;Software;Computer science;Information technology;Information systems,pattern classification;software engineering;software tools,empirical software engineering;Web browsers;condition random field based approach;low-level actions;data collection;high-level development activities;fine-grained event sequences;collected behavior data;work settings;software tools;uninstrumented applications;CRF model;one-week work;professional developers;working data;ActivitySpace framework;meaningful development activities","keywords":"Software development;Developers' interaction data;Condition Random Field","startPage":"897","endPage":"897","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453167","citationCount":0,"referenceCount":0,"year":2018,"authors":"L. Bao; Z. Xing; X. Xia; D. Lo; A. E. Hassan","affiliations":"Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Res. Sch. of Comput. Sci., Australian Nat. Univ., Canberra, ACT, Australia; Fac. of Inf. Technol., Monash Univ., Clayton, VIC, Australia; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Sch. of Comput., Queen's Univ., Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3509"},"title":"Predicting Future Developer Behavior in the IDE Using Topic Models","abstract":"Interaction data, gathered from developers' daily clicks and key presses in the IDE, has found use in both empirical studies and in recommendation systems for software engineering. We observe that this data has several characteristics, common across IDEs: 1) exponentially distributed - some events or commands dominate the trace (e.g., cursor movement commands), while most other commands occur relatively infrequently; 2) noisy - the traces include spurious commands (or clicks), or unrelated events, that may not be important to the behavior of interest; 3) comprise of overlapping events and commands - specific commands can be invoked by separate mechanisms, and similar events can be triggered by different sources. These characteristics of this data are analogous to the characteristics of synonymy and polysemy in natural language corpora. Therefore, this paper (and presentation) presents a new modeling approach for this type of data, leveraging topic models typically applied to streams of natural language text.","conference":"IEEE","terms":"Software engineering,natural language processing;recommender systems;software engineering;text analysis,modeling approach;leveraging topic models;IDE;interaction data;key presses;recommendation systems;software engineering;spurious commands;unrelated events;future developer behavior prediction;developers daily clicks;cursor movement commands;overlapping events;overlapping commands;synonymy;polysemy;natural language corpora;natural language text","keywords":"command recommendation systems;IDE interaction data","startPage":"932","endPage":"932","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453171","citationCount":0,"referenceCount":0,"year":2018,"authors":"K. Damevski; H. Chen; D. C. Shepherd; N. A. Kraft; L. Pollock","affiliations":"Virginia Commonwealth Univ., Richmond, VA, USA; Brooklyn, New York, NY, USA; ABB Corp. Res., Raleigh, NC, USA; ABB Corp. Res., Raleigh, NC, USA; Univ. of Delaware, Newark, DE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d350a"},"title":"[Journal First] Privacy by Designers: Software Developers' Privacy Mindset","abstract":"Privacy by design (PbD) is a policy measure that guides software developers to apply inherent solutions to achieve better privacy protection. For PbD to be a viable option, it is important to understand developers' perceptions, interpretation and practices as to informational privacy (or data protection). To this end, we conducted in-depth interviews with 27 developers from different domains, who practice software design. Grounded analysis of the data revealed an interplay between several different forces affecting the way in which developers handle privacy concerns. Borrowing the schema of Social Cognitive Theory (SCT), we classified and analyzed the cognitive, organizational and behavioral factors that play a role in developers' privacy decision making. Our findings indicate that developers use the vocabulary of data security to approach privacy challenges, and that this vocabulary limits their perceptions of privacy mainly to third-party threats coming from outside of the organization; that organizational privacy climate is a powerful means for organizations to guide developers toward particular practices of privacy; and that software architectural patterns frame privacy solutions that are used throughout the development process, possibly explaining developers' preference of policy-based solutions to architectural solutions. Further, we show, through the use of the SCT schema for framing the findings of this study, how a theoretical model of the factors that influence developers' privacy practices can be conceptualized and used as a guide for future research toward effective implementation of PbD.","conference":"IEEE","terms":"Privacy;Data privacy;Meteorology;Software;Information systems;Vocabulary;Software engineering,cognition;data privacy;decision making;security of data;software architecture,decision making;privacy by design;organizational privacy;data protection;software development process;software architectural patterns;informational privacy;privacy protection;PbD;policy-based solutions;data security;behavioral factors;Social Cognitive Theory;software design","keywords":"Data protection;Privacy;Privacy by design;Qualitative research;Grounded analysis;Social cognitive theory;Organizational climate","startPage":"396","endPage":"396","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453098","citationCount":0,"referenceCount":0,"year":2018,"authors":"I. Hadar; T. Hasson; O. Ayalon; E. Toch; M. Birnhack; S. Sherman; A. Balissa","affiliations":"Dept. of Inf. Syst., Univ. of Haifa, Haifa, Israel; Dept. of Inf. Syst., Univ. of Haifa, Haifa, Israel; Fac. of Eng., Tel Aviv Univ., Tel Aviv, Israel; Fac. of Eng., Tel Aviv Univ., Tel Aviv, Israel; Fac. of Eng., Tel Aviv Univ., Tel Aviv, Israel; Dept. of Inf. Syst., Univ. of Haifa, Haifa, Israel; Fac. of Eng., Tel Aviv Univ., Tel Aviv, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d350b"},"title":"TypeDevil: Dynamic Type Inconsistency Analysis for JavaScript","abstract":"Dynamic languages, such as JavaScript, give programmers the freedom to ignore types, and enable them to write concise code in short time. Despite this freedom, many programs follow implicit type rules, for example, that a function has a particular signature or that a property has a particular type. Violations of such implicit type rules often correlate with problems in the program. This paper presents Type Devil, a mostly dynamic analysis that warns developers about inconsistent types. The key idea is to assign a set of observed types to each variable, property, and function, to merge types based in their structure, and to warn developers about variables, properties, and functions that have inconsistent types. To deal with the pervasiveness of polymorphic behavior in real-world JavaScript programs, we present a set of techniques to remove spurious warnings and to merge related warnings. Applying Type Devil to widely used benchmark suites and real-world web applications reveals 15 problematic type inconsistencies, including correctness problems, performance problems, and dangerous coding practices.","conference":"IEEE","terms":"Instruments;Performance analysis;Runtime;Computer crashes;Arrays;Receivers;Benchmark testing,Java,TypeDevil;dynamic type inconsistency analysis;JavaScript;dynamic language;polymorphic behavior pervasiveness","keywords":"Dynamic languages;types;dynamic analysis;JavaScript;program analysis","startPage":"314","endPage":"324","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194584","citationCount":28,"referenceCount":45,"year":2015,"authors":"M. Pradel; P. Schuh; K. Sen","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d350c"},"title":"Performance Analysis Using Subsuming Methods: An Industrial Case Study","abstract":"Large-scale object-oriented applications consist of tens of thousands of methods and exhibit highly complex runtime behaviour that is difficult to analyse for performance. Typical performance analysis approaches that aggregate performance measures in a method-centric manner result in thinly distributed costs and few easily identifiable optimisation opportunities. Subsuming methods analysis is a new approach that aggregates performance costs across repeated patterns of method calls that occur in the application's runtime behaviour. This allows automatic identification of patterns that are expensive and represent practical optimisation opportunities. To evaluate the practicality of this analysis with a real world large-scale object-oriented application we completed a case study with the developers of letterboxd.com - a social network website for movie goers. Using the results of the analysis we were able to rapidly implement changes resulting in a 54.8% reduction in CPU load and an 49.6% reduction in average response time.","conference":"IEEE","terms":"Servers;Time factors;Runtime;Optimization;Performance analysis;Databases;Context,object-oriented methods;optimisation;pattern recognition;social networking (online);software performance evaluation,performance analysis;subsuming methods;large-scale object-oriented applications;complex runtime behaviour;optimisation opportunities;subsuming method analysis;application runtime behaviour;automatic pattern identification;large-scale object-oriented application;social network Website;CPU load reduction;average response time reduction","keywords":"Subsuming methods;Runtime bloat;Performance analysis;Object oriented software","startPage":"149","endPage":"158","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202959","citationCount":5,"referenceCount":27,"year":2015,"authors":"D. Maplesden; K. v. Randow; E. Tempero; J. Hosking; J. Grundy","affiliations":"Univ. of Auckland, Auckland, New Zealand; NA; Univ. of Auckland, Auckland, New Zealand; Univ. of Auckland, Auckland, New Zealand; Swinburne Univ. of Technol., Hawthorn, VIC, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d350d"},"title":"Design and Evaluation of a Customizable Multi-Domain Reference Architecture on Top of Product Lines of Self-Driving Heavy Vehicles - An Industrial Case Study","abstract":"Self-driving vehicles for commercial use cases like logistics or overcast mines increase their owners' economic competitiveness. Volvo maintains, evolves, and distributes a vehicle control product line for different brands like Volvo Trucks, Renault, and Mack in more than 190 markets world-wide. From the different application domains of their customers originates the need for a multi-domain reference architecture concerned with transport mission planning, execution, and tracking on top of the vehicle control product line. This industrial case study is the first of its kind reporting about the systematic process to design such a reference architecture involving all relevant external and internal stakeholders, development documents, low level artifacts, and literature. Quantitative and qualitative metrics were applied to evaluate non-functional requirements on the reference architecture level before a concrete variant was evaluated using a Volvo FMX truck in an exemplary construction site setting.","conference":"IEEE","terms":"Computer architecture;Vehicles;Stakeholders;Bibliographies;Interviews;Conferences;Planning,control engineering computing;logistics;mobile robots;road vehicles,customizable multidomain reference architecture;product lines;self-driving heavy vehicles;industrial case study;commercial use cases;logistics;overcast mines;economic competitiveness;vehicle control product line;Renault;Mack;multidomain reference architecture;transport mission planning;quantitative metrics;qualitative metrics;nonfunctional requirements;Volvo FMX truck;construction site setting","keywords":"self-driving vehicles;reference architecture;design;evaluation;variability;industrial case study","startPage":"189","endPage":"198","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202963","citationCount":2,"referenceCount":64,"year":2015,"authors":"J. Schroeder; D. Holzner; C. Berger; C. Hoel; L. Laine; A. Magnusson","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d350e"},"title":"When App Stores Listen to the Crowd to Fight Bugs in the Wild","abstract":"App stores are digital distribution platforms that put available apps that run on mobile devices. Current stores are software repositories that deliver apps upon user requests. However, when an app has a bug, the store continues delivering defective apps until the developer uploads a fixed version, thus impacting on the reputation of both store and app developer. In this paper, we envision a new generation of app stores that: (a) reduce human intervention to maintain mobile apps; and (b) enhance store services with smart and autonomous functionalities to automatically increase the quality of the delivered apps. We sketch a prototype of our envisioned app store and we discuss the functionalities that current stores an enhance by incorporating automatic software repair techniques.","conference":"IEEE","terms":"Monitoring;Computer bugs;Androids;Humanoid robots;Maintenance engineering;Google,mobile computing;software quality,App stores;digital distribution platforms;mobile devices;software repository;automatic software repair techniques","keywords":"","startPage":"567","endPage":"570","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203014","citationCount":5,"referenceCount":11,"year":2015,"authors":"M. Gomez; M. Martineza; M. Monperrus; R. Rouvoy","affiliations":"Inria Lille - Nord Eur., Univ. of Lille 1, Lille, France; Inria Lille - Nord Eur., Univ. of Lille 1, Lille, France; Inria Lille - Nord Eur., Univ. of Lille 1, Lille, France; Inria Lille - Nord Eur., Univ. of Lille 1, Lille, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d350f"},"title":"Interdisciplinary Design Patterns for Socially Aware Computing","abstract":"The success of software applications that collect and process personal data does not only depend on technical aspects, but is also linked to social compatibility and user acceptance. It requires experts from different disciplines to ensure legal compliance, to foster the users' trust, to enhance the usability of the application and to finally realize the application. Multidisciplinary requirements have to be formulated, interwoven and implemented. We advocate the use of interdisciplinary design patterns that capture the design know-how of typical, recurring features in socially aware applications with particular concern for socio-technical requirements. The proposed patterns address interdisciplinary concerns in a tightly interwoven manner and are intended to facilitate the development of accepted and acceptable applications that in particular deal with sensitive user context information.","conference":"IEEE","terms":"Law;Context;Venus;Software engineering;Usability,human factors;software engineering,interdisciplinary design patterns;socially aware computing;software applications;social compatibility;user acceptance;legal compliance;user trust;socially aware applications;socio-technical requirements;user context information","keywords":"Interdisciplinary Design Patterns;Socially Aware Computing","startPage":"477","endPage":"486","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202998","citationCount":7,"referenceCount":32,"year":2015,"authors":"H. Baraki; K. Geihs; C. Voigtmann; A. Hoffmann; R. Kniewel; B. Macek; J. Zirfas","affiliations":"Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany; Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany; Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany; Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany; Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany; Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany; Zentrum fur Informationstechnik-Gestaltung (ITeG), Univ. of Kassel, Kassel, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3510"},"title":"ChangeScribe: A Tool for Automatically Generating Commit Messages","abstract":"During software maintenances tasks, commit messages are an important source of information, knowledge, and documentation that developers rely upon. However, the number and nature of daily activities and interruptions can influence the quality of resulting commit messages. This formal demonstration paper presents ChangeScribe, a tool for automatically generating commit messages. ChangeScribe is available at http://www.cs.wm.edu/semeru/changescribe (Eclipse plugin, instructions, demos and the source code).","conference":"IEEE","terms":"Software;Context;Java;Semantics;XML;Visualization;Documentation,software maintenance;system documentation,ChangeScribe;automatic commit message generation;software maintenance task;information source;knowledge source;documentation;Eclipse plugin","keywords":"Commit message;summarization;code changes","startPage":"709","endPage":"712","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203049","citationCount":26,"referenceCount":19,"year":2015,"authors":"M. Linares-Vásquez; L. F. Cortés-Coy; J. Aponte; D. Poshyvanyk","affiliations":"Coll. of William \u0026 Mary, Williamsburg, VA, USA; Univ. Nac. de Colombia, Bogota, Colombia; Univ. Nac. de Colombia, Bogota, Colombia; Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3511"},"title":"What Makes a Great Software Engineer?","abstract":"Good software engineers are essential to the creation of good software. However, most of what we know about software-engineering expertise are vague stereotypes, such as 'excellent communicators' and 'great teammates'. The lack of specificity in our understanding hinders researchers from reasoning about them, employers from identifying them, and young engineers from becoming them. Our understanding also lacks breadth: what are all the distinguishing attributes of great engineers (technical expertise and beyond)? We took a first step in addressing these gaps by interviewing 59 experienced engineers across 13 divisions at Microsoft, uncovering 53 attributes of great engineers. We explain the attributes and examine how the most salient of these impact projects and teams. We discuss implications of this knowledge on research and the hiring and training of engineers.","conference":"IEEE","terms":"Software;Software engineering;Interviews;Lead;Knowledge engineering;Companies,project management;software engineering,software engineering expertise;engineer attribute;software project","keywords":"Software engineers;expertise;teamwork","startPage":"700","endPage":"710","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194618","citationCount":14,"referenceCount":37,"year":2015,"authors":"P. L. Li; A. J. Ko; J. Zhu","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3512"},"title":"Coexecutability for Efficient Verification of Data Model Updates","abstract":"Modern applications use back-end data stores for persistent data. Automated verification of the code that updates the data store would prevent bugs that can cause loss or corruption of data. In this paper, we focus on the most challenging part of this problem: automated verification of code that updates the data store and contains loops. Due to dependencies between loop iterations, verification of code that contains loops is a hard problem, and typically requires manual assistance in the form of loop invariants. We present a fully automated technique that improves verifiability of loops. We first define co execution, a method for modeling loop iterations that simplifies automated reasoning about loops. Then, we present a fully automated static program analysis that detects whether the behavior of a given loop can be modeled using co execution. We provide a customized verification technique for co executable loops that results in more effective verification. In our experiments we observed that, in 45% of cases, modeling loops using co execution reduces verification time between 1 and 4 orders of magnitude. In addition, the rate of inconclusive verification results in the presence of loops is reduced from 65% down to 24%, all without requiring loop invariants or any manual intervention.","conference":"IEEE","terms":"Data models;Cognition;Object oriented modeling;Semantics;Rails;Computer bugs;Manuals,data handling;program control structures;program diagnostics;reasoning about programs,coexecutability;data model update verification;back-end data store;persistent data;automated code verification;data store update;bug prevention;data loss;data corruption;loop invariants;loop verifiability;loop iteration modeling;automated reasoning about loops;fully automated static program analysis;loop behavior;customized verification technique","keywords":"Data Models;Logic-based Verification;Loop Analysis","startPage":"744","endPage":"754","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194622","citationCount":4,"referenceCount":32,"year":2015,"authors":"I. Bocic; T. Bultan","affiliations":"Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3513"},"title":"AutoCSP: Automatically Retrofitting CSP to Web Applications","abstract":"Web applications often handle sensitive user data, which makes them attractive targets for attacks such as cross-site scripting (XSS). Content security policy (CSP) is a content-restriction mechanism, now supported by all major browsers, that offers thorough protection against XSS. Unfortunately, simply enabling CSP for a web application would affect the application's behavior and likely disrupt its functionality. To address this issue, we propose AutoCSP, an automated technique for retrofitting CSP to web applications. AutoCSP (1) leverages dynamic taint analysis to identify which content should be allowed to load on the dynamically-generated HTML pages of a web application and (2) automatically modifies the server-side code to generate such pages with the right permissions. Our evaluation, performed on a set of real-world web applications, shows that AutoCSP can retrofit CSP effectively and efficiently.","conference":"IEEE","terms":"HTML;Web pages;Browsers;Heuristic algorithms;Security;Servers;Algorithm design and analysis,Internet;security of data,AutoCSP policy;content security policy;CSP retrofitting;Web applications;cross-site scripting;CSP content-restriction mechanism;XSS protection;dynamic taint analysis;dynamically-generated HTML pages;server-side code modification","keywords":"Content security policy;cross-site scripting","startPage":"336","endPage":"346","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194586","citationCount":12,"referenceCount":31,"year":2015,"authors":"M. Fazzini; P. Saxena; A. Orso","affiliations":"Georgia Inst. of Technol., Atlanta, GA, USA; Nat. Univ. of Singapore, Singapore, Singapore; Georgia Inst. of Technol., Atlanta, GA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3514"},"title":"Morpheus: Variability-Aware Refactoring in the Wild","abstract":"Today, many software systems are configurable with conditional compilation. Just like any software system, configurable systems need to be refactored in their evolution, but their inherent variability induces an additional dimension of complexity that is not addressed well by current academic and industrial refactoring engines. To improve the state of the art, we propose a variability-aware refactoring approach that relies on a canonical variability representation and recent work on variability-aware analysis. The goal is to preserve the behavior of all variants of a configurable system, without compromising general applicability and scalability. To demonstrate practicality, we developed Morpheus, a sound, variability-aware refactoring engine for C code with preprocessor directives. We applied Morpheus to three substantial real-world systems (Busybox, OpenSSL, and SQLite) showing that it scales reasonably well, despite of its heavy reliance on satisfiability solvers. By extending a standard approach of testing refactoring engines with support for variability, we provide evidence for the correctness of the refactorings implemented.","conference":"IEEE","terms":"Engines;Standards;Data structures;Software systems;Complexity theory;Scalability;Testing,C language;computability;program compilers;program diagnostics;software maintenance;software reliability,Morpheus;software systems;conditional compilation;variability-aware refactoring approach;canonical variability representation;variability-aware analysis;C code;preprocessor directives;satisfiability solvers;testing refactoring engines","keywords":"","startPage":"380","endPage":"391","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194590","citationCount":16,"referenceCount":62,"year":2015,"authors":"J. Liebig; A. Janker; F. Garbe; S. Apel; C. Lengauer","affiliations":"Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3515"},"title":"Avoiding Security Pitfalls with Functional Programming: A Report on the Development of a Secure XML Validator","abstract":"While the use of XML is pervading all areas of IT, security challenges arise when XML files are used to transfer security data such as security policies. To tackle this issue, we have developed a lightweight secure XML validator and have chosen to base the development on the strongly typed functional language OCaml. The initial development took place as part of the LaFoSec Study which aimed at investigating the impact of using functional languages for security. We then turned the validator into an industrial application, which was successfully evaluated at EAL4+ level by independent assessors. In this paper, we explain the challenges involved in processing XML data in a critical context, we describe our choices in designing a secure XML validator, and we detail how we used features of functional languages to enforce security requirements.","conference":"IEEE","terms":"XML;Syntactics;Software engineering;Computer crime;Standards;Context,security of data;XML,security pitfalls avoidance;functional programming;secure XML validator;extensible markup language;security policy;OCaml functional language;LaFoSec Study;XML data processing;security requirements","keywords":"Security;Software Engineering;Functional Programming;XML Security","startPage":"209","endPage":"218","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202965","citationCount":2,"referenceCount":14,"year":2015,"authors":"D. Doligez; C. Faure; T. Hardin; M. Maarek","affiliations":"Inria, Le Chesnay, France; SafeRiver, Montrouge, France; SafeRiver, Montrouge, France; SafeRiver, Montrouge, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3516"},"title":"An Initiative to Improve Reproducibility and Empirical Evaluation of Software Testing Techniques","abstract":"The current concern regarding quality of evaluation performed in existing studies reveals the need for methods and tools to assist in the definition and execution of empirical studies and experiments. However, when trying to apply general methods from empirical software engineering in specific fields, such as evaluation of software testing techniques, new obstacles and threats to validity appears, hindering researchers' use of empirical methods. This paper discusses those issues specific for evaluation of software testing techniques and proposes an initiative for a collaborative effort to encourage reproducibility of experiments evaluating software testing techniques (STT). We also propose the development of a tool that enables automatic execution and analysis of experiments producing a reproducible research compendia as output that is, in turn, shared among researchers. There are many expected benefits from this Endeavour, such as providing a foundation for evaluation of existing and upcoming STT, and allowing researchers to devise and publish better experiments.","conference":"IEEE","terms":"Software engineering;Software testing;Software;Collaboration;Guidelines;Conferences,program testing;software engineering,reproducibility;software testing technique;empirical software engineering;STT","keywords":"Empirical Software Engineering;Software Testing;Reproducibility in Software Engineering","startPage":"575","endPage":"578","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203016","citationCount":3,"referenceCount":13,"year":2015,"authors":"F. G. d. Oliveira Neto; R. Torkar; P. D. L. Machado","affiliations":"NA; Dept. of Comput. Sci. \u0026 Eng., Chalmers \u0026 the Univ. of Gothenburg, Gothenburg, Sweden; Software Practices Lab., Fed. Univ. of Campina Grande, Campina Grande, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3517"},"title":"Security Toolbox for Detecting Novel and Sophisticated Android Malware","abstract":"This paper presents a demo of our Security Toolbox to detect novel malware in Android apps. This Toolbox is developed through our recent research project funded by the DARPA Automated Program Analysis for Cybersecurity (APAC) project. The adversarial challenge (\"Red\") teams in the DARPA APAC program are tasked with designing sophisticated malware to test the bounds of malware detection technology being developed by the research and development (\"Blue\") teams. Our research group, a Blue team in the DARPA APAC program, proposed a \"human-in-the-loop program analysis\" approach to detect malware given the source or Java bytecode for an Android app. Our malware detection apparatus consists of two components: a general-purpose program analysis platform called Atlas, and a Security Toolbox built on the Atlas platform. This paper describes the major design goals, the Toolbox components to achieve the goals, and the workflow for auditing Android apps. The accompanying video illustrates features of the Toolbox through a live audit.","conference":"IEEE","terms":"Androids;Humanoid robots;Malware;Semantics;XML;Software,invasive software;Java;program diagnostics;research and development;smart phones;source code (software),security toolbox;Android apps;DARPA automated program analysis for cybersecurity;APAC program;research and development teams;blue teams;human-in-the-loop program analysis approach;source bytecode;Java bytecode;malware detection apparatus;general-purpose program analysis platform;Atlas platform;live audit","keywords":"Android;malware;program analysis;mobile security","startPage":"733","endPage":"736","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203055","citationCount":6,"referenceCount":18,"year":2015,"authors":"B. Holland; T. Deering; S. Kothari; J. Mathews; N. Ranade","affiliations":"Dept. of Electr. \u0026 Comput. Eng., Iowa State Univ., Ames, IA, USA; Dept. of Electr. \u0026 Comput. Eng., Iowa State Univ., Ames, IA, USA; Dept. of Electr. \u0026 Comput. Eng., Iowa State Univ., Ames, IA, USA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3518"},"title":"A Programming Model for Sustainable Software","abstract":"This paper presents a novel energy-aware and temperature-aware programming model with first-class support for sustainability. A program written in the new language, named Eco, may adaptively adjusts its own behaviors to stay on a given (energy or temperature) budget, avoiding both deficit that would lead to battery drain or CPU overheating, and surplus that could have been used to improve the quality of results. Sustainability management in Eco is captured as a form of supply and demand matching, and the language runtime consistently maintains the equilibrium between supply and demand. Among the efforts of energy-adaptive and temperature-adaptive systems, Eco is distinctive in its role in bridging the programmer and the underlying system, and in particular, bringing both programmer knowledge and application-specific traits into energy optimization. Through a number of intuitive programming abstractions, Eco reduces challenging issues in this domain --- such as workload characterization and decision making in adaptation --- to simple programming tasks, ultimately offering fine-grained, programmable, and declarative sustainability to energy-efficient computing. Eco is an minimal extension to Java, and has been implemented as an open-source compiler. We validate the usefulness of Eco by upgrading real-world Java applications with energy awareness and temperature awareness.","conference":"IEEE","terms":"Programming;Batteries;Runtime;Calibration;Thermal management;Software;Java,decision making;Java;program compilers;public domain software;sustainable development,sustainable software;energy-aware programming model;temperature-aware programming model;Eco;sustainability management;supply and demand matching;energy-adaptive system;temperature-adaptive system;energy optimization;intuitive programming abstractions;workload characterization;decision making;programming tasks;energy-efficient computing;open-source compiler;Java applications;energy awareness;temperature awareness","keywords":"sustainability;programming models;energy management;thermal management","startPage":"767","endPage":"777","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194624","citationCount":12,"referenceCount":41,"year":2015,"authors":"H. S. Zhu; C. Lin; Y. D. Liu","affiliations":"SUNY Binghamton, Binghamton, NY, USA; SUNY Binghamton, Binghamton, NY, USA; SUNY Binghamton, Binghamton, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3519"},"title":"Work Practices and Challenges in Pull-Based Development: The Integrator's Perspective","abstract":"In the pull-based development model, the integrator has the crucial role of managing and integrating contributions. This work focuses on the role of the integrator and investigates working habits and challenges alike. We set up an exploratory qualitative study involving a large-scale survey of 749 integrators, to which we add quantitative data from the integrator's project. Our results provide insights into the factors they consider in their decision making process to accept or reject a contribution. Our key findings are that integrators struggle to maintain the quality of their projects and have difficulties with prioritizing contributions that are to be merged. Our insights have implications for practitioners who wish to use or improve their pull-based development process, as well as for researchers striving to understand the theoretical implications of the pull-based model in software development.","conference":"IEEE","terms":"Software;Electronic mail;Collaboration;Face;Databases;Inspection;Birds,decision making;software quality,pull-based software development model;decision making process;software quality","keywords":"Distributed Software Development;Pull-based Development;Pull Request;GitHub","startPage":"358","endPage":"368","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194588","citationCount":73,"referenceCount":24,"year":2015,"authors":"G. Gousios; A. Zaidman; M. Storey; A. v. Deursen","affiliations":"Radboud Univ. Nijmegen, Nijmegen, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Univ. of Victoria, Victoria, BC, Canada; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d351a"},"title":"When and Why Your Code Starts to Smell Bad","abstract":"In past and recent years, the issues related to managing technical debt received significant attention by researchers from both industry and academia. There are several factors that contribute to technical debt. One of these is represented by code bad smells, i.e., Symptoms of poor design and implementation choices. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced. To fill this gap, we conducted a large empirical study over the change history of 200 open source projects from different software ecosystems and investigated when bad smells are introduced by developers, and the circumstances and reasons behind their introduction. Our study required the development of a strategy to identify smell-introducing commits, the mining of over 0.5M commits, and the manual analysis of 9,164 of them (i.e., Those identified as smell-introducing). Our findings mostly contradict common wisdom stating that smells are being introduced during evolutionary tasks. In the light of our results, we also call for the need to develop a new generation of recommendation systems aimed at properly planning smell refactoring activities.","conference":"IEEE","terms":"Measurement;Software;Ecosystems;History;Androids;Humanoid robots;Maintenance engineering,recommender systems;software maintenance;software quality,technical debt management;code quality;software ecosystems;smell-introducing commits identification;recommendation systems;smell refactoring activities","keywords":"bad code smells;mining software repositories;empirical study","startPage":"403","endPage":"414","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194592","citationCount":59,"referenceCount":51,"year":2015,"authors":"M. Tufano; F. Palomba; G. Bavota; R. Oliveto; M. Di Penta; A. De Lucia; D. Poshyvanyk","affiliations":"Coll. of William \u0026 Mary, Williamsburg, VA, USA; Univ. of Salerno, Fisciano, Italy; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Univ. of Molise, Pesche, Italy; Univ. of Sannio, Benevento, Italy; Univ. of Salerno, Fisciano, Italy; Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d351b"},"title":"A Field Study on Fostering Structural Navigation with Prodet","abstract":"Past studies show that developers who navigate code in a structural manner complete tasks faster and more correctly than those whose behavior is more opportunistic. The goal of this work is to move professional developers towards more effective program comprehension and maintenance habits by providing an approach that fosters structural code navigation. To this end, we created a Visual Studio plugin called Prodet that integrates an always-on navigable visualization of the most contextually relevant portions of the call graph. We evaluated the effectiveness of our approach by deploying it in a six week field study with professional software developers. The study results show a statistically significant increase in developers' use of structural navigation after installing Prodet. The results also show that developers continuously used the filtered and navigable call graph over the three week period in which it was deployed in production. These results indicate the maturity and value of our approach to increase developers' effectiveness in a practical and professional environment.","conference":"IEEE","terms":"Navigation;Context;Visualization;Software engineering;Maintenance engineering;Software;History,program visualisation;software maintenance;source code (software),structural code navigation;Prodet;program comprehension;program maintenance habits;Visual Studio plugin;always-on navigable visualization;call graph;professional software developers","keywords":"Software Maintenance;Structural Navigation;Code Recommendation;Field Study","startPage":"229","endPage":"238","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202967","citationCount":5,"referenceCount":36,"year":2015,"authors":"V. Augustine; P. Francis; X. Qu; D. Shepherd; W. Snipes; C. Braunlich; T. Fritz","affiliations":"ABB Corp. Res., Raleigh, NC, USA; ABB Corp. Res., Raleigh, NC, USA; ABB Corp. Res., Raleigh, NC, USA; ABB Corp. Res., Raleigh, NC, USA; ABB Corp. Res., Raleigh, NC, USA; Wotan Eng. GmbH, Otelfingen, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d351c"},"title":"Effectiveness of Persona with Personality Traits on Conceptual Design","abstract":"Conceptual design is an important skill in Software Engineering. Teaching conceptual design that can deliver a useful product is challenging, particularly when access to real users is limited. This study explores the effects of the use of Holistic Personas (i.e. a persona enriched with personality traits) on students' performance in creating conceptual designs. Our results indicate that the students were able to identify the personality traits of personas and their ratings of the personalities match closely with the intended personalities. A majority of the participants stated that their designs were tailored to meet the needs of the given personas' personality traits. Results suggest that the Holistic Personas can help students to take into account personality traits in the conceptual design process. Further studies are warranted to assess the value of incorporating Holistic Personas in conceptual design training for imparting skills of producing in-depth design by taking personalities into account.","conference":"IEEE","terms":"Software;Software engineering;Training;Context;Joints;Stability analysis,computer science education;educational courses;software engineering;teaching,personality traits;software engineering;conceptual design teaching;holistic personas;conceptual design process;conceptual design training;in-depth design","keywords":"User-Centered Design;persona;personality;conceptual design;software engineering education","startPage":"263","endPage":"272","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202971","citationCount":8,"referenceCount":38,"year":2015,"authors":"F. Anvari; D. Richards; M. Hitchens; M. A. Babar","affiliations":"Dept. of Comput., Macquarie Univ., Sydney, NSW, Australia; Dept. of Comput., Macquarie Univ., Sydney, NSW, Australia; Dept. of Comput., Macquarie Univ., Sydney, NSW, Australia; CREST - The Centre for Res. on Eng. Software Technol., Univ. of Adelaide, Adelaide, SA, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d351d"},"title":"Automated Planning for Self-Adaptive Systems","abstract":"Self-adaptation has been proposed as a viable solution to alleviate the management burden that is induced by the dynamic nature and increasing complexity of computer systems. In this context, architectural-based self-adaptation has emerged as one of the most promising approaches to automatically manage such systems, resorting to a control loop that includes monitoring, analyzing, planning, and executing adequate actions. This work addresses the challenges of adaptation planning -the decision-making process for selecting an appropriate course of action- with a focus on the problem of provisioning automated mechanisms for assembling adaptation plans, as a means to enhance adaptive capabilities under uncertainty. To this purpose, adaptations are modeled in a hierarchical manner, defining primitive actions, guarded actions, and deliberate plans, which may guide the system towards a desired state.","conference":"IEEE","terms":"Planning;Adaptation models;Measurement;Uncertainty;Software;Adaptive systems;Medical services,decision making;fault tolerant computing;planning,automated planning;self-adaptive systems;computer systems;architectural-based self-adaptation;system management;control loop;adaptation planning;decision-making process","keywords":"automated planning;PDDL;self-adaptive;self-management;planning","startPage":"839","endPage":"842","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203094","citationCount":0,"referenceCount":26,"year":2015,"authors":"R. Gil","affiliations":"Inst. Super. Tecnico, Univ. de Lisboa, Lisbon, Portugal","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d351e"},"title":"Fast Feedback Cycles in Empirical Software Engineering Research","abstract":"Background/Context: Gathering empirical knowledge is a time consuming task and the results from empirical studies often are soon outdated by new technological solutions. As a result, the impact of empirical results on software engineering practice is often not guaranteed.Objective/Aim: In this paper, we summarise the ongoing discussion on \"Empirical Software Engineering 2.0\" as a way to improve the impact of empirical results on industrial practices. We propose a way to combine data mining and analysis with domain knowledge to enable fast feedback cycles in empirical software engineering research.Method: We identify the key concepts on gathering fast feedback in empirical software engineering by following an experience-based line of reasoning by argument. Based on the identified key concepts, we design and execute a small proof of concept with a company to demonstrate potential benefits of the approach.Results: In our example, we observed that a simple double feedback mechanism notably increased the precision of the data analysis and improved the quality of the knowledge gathered.Conclusion: Our results serve as a basis to foster discussion and collaboration within the research community for a development of the idea.","conference":"IEEE","terms":"Software engineering;Software;Data mining;Stakeholders;Collaboration;Data analysis;Physics,data analysis;data mining;inference mechanisms;software engineering,feedback cycle;empirical software engineering 2.0;EMSE 2.0;data mining;data analysis;domain knowledge;double feedback mechanism","keywords":"Empirical methods;Research methods;Data mining;Knowledge transfer","startPage":"583","endPage":"586","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203018","citationCount":4,"referenceCount":22,"year":2015,"authors":"A. Vetrò; S. Ognawala; D. M. Fernández; S. Wagner","affiliations":"Tech. Univ. Munchen, Munich, Germany; Tech. Univ. Munchen, Munich, Germany; Tech. Univ. Munchen, Munich, Germany; Univ. of Stuttgart, Stuttgart, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d351f"},"title":"Information Transformation: An Underpinning Theory for Software Engineering","abstract":"Software engineering lacks underpinning scientific theories both for the software it produces and the processes by which it does so. We propose that an approach based on information theory can provide such a theory, or rather many theories. We envision that such a benefit will be realised primarily through research based on the quantification of information involved and a mathematical study of the limiting laws that arise. However, we also argue that less formal but more qualitative uses for information theory will be useful. The main argument in support of our vision is based on the fact that both a program and an engineering process to develop such a program are fundamentally processes that transform information. To illustrate our argument we focus on software testing and develop an initial theory in which a test suite is input/output adequate if it achieves the channel capacity of the program as measured by the mutual information between its inputs and its outputs. We outline a number of problems, metrics and concrete strategies for improving software engineering, based on information theoretical analyses. We find it likely that similar analyses and subsequent future research to detail them would be generally fruitful for software engineering.","conference":"IEEE","terms":"Software engineering;Entropy;Software;Channel capacity;Software testing;Measurement,information theory;program testing;software engineering,information transformation theory;software engineering;information quantification;software testing","keywords":"information theory;software engineering;software testing","startPage":"599","endPage":"602","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203022","citationCount":4,"referenceCount":20,"year":2015,"authors":"D. Clark; R. Feldt; S. Poulding; S. Yoo","affiliations":"Dept. of Comput. Sci., Univ. Coll. London, London, UK; Dept. of Software Eng., Blekinge Inst. of Technol., Blekinge, Sweden; Dept. of Software Eng., Blekinge Inst. of Technol., Blekinge, Sweden; Dept. of Comput. Sci., Univ. Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3520"},"title":"Revisiting the Impact of Classification Techniques on the Performance of Defect Prediction Models","abstract":"Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., Logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., Contains erroneous entries and (b) biased, i.e., Only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., The impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., The choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others.","conference":"IEEE","terms":"Predictive models;NASA;Software;Decision trees;Training;Logistics;Complexity theory,pattern classification;quality assurance;software quality,classification techniques;defect prediction models;software quality assurance teams;defect-prone software modules;NASA dataset;PROMISE dataset;open source software","keywords":"Defect Prediction;Classification Techniques","startPage":"789","endPage":"800","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194626","citationCount":96,"referenceCount":65,"year":2015,"authors":"B. Ghotra; S. McIntosh; A. E. Hassan","affiliations":"Software Anal. \u0026 Intell. Lab. (SAIL), Queen's Univ., Kingston, ON, Canada; Software Anal. \u0026 Intell. Lab. (SAIL), Queen's Univ., Kingston, ON, Canada; Software Anal. \u0026 Intell. Lab. (SAIL), Queen's Univ., Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3521"},"title":"Dynamic Generation of Likely Invariants for Multithreaded Programs","abstract":"We propose a new method for dynamically generating likely invariants from multithreaded programs.While existing invariant generation tools work well on sequential programs, they are ineffective at reasoning about multithreaded programs both in terms of the number of real invariants generated and in terms of their usefulness in helping programmers. We address this issue by developing a new dynamic invariant generator consisting of an LLVM based code instrumentation front end, a systematic thread interleaving explorer, and a customized invariant inference engine. We show that efficient interleaving exploration strategies can be used to generate a diversified set of executions with little runtime overhead. Furthermore, we show that focusing on a small subset of thread-local transition invariants is often sufficient for reasoning about the concurrency behavior of programs. We have evaluated our new method on a set of open-source multithreaded C/C++ benchmarks. Our experiments show that our method can generate invariants that are significantly higher in quality than the previous state-of-the-art.","conference":"IEEE","terms":"Concurrent computing;Instruments;Instruction sets;Engines;Systematics;Schedules;Programming,C++ language;multi-threading;software tools,dynamic likely invariant generation;multithreaded programs;invariant generation tools;sequential programs;dynamic invariant generator;LLVM;code instrumentation front end;systematic thread interleaving explorer;customized invariant inference engine;interleaving exploration strategies;thread-local transition invariants;concurrency behavior;open-source multithreaded C/C++ benchmarks","keywords":"invariant generation;likely invariant;concurrent program;transition invariant;partial order reduction","startPage":"835","endPage":"846","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194630","citationCount":4,"referenceCount":58,"year":2015,"authors":"M. Kusano; A. Chattopadhyay; C. Wang","affiliations":"Dept. of ECE, Virginia Tech, Blacksburg, VA, USA; Dept. of ECE, Virginia Tech, Blacksburg, VA, USA; Dept. of ECE, Virginia Tech, Blacksburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3522"},"title":"Mining Apps for Abnormal Usage of Sensitive Data","abstract":"What is it that makes an app malicious? One important factor is that malicious apps treat sensitive data differently from benign apps. To capture such differences, we mined 2,866 benign Android applications for their data flow from sensitive sources, and compare these flows against those found in malicious apps. We find that (a) for every sensitive source, the data ends up in a small number of typical sinks; (b) these sinks differ considerably between benign and malicious apps; (c) these differences can be used to flag malicious apps due to their abnormal data flow; and (d) malicious apps can be identified by their abnormal data flow alone, without requiring known malware samples. In our evaluation, our MUDFLOW prototype correctly identified 86.4% of all novel malware, and 90.1% of novel malware leaking sensitive data.","conference":"IEEE","terms":"Malware;Androids;Humanoid robots;Smart phones;Google;Data mining;Twitter,data mining;invasive software;mobile computing;smart phones,data mining;Android application;abnormal data flow;malware","keywords":"","startPage":"426","endPage":"436","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194594","citationCount":61,"referenceCount":30,"year":2015,"authors":"V. Avdiienko; K. Kuznetsov; A. Gorla; A. Zeller; S. Arzt; S. Rasthofer; E. Bodden","affiliations":"Saarland Univ., Saarbrucken, Germany; Saarland Univ., Saarbrucken, Germany; IMDEA Software Inst., Madrid, Spain; Saarland Univ., Saarbrucken, Germany; Tech. Univ. Darmstadt, Darmstadt, Germany; Tech. Univ. Darmstadt, Darmstadt, Germany; Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3523"},"title":"Evolution of Software Development Strategies","abstract":"The development of discipline-specific cognitive and meta-cognitive skills is fundamental to the successful mastery of software development skills and processes. This development happens over time and is influenced by many factors, however its understanding by teachers is crucial in order to develop activities and materials to transform students from novice to expert software engineers. In this paper, we analyse the evolution of learning strategies of novice, first year students, to expert, final year students. We analyse reflections on software development processes from students in an introductory software development course, and compare them to those of final year students, in a distributed systems development course. Our study shows that computer science - specific strategies evolve as expected, with the majority of final year students including design before coding in their software development process, but that several areas still require scaffolding activities to assist in learning development.","conference":"IEEE","terms":"Software;Encoding;Software engineering;Planning;Context;Programming profession,computer science education;educational courses;software engineering;teaching,software development strategies;discipline-specific meta-cognitive skills development;software development skills;learning strategies;software development course;distributed system development course;computer science specific strategies;software coding;scaffolding activities","keywords":"software development strategies;self regulated learning behavior","startPage":"243","endPage":"252","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202969","citationCount":1,"referenceCount":40,"year":2015,"authors":"K. Falkner; C. Szabo; R. Vivian; N. Falkner","affiliations":"Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3524"},"title":"Novice Code Understanding Strategies during a Software Maintenance Assignment","abstract":"Existing efforts on teaching software maintenance have focussed on constructing adequate codebases that students with limited knowledge could maintain, with little focus on the learning outcomes of such exercises and of the approaches that students employ while performing maintenance. An analysis of the code understanding strategies employed by novice students as they perform software maintenance exercises is fundamental for the effective teaching of software maintenance. In this paper, we analyze the strategies employed by second year students in a maintenance exercise over a large codebase. We analyze student reflections on their code understanding, maintenance process and the use of tools. We show that students are generally capable of working with large codebases. Our study also finds that the majority of students follow a systematic approach to code understanding, but that their approach can be significantly improved through the use of tools and a better understanding of reverse engineering approaches.","conference":"IEEE","terms":"Software maintenance;Maintenance engineering;Software engineering;Education;Testing;Systematics,computer science education;reverse engineering;software maintenance,novice code understanding strategies;software maintenance assignment;adequate codebases;learning outcomes;novice students;software maintenance exercises;second year students;reverse engineering approaches","keywords":"software maintenance;software engineering;cognitive models","startPage":"276","endPage":"284","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202973","citationCount":1,"referenceCount":20,"year":2015,"authors":"C. Szabo","affiliations":"Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3525"},"title":"Scalable Formal Verification of UML Models","abstract":"UML (Unified Modeling Language) has been used for years in diverse domains. Its notations usually come with a reasonably well-defined syntax, but its semantics is left under-specified and open to different interpretations. This freedom hampers the formal verification of produced specifications and calls for more rigor and precision. This work aims to bridge this gap and proposes a flexible and modular formalization approach based on temporal logic. We studied the different interpretations for some of its constructs, and our framework allows one to assemble the semantics of interest by composing the selected formalizations for the different pieces. However, the formalization per-se is not enough. The verification process, in general, becomes slow and impossible -as the model grows in size. To tackle the scalability problem, this work also proposes a bit-vector-based encoding of LTL formulae. The first results witness a significant increase in the size of analyzable models, not only for our formalization of UML models, but also for numerous other models that can be reduced to bounded satisfiability checking of LTL formulae.","conference":"IEEE","terms":"Unified modeling language;Encoding;Semantics;Model checking;Scalability;Analytical models;Conferences,formal verification;temporal logic;Unified Modeling Language,scalable formal verification;UML model;unified modeling language;modular formalization approach;temporal logic;bit-vector-based encoding;LTL formulae;satisfiability checking","keywords":"UML;Formal Methods;Temporal Logic;Bounded Model Checking;Bit-Vector Logic","startPage":"847","endPage":"850","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203096","citationCount":1,"referenceCount":14,"year":2015,"authors":"M. M. Pourhashem Kallehbasti","affiliations":"Dipt. di Elettron., Inf. e Bioingegneria, Politec. di Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3526"},"title":"Views on Internal and External Validity in Empirical Software Engineering","abstract":"Empirical methods have grown common in software engineering, but there is no consensus on how to apply them properly. Is practical relevance key? Do internally valid studies have any value? Should we replicate more to address the tradeoff between internal and external validity? We asked the community how empirical research should take place in software engineering, with a focus on the tradeoff between internal and external validity and replication, complemented with a literature review about the status of empirical research in software engineering. We found that the opinions differ considerably, and that there is no consensus in the community when to focus on internal or external validity and how to conduct and review replications.","conference":"IEEE","terms":"Software engineering;Guidelines;Bibliographies;Computer languages;Context;Standards;History,software engineering,external validity;internal validity;software engineering;empirical research","keywords":"","startPage":"9","endPage":"19","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194557","citationCount":46,"referenceCount":37,"year":2015,"authors":"J. Siegmund; N. Siegmund; S. Apel","affiliations":"Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3527"},"title":"Making System User Interactive Tests Repeatable: When and What Should We Control?","abstract":"System testing and invariant detection is usually conducted from the user interface perspective when the goal is to evaluate the behavior of an application as a whole. A large number of tools and techniques have been developed to generate and automate this process, many of which have been evaluated in the literature or internally within companies. Typical metrics for determining effectiveness of these techniques include code coverage and fault detection, however, with the assumption that there is determinism in the resulting outputs. In this paper we examine the extent to which a common set of factors such as the system platform, Java version, application starting state and tool harness configurations impact these metrics. We examine three layers of testing outputs: the code layer, the behavioral (or invariant) layer and the external (or user interaction) layer. In a study using five open source applications across three operating system platforms, manipulating several factors, we observe as many as 184 lines of code coverage difference between runs using the same test cases, and up to 96 percent false positives with respect to fault detection. We also see some a small variation among the invariants inferred. Despite our best efforts, we can reduce, but not completely eliminate all possible variation in the output. We use our findings to provide a set of best practices that should lead to better consistency and smaller differences in test outcomes, allowing more repeatable and reliable testing and experimentation.","conference":"IEEE","terms":"Testing;Entropy;Graphical user interfaces;Java;Delays;Operating systems,graphical user interfaces;operating systems (computers);program testing;public domain software;software fault tolerance,user interactive application testing;code coverage;fault detection;open source application;operating system platform;graphical user interface;GUI","keywords":"software testing; experimentation; benchmarking; graphical user interfaces","startPage":"55","endPage":"65","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194561","citationCount":9,"referenceCount":40,"year":2015,"authors":"Z. Gao; Y. Liang; M. B. Cohen; A. M. Memon; Z. Wang","affiliations":"Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Dept. of Comput. Sci., Univ. of Maryland, College Park, MD, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska-Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3528"},"title":"The Impact of Mislabelling on the Performance and Interpretation of Defect Prediction Models","abstract":"The reliability of a prediction model depends on the quality of the data from which it was trained. Therefore, defect prediction models may be unreliable if they are trained using noisy data. Recent research suggests that randomly-injected noise that changes the classification (label) of software modules from defective to clean (and vice versa) can impact the performance of defect models. Yet, in reality, incorrectly labelled (i.e., mislabelled) issue reports are likely non-random. In this paper, we study whether mislabelling is random, and the impact that realistic mislabelling has on the performance and interpretation of defect models. Through a case study of 3,931 manually-curated issue reports from the Apache Jackrabbit and Lucene systems, we find that: (1) issue report mislabelling is not random; (2) precision is rarely impacted by mislabelled issue reports, suggesting that practitioners can rely on the accuracy of modules labelled as defective by models that are trained using noisy data; (3) however, models trained on noisy data typically achieve 56%-68% of the recall of models trained on clean data; and (4) only the metrics in top influence rank of our defect models are robust to the noise introduced by mislabelling, suggesting that the less influential metrics of models that are trained on noisy data should not be interpreted or used to make decisions.","conference":"IEEE","terms":"Predictive models;Data models;Noise measurement;Noise;Data mining;Software,software performance evaluation;software reliability,mislabelling impact;defect prediction model performance;defect prediction model interpretation;prediction model reliability;defect prediction models;randomly-injected noise;software modules;Apache Jackrabbit system;Lucene system","keywords":"Software Quality Assurance;Software Defect Prediction;Data Quality;Mislabelling","startPage":"812","endPage":"823","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194628","citationCount":33,"referenceCount":51,"year":2015,"authors":"C. Tantithamthavorn; S. McIntosh; A. E. Hassan; A. Ihara; K. Matsumoto","affiliations":"Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3529"},"title":"Graph-Based Statistical Language Model for Code","abstract":"n-gram statistical language model has been successfully applied to capture programming patterns to support code completion and suggestion. However, the approaches using n-gram face challenges in capturing the patterns at higher levels of abstraction due to the mismatch between the sequence nature in n-grams and the structure nature of syntax and semantics in source code. This paper presents GraLan, a graph-based statistical language model and its application in code suggestion. GraLan can learn from a source code corpus and compute the appearance probabilities of any graphs given the observed (sub)graphs. We use GraLan to develop an API suggestion engine and an AST-based language model, ASTLan. ASTLan supports the suggestion of the next valid syntactic template and the detection of common syntactic templates. Our empirical evaluation on a large corpus of open-source projects has shown that our engine is more accurate in API code suggestion than the state-of-the-art approaches, and in 75% of the cases, it can correctly suggest the API with only five candidates. ASTLan has also high accuracy in suggesting the next syntactic template and is able to detect many useful and common syntactic templates.","conference":"IEEE","terms":"Context;Syntactics;Probability;Engines;Programming;Accuracy;Computational modeling,graph theory;object-oriented programming;probability;public domain software;source code (software),graph-based statistical language model;n-gram statistical language model;programming patterns;code completion;code suggestion;GraLan;source code corpus;appearance probability;API suggestion engine;AST-based language model;ASTLan;valid syntactic template;common syntactic template detection;open-source projects","keywords":"Graph-based Language Model;API Suggestion;Syntactic Template Suggestion","startPage":"858","endPage":"868","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194632","citationCount":36,"referenceCount":43,"year":2015,"authors":"A. T. Nguyen; T. N. Nguyen","affiliations":"Iowa State Univ., Ames, IA, USA; Iowa State Univ., Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d352a"},"title":"DirectFix: Looking for Simple Program Repairs","abstract":"Recent advances in program repair techniques have raised the possibility of patching bugs automatically. For an automatically generated patch to be accepted by developers, it should not only resolve the bug but also satisfy certain human-related factors including readability and comprehensibility. In this paper, we focus on the simplicity of patches (the size of changes). We present a novel semantics-based repair method that generates the simplest patch such that the program structure of the buggy program is maximally preserved. To take into account the simplicity of repairs in an efficient way (i.e., Without explicitly enumerating each repair candidate for each fault location), our method fuses fault localization and repair generation into one step. We do so by leveraging partial Max SAT constraint solving and component-based program synthesis. We compare our prototype implementation, Direct Fix, with the state-of-the-art semantics-based repair tool Sem Fix, that performs fault localization before repair generation. In our experiments with SIR programs and GNU Coreutils, Direct Fix generates repairs that are simpler than those generated by Sem Fix. Since both Direct Fix and Sem Fix are test-driven repair tools, they can introduce regressions for other tests which do not drive the repair. We found that Direct Fix causes substantially less regression errors than Sem Fix.","conference":"IEEE","terms":"Maintenance engineering;Semantics;Encoding;Computer bugs;Fault location;Fuses,human factors;object-oriented programming;program debugging;regression analysis;software fault tolerance;software maintenance,DirectFix;program repair technique;bug patching;human-related factor;readability;comprehensibility;patch simplicity;semantics-based repair method;fault location;repair generation;Max SAT constraint solving;component-based program synthesis;regression error","keywords":"Automated Repair;SMT solving;Program Synthesis","startPage":"448","endPage":"458","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194596","citationCount":79,"referenceCount":40,"year":2015,"authors":"S. Mechtaev; J. Yi; A. Roychoudhury","affiliations":"Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d352b"},"title":"Automated Data Structure Generation: Refuting Common Wisdom","abstract":"Common wisdom in the automated data structure generation community states that declarative techniques have better usability than imperative techniques, while imperative techniques have better performance. We show that this reasoning is fundamentally flawed: if we go to the declarative limit and employ constraint logic programming (CLP), the CLP data structure generation has orders of magnitude better performance than comparable imperative techniques. Conversely, we observe and argue that when it comes to realistically complex data structures and properties, the CLP specifications become more obscure, indirect, and difficult to implement and understand than their imperative counterparts. We empirically evaluate three competing generation techniques, CLP, Korat, and UDITA, to validate these observations on more complex and interesting data structures than any prior work in this area. We explain why these observations are true, and discuss possible techniques for attaining the best of both worlds.","conference":"IEEE","terms":"Data structures;Java;Usability;Engines;Search problems;Semantics;Generators,constraint handling;data structures,automated data structure generation;common wisdom;constraint logic programming;CLP;Korat;UDITA","keywords":"Constraint Logic Programming;Data Structure Generation","startPage":"32","endPage":"43","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194559","citationCount":3,"referenceCount":38,"year":2015,"authors":"K. Dewey; L. Nichols; B. Hardekopf","affiliations":"Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Univ. of California, Santa Barbara, Santa Barbara, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d352c"},"title":"Composite Constant Propagation: Application to Android Inter-Component Communication Analysis","abstract":"Many program analyses require statically inferring the possible values of composite types. However, current approaches either do not account for correlations between object fields or do so in an ad hoc manner. In this paper, we introduce the problem of composite constant propagation. We develop the first generic solver that infers all possible values of complex objects in an interprocedural, flow and context-sensitive manner, taking field correlations into account. Composite constant propagation problems are specified using COAL, a declarative language. We apply our COAL solver to the problem of inferring Android Inter-Component Communication (ICC) values, which is required to understand how the components of Android applications interact. Using COAL, we model ICC objects in Android more thoroughly than the state-of-the-art. We compute ICC values for 460 applications from the Play store. The ICC values we infer are substantially more precise than previous work. The analysis is efficient, taking slightly over two minutes per application on average. While this work can be used as the basis for many whole-program analyses of Android applications, the COAL solver can also be used to infer the values of composite objects in many other contexts.","conference":"IEEE","terms":"Coal;Androids;Humanoid robots;Data models;Analytical models;Computational modeling;Correlation,mobile computing;program diagnostics;smart phones,composite constant propagation;Android application;intercomponent communication analysis;ICC;program analysis;COAL solver;declarative language","keywords":"inter-component communication;composite constant;constant propagation;Android;Android IPC;Android ICC","startPage":"77","endPage":"88","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194563","citationCount":42,"referenceCount":0,"year":2015,"authors":"D. Octeau; D. Luchaup; M. Dering; S. Jha; P. McDaniel","affiliations":"Dept. of Comput. Sci., Univ. of Wisconsin, Madison, WI, USA; Dept. of Comput. Sci., Univ. of Wisconsin, Madison, WI, USA; Dept. of Comput. Sci. \u0026 Eng., Pennsylvania State Univ., University Park, PA, USA; Dept. of Comput. Sci., Univ. of Wisconsin, Madison, WI, USA; Dept. of Comput. Sci. \u0026 Eng., Pennsylvania State Univ., University Park, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d352d"},"title":"How Can I Use This Method?","abstract":"Code examples are small source code fragments whose purpose is to illustrate how a programming language construct, an API, or a specific function/method works. Since code examples are not always available in the software documentation, researchers have proposed techniques to automatically extract them from existing software or to mine them from developer discussions. In this paper we propose MUSE (Method USage Examples), an approach for mining and ranking actual code examples that show how to use a specific method. MUSE combines static slicing (to simplify examples) with clone detection (to group similar examples), and uses heuristics to select and rank the best examples in terms of reusability, understandability, and popularity. MUSE has been empirically evaluated using examples mined from six libraries, by performing three studies involving a total of 140 developers to: (i) evaluate the selection and ranking heuristics, (ii) provide their perception on the usefulness of the selected examples, and (iii) perform specific programming tasks using the MUSE examples. The results indicate that MUSE selects and ranks examples close to how humans do, most of the code examples (82%) are perceived as useful, and they actually help when performing programming tasks.","conference":"IEEE","terms":"Cloning;Libraries;Software;Documentation;Measurement;Data mining;Context,program slicing;programming languages;software engineering;source code (software),clone detection;static slicing;MUSE;method usage examples;software development;software documentation;programming language;source code fragment","keywords":"Code examples;Empirical studies;Software documentation;Static software analysis","startPage":"880","endPage":"890","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194634","citationCount":22,"referenceCount":21,"year":2015,"authors":"L. Moreno; G. Bavota; M. Di Penta; R. Oliveto; A. Marcus","affiliations":"Univ. of Texas at Dallas, Dallas, TX, USA; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Univ. of Sannio, Benevento, Italy; Univ. of Molise, Campobasso, Italy; Univ. of Texas at Dallas, Dallas, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d352e"},"title":"relifix: Automated Repair of Software Regressions","abstract":"Regression occurs when code changes introduce failures in previously passing test cases. As software evolves, regressions may be introduced. Fixing regression errors manually is time-consuming and error-prone. We propose an approach of automated repair of software regressions, called relifix, that considers the regression repair problem as a problem of reconciling problematic changes. Specifically, we derive a set of code transformations obtained from our manual inspection of 73 real software regressions; this set of code transformations uses syntactical information from changed statements. Regression repair is then accomplished via a search over the code transformation operators - which operator to apply, and where. Our evaluation compares the repairability of relifix with GenProg on 35 real regression errors. relifix repairs 23 bugs, while GenProg only fixes five bugs. We also measure the likelihood of both approaches in introducing new regressions given a reduced test suite. Our experimental results shows that our approach is less likely to introduce new regressions than GenProg.","conference":"IEEE","terms":"Maintenance engineering;Software;Computer bugs;Benchmark testing;Inspection;Manuals,regression analysis;software maintenance,relifix;automated repair;software regressions;regression repair problem;code transformations;manual inspection;code transformation operators;GenProg","keywords":"Automated Repair;Genetic Programming;Real-life Software Regressions","startPage":"471","endPage":"482","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194598","citationCount":28,"referenceCount":55,"year":2015,"authors":"S. H. Tan; A. Roychoudhury","affiliations":"Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d352f"},"title":"Improving Student Group Work with Collaboration Patterns: A Case Study","abstract":"Group work skills are essential for Computer Scientists and especially Software Engineers. Group work is included in most CS curricula in order to support students in acquiring these skills. During group work, problems can occur related to a variety of factors, such as unstable group constellations or (missing) instructor support. Students need to find strategies for solving or preventing such problems. Student collaboration patterns offer a way of supporting students by providing problem-solving strategies that other students have already applied successfully. In this work we describe how student collaboration patterns were applied in an interdisciplinary software engineering project, and show that their application was generally experienced as helpful by the students.","conference":"IEEE","terms":"Teamwork;Software engineering;Software;Conferences;Team working;Computer science,computer science education;software engineering,student group work skills;student collaboration patterns;CS curricula;problem-solving strategy;interdisciplinary software engineering project","keywords":"Group Work;Student Projects;Collaboration Patterns","startPage":"303","endPage":"306","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202977","citationCount":0,"referenceCount":13,"year":2015,"authors":"C. Köppe; M. v. Eekelen; S. Hoppenbrouwers","affiliations":"HAN Univ. of Appl. Sci., Arnhem, Netherlands; NA; HAN Univ. of Appl. Sci., Arnhem, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3530"},"title":"Collaborative and Cooperative-Learning in Software Engineering Courses","abstract":"Collaborative learning is a key component of software engineering (SE) courses in most undergraduate computing curricula. Thus these courses include fairly intensive team projects, the intent being to ensure that not only do students develop an understanding of key software engineering concepts and practices, but also develop the skills needed to work effectively in large design and development teams. But there is a definite risk in collaborative learning in that there is a potential that individual learning gets lost in the focus on the team's success in completing the project (s). While the team's success is indeed the primary goal of an industrial SE team, ensuring individual learning is obviously an essential goal of SE courses. We have developed a novel approach that exploits the affordances of mobile and web technologies to help ensure that individual students in teams in SE courses develop a thorough understanding of the relevant concepts and practices while working on team projects, indeed, that the team contributes in an essential manner to the learning of each member of the team. We describe the learning theory underlying our approach, provide some details concerning the prototype implementation of a tool based on the approach, and describe how we are using it in an SE course in our program.","conference":"IEEE","terms":"Information services;Electronic publishing;Internet;Collaborative work;Software engineering;Education;Collaboration,computer science education;educational courses;Internet;mobile computing;software engineering,software engineering courses;cooperative-learning;collaborative learning;SE courses;undergraduate computing curricula;industrial SE team;Web technology;mobile technology;learning theory","keywords":"Collaborative Learning;Cooperative Learning;Use of technology in classroom","startPage":"319","endPage":"322","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202981","citationCount":2,"referenceCount":14,"year":2015,"authors":"N. Soundarajan; S. Joshi; R. Ramnath","affiliations":"Comput. Sci. \u0026 Eng., Ohio State Univ., Columbus, OH, USA; Comput. Sci. \u0026 Eng., Ohio State Univ., Columbus, OH, USA; Comput. Sci. \u0026 Eng., Ohio State Univ., Columbus, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3531"},"title":"Leveraging Informal Documentation to Summarize Classes and Methods in Context","abstract":"Critical information related to a software developer'scurrent task is trapped in technical developer discussions,bug reports, code reviews, and other software artefacts. Muchof this information pertains to the proper use of code elements(e.g., methods and classes) that capture vital problem domainknowledge. To understand the purpose of these code elements, software developers must either access documentation and online posts and understand the source code or peruse a substantial amount of text. In this paper, we use the context that surrounds code elements in StackOverflow posts to summarize the use and purpose of code elements. To provide focus to our investigation, we consider the generation of summaries for library identifiers discussed in StackOverflow. Our automatic summarization approach was evaluated on a sample of 100 randomly-selected library identifiers with respect to a benchmark of summaries provided by two annotators. The results show that the approach attains an R-precision of 54%, which is appropriate given the diverse ways in which code elements can be used.","conference":"IEEE","terms":"Context;Documentation;Libraries;Java;Androids;Humanoid robots;Software,software engineering;source code (software),informal documentation;software artefacts;code elements;source code;StackOverflow;library identifiers;automatic summarization approach","keywords":"","startPage":"639","endPage":"642","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203032","citationCount":11,"referenceCount":22,"year":2015,"authors":"L. Guerrouj; D. Bourque; P. C. Rigby","affiliations":"Dept. of Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. of Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. of Software Eng., Concordia Univ., Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3532"},"title":"A Flexible and Non-intrusive Approach for Computing Complex Structural Coverage Metrics","abstract":"Software analysis tools and techniques often leverage structural code coverage information to reason about the dynamic behavior of software. Existing techniques instrument the code with the required structural obligations and then monitor the execution of the compiled code to report coverage. Instrumentation based approaches often incur considerable runtime overhead for complex structural coverage metrics such as Modified Condition/Decision (MC/DC). Code instrumentation, in general, has to be approached with great care to ensure it does not modify the behavior of the original code. Furthermore, instrumented code cannot be used in conjunction with other analyses that reason about the structure and semantics of the code under test. In this work, we introduce a non-intrusive preprocessing approach for computing structural coverage information. It uses a static partial evaluation of the decisions in the source code and a source-to-bytecode mapping to generate the information necessary to efficiently track structural coverage metrics during execution. Our technique is flexible; the results of the preprocessing can be used by a variety of coverage-driven software analysis tasks, including automated analyses that are not possible for instrumented code. Experimental results in the context of symbolic execution show the efficiency and flexibility of our non- intrusive approach for computing code coverage information.","conference":"IEEE","terms":"Instruments;Measurement;Software;Java;Monitoring;Runtime;Electronic mail,partial evaluation (compilers);program control structures;program diagnostics;reasoning about programs;software metrics,flexible nonintrusive approach;software analysis tools;software analysis technique;structural code coverage information;software dynamic behavior reasoning;structural obligation;compiled code execution monitoring;coverage reporting;instrumentation based approach;runtime overhead;modified condition-decision;code instrumentation;code behavior;code semantics;nonintrusive preprocessing approach;structural coverage information;static partial evaluation;source code;source-to-bytecode mapping;structural coverage metrics tracking;coverage-driven software analysis;symbolic execution","keywords":"","startPage":"506","endPage":"516","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194601","citationCount":3,"referenceCount":40,"year":2015,"authors":"M. W. Whalen; S. Person; N. Rungta; M. Staats; D. Grijincu","affiliations":"Univ. of Minnesota, Minneapolis, MN, USA; NASA Langley Res. Center, Hampton, VA, USA; NASA Ames Res. Center, Moffett Field, CA, USA; Google Inc., Zurich, Switzerland; Univ. of St. Andrews, St. Andrews, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3533"},"title":"Truth in Advertising: The Hidden Cost of Mobile Ads for Software Developers","abstract":"The \"free app\" distribution model has been extremely popular with end users and developers. Developers use mobile ads to generate revenue and cover the cost of developing these free apps. Although the apps are ostensibly free, they in fact do come with hidden costs. Our study of 21 real world Android apps shows that the use of ads leads to mobile apps that consume significantly more network data, have increased energy consumption, and require repeated changes to ad related code. We also found that complaints about these hidden costs are significant and can impact the ratings given to an app. Our results provide actionable information and guidance to software developers in weighing the tradeoffs of incorporating ads into their mobile apps.","conference":"IEEE","terms":"Mobile communication;Energy consumption;Advertising;Mobile handsets;Performance evaluation;Instruments,advertising data processing;mobile computing;smart phones;software engineering,mobile advertising;software developer;free app distribution model;Android app","keywords":"Mobile advertisements;mobile devices","startPage":"100","endPage":"110","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194565","citationCount":42,"referenceCount":46,"year":2015,"authors":"J. Gui; S. Mcilroy; M. Nagappan; W. G. J. Halfond","affiliations":"Univ. of Southern California, Los Angeles, CA, USA; Queen's Univ., Kingston, ON, Canada; Rochester Inst. of Technol., Rochester, NY, USA; Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3534"},"title":"Enron's Spreadsheets and Related Emails: A Dataset and Analysis","abstract":"Spreadsheets are used extensively in business processes around the world and as such, are a topic of research interest. Over the past few years, many spreadsheet studies have been performed on the EUSES spreadsheet corpus. While this corpus has served the spreadsheet community well, the spreadsheets it contains are mainly gathered with search engines and might therefore not represent spreadsheets used in companies. This paper presents an analysis of a new dataset, extracted from the Enron email archive, containing over 15,000 spreadsheets used within the Enron Corporation. In addition to the spreadsheets, we also present an analysis of the associated emails, where we look into spreadsheet-specific email behavior. Our analysis shows that 1) 24% of Enron spreadsheets with at least one formula contain an Excel error, 2) there is little diversity in the functions used in spreadsheets: 76% of spreadsheets in the presented corpus use the same 15 functions and, 3) the spreadsheets are substantially more smelly than the EUSES corpus, especially in terms of long calculation chains. Regarding the emails, we observe that spreadsheets 1) are a frequent topic of email conversation with 10% of emails either referring to or sending spreadsheets and 2) the emails are frequently discussing errors in and updates to spreadsheets.","conference":"IEEE","terms":"Electronic mail;Software engineering;Software;Measurement;Companies;Economics;Industries,data analysis;electronic mail;financial data processing;spreadsheet programs,email conversation;calculation chain;Excel error;spreadsheet-specific email behavior;email analysis;Enron Corporation;Enron email archive;EUSES spreadsheet corpus;business process;emails;Enron spreadsheet","keywords":"","startPage":"7","endPage":"16","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202944","citationCount":29,"referenceCount":39,"year":2015,"authors":"F. Hermans; E. Murphy-Hill","affiliations":"Delft Univ. of Technol., Delft, Netherlands; North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3535"},"title":"CARAMEL: Detecting and Fixing Performance Problems That Have Non-Intrusive Fixes","abstract":"Performance bugs are programming errors that slow down program execution. While existing techniques can detect various types of performance bugs, a crucial and practical aspect of performance bugs has not received the attention it deserves: how likely are developers to fix a performance bug? In practice, fixing a performance bug can have both benefits and drawbacks, and developers fix a performance bug only when the benefits outweigh the drawbacks. Unfortunately, for many performance bugs, the benefits and drawbacks are difficult to assess accurately. This paper presents CARAMEL, a novel static technique that detects and fixes performance bugs that have non-intrusive fixes likely to be adopted by developers. Each performance bug detected by CARAMEL is associated with a loop and a condition. When the condition becomes true during the loop execution, all the remaining computation performed by the loop is wasted. Developers typically fix such performance bugs because these bugs waste computation in loops and have non-intrusive fixes: when some condition becomes true dynamically, just break out of the loop. Given a program, CARAMEL detects such bugs statically and gives developers a potential source-level fix for each bug. We evaluate CARAMEL on real-world applications, including 11 popular Java applications (e.g., Groovy, Log4J, Lucene, Struts, Tomcat, etc) and 4 widely used C/C++ applications (Chromium, GCC, Mozilla, and My SQL). CARAMEL finds 61 new performance bugs in the Java applications and 89 new performance bugs in the C/C++ applications. Based on our bug reports, developers so far have fixed 51 and 65 performance bugs in the Java and C/C++ applications, respectively. Most of the remaining bugs are still under consideration by developers.","conference":"IEEE","terms":"Computer bugs;Java;Algorithm design and analysis;Complexity theory;Cognition;Software engineering;Programming,C++ language;Java;program control structures;program debugging,CARAMEL;fixing performance problems;detecting performance problems;nonintrusive fixes;performance bugs;programming errors;static technique;loop execution;Java;C-C++ applications","keywords":"performance bugs;bug detection;automatic bug fixing;static analysis;real-world bugs","startPage":"902","endPage":"912","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194636","citationCount":40,"referenceCount":62,"year":2015,"authors":"A. Nistor; P. Chang; C. Radoi; S. Lu","affiliations":"Chapman Univ., Orange, NJ, USA; Univ. of Wisconsin-Madison, Madison, WI, USA; Univ. of Illinois, Urbana, IL, USA; Univ. of Chicago, Chicago, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3536"},"title":"Dynamic Data Flow Testing of Object Oriented Systems","abstract":"Data flow testing has recently attracted new interest in the context of testing object oriented systems, since data flow information is well suited to capture relations among the object states, and can thus provide useful information for testing method interactions. Unfortunately, classic data flow testing, which is based on static analysis of the source code, fails to identify many important data flow relations due to the dynamic nature of object oriented systems. In this paper, we propose a new technique to generate test cases for object oriented software. The technique exploits useful inter-procedural data flow information extracted dynamically from execution traces for object oriented systems. The technique is designed to enhance an initial test suite with test cases that exercise complex state based method interactions. The experimental results indicate that dynamic data flow testing can indeed generate test cases that exercise relevant behaviors otherwise missed by both the original test suite and by test suites that satisfy classic data flow criteria.","conference":"IEEE","terms":"Testing;Object recognition;Runtime;Object oriented modeling;Performance analysis;Data models;Context,data flow computing;object-oriented programming;program diagnostics;source code (software),dynamic data flow testing;object oriented systems;static analysis;source code;interprocedural data flow information;state based method interactions","keywords":"","startPage":"947","endPage":"958","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194640","citationCount":9,"referenceCount":66,"year":2015,"authors":"G. Denaro; A. Margara; M. Pezzè; M. Vivanti","affiliations":"Univ. of Milano Bicocca, Milan, Italy; Univ. della Svizzera Italiana (USI), Lugano, Switzerland; Univ. della Svizzera Italiana (USI), Lugano, Switzerland; Univ. della Svizzera Italiana (USI), Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3537"},"title":"Masters-Level Software Engineering Education and the Enriched Student Context","abstract":"Currently, adult higher education software engineering pedagogy isolates the student in a controlled environment during delivery, with application of their learning temporally distant from their professional practice. Delivering software engineering teaching that is immediately relevant to professional practice remains an open challenge. In this paper, we discuss a new pedagogical model which addresses this problem by embedding the validation of the student's learning within their rich professional context. We discuss our experience of applying the model to the design and delivery of a new post-graduate software development module, a core component in our new software engineering Masters qualification at the Open University, UK, a market leader in adult higher education at a distance.","conference":"IEEE","terms":"Software engineering;Education;Context;Software;Context modeling;Unified modeling language;Knowledge engineering,computer science education;further education;software engineering,masters-level software engineering education;adult higher education software engineering pedagogy;pedagogical model;professional context;post-graduate software development module;Open University","keywords":"software engineering education;Masters;distance education","startPage":"311","endPage":"314","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202979","citationCount":1,"referenceCount":9,"year":2015,"authors":"J. G. Hall; L. Rapanotti","affiliations":"Dept. of Comput. \u0026 Commun., Open Univ., Milton Keynes, UK; Dept. of Comput. \u0026 Commun., Open Univ., Milton Keynes, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3538"},"title":"System Thinking: Educating T-Shaped Software Engineers","abstract":"With respect to system thinking, a T-shaped person is one who has technical depth in at least one aspect of the system's content, and a workable level of understanding of a fair number of the other system aspects. Many pure computer science graduates are strongly I-shaped, with a great deal of depth in software technology, but little understanding of the other disciplines involved in such areas as business, medicine, transportation, or Internets of Things. This leaves them poorly prepared to participate in the increasing numbers of projects involving multi-discipline system thinking, and in strong need of software skills. We have developed and evolved an MS-level software engineering curriculum that enables CS majors to become considerably more T-shaped than when they entered. It includes courses in software management and economics, human-computer interaction, embedded software systems, systems and software requirements, architecture, and V\u0026V, and a two-semester, real-client team project course that gives students experience in applying these skills. We find via feedback on the students' internships and job experiences that they and their employers have high rates of success in job offers and job performance.","conference":"IEEE","terms":"Software;Software engineering;Hardware;Stakeholders;Aircraft;Aircraft propulsion,computer science education;embedded systems;further education;human computer interaction;Internet of Things;software architecture;software management;systems engineering,system thinking;system content;computer science graduate;I-shaped;software technology;Internet of Things;multidiscipline system;software skill;MS-level software engineering curriculum;software management;human-computer interaction;embedded software system;software requirement;software architecture;real-client team project course","keywords":"Software Engineering;Systems Engineering;System Thinking;T-shaped;Curriculum;Education and Training;The Incremental Commitment Spiral Model","startPage":"333","endPage":"342","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202983","citationCount":1,"referenceCount":15,"year":2015,"authors":"B. Boehm; S. K. Mobasser","affiliations":"Center for Syst. \u0026 Software Eng., Univ. of Southern California, Los Angeles, CA, USA; Center for Syst. \u0026 Software Eng., Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3539"},"title":"TaskNav: Task-Based Navigation of Software Documentation","abstract":"To help developers navigate documentation, we introduce Task Nav, a tool that automatically discovers and indexes task descriptions in software documentation. With Task Nav, we conceptualize tasks as specific programming actions that have been described in the documentation. Task Nav presents these extracted task descriptions along with concepts, code elements, and section headers in an auto-complete search interface. Our preliminary evaluation indicates that search results identified through extracted task descriptions are more helpful to developers than those found through other means, and that they help bridge the gap between documentation structure and the information needs of software developers. Video: https://www.youtube.com/watch?v=opnGYmMGnqY.","conference":"IEEE","terms":"Documentation;Instruction sets;Software engineering;Interference;Navigation;Electronic mail,document handling;information retrieval;software engineering,task-based navigation;software documentation;TaskNav;task descriptions;programming actions;code elements;section headers;auto-complete search interface;extracted task descriptions;documentation structure;software developers","keywords":"Software Documentation;Development Tasks;Navigation;Auto-Complete;Natural Language Processing","startPage":"649","endPage":"652","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203034","citationCount":8,"referenceCount":16,"year":2015,"authors":"C. Treude; M. Sicard; M. Klocke; M. Robillard","affiliations":"Dept. de Inf. e Mat. Aplic., Univ. Fed. do Rio Grande do Norte, Natal, Brazil; Sch. of Comput. Sci., McGill Univ., Montreal, QC, Canada; Sch. of Comput. Sci., McGill Univ., Montreal, QC, Canada; Sch. of Comput. Sci., McGill Univ., Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d353a"},"title":"A Genetic Algorithm for Detecting Significant Floating-Point Inaccuracies","abstract":"It is well-known that using floating-point numbers may inevitably result in inaccurate results and sometimes even cause serious software failures. Safety-critical software often has strict requirements on the upper bound of inaccuracy, and a crucial task in testing is to check whether significant inaccuracies may be produced. The main existing approach to the floating-point inaccuracy problem is error analysis, which produces an upper bound of inaccuracies that may occur. However, a high upper bound does not guarantee the existence of inaccuracy defects, nor does it give developers any concrete test inputs for debugging. In this paper, we propose the first metaheuristic search-based approach to automatically generating test inputs that aim to trigger significant inaccuracies in floating-point programs. Our approach is based on the following two insights: (1) with FPDebug, a recently proposed dynamic analysis approach, we can build a reliable fitness function to guide the search; (2) two main factors - the scales of exponents and the bit formations of significands - may have significant impact on the accuracy of the output, but in largely different ways. We have implemented and evaluated our approach over 154 real-world floating-point functions. The results show that our approach can detect significant inaccuracies in the subjects.","conference":"IEEE","terms":"Genetic algorithms;Sociology;Statistics;Algorithm design and analysis;Accuracy;Search problems;Software,floating point arithmetic;genetic algorithms;program debugging;search problems;system monitoring,genetic algorithm;floating-point inaccuracy detection;floating-point numbers;software failures;safety-critical software;floating-point inaccuracy problem;error analysis;debugging;metaheuristic search-based approach;floating-point programs;FPDebug;dynamic analysis approach;fitness function","keywords":"","startPage":"529","endPage":"539","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194603","citationCount":13,"referenceCount":34,"year":2015,"authors":"D. Zou; R. Wang; Y. Xiong; L. Zhang; Z. Su; H. Mei","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d353b"},"title":"Automated Decomposition of Build Targets","abstract":"A (build) target specifies the information that is needed to automatically build a software artifact. This paper focuses on underutilized targets - an important dependency problem that we identified at Google. An underutilized target is one with files not needed by some of its dependents. Underutilized targets result in less modular code, overly large artifacts, slow builds, and unnecessary build and test triggers. To mitigate these problems, programmers decompose underutilized targets into smaller targets. However, manually decomposing a target is tedious and error-prone. Although we prove that finding the best target decomposition is NP-hard, we introduce a greedy algorithm that proposes a decomposition through iterative unification of the strongly connected components of the target. Our tool found that 19,994 of 40,000 Java library targets at Google can be decomposed to at least two targets. The results show that our tool is (1) efficient because it analyzes a target in two minutes on average and (2) effective because for each of 1,010 targets, it would save at least 50% of the total execution time of the tests triggered by the target.","conference":"IEEE","terms":"Google;Java;Libraries;Servers;Software;Target tracking;Greedy algorithms,graph theory;greedy algorithms;Java;program testing;software libraries,automated build target decomposition;software artifact;underutilized targets;dependency problem;Google;modular code;build trigger;test trigger;manual target decomposition;NP-hard problem;greedy algorithm;iterative unification;strongly-connected components;Java library targets;total execution time","keywords":"software evolution;refactoring;build;modularity;regression testing;continuous integration;empirical","startPage":"123","endPage":"133","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194567","citationCount":8,"referenceCount":53,"year":2015,"authors":"M. Vakilian; R. Sauciuc; J. D. Morgenthaler; V. Mirrokni","affiliations":"Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA; Google Inc., Mountain View, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d353c"},"title":"Gray Computing: An Analysis of Computing with Background JavaScript Tasks","abstract":"Websites routinely distribute small amounts of work to visitors' browsers in order to validate forms, render animations, and perform other computations. This paper examines the feasibility, cost effectiveness, and approaches for increasing the workloads offloaded to web visitors' browsers in order to turn them into a large-scale distributed data processing engine, which we term gray computing. Past research has looked primarily at either non-browser based volunteer computing or browser-based volunteer computing where the visitors keep their browsers open to a single web page for a long period of time. This paper provides a deep analysis of the architectural, cost effectiveness, user experience, performance, security, and other issues of gray computing distributed data processing engines with high heterogeneity, non-uniform page view times, and high computing pool volatility.","conference":"IEEE","terms":"Servers;Data processing;Browsers;Engines;Cloud computing;Computer architecture;Distributed processing,authoring languages;Java;online front-ends;volunteer computing;Web sites,gray computing;background JavaScript tasks;Web sites;animation rendering;distributed data processing engine;nonbrowser based volunteer computing;browser-based volunteer computing;deep analysis;cost effectiveness;user experience;security","keywords":"","startPage":"167","endPage":"177","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194571","citationCount":3,"referenceCount":26,"year":2015,"authors":"Y. Pan; J. White; Y. Sun; J. Gray","affiliations":"Vanderbilt Univ., Nashville, TN, USA; Vanderbilt Univ., Nashville, TN, USA; California State Polytech. Univ., Pomona, CA, USA; Univ. of Alabama, Tuscaloosa, AL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d353d"},"title":"Automatic and Continuous Software Architecture Validation","abstract":"Software systems tend to suffer from architectural problems as they are being developed. While modern software development methodologies such as Agile and Dev-Ops suggest different ways of assuring code quality, very little attention is paid to maintaining high quality of the architecture of the evolving systems. By detecting and alerting about violations of the intended software architecture, one can often avoid code-level bad smells such as spaghetti code. Typically, if one wants to reason about the software architecture, the burden of first defining the intended architecture falls on the developer's shoulders. This includes definition of valid and invalid dependencies between software components. However, the developers are seldom familiar with the entire software system, which makes this task difficult, time consuming and error-prone. We propose and implement a solution for automatic detection of architectural violations in software artifacts. The solution, which utilizes a number of predefined and user-defined patterns, does not require prior knowledge of the system or its intended architecture. We propose to leverage this solution as part of the nightly build process used by development teams, thus achieving continuous automatic validation of the system's software architecture. As we show in multiple open-source and proprietary cases, a small set of predefined patterns can detect architectural violations as they are introduced over the course of development, and also capture deterioration in existing architectural problems. By evaluating the tool on relatively large open-source projects, we also validate its scalability and practical applicability to large software systems.","conference":"IEEE","terms":"Production facilities;Software architecture;Software systems;Computer architecture;Semantics,object-oriented programming;program verification;software architecture;software quality;source code (software),automatic software architecture validation;continuous software architecture validation;software systems;architectural problems;agile software development methodologies;Dev-Ops;code quality;quality maintenance;systems architecture;code-level;spaghetti code;software components;architectural violations automatic detection;software artifacts;predefined patterns;user-defined patterns;development teams","keywords":"","startPage":"59","endPage":"68","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202950","citationCount":2,"referenceCount":44,"year":2015,"authors":"M. Goldstein; I. Segall","affiliations":"IBM Res. - Haifa, Haifa, Israel; IBM Res. - Haifa, Haifa, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d353e"},"title":"Does the Failing Test Execute a Single or Multiple Faults? An Approach to Classifying Failing Tests","abstract":"Debugging is an indispensable yet frustrating activity in software development and maintenance. Thus, numerous techniques have been proposed to aid this task. Despite the demonstrated effectiveness and future potential of these techniques, many of them have the unrealistic single-fault failure assumption. To alleviate this problem, we propose a technique that can be used to distinguish failing tests that executed a single fault from those that executed multiple faults in this paper. The technique suitably combines information from (i) a set of fault localization ranked lists, each produced for a certain failing test and (ii) the distance between a failing test and the passing test that most resembles it to achieve this goal. An experiment on 5 real-life medium-sized programs with 18, 920 multiple-fault versions, which are shipped with number of faults ranging from 2 to 8, has been conducted to evaluate the technique. The results indicate that the performance of the technique in terms of evaluation measures precision, recall, and F-measure is promising. In addition, for the identified failing tests that executed a single fault, the technique can also properly cluster them.","conference":"IEEE","terms":"Maintenance engineering;Fault diagnosis;Debugging;Indexes;Software;Classification algorithms;Testing,pattern classification;program debugging;program testing;software fault tolerance;software maintenance,failing test classification;debugging;software development;software maintenance;single-fault failure assumption;fault localization;evaluation measures precision;recall;F-measure","keywords":"distance calculation;fault localization;binary classification;debugging","startPage":"924","endPage":"935","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194638","citationCount":1,"referenceCount":46,"year":2015,"authors":"Z. Yu; C. Bai; K. Cai","affiliations":"Dept. of Autom. Control, Beihang Univ., Beijing, China; Dept. of Autom. Control, Beihang Univ., Beijing, China; Dept. of Autom. Control, Beihang Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d353f"},"title":"Engineering Sustainability Through Language","abstract":"As our understanding and care for sustainability concerns increases, so does the demand for incorporating these concerns into software. Yet, existing programming language constructs are not well-aligned with concepts of the sustainability domain. This undermines what we term technical sustainability of the software due to (i) increased complexity in programming of such concerns and (ii) continuous code changes to keep up with changes in (environmental, social, legal and other) sustainability-related requirements. In this paper we present a proof-of-concept approach on how technical sustainability support for new and existing concerns can be provided through flexible language-level programming. We propose to incorporate sustainability-related behaviour into programs through micro-languages enabling such behaviour to be updated and/or redefined as and when required.","conference":"IEEE","terms":"Semantics;Software;Syntactics;Batteries;Programming;Software engineering;Computer languages,programming languages;software engineering;sustainable development,engineering sustainability;programming language;continuous code;sustainability-related requirements;technical sustainability support;flexible language-level programming;sustainability-related behaviour;microlanguages","keywords":"sustainabiity;micro-languages;adaptability;change management","startPage":"501","endPage":"504","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203001","citationCount":5,"referenceCount":17,"year":2015,"authors":"R. Chitchyan; W. Cazzola; A. Rashid","affiliations":"Dept. of Comput. Sci., Univ. of Leicester, Leicester, UK; Dept. of Comput. Sci., Univ. degli Studi di Milano, Milan, Italy; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3540"},"title":"Educating Software Engineering Managers - Revisited What Software Project Managers Need to Know Today","abstract":"In 2003, the original paper with this title was published as part of CSEET 2003. It focused on resolving communication issues between software project managers and developers and introduced a corporate strategy based means of evaluating software engineers. Now, more than a decade later, we could benefit from what we have learned in other fields about managing people involved in knowledge work and how to improve our success in software development. But are we? This paper is intended to present what Software Engineering students can be taught today that will help them to be successful as software project managers now and in the future. It is based on the premise that effective software project managers are not born but made through education.","conference":"IEEE","terms":"Software;Software engineering;Project management;Schedules;Training,computer science education;project management;software development management,software engineering manager education;software project managers;software project developers;corporate strategy;software engineering students","keywords":"Software Project Management;Software Project Management Curriculum;Project Management","startPage":"353","endPage":"359","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202985","citationCount":4,"referenceCount":54,"year":2015,"authors":"L. Peters; A. M. Moreno","affiliations":"Univ. Politec. de Madrid, Madrid, Spain; Univ. Politec. de Madrid, Madrid, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911aeee8435e8e7d3541"},"title":"Borrowing from the Crowd: A Study of Recombination in Software Design Competitions","abstract":"One form of crowdsourcing is the competition, which poses an open call for competing solutions. Commercial systems such as TopCoder have begun to explore the application of competitions to software development, but have important limitations diminishing the potential benefits drawn from the crowd. In particular, they employ a model of independent work that ignores the opportunity for designs to arise from the ideas of multiple designers. In this paper, we examine the potential for software design competitions to incorporate recombination, in which competing designers are given the designs of others and encouraged to use them to revise their own designs. To explore this, we conducted two software design competitions in which participants were asked to produce both an initial and a revised design, drawing on lessons learned from the crowd. We found that, in both competitions, all participants borrowed ideas and most improved the quality of their designs. Our findings demonstrate the potential benefits of recombination in software design and suggest several ways in which software design competitions can be improved.","conference":"IEEE","terms":"Crowdsourcing;Software design;Interviews;Software engineering;Distance measurement;User interfaces,outsourcing;software engineering,crowdsourcing;software design competition recombination;software development","keywords":"crowdsourcing;software design;collective intelligence;collaborative design","startPage":"551","endPage":"562","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194605","citationCount":16,"referenceCount":40,"year":2015,"authors":"T. D. LaToza; M. Chen; L. Jiang; M. Zhao; A. v. d. Hoek","affiliations":"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Grad. Sch. of Inf., Univ. of Amsterdam, Amsterdam, Netherlands; Grad. Sch. of Inf., Univ. of Amsterdam, Amsterdam, Netherlands; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3542"},"title":"Data-Delineation in Software Binaries and its Application to Buffer-Overrun Discovery","abstract":"Detecting memory-safety violations in binaries is complicated by the lack of knowledge of the intended data layout, i.e., the locations and sizes of objects. We present lightweight, static, heuristic analyses for recovering the intended layout of data in a stripped binary. Comparison against DWARF debugging information shows high precision and recall rates for inferring source-level object boundaries. On a collection of benchmarks, our analysis eliminates a third to a half of incorrect object boundaries identified by an IDA Pro-inspired heuristic, while retaining nearly all valid object boundaries. In addition to measuring their accuracy directly, we evaluate the effect of using the recovered data for improving the precision of static buffer-overrun detection in the defect-detection tool CodeSonar/x86. We demonstrate that CodeSonar's false-positive rate drops by about 80% across our internal evaluation suite for the tool, while our approximation of CodeSonar's recall only degrades about 25%.","conference":"IEEE","terms":"Registers;Layout;Benchmark testing;Approximation methods;Libraries;Accuracy;Optimization,program diagnostics;security of data;software tools,software security;CodeSonar/x86;defect-detection tool;static analysis;memory-safety violation detection;buffer-overrun discovery;software binary;DDA;data delineation analysis","keywords":"reverse engineering;data delineation;binary analysis;static analysis;buffer overrun detection","startPage":"145","endPage":"155","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194569","citationCount":6,"referenceCount":29,"year":2015,"authors":"D. Gopan; E. Driscoll; D. Nguyen; D. Naydich; A. Loginov; D. Melski","affiliations":"GrammaTech, Inc., Madison, WI, USA; GrammaTech, Inc., Madison, WI, USA; GrammaTech, Inc., Ithaca, NY, USA; GrammaTech, Inc., Ithaca, NY, USA; GrammaTech, Inc., Ithaca, NY, USA; GrammaTech, Inc., Ithaca, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3543"},"title":"Symbolic Model Checking of Product-Line Requirements Using SAT-Based Methods","abstract":"Product line (PL) engineering promotes the development of families of related products, where individual products are differentiated by which optional features they include. Modelling and analyzing requirements models of PLs allows for early detection and correction of requirements errors -- including unintended feature interactions, which are a serious problem in feature-rich systems. A key challenge in analyzing PL requirements is the efficient verification of the product family, given that the number of products is too large to be verified one at a time. Recently, it has been shown how the high-level design of an entire PL, that includes all possible products, can be compactly represented as a single model in the SMV language, and model checked using the NuSMV tool. The implementation in NuSMV uses BDDs, a method that has been outperformed by SAT-based algorithms. In this paper we develop PL model checking using two leading SAT-based symbolic model checking algorithms: IMC and IC3. We describe the algorithms, prove their correctness, and report on our implementation. Evaluating our methods on three PL models from the literature, we demonstrate an improvement of up to 3 orders of magnitude over the existing BDD-based method.","conference":"IEEE","terms":"Model checking;Interpolation;Adaptation models;Computational modeling;Data structures;Boolean functions;Analytical models,binary decision diagrams;computability;program verification;software product lines;systems analysis,product-line requirements;SAT-based method;product line engineering;requirement error detection;requirement error correction;feature interaction;product family verification;high-level design;SMV language;NuSMV tool;BDD method;PL model checking;SAT-based symbolic model checking algorithm;IMC algorithm;IC3 algorithm;correctness proving","keywords":"Product Lines;Symbolic Model Checking;IC3;IMC","startPage":"189","endPage":"199","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194573","citationCount":5,"referenceCount":0,"year":2015,"authors":"S. Ben-David; B. Sterin; J. M. Atlee; S. Beidu","affiliations":"David Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada; Dept. of EECS, Univ. of California, Berkeley, Berkeley, CA, USA; David Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada; David Cheriton Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3544"},"title":"Empirically Detecting False Test Alarms Using Association Rules","abstract":"Applying code changes to software systems and testing these code changes can be a complex task that involves many different types of software testing strategies, e.g. system and integration tests. However, not all test failures reported during code integration are hinting towards code defects. Testing large systems such as the Microsoft Windows operating system requires complex test infrastructures, which may lead to test failures caused by faulty tests and test infrastructure issues. Such false test alarms are particular annoying as they raise engineer attention and require manual inspection without providing any benefit. The goal of this work is to use empirical data to minimize the number of false test alarms reported during system and integration testing. To achieve this goal, we use association rule learning to identify patterns among failing test steps that are typically for false test alarms and can be used to automatically classify them. A successful classification of false test alarms is particularly valuable for product teams as manual test failure inspection is an expensive and time-consuming process that not only costs engineering time and money but also slows down product development. We evaluating our approach on system and integration tests executed during Windows 8.1 and Microsoft Dynamics AX development. Performing more than 10,000 classifications for each product, our model shows a mean precision between 0.85 and 0.90 predicting between 34% and 48% of all false test alarms.","conference":"IEEE","terms":"Association rules;Testing;Manuals;Software engineering;Inspection;Software systems,data mining;learning (artificial intelligence);operating systems (computers);program testing;software reliability,false test alarm detection;software systems;software testing strategy;integration tests;test failures;code integration;code defects;Microsoft Windows operating system;complex test infrastructures;association rule learning;manual test failure inspection;Microsoft Dynamics AX development;Windows 8.1","keywords":"","startPage":"39","endPage":"48","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202948","citationCount":18,"referenceCount":44,"year":2015,"authors":"K. Herzig; N. Nagappan","affiliations":"Microsoft Res., Cambridge, UK; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3545"},"title":"SPF: A Middleware for Social Interaction in Mobile Proximity Environments","abstract":"Smart interconnected devices are changing our lives and are turning conventional spaces into smart ones. Physical proximity, a key enabler of social interactions in the old days is not exploited by smart solutions, where the social dimension is always managed through the Internet. This paper aims to blend the two forces and proposes the idea of social smart space, where modern technologies can help regain and renew social interactions, and where proximity is seen as enabler for dedicated and customized functionality provided by users to users. A Social Proximity Framework (SPF) provides the basis for the creation of this new flavor of smart spaces. Two different versions of the SPF, based on different communication infrastructures, help explain the characteristics of the different components, and show how the SPF can benefit from emerging connection-less communication protocols. A first assessment of the two implementations concludes the paper.","conference":"IEEE","terms":"Middleware;Context;Androids;Humanoid robots;IEEE 802.11 Standard;Protocols,Internet;middleware;mobile computing;protocols,social interaction middleware;mobile proximity environments;smart interconnected devices;conventional spaces;smart solutions;Internet;social smart space;social proximity framework;communication infrastructures;SPF;connection-less communication protocols","keywords":"","startPage":"79","endPage":"88","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202952","citationCount":2,"referenceCount":19,"year":2015,"authors":"L. Baresi; L. Goix; S. Guinea; V. Panzica La Manna; J. Aliprandi; D. Archetti","affiliations":"Dipt. di Elettron. Inf. e Bioingegneria, Politec. di Milano, Milan, Italy; Econocom-Osiatis, Lyon, France; Dipt. di Elettron. Inf. e Bioingegneria, Politec. di Milano, Milan, Italy; MIT Media Lab., Cambridge, MA, USA; Dipt. di Elettron. Inf. e Bioingegneria, Politec. di Milano, Milan, Italy; Dipt. di Elettron. Inf. e Bioingegneria, Politec. di Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3546"},"title":"AppCivist - A Service-Oriented Software Platform for Socially Sustainable Activism","abstract":"The increased adoption of mobile devices and social networking is drastically changing the way people monitor and share knowledge about their environment. Here, information and communication technologies (ICT) offer significant new ways to support social activism in cities by providing residents with new digital tools to articulate projects and mobilize activities. However, the development of ICT for activism is still in its infancy, with activists using basic tools stitched together in an ad hoc manner for their needs. Still, Internet-based technologies and related software architectures feature various enablers for civic action beyond base social networking. To that end, this paper discusses the vision and initial details of AppCivist, a platform that builds on cross-domain research among social scientists and computer scientists to revisit service-oriented architecture and relevant services to further social activism. We discuss the ICT challenges inherent in this project and present our recent work to address them.","conference":"IEEE","terms":"Assembly;Organizations;Software engineering;Software;Context;Computer architecture;Ontologies,Internet;service-oriented architecture;social networking (online),service-oriented software platform;socially sustainable activism;AppCivist platform;mobile devices;social networking;information and communication technology;ICT;digital tools;Internet-based technology;software architectures","keywords":"service-oriented computing;social activism;social sustainability","startPage":"515","endPage":"518","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203003","citationCount":5,"referenceCount":9,"year":2015,"authors":"A. Pathak; V. Issarny; J. Holston","affiliations":"Inria, Rennes, France; Inria, Rennes, France; Univ. of California, Berkeley, Berkeley, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3547"},"title":"Concurrent Software Engineering and Robotics Education","abstract":"This paper presents a new, multidisciplinary robotics programming course, reports initial results, and describes subsequent improvements. With equal emphasis on software engineering and robotics, the course teaches students how software engineering applies to robotics. Students learn independently and interactively and gain hands-on experience by implementing robotics algorithms on a real robot. To understand the effects of the course, we conducted an exit and an 8-month survey and measured software quality of the students' solutions. The analysis shows that the hands-on experience helped everyone learn and retain robotics well, but the students' knowledge gain in software engineering depended on their prior programming knowledge. Based on these findings, we propose improvements to the course. Lastly, we reflect our experience on andragogy, minimalism, and interactive learning.","conference":"IEEE","terms":"Software engineering;Robot sensing systems;Computer science;Education;Software quality,computer science education;concurrency control;control engineering computing;control engineering education;robot programming;software quality;teaching,concurrent software engineering;robotics education;multidisciplinary robotics programming course;teaching;students independent learning;interactive learning;hands-on experience;robotics algorithms;software quality;student knowledge gain;programming knowledge;andragogy;minimalism","keywords":"","startPage":"370","endPage":"379","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202987","citationCount":2,"referenceCount":30,"year":2015,"authors":"J. Shin; A. Rusakov; B. Meyer","affiliations":"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3548"},"title":"Does Outside-In Teaching Improve the Learning of Object-Oriented Programming?","abstract":"Object-oriented programming (OOP) is widely used in the software industry and university introductory courses today. Following the structure of most textbooks, such courses frequently are organised starting with the concepts of imperative and structured programming and only later introducing OOP. An alternative approach is to begin directly with OOP following the Outside-In teaching method as proposed by Meyer. Empirical results for the effects of Outside-In teaching on students and lecturers are sparse, however. We describe the conceptual design and empirical evaluation of two OOP introductory courses from different universities based on Outside-In teaching. The evaluation results are compared to those from a third course serving as the control group, which was taught OOP the \"traditional\" way. We evaluate the initial motivation and knowledge of the participants and the learning outcomes. In addition, we analyse results of the end term exams and qualitatively analyse the results of interviews with the lecturers and tutors. Regarding the learning outcomes, the results show no significant differences between the Outside-In and the \"traditional\" teaching method. In general, students found it harder to solve and implement algorithmic problems than to understand object oriented (OO) concepts. Students taught OOP by the Outside-In method, however, were less afraid that they would not pass the exam at the end of term and understood the OO paradigm more quickly. Therefore, the Outside-In method is no silver bullet for teaching OOP regarding the learning outcomes but has positive effects on motivation and interest.","conference":"IEEE","terms":"Education;Java;Programming profession;Software;Computers,computer aided instruction;computer science education;object-oriented programming,object-oriented programming;OOP;software industry;university introductory courses;structured programming;imperative programming;outside-in teaching method;algorithmic problems","keywords":"","startPage":"408","endPage":"417","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202991","citationCount":1,"referenceCount":21,"year":2015,"authors":"E. Janke; P. Brune; S. Wagner","affiliations":"Univ. of Appl. Sci. Neu-Ulm, Neu-Ulm, Germany; Univ. of Appl. Sci. Neu-Ulm, Neu-Ulm, Germany; Univ. of Stuttgart, Stuttgart, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3549"},"title":"Open Source-Style Collaborative Development Practices in Commercial Projects Using GitHub","abstract":"Researchers are currently drawn to study projects hosted on GitHub due to its popularity, ease of obtaining data, and its distinctive built-in social features. GitHub has been found to create a transparent development environment, which together with a pull request-based workflow, provides a lightweight mechanism for committing, reviewing and managing code changes. These features impact how GitHub is used and the benefits it provides to teams' development and collaboration. While most of the evidence we have is from GitHub's use in open source software (OSS) projects, GitHub is also used in an increasing number of commercial projects. It is unknown how GitHub supports these projects given that GitHub's workflow model does not intuitively fit the commercial development way of working. In this paper, we report findings from an online survey and interviews with GitHub users on how GitHub is used for collaboration in commercial projects. We found that many commercial projects adopted practices that are more typical of OSS projects including reduced communication, more independent work, and self-organization. We discuss how GitHub's transparency and popular workflow can promote open collaboration, allowing organizations to increase code reuse and promote knowledge sharing across their teams.","conference":"IEEE","terms":"Interviews;Organizations;Software;Encoding;Writing;Collaborative software,computer software;public domain software,open source-style collaborative development practices;commercial projects;built-in social features;transparent development environment;pull request-based workflow;code changes;open source software projects;OSS projects;GitHub users;self-organization;knowledge sharing;online code hosting service","keywords":"GitHub;open source;collaboration;coordination;workflow;pull requests;practices;commercial projects","startPage":"574","endPage":"585","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194607","citationCount":21,"referenceCount":49,"year":2015,"authors":"E. Kalliamvakou; D. Damian; K. Blincoe; L. Singer; D. M. German","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d354a"},"title":"DASE: Document-Assisted Symbolic Execution for Improving Automated Software Testing","abstract":"We propose and implement a new approach, Document-Assisted Symbolic Execution (DASE), to improve automated test generation and bug detection. DASE leverages natural language processing techniques and heuristics to analyze program documentation to extract input constraints automatically. DASE then uses the input constraints to guide symbolic execution to focus on inputs that are semantically more important.We evaluated DASE on 88 programs from 5 mature real-world software suites: COREUTILS, FINDUTILS, GREP, BINUTILS, and ELFTOOLCHAIN. DASE detected 12 previously unknown bugs that symbolic execution without input constraints failed to detect, 6 of which have already been confirmed by the developers. In addition, DASE increases line coverage, branch coverage, and call coverage by 14.2 -- 120.3%, 2.3 -- 167.7%, and 16.9 -- 135.2% respectively, which are 6.0 -- 21.1 percentage points (pp), 1.6 -- 18.9 pp, and 2.8 -- 20.1 pp increases. The accuracies of input constraint extraction are 97.8 -- 100%.","conference":"IEEE","terms":"Arrays;Computer bugs;Testing;Ground penetrating radar;Geophysical measurement techniques;Search problems;Grammar,natural language processing;program debugging;program testing;system documentation,document-assisted symbolic execution;DASE;automated software testing;automated test generation;bug detection;natural language processing techniques;program documentation;software suites;COREUTILS;FINDUTILS;GREP;BINUTILS;ELFTOOLCHAIN;line coverage;branch coverage;call coverage","keywords":"natural language processing;symbolic execution;software testing","startPage":"620","endPage":"631","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194611","citationCount":8,"referenceCount":61,"year":2015,"authors":"E. Wong; L. Zhang; S. Wang; T. Liu; L. Tan","affiliations":"Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d354b"},"title":"On the Architecture-Driven Development of Software-Intensive Systems-of-Systems","abstract":"Nowadays, complex software-intensive systems have resulted from the integration of heterogeneous independent systems, thus leading to a new class of systems called Systems-of-Systems (SoS). As in any system, SoS architectures have been regarded as an important element for determining their success. However, the state of the art reveals shortcomings that contribute to compromise the quality of these systems, as their inherent characteristics (such as emergent behavior and evolutionary development) are often not properly addressed. In this context, this PhD research aims at investigating how SoS software architectures can be used to model and evolve these systems. As main contribution, an architecture-centric approach for developing software-intensive SoS with focus on the formal specification and dynamic reconfiguration of their architectures is proposed. Such an approach mainly intends to contribute to fill some of the relevant existing gaps regarding the development of software-intensive SoS driven by their software architectures.","conference":"IEEE","terms":"Computer architecture;Runtime;Context;Middleware,formal specification;software architecture;software quality,architecture-driven development;software-intensive systems-of-systems;heterogeneous independent systems;SoS software architectures;architecture-centric approach;formal specification","keywords":"systems-of-systems;SoS;software architecture;architecture-driven engineering","startPage":"899","endPage":"902","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203109","citationCount":0,"referenceCount":18,"year":2015,"authors":"E. Cavalcante","affiliations":"Dept. of Inf. \u0026 Appl. Math., Fed. Univ. of Rio Grande do Norte, Natal, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d354c"},"title":"Tempura: Temporal Dimension for IDEs","abstract":"Modern integrated development environments (IDEs) make many software engineering tasks easier by providing automated programming support such as code completion and navigation. However, such support -- and therefore IDEs as awhole -- operate on one revision of the code at a time, and leave handling of code history to external tools or plugins, such asEGit for Eclipse. For example, when a method is removed froma class, developers can no longer find the method through code completion. This forces developers to manually switch across different revisions or resort to using external tools when they need to learn about previous code revisions.We propose a novel approach of adding a temporal dimensionto IDEs, enabling code completion and navigation to operate on multiple revisions of code at a time. We previously introduced the idea of temporal code completion and navigation,and presented a vision for how that idea may be realized.This paper realizes that vision by implementing and evaluatinga prototype tool called Tempura. We describe our algorithmfor processing and indexing historical code information from repositories for Tempura, and demonstrate Tempura's scalability with three large Eclipse projects. We also evaluate Tempura's usability through a controlled user study. The study participantslearned about the code history with more accuracy when usingTempura compared to EGit. Although the sample size was notlarge enough to provide strong statistical significance, the resultsshow a promising outlook for our approach.","conference":"IEEE","terms":"Proposals;History;Navigation;Receivers;Context;Indexing;Java,software tools,Tempura;prototype tool;temporal dimension;integrated development environment;IDE;software engineering;code revision;temporal code completion","keywords":"","startPage":"212","endPage":"222","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194575","citationCount":0,"referenceCount":44,"year":2015,"authors":"Y. Y. Lee; D. Marinov; R. E. Johnson","affiliations":"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d354d"},"title":"Online Defect Prediction for Imbalanced Data","abstract":"Many defect prediction techniques are proposed to improve software reliability. Change classification predicts defects at the change level, where a change is the modifications to one file in a commit. In this paper, we conduct the first study of applying change classification in practice. We identify two issues in the prediction process, both of which contribute to the low prediction performance. First, the data are imbalanced -- there are much fewer buggy changes than clean changes. Second, the commonly used cross-validation approach is inappropriate for evaluating the performance of change classification. To address these challenges, we apply and adapt online change classification, resampling, and updatable classification techniques to improve the classification performance. We perform the improved change classification techniques on one proprietary and six open source projects. Our results show that these techniques improve the precision of change classification by 12.2-89.5% or 6.4 -- 34.8 percentage points (pp.) on the seven projects. In addition, we integrate change classification in the development process of the proprietary project. We have learned the following lessons: 1) new solutions are needed to convince developers to use and believe prediction results, and prediction results need to be actionable, 2) new and improved classification algorithms are needed to explain the prediction results, and insensible and unactionable explanations need to be filtered or refined, and 3) new techniques are needed to improve the relatively low precision.","conference":"IEEE","terms":"Training;Software;Predictive models;Feature extraction;Computer bugs;Data models;Software engineering,pattern classification;public domain software;software performance evaluation;software reliability,online defect prediction techniques;imbalanced data;software reliability;change classification performance evaluation;low prediction performance;cross-validation approach;updatable classification techniques;resampling classification techniques;online change classification techniques;open source projects;proprietary project development process","keywords":"","startPage":"99","endPage":"108","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202954","citationCount":51,"referenceCount":53,"year":2015,"authors":"M. Tan; L. Tan; S. Dara; C. Mayeux","affiliations":"Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Cisco Syst., Bangalore, India; Cisco Syst., Bangalore, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d354e"},"title":"Managing Emergent Ethical Concerns for Software Engineering in Society","abstract":"This paper presents an initial framework for managing emergent ethical concerns during software engineering in society projects. We argue that such emergent considerations can neither be framed as absolute rules about how to act in relation to fixed and measurable conditions. Nor can they be addressed by simply framing them as non-functional requirements to be satisficed. Instead, a continuous process is needed that accepts the 'messiness' of social life and social research, seeks to understand complexity (rather than seek clarity), demands collective (not just individual) responsibility and focuses on dialogue over solutions. The framework has been derived based on retrospective analysis of ethical considerations in four software engineering in society projects in three different domains.","conference":"IEEE","terms":"Software engineering;Ethics;Software;Media;Societies;Stakeholders;Law enforcement,ethical aspects;software engineering;software management,emergent ethical concern management;software engineering;society projects","keywords":"ethics;software in society;cyber crime;citizen science","startPage":"523","endPage":"526","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203005","citationCount":2,"referenceCount":13,"year":2015,"authors":"A. Rashid; K. Moore; C. May-Chahal; R. Chitchyan","affiliations":"Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Dept. of Comput. Sci., Univ. of Leicester, Leicester, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d354f"},"title":"Software Design Studio: A Practical Example","abstract":"We have been generally successful for transferring software engineering knowledge to industry through various forms of education. However, many challenges in software engineering training remain. A key amongst these is how best to energise software engineering education with real-world software engineering practices. This paper describes our experience of delivering a radically different approach based on the notion of a Software Design Studio. The Software Design Studio is both a lab for students engaged in conceiving, designing and developing software products as well as an approach for teaching software engineering in the lab which emphasizes practical hands-on work and experimentation. The feedback on the Software Design Studio -- from both staff and students -- has been outstanding. Although the programme is designed as a small, elite programme there is interest to see if the teaching methods can be transferred across to the much larger undergraduate programme in Computer Science. In this paper, we provide a detailed description of how our studio works in practice so that others, thinking of tak-ing a studio or studio-inspired approach, can use in designing their own courses.","conference":"IEEE","terms":"Software engineering;Software design;Testing;Conferences;Training,computer science education;educational courses;product development;software development management;teaching,Software Design Studio;software engineering knowledge;software engineering training;software engineering education;software product conceiving;software product design;software product development;software engineering teaching;teaching methods;computer science;studio approach;studio-inspired approach;courses","keywords":"Software engineering education;software design studio;reflective teaching approach.","startPage":"389","endPage":"397","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202989","citationCount":6,"referenceCount":15,"year":2015,"authors":"J. Lee; G. Kotonya; J. Whittle; C. Bull","affiliations":"Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3550"},"title":"In Search of the Emotional Design Effect in Programming","abstract":"A small number of recent studies have suggested that learning is enhanced when the illustrations in instructional materials are designed to appeal to the learners' emotions through the use of color and the personification of key elements. We sought to replicate this emotional design effect in the context of introductory object-oriented programming (OOP). In this preliminary study, a group of freshmen studied a text on objects which was illustrated using anthropomorphic graphics while a control group had access to abstract graphics. We found no significant difference in the groups' scores on a comprehension post-test, but the experimental group spent substantially less time on the task than the control group. Among those participants who had no prior programming experience, the materials inspired by emotional design were perceived as less intelligible and appealing and led to lower self-reported concentration levels. Although this result does not match the pattern of results from earlier studies, it shows that the choice of illustrations in learning materials matters and calls for more research that addresses the limitations of this preliminary study.","conference":"IEEE","terms":"Visualization;Programming profession;Object oriented programming;Education;Multimedia communication,computer science education;object-oriented programming;social aspects of automation,emotional design effect;instructional materials;object-oriented programming;OOP;anthropomorphic graphics;abstract graphics;comprehension post-test;control group;learning materials;self-reported concentration levels","keywords":"emotional design;programming;learning;multimedia","startPage":"428","endPage":"434","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202993","citationCount":5,"referenceCount":21,"year":2015,"authors":"L. Haaranen; P. Ihantola; J. Sorva; A. Vihavainen","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3551"},"title":"Interactive Synthesis Using Free-Form Queries","abstract":"We present a new code assistance tool for integrated development environments. Our system accepts free-form queries allowing a mixture of English and Java as an input, and produces Java code fragments that take the query into account and respect syntax, types, and scoping rules of Java as well as statistical usage patterns. The returned results need not have the structure of any previously seen code fragment. As part of our system we have constructed a probabilistic context free grammar for Java constructs and library invocations, as well as an algorithm that uses a customized natural language processing tool chain to extract information from free-form text queries. The evaluation results show that our technique can tolerate much of the flexibility present in natural language, and can also be used to repair incorrect Java expressions that contain useful information about the developer's intent. Our demo video is available at http://youtu.be/tx4-XgAZkKU.","conference":"IEEE","terms":"Java;Context;Natural language processing;Programming;Grammar;Chapters,Java;natural language processing;query processing;software libraries;statistical analysis,interactive synthesis;code assistance tool;integrated development environments;free-form queries;English;Java code fragments;statistical usage patterns;library invocations;customized natural language processing tool chain;free-form text queries","keywords":"Program Synthesis;Program Repair;Natural Language Processing;Code Completion","startPage":"689","endPage":"692","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203044","citationCount":8,"referenceCount":28,"year":2015,"authors":"T. Gvero; V. Kuncak","affiliations":"Ecole Polytech. Fed. de Lausanne (EPFL), Lausanne, Switzerland; Ecole Polytech. Fed. de Lausanne (EPFL), Lausanne, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3552"},"title":"Tricorder: Building a Program Analysis Ecosystem","abstract":"Static analysis tools help developers find bugs, improve code readability, and ensure consistent style across a project. However, these tools can be difficult to smoothly integrate with each other and into the developer workflow, particularly when scaling to large codebases. We present Tricorder, a program analysis platform aimed at building a data-driven ecosystem around program analysis. We present a set of guiding principles for our program analysis tools and a scalable architecture for an analysis platform implementing these principles. We include an empirical, in-situ evaluation of the tool as it is used by developers across Google that shows the usefulness and impact of the platform.","conference":"IEEE","terms":"Google;Computer bugs;Buildings;Ecosystems;Java;Libraries;Usability,ecology;program diagnostics;software architecture,program analysis ecosystem;tricorder;static analysis tools;code readability;developer workflow;codebases;scalable architecture;Google","keywords":"program analysis;static analysis","startPage":"598","endPage":"608","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194609","citationCount":50,"referenceCount":43,"year":2015,"authors":"C. Sadowski; J. v. Gogh; C. Jaspan; E. Söderberg; C. Winter","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3553"},"title":"Regular Property Guided Dynamic Symbolic Execution","abstract":"A challenging problem in software engineering is to check if a program has an execution path satisfying a regular property. We propose a novel method of dynamic symbolic execution (DSE) to automatically find a path of a program satisfying a regular property. What makes our method distinct is when exploring the path space, DSE is guided by the synergy of static analysis and dynamic analysis to find a target path as soon as possible. We have implemented our guided DSE method for Java programs based on JPF and WALA, and applied it to 13 real-world open source Java programs, a total of 225K lines of code, for extensive experiments. The results show the effectiveness, efficiency, feasibility and scalability of the method. Compared with the pure DSE on the time to find the first target path, the average speedup of the guided DSE is more than 258X when analyzing the programs that have more than 100 paths.","conference":"IEEE","terms":"Monitoring;History;Java;Context;Software engineering;Space exploration;Algorithm design and analysis,Java;program diagnostics;public domain software;software engineering;symbol manipulation,dynamic symbolic execution;regular property;software engineering;execution path;path space;synergy;static analysis;guided DSE method;JPF;WALA;real-world open source Java programs","keywords":"Dynamic Symbolic Execution;Regular Property;Finite State Machine;Static Analysis;Dynamic Analysis","startPage":"643","endPage":"653","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194613","citationCount":13,"referenceCount":53,"year":2015,"authors":"Y. Zhang; Z. Chen; J. Wang; W. Dong; Z. Liu","affiliations":"Key Lab. of High Performance Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Key Lab. of High Performance Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Centre for Software Eng., Birmingham City Univ., Birmingham, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3554"},"title":"Cascade: A Universal Programmer-Assisted Type Qualifier Inference Tool","abstract":"Type qualifier inference tools usually operate in batch mode and assume that the program must not be changed except to add the type qualifiers. In practice, programs must be changed to make them type-correct, and programmers must understand them. Cascade is an interactive type qualifier inference tool that is easy to implement and universal (i.e., it can work for any type qualifier system for which a checker is implemented). It shows that qualifier inference can achieve better results by involving programmers rather than relying solely on automation.","conference":"IEEE","terms":"Java;Tutorials;Prototypes;Software;Safety;Automation;Ports (Computers),inference mechanisms;software engineering;software tools,Cascade;universal programmer-assisted type qualifier inference tool;interactive type qualifier inference tool;programmers;type-correct programs","keywords":"software evolution;refactoring;human-computer interaction;design;type system;type qualifier inference","startPage":"234","endPage":"245","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194577","citationCount":2,"referenceCount":46,"year":2015,"authors":"M. Vakilian; A. Phaosawasdi; M. D. Ernst; R. E. Johnson","affiliations":"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Washington, Seattle, WA, USA; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3555"},"title":"IccTA: Detecting Inter-Component Privacy Leaks in Android Apps","abstract":"Shake Them All is a popular \"Wallpaper\" application exceeding millions of downloads on Google Play. At installation, this application is given permission to (1) access the Internet (for updating wallpapers) and (2) use the device microphone (to change background following noise changes). With these permissions, the application could silently record user conversations and upload them remotely. To give more confidence about how Shake Them All actually processes what it records, it is necessary to build a precise analysis tool that tracks the flow of any sensitive data from its source point to any sink, especially if those are in different components. Since Android applications may leak private data carelessly or maliciously, we propose IccTA, a static taint analyzer to detect privacy leaks among components in Android applications. IccTA goes beyond state-of-the-art approaches by supporting inter- component detection. By propagating context information among components, IccTA improves the precision of the analysis. IccTA outperforms existing tools on two benchmarks for ICC-leak detectors: DroidBench and ICC-Bench. Moreover, our approach detects 534 ICC leaks in 108 apps from MalGenome and 2,395 ICC leaks in 337 apps in a set of 15,000 Google Play apps.","conference":"IEEE","terms":"Androids;Humanoid robots;Malware;Privacy;Google;Data privacy;Java,Android (operating system);data privacy;Internet;mobile computing,IccTA;intercomponent privacy leak detection;Android application;Google Play;Internet","keywords":"","startPage":"280","endPage":"291","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194581","citationCount":187,"referenceCount":51,"year":2015,"authors":"L. Li; A. Bartel; T. F. Bissyandé; J. Klein; Y. Le Traon; S. Arzt; S. Rasthofer; E. Bodden; D. Octeau; P. McDaniel","affiliations":"NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3556"},"title":"A Large-Scale Technology Evaluation Study: Effects of Model-based Analysis and Testing","abstract":"Besides model-based development, model-based quality assurance and the tighter integration of static and dynamic quality assurance activities are becoming increasingly relevant in the development of software-intensive systems. Thus, this paper reports on an empirical study aimed at investigating the promises regarding quality improvements and cost savings. The evaluation comprises data from 13 industry case studies conducted during a three-year large-scale research project in the transportation domain (automotive, avionics, rail system). During the evaluation, we identified major goals and strategies associated with (integrated) model-based analysis and testing and evaluated the improvements achieved. The aggregated results indicate an average cost reduction of between 29% and 34% for verification and validation and of between 22% and 32% for defect removal. Compared with these cost savings, improvements regarding test coverage (~8%), number of remaining defects (~13%), and time to market (~8%) appear less noticeable.","conference":"IEEE","terms":"Analytical models;Testing;Data models;Context;Software engineering;Software;Atmospheric measurements,program testing;program verification;software cost estimation;software quality,large-scale technology evaluation study;model-based development;model-based quality assurance;static quality assurance activities;dynamic quality assurance activities;software-intensive systems development;quality improvements;cost savings;transportation domain;automotive system;avionics;rail system;model-based analysis;testing;cost reduction;verification;validation;defect removal;test coverage;time to market;software quality assurance","keywords":"Empirical study;embedded software quality assurance;multiple case study;GQM+Strategies;quantitative technology evaluation;model-based testing;internal baselines","startPage":"119","endPage":"128","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202956","citationCount":1,"referenceCount":36,"year":2015,"authors":"M. Kläs; T. Bauer; A. Dereani; T. Söderqvist; P. Helle","affiliations":"Fraunhofer Inst. for Exp. Software Eng., Kaiserslautern, Germany; Fraunhofer Inst. for Exp. Software Eng., Kaiserslautern, Germany; Daimler AG, Sindelfingen, Germany; Adv. Technol. \u0026 Res., Volvo Group Trucks Technol., Gothenburg, Sweden; Airbus Group Innovations, Hamburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3557"},"title":"An Industrial Case Study on the Automated Detection of Performance Regressions in Heterogeneous Environments","abstract":"A key goal of performance testing is the detection of performance degradations (i.e., regressions) compared to previous releases. Prior research has proposed the automation of such analysis through the mining of historical performance data (e.g., CPU and memory usage) from prior test runs. Nevertheless, such research has had limited adoption in practice. Working with a large industrial performance testing lab, we noted that a major hurdle in the adoption of prior work (including our own work) is the incorrect assumption that prior tests are always executed in the same environment (i.e., labs). All too often, tests are performed in heterogenous environments with each test being run in a possibly different lab with different hardware and software configurations. To make automated performance regression analysis techniques work in industry, we propose to model the global expected behaviour of a system as an ensemble (combination) of individual models, one for each successful previous test run (and hence configuration). The ensemble of models of prior test runs are used to flag performance deviations (e.g., CPU counters showing higher usage) in new tests. The deviations are then aggregated using simple voting or more advanced weighting to determine whether the counters really deviate from the expected behaviour or whether it was simply due to an environment-specific variation. Case studies on two open-source systems and a very large scale industrial application show that our weighting approach outperforms a state-of-the-art environment-agnostic approach. Feedback from practitioners who used our approach over a 4 year period (across several major versions) has been very positive.","conference":"IEEE","terms":"Radiation detectors;Association rules;Throughput;Testing;Databases;Bagging,program testing;public domain software;regression analysis;software performance evaluation,automated performance regression detection;heterogeneous environments;performance testing;performance degradation detection;automated performance regression analysis techniques;open-source systems;weighting approach","keywords":"load testing;performance testing;performance analysis;performance regression testing;performance regression","startPage":"159","endPage":"168","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202960","citationCount":14,"referenceCount":37,"year":2015,"authors":"K. C. Foo; Z. M. Jiang; B. Adams; A. E. Hassan; Y. Zou; P. Flora","affiliations":"BlackBerry, Canada; York Univ., Toronto, ON, Canada; Polytech. Montreal, Montreal, QC, Canada; Queen's Univ., Kingston, ON, Canada; Queen's Univ., Kingston, ON, Canada; BlackBerry, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3558"},"title":"Cognitively Sustainable ICT with Ubiquitous Mobile Services - Challenges and Opportunities","abstract":"Information and Communication Technology (ICT) has led to an unprecedented development in almost all areas of human life. It forms the basis for what is called \"the cognitive revolution\" -- a fundamental change in the way we communicate, feel, think and learn based on an extension of individual information processing capacities by communication with other people through technology. This so-called \"extended cognition\" shapes human relations in a radically new way. It is accompanied by a decrease of shared attention and affective presence within closely related groups. This weakens the deepest and most important bonds, that used to shape human identity. Sustainability, both environmental and social (economic, technological, political and cultural) is one of the most important issues of our time. In connection with \"extended cognition\" we have identified a new, basic type of social sustainability that everyone takes for granted, and which we claim is in danger due to our changed ways of communication. We base our conclusion on a detailed analysis of the current state of the practice and observed trends. The contribution of our article consists of identifying cognitive sustainability and explaining its central role for all other aspects of sustainability, showing how it relates to the cognitive revolution, its opportunities and challenges. Complex social structures with different degrees of proximity have always functioned as mechanisms behind belongingness and identity. To create a long-term cognitive sustainability, we need to rethink and design new communication technologies that support differentiated and complex social relationships.","conference":"IEEE","terms":"Cognition;Games;Mobile communication;Mobile handsets;Software engineering;Computers,cognition;cultural aspects;environmental economics;mobile computing;politics;social sciences computing;socio-economic effects;sustainable development,cognitively sustainable ICT;ubiquitous mobile services;information and communication technology;cognitive revolution;information processing capacities;extended cognition;shared attention;affective presence;closely-related groups;human identity;environmental sustainability;social sustainability;economic sustainability;technological sustainability;political sustainability;cultural sustainability;cognitive sustainability;complex social structures;proximity degrees;differentiated relationships","keywords":"Cognitive sustainability;Social sustainability;Sustainable ICT;Cognitive revolution;Privacy;Shared attention;Social cognition;Software engineering for social good.","startPage":"531","endPage":"540","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203007","citationCount":3,"referenceCount":54,"year":2015,"authors":"M. Jägemar; G. Dodig-Crnkovic","affiliations":"Sch. of Innovation, Design \u0026 Eng., Malardalen Univ., Vasteras, Sweden; Dept. of Appl. Inf. Technol., Chalmers Univ. of Technol., Gothenburg, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3559"},"title":"Teaching Software Architecture to Undergraduate Students: An Experience Report","abstract":"Software architecture lies at the heart of system thinking skills for software. Teaching software architecture requires contending with the problem of how to make the learning realistic -- most systems which students can learn quickly are too simple for them to express architectural issues. We address here the ten years' history of teaching an undergraduate software architecture course, as a part of a bachelor's program in software engineering. Included are descriptions of what we perceive the realistic goals to be, of teaching software architecture at this level. We go on to analyze the successes and issues of various approaches we have taken over the years. We finish with recommendations for others who teach this same subject, either as a standalone undergraduate course or integrated into a software engineering course.","conference":"IEEE","terms":"Software architecture;Computer architecture;Software;History;Training,computer science education;educational courses;further education;software architecture;teaching,undergraduate students;undergraduate software architecture course teaching;software engineering course","keywords":"Software Architecture;Project-Based Learning;Course Evolution","startPage":"445","endPage":"454","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202995","citationCount":8,"referenceCount":18,"year":2015,"authors":"C. R. Rupakheti; S. V. Chenoweth","affiliations":"Dept. of Comput. Sci. \u0026 Software Eng., Rose-Hulman Inst. of Technol., Terre Haute, IN, USA; Dept. of Comput. Sci. \u0026 Software Eng., Rose-Hulman Inst. of Technol., Terre Haute, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d355a"},"title":"Are Students Representatives of Professionals in Software Engineering Experiments?","abstract":"Background: Most of the experiments in software engineering (SE) employ students as subjects. This raises concerns about the realism of the results acquired through students and adaptability of the results to software industry. Aim: We compare students and professionals to understand how well students represent professionals as experimental subjects in SE research. Method: The comparison was made in the context of two test-driven development experiments conducted with students in an academic setting and with professionals in a software organization. We measured the code quality of several tasks implemented by both subject groups and checked whether students and professionals perform similarly in terms of code quality metrics. Results: Except for minor differences, neither of the subject groups is better than the other. Professionals produce larger, yet less complex, methods when they use their traditional development approach, whereas both subject groups perform similarly when they apply a new approach for the first time. Conclusion: Given a carefully scoped experiment on a development approach that is new to both students and professionals, similar performances are observed. Further investigation is necessary to analyze the effects of subject demographics and level of experience on the results of SE experiments.","conference":"IEEE","terms":"Measurement;Industries;Software;Context;Training;Inspection;Complexity theory,computer science education;DP industry;software engineering,student;software engineering;SE;software industry;test-driven development experiment;software organization;code quality metrics","keywords":"experimentation;empirical study;test-driven development;code quality","startPage":"666","endPage":"676","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194615","citationCount":59,"referenceCount":30,"year":2015,"authors":"I. Salman; A. T. Misirli; N. Juristo","affiliations":"Dept. of Inf. Process. Sci., Univ. of Oulu, Oulu, Finland; Fac. of Comput. \u0026 Inf., Istanbul Tech. Univ., Istanbul, Turkey; Dept. of Inf. Process. Sci., Univ. of Oulu, Oulu, Finland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d355b"},"title":"A Synergistic Analysis Method for Explaining Failed Regression Tests","abstract":"We propose a new automated debugging method for regression testing based on a synergistic application of both dynamic and semantic analysis. Our method takes a failure- inducing test input, a buggy program, and an earlier correct version of the same program, and computes a minimal set of code changes responsible for the failure, as well as explaining how the code changes lead to the failure. Although this problem has been the subject of intensive research in recent years, existing methods are rarely adopted by developers in practice since they do not produce sufficiently accurate fault explanations for real applications. Our new method is significantly faster and more accurate than existing methods for explaining failed regression tests in real applications, due to its synergistic analysis framework that iteratively applies both dynamic analysis and a constraint solver based semantic analysis to leverage their complementary strengths. We have implemented our new method in a software tool based on the LLVMcompiler and the KLEE symbolic virtual machine. Our experiments on large real Linux applications show that the new method is both efficient and effective in practice.","conference":"IEEE","terms":"Semantics;Algorithm design and analysis;Generators;Debugging;Heuristic algorithms;Reactive power;Testing,Linux;program debugging;program testing;software tools,synergistic analysis method;regression testing failure;automated debugging method;dynamic analysis;semantic analysis;software tool;LLVM compiler;KLEE symbolic virtual machine;Linux application","keywords":"fault localization;error diagnosis;regression testing;delta debugging;weakest precondition;SMT solver","startPage":"257","endPage":"267","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194579","citationCount":5,"referenceCount":38,"year":2015,"authors":"Q. Yi; Z. Yang; J. Liu; C. Zhao; C. Wang","affiliations":"Inst. of Software, Beijing, China; Dept. of Comput. Sci., Western Michigan Univ., Kalamazoo, MI, USA; Inst. of Software, Beijing, China; Inst. of Software, Beijing, China; Dept. of Electr. \u0026 Comput. Eng., Virginia Tech, Blacksburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d355c"},"title":"AppContext: Differentiating Malicious and Benign Mobile App Behaviors Using Context","abstract":"Mobile malware attempts to evade detection during app analysis by mimicking security-sensitive behaviors of benign apps that provide similar functionality (e.g., sending SMS messages), and suppressing their payload to reduce the chance of being observed (e.g., executing only its payload at night). Since current approaches focus their analyses on the types of security-sensitive resources being accessed (e.g., network), these evasive techniques in malware make differentiating between malicious and benign app behaviors a difficult task during app analysis. We propose that the malicious and benign behaviors within apps can be differentiated based on the contexts that trigger security-sensitive behaviors, i.e., the events and conditions that cause the security-sensitive behaviors to occur. In this work, we introduce AppContext, an approach of static program analysis that extracts the contexts of security-sensitive behaviors to assist app analysis in differentiating between malicious and benign behaviors. We implement a prototype of AppContext and evaluate AppContext on 202 malicious apps from various malware datasets, and 633 benign apps from the Google Play Store. AppContext correctly identifies 192 malicious apps with 87.7% precision and 95% recall. Our evaluation results suggest that the maliciousness of a security-sensitive behavior is more closely related to the intention of the behavior (reflected via contexts) than the type of the security-sensitive resources that the behavior accesses.","conference":"IEEE","terms":"Context;Malware;Androids;Humanoid robots;Mobile communication;Electrocardiography;Payloads,invasive software;mobile computing;program diagnostics,AppContext;malicious mobile app behavior;benign mobile app behavior;mobile malware;app analysis;security-sensitive behaviors;static program analysis;Google Play Store","keywords":"Mobile Security; Context; Program Analysis","startPage":"303","endPage":"313","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194583","citationCount":65,"referenceCount":51,"year":2015,"authors":"W. Yang; X. Xiao; B. Andow; S. Li; T. Xie; W. Enck","affiliations":"Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; NEC Labs. America, Princeton, NJ, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d355d"},"title":"Improving Predictability, Efficiency and Trust of Model-Based Proof Activity","abstract":"We report on our industrial experience in using formal methods for the analysis of safety-critical systems developed in a model-based design framework. We first highlight the formal proof workflow devised for the verification and validation of embedded systems developed in Matlab/Simulink. In particular, we show that there is a need to: determine the compatibility of the model to be analysed with the proof engine, establish whether the model facilitates proof convergence or when optimisation is required, and avoid over-specification when specifying the hypotheses constraining the inputs of the model during analysis. We also stress on the importance of having a certain harness over the proof activity and present a set of tools we developed to achieve this purpose. Finally, we give a list of best practices, methods and any necessary tools aiming at guaranteeing the validity of the verification results obtained.","conference":"IEEE","terms":"Analytical models;Safety;Mathematical model;Software packages;Computational modeling;Convergence;Complexity theory,formal specification;program verification;safety-critical software,predictability improvement;efficiency improvement;trust improvement;model-based proof activity;formal methods;safety-critical system analysis;model-based design framework;formal proof workflow;embedded system verification;embedded system validation;Matlab/Simulink;model compatibility analysis;proof engine;proof convergence","keywords":"Verification and Validation;Model-Based Design;Functional Hazard Analysis (FHA);Model Checking;Matlab/Simulink","startPage":"139","endPage":"148","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202958","citationCount":0,"referenceCount":15,"year":2015,"authors":"J. Etienne; M. Maarek; F. Anseaume; V. Delebarre","affiliations":"SafeRiver, Montrouge, France; SafeRiver, Montrouge, France; SafeRiver, Montrouge, France; SafeRiver, Montrouge, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d355e"},"title":"A Case Study in Locating the Architectural Roots of Technical Debt","abstract":"Our recent research has shown that, in large-scale software systems, defective files seldom exist alone. They are usually architecturally connected, and their architectural structures exhibit significant design flaws which propagate bugginess among files. We call these flawed structures the architecture roots, a type of technical debt that incurs high maintenance penalties. Removing the architecture roots of bugginess requires refactoring, but the benefits of refactoring have historically been difficult for architects to quantify or justify. In this paper, we present a case study of identifying and quantifying such architecture debts in a large-scale industrial software project. Our approach is to model and analyze software architecture as a set of design rule spaces (DRSpaces). Using data extracted from the project's development artifacts, we were able to identify the files implicated in architecture flaws and suggest refactorings based on removing these flaws. Then we built economic models of the before and (predicted) after states, which gave the organization confidence that doing the refactorings made business sense, in terms of a handsome return on investment.","conference":"IEEE","terms":"Computer architecture;Software engineering;Business;History;Sonar detection;Microprocessors,software architecture;software maintenance,technical debt;large-scale software systems;architecture roots;software refactoring;large-scale industrial software project;software architecture flaws;design rule spaces;DRSpaces","keywords":"","startPage":"179","endPage":"188","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202962","citationCount":27,"referenceCount":33,"year":2015,"authors":"R. Kazman; Y. Cai; R. Mo; Q. Feng; L. Xiao; S. Haziyev; V. Fedak; A. Shapochka","affiliations":"SEU/CMU, Univ. of Hawaii, Honolulu, HI, USA; Drexel Univ., Philadelphia, PA, USA; Drexel Univ., Philadelphia, PA, USA; Drexel Univ., Philadelphia, PA, USA; Drexel Univ., Philadelphia, PA, USA; SoftServe Inc., Lviv, Ukraine; SoftServe Inc., Lviv, Ukraine; SoftServe Inc., Lviv, Ukraine","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d355f"},"title":"Virtual Reality in Software Engineering: Affordances, Applications, and Challenges","abstract":"Software engineers primarily interact with source code using a keyboard and mouse, and typically view software on a small number of 2D monitors. This interaction paradigm does not take advantage of many affordances of natural human movement and perception. Virtual reality (VR) can use these affordances more fully than existing developer environments to enable new creative opportunities and potentially result in higher productivity, lower learning curves, and increased user satisfaction. This paper describes the affordances offered by VR, demonstrates the benefits of VR and software engineering in prototypes for live coding and code review, and discusses future work, open questions, and the challenges of VR.","conference":"IEEE","terms":"Three-dimensional displays;Software;Navigation;Software engineering;Cognition;Keyboards;Encoding,software engineering;software reviews;source code (software);virtual reality,virtual reality;VR;software engineering;source code;live coding;code review","keywords":"virtual reality;software engineering;live coding;code review","startPage":"547","endPage":"550","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203009","citationCount":17,"referenceCount":21,"year":2015,"authors":"A. Elliott; B. Peiris; C. Parnin","affiliations":"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3560"},"title":"A Vision of Crowd Development","abstract":"Crowdsourcing has had extraordinary success in solving a diverse set of problems, ranging from digitization of libraries and translation of the Internet, to scientific challenges such as classifying elements in the galaxy or determining the 3D shape of an enzyme. By leveraging the power of the masses, it is feasible to complete tasks in mere days and sometimes even hours, and to take on tasks that were previously impossible because of their sheer scale. Underlying the success of crowdsourcing is a common theme - the microtask. By breaking down the overall task at hand into microtasks providing short, self-contained pieces of work, work can be performed independently, quickly, and in parallel - enabling numerous and often untrained participants to chip in. This paper puts forth a research agenda, examining the question of whether the same kinds of successes that microtask crowdsourcing is having in revolutionizing other domains can be brought to software development. That is, we ask whether it is possible to push well beyond the open source paradigm, which still relies on traditional, coarse-grained tasks, to a model in which programming proceeds through microtasks performed by vast numbers of crowd developers.","conference":"IEEE","terms":"Crowdsourcing;Games;Context;Libraries;Programming;Open source software,Internet;public domain software;software engineering,crowd development vision;digital libraries;3D shape;Internet;microtask crowdsourcing;software development;open source paradigm;coarse-grained tasks","keywords":"crowdsourcing;collaborative software development;open source software development","startPage":"563","endPage":"566","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203013","citationCount":12,"referenceCount":20,"year":2015,"authors":"T. D. LaToza; A. v. d. Hoek","affiliations":"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3561"},"title":"Sustainability Design and Software: The Karlskrona Manifesto","abstract":"Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems.The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.","conference":"IEEE","terms":"Conferences;Software engineering;Software systems;Economics;Meteorology;History,software maintenance,sustainability design;Karlskrona manifesto;ecological systems;social systems;software engineering community;software-intensive systems;collaborative reflective writing process","keywords":"Sustainability;sustainability design;systems thinking;software engineering;long-term thinking;environmental sustainability;social sustainability;economic sustainability;societal sustainability;technical sustainability;ethics","startPage":"467","endPage":"476","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202997","citationCount":45,"referenceCount":52,"year":2015,"authors":"C. Becker; R. Chitchyan; L. Duboc; S. Easterbrook; B. Penzenstadler; N. Seyff; C. C. Venters","affiliations":"Fac. of Inf., Univ. of Toronto, Toronto, ON, Canada; Dept. of Comput. Sci., Univ. of Leicester, Leicester, UK; Dept. of Inf. \u0026 Comput. Sci., State Univ. of Rio de Janeiro, Rio de Janeiro, Brazil; Dept. of Comput. Sci., Univ. of Toronto, Toronto, ON, Canada; Inst. for Software Res., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Sch. of Comput. \u0026 Eng., Univ. of Huddersfield, Huddersfield, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3562"},"title":"Stuck and Frustrated or in Flow and Happy: Sensing Developers' Emotions and Progress","abstract":"Software developers working on change tasks commonly experience a broad range of emotions, ranging from happiness all the way to frustration and anger. Research, primarily in psychology, has shown that for certain kinds of tasks, emotions correlate with progress and that biometric measures, such as electro-dermal activity and electroencephalography data, might be used to distinguish between emotions. In our research, we are building on this work and investigate developers' emotions, progress and the use of biometric measures to classify them in the context of software change tasks. We conducted a lab study with 17 participants working on two change tasks each. Participants were wearing three biometric sensors and had to periodically assess their emotions and progress. The results show that the wide range of emotions experienced by developers is correlated with their perceived progress on the change tasks. Our analysis also shows that we can build a classifier to distinguish between positive and negative emotions in 71.36% and between low and high progress in 67.70% of all cases. These results open up opportunities for improving a developer's productivity. For instance, one could use such a classifier for providing recommendations at opportune moments when a developer is stuck and making no progress.","conference":"IEEE","terms":"Biosensors;Software;Atmospheric measurements;Particle measurements;Electroencephalography;Software engineering;Psychology,behavioural sciences computing;industrial psychology;signal classification;software development management,software developer emotions;software developer progress;psychology;biometric measures;electro-dermal activity;electroencephalography data;software change tasks;pattern classifier","keywords":"","startPage":"688","endPage":"699","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194617","citationCount":43,"referenceCount":69,"year":2015,"authors":"S. C. Müller; T. Fritz","affiliations":"Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3563"},"title":"Specifying Event-Based Systems with a Counting Fluent Temporal Logic","abstract":"Fluent linear temporal logic is a formalism for specifying properties of event-based systems, based on propositions called fluents, defined in terms of activating and deactivating events. In this paper, we propose complementing the notion of fluent by the related concept of counting fluent. As opposed to the boolean nature of fluents, counting fluents are numerical values, that enumerate event occurrences, and allow us to specify naturally some properties of reactive systems. Although by extending fluent linear temporal logic with counting fluents we obtain an undecidable, strictly more expressive formalism, we develop a sound (but incomplete) model checking approach for the logic, that reduces to traditional temporal logic model checking, and allows us to automatically analyse properties involving counting fluents, on finite event-based systems. Our experiments, based on relevant models taken from the literature, show that: (i) counting fluent temporal logic is better suited than traditional temporal logic for expressing properties in which the number of occurrences of certain events is relevant, and (ii) our model checking approach on counting fluent specifications is more efficient and scales better than model checking equivalent fluent temporal logic specifications.","conference":"IEEE","terms":"Bridges;Model checking;Monitoring;Safety;Software;Mechanical factors;Analytical models,formal specification;formal verification;temporal logic,event-based systems specification;counting fluent temporal logic;fluent linear temporal logic;event activation;event deactivation;counting fluent concept;reactive systems;model checking approach;temporal logic model checking;finite event-based systems;counting fluent specifications","keywords":"Model checking;Linear Temporal Logics;Event-Based System Analysis","startPage":"733","endPage":"743","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194621","citationCount":2,"referenceCount":34,"year":2015,"authors":"G. Regis; R. Degiovanni; N. D'Ippolito; N. Aguirre","affiliations":"Dept. de Comput., Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Dept. de Comput., Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Dept. de Comput., Univ. de Buenos Aires, Buenos Aires, Argentina; Dept. de Comput., Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3564"},"title":"Detecting Inconsistencies in JavaScript MVC Applications","abstract":"Higher demands for more reliable and maintainable JavaScript-based web applications have led to the recent development of MVC (Model-View-Controller) frameworks. One of the main advantages of using these frameworks is that they abstract out DOM API method calls, which are one of the leading causes of web application faults, due to their often complicated interaction patterns. However, MVC frameworks are susceptible to inconsistencies between the identifiers and types of variables and functions used throughout the application. In response to this problem, we introduce a formal consistency model for web applications made using MVC frameworks. We propose an approach -- called Aurebesh -- that automatically detects inconsistencies in such applications. We evaluate Aurebesh by conducting a fault injection experiment and by running it on real applications. Our results show that Aurebesh is accurate, with an overall recall of 96.1% and a precision of 100%. It is also useful in detecting bugs, allowing us to find 15 real-world bugs in applications built on Angular JS, a popular MVC framework.","conference":"IEEE","terms":"HTML;Data models;Motion pictures;Computer bugs;Detectors;Analytical models;Reliability,Internet;Java;program debugging;software fault tolerance,MVC applications;JavaScript-based Web applications;model-view-controller frameworks;formal consistency model;AUREBESH;fault injection;bugs detection;AngularJS;inconsistencies detection","keywords":"","startPage":"325","endPage":"335","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194585","citationCount":11,"referenceCount":35,"year":2015,"authors":"F. S. Ocariza; K. Pattabiraman; A. Mesbah","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3565"},"title":"Approximating Attack Surfaces with Stack Traces","abstract":"Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.","conference":"IEEE","terms":"Computer crashes;Security;Measurement;Approximation methods;Software;Predictive models;Surface treatment,decision making;program diagnostics;project management;software engineering,attack surface approximation;security testing;effort reviewing;software projects;vulnerable code identification;decision-making;software development;stack trace analysis;attack surface measurement;Windows 8","keywords":"stack traces;security;vulnerability;models;testing;reliability;attack surface","startPage":"199","endPage":"208","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964","citationCount":17,"referenceCount":44,"year":2015,"authors":"C. Theisen; K. Herzig; P. Morrison; B. Murphy; L. Williams","affiliations":"Dept. of Comput. Sci., NCSU, Raleigh, NC, USA; Microsoft Res., Cambridge, UK; Dept. of Comput. Sci., NCSU, Raleigh, NC, USA; Microsoft Res., Cambridge, UK; Dept. of Comput. Sci., NCSU, Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3566"},"title":"The Role of Design Thinking and Physical Prototyping in Social Software Engineering","abstract":"Social Software Engineering (Social SE), that is SE aiming to promote positive social change, is a rapidly emerging area. Here, software and digital artefacts are seen as tools for social change, rather than end products or 'solutions'. Moreover, Social SE requires a sustained buy-in from a range of stakeholders and end-users working in partnership with multidisciplinary software development teams often at a distance. This context poses new challenges to software engineering: it requires both an agile approach for handling uncertainties in the software development process, and the application of participatory, creative design processes to bridge the knowledge asymmetries and the geographical distances in the partnership. This paper argues for the role of design thinking in Social SE and highlights its implications for software engineering in general. It does so by reporting on the contributions that design thinking - and in particular physical design - has brought to (1) the problem space definition, (2) user requirements capture and (3) system feature design of a renewable energy forecasting system developed in partnership with a remote Scottish Island community.","conference":"IEEE","terms":"Conferences;Prototypes;Software;Software engineering;Context;Games;Space exploration,load forecasting;power engineering computing;renewable energy sources;software prototyping,design thinking;physical prototyping;social software engineering;social SE;sustained buy-in;multidisciplinary software development teams;agile approach;creative design processes;knowledge asymmetries;geographical distances;physical design;problem space definition;user requirements capture;system feature design;renewable energy forecasting system;remote Scottish island community","keywords":"","startPage":"487","endPage":"496","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202999","citationCount":8,"referenceCount":41,"year":2015,"authors":"P. Newman; M. A. Ferrario; W. Simm; S. Forshaw; A. Friday; J. Whittle","affiliations":"Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Manage. Sch., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; LICA, Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3567"},"title":"Efficient Scalable Verification of LTL Specifications","abstract":"Linear Temporal Logic (LTL) has been used in computer science for decades to formally specify programs, systems, desired properties, and relevant behaviors. This paper presents a novel, efficient technique for verifying LTL specifications in a fully automated way. Our technique belongs to the category of Bounded Satisfiability Checking approaches, where LTL formulae are encoded as formulae of another decidable logic that can be solved through modern satisfiability solvers. The target logic in our approach is Bit-Vector Logic. We present our novel encoding, show its correctness, and experimentally compare it against existing encodings implemented in well-known formal verification tools.","conference":"IEEE","terms":"Encoding;Semantics;Unified modeling language;Model checking;Systematics;Analytical models,formal verification;temporal logic,scalable verification;LTL specification;linear temporal logic;computer science;bounded satisfiability checking approach;LTL formulae;target logic;bit-vector logic;formal verification tool","keywords":"Formal Methods;Temporal Logic;Bounded Model Checking;Bit-Vector Logic","startPage":"711","endPage":"721","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194619","citationCount":5,"referenceCount":28,"year":2015,"authors":"L. Baresi; M. M. Pourhashem Kallehbasti; M. Rossi","affiliations":"Dipt. di Elettron., Inf. e Bioingegneria, Politec. di Milano, Milan, Italy; Dipt. di Elettron., Inf. e Bioingegneria, Politec. di Milano, Milan, Italy; Dipt. di Elettron., Inf. e Bioingegneria, Politec. di Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3568"},"title":"Assert Use in GitHub Projects","abstract":"Asserts have long been a strongly recommended (if non-functional) adjunct to programs. They certainly don't add any user-evident feature value; and it can take quite some skill and effort to devise and add useful asserts. However, they are believed to add considerable value to the developer. Certainly, they can help with automated verification; but even in the absence of that, claimed advantages include improved understandability, maintainability, easier fault localization and diagnosis, all eventually leading to better software quality. We focus on this latter claim, and use a large dataset of asserts in C and C++ programs to explore the connection between asserts and defect occurrence. Our data suggests a connection: functions with asserts do have significantly fewer defects. This indicates that asserts do play an important role in software quality; we therefore explored further the factors that play a role in assertion placement: specifically, process factors (such as developer experience and ownership) and product factors, particularly interprocedural factors, exploring how the placement of assertions in functions are influenced by local and global network properties of the callgraph. Finally, we also conduct a differential analysis of assertion use across different application domains.","conference":"IEEE","terms":"Software;History;Computer bugs;Java;Runtime;Collaboration;Gain measurement,formal verification;software quality,GitHub projects;assert use;automated verification;C program;C++ program;asserts occurrence;defect occurrence;software quality;assertion placement;process factors;product factors;interprocedural factors;callgraph local network property;callgraph global network property;differential analysis;application domain","keywords":"Assertions;GitHub;Defects","startPage":"755","endPage":"766","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194623","citationCount":13,"referenceCount":50,"year":2015,"authors":"C. Casalnuovo; P. Devanbu; A. Oliveira; V. Filkov; B. Ray","affiliations":"Comput. Sci. Dept., Univ. of California, Davis, Davis, CA, USA; Comput. Sci. Dept., Univ. of California, Davis, Davis, CA, USA; Comput. Sci. Dept., Univ. of California, Davis, Davis, CA, USA; Comput. Sci. Dept., Univ. of California, Davis, Davis, CA, USA; Comput. Sci. Dept., Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3569"},"title":"How Much Up-Front? A Grounded theory of Agile Architecture","abstract":"The tension between software architecture and agility is not well understood by agile practitioners or researchers. If an agile software team spends too little time designing architecture up-front then the team faces increased risk and higher chance of failure, if the team spends too much time the delivery of value to the customer is delayed, and responding to change can become extremely difficult. This paper presents a grounded theory of agile architecture that describes how agile software teams answer the question of how much upfront architecture design effort is enough. This theory, based on grounded theory research involving 44 participants, presents six forces that affect the team's context and five strategies that teams use to help them determine how much effort they should put into up-front design.","conference":"IEEE","terms":"Computer architecture;Scrum (Software development);Planning;Software;Complexity theory;Business;Interviews,software architecture;software prototyping;team working,grounded theory;agile architecture;software architecture;software agility;agile software teams;upfront architecture design effort","keywords":"","startPage":"347","endPage":"357","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194587","citationCount":10,"referenceCount":54,"year":2015,"authors":"M. Waterman; J. Noble; G. Allan","affiliations":"Specialised Archit. Services Ltd., Wellington, New Zealand; Sch. of Eng. \u0026 Comput. Sci., Victoria Univ. of Wellington, Wellington, New Zealand; Sch. of Eng. \u0026 Comput. Sci., Victoria Univ. of Wellington, Wellington, New Zealand","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d356a"},"title":"Does Automated Refactoring Obviate Systematic Editing?","abstract":"When developers add features and fix bugs, they often make systematic edits-similar edits to multiple locations. Systematic edits may indicate that developers should instead refactor to eliminate redundancy. This paper explores this question by designing and implementing a fully automated refactoring tool called RASE, which performs clone removal. RASE (1) extracts common code guided by a systematic edit; (2) creates new types and methods as needed; (3) parameterizes differences in types, methods, variables, and expressions; and (4) inserts return objects and exit labels based on control and data flow. To our knowledge, this functionality makes RASE the most advanced refactoring tool for automated clone removal. We evaluate RASE with real-world systematic edits and compare to method based clone removal. RASE successfully performs clone removal in 30 of 56 method pairs (n=2) and 20 of 30 method groups (n≥3) with systematic edits. We find that scoping refactoring based on systematic edits (58%), rather than the entire method (33%), increases the applicability of automated clone removal. Automated refactoring is not feasible in the other 42% cases, which indicates that automated refactoring does not obviate the need for systematic editing.","conference":"IEEE","terms":"Systematics;Cloning;Concrete;Data mining;Software;Merging;Context,data flow computing;software maintenance;software tools,systematic editing;automated refactoring tool;RASE;data flow;automated clone removal;scoping refactoring","keywords":"","startPage":"392","endPage":"402","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194591","citationCount":5,"referenceCount":34,"year":2015,"authors":"Na Meng; L. Hua; Miryung Kim; K. S. McKinley","affiliations":"Univ. of Texas at Austin, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d356b"},"title":"\"Should We Move to Stack Overflow?\" Measuring the Utility of Social Media for Developer Support","abstract":"Stack Overflow is an enormously popular question-and-answer web site intended for software developers to help each other with programming issues. Some software projects aimed at developers (for example, application programming interfaces, application engines, cloud services, development frameworks, and the like) are closing their self-supported developer discussion forums and mailing lists and instead directing developers to use special-purpose tags on Stack Overflow. The goals of this paper are to document the main reasons given for moving developer support to Stack Overflow, and then to collect and analyze data from a group of software projects that have done this, in order to show whether the expected quality of support was actually achieved. The analysis shows that for all four software projects in this study, two of the desired quality indicators, developer participation and response time, did show improvements on Stack Overflow as compared to mailing lists and forums. However, we also found several projects that moved back from Stack Overflow, despite achieving these desired improvements. The results of this study are applicable to a wide variety of software projects that provide developer support using social media.","conference":"IEEE","terms":"Software;Google;Time factors;Media;Documentation;Message systems;Software engineering,social networking (online);software engineering,stack overflow;social media;developer support;question-and-answer Web site;software developers;software projects;self-supported developer discussion forums;mailing lists;special-purpose tags;support quality","keywords":"developer support;technical support;quality;Stack Overflow;mailing list;forums;metrics;social media","startPage":"219","endPage":"228","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202966","citationCount":18,"referenceCount":18,"year":2015,"authors":"M. Squire","affiliations":"Dept. of Comput. Sci., Elon Univ., Elon, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d356c"},"title":"Drawing Insight from Student Perceptions of Reflective Design Learning","abstract":"While design and designing are core elements in computer science and software engineering, conventional curricular structures do not adequately support design learning. Current methods tend to isolate the study of design within specific subject matter and lack a strong emphasis on reflection. This paper reports on insights and lessons learned from a user study in the context of ongoing work on developing an educational intervention that better supports design learning with a particular emphasis on learner-driven reflection. Insights drawn from this study relate to general aspects of design learning, such as the importance of collaborative reflection and the impact of learner perceptions regarding their abilities, as well as to specific improvements to our approach.","conference":"IEEE","terms":"Context;Marine vehicles;Computer science;Software engineering;Training;Joints;Software,computer science education;software engineering,student perceptions;reflective design learning;computer science;software engineering;curricular structures;educational intervention;learner-driven reflection;collaborative reflection","keywords":"software engineering education;software design;design learning;reflection;structured reflection","startPage":"253","endPage":"262","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202970","citationCount":1,"referenceCount":15,"year":2015,"authors":"T. V. Wilkins; J. C. Georgas","affiliations":"Dept. of Electr. Eng. \u0026 Comput. Sci., Northern Arizona Univ., Flagstaff, AZ, USA; Dept. of Electr. Eng. \u0026 Comput. Sci., Northern Arizona Univ., Flagstaff, AZ, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d356d"},"title":"Inferring Behavioral Specifications from Large-scale Repositories by Leveraging Collective Intelligence","abstract":"Despite their proven benefits, useful, comprehensible, and efficiently checkable specifications are not widely available. This is primarily because writing useful, non-trivial specifications from scratch is too hard, time consuming, and requires expertise that is not broadly available. Furthermore, the lack of specifications for widely-used libraries and frameworks, caused by the high cost of writing specifications, tends to have a snowball effect. Core libraries lack specifications, which makes specifying applications that use them expensive. To contain the skyrocketing development and maintenance costs of high assurance systems, this self-perpetuating cycle must be broken. The labor cost of specifying programs can be significantly decreased via advances in specification inference and synthesis, and this has been attempted several times, but with limited success. We believe that practical specification inference and synthesis is an idea whose time has come. Fundamental breakthroughs in this area can be achieved by leveraging the collective intelligence available in software artifacts from millions of open source projects. Fine-grained access to such data sets has been unprecedented, but is now easily available. We identify research directions and report our preliminary results on advances in specification inference that can be had by using such data sets to infer specifications.","conference":"IEEE","terms":"Data mining;Software;Software engineering;History;Writing;Libraries;Maintenance engineering,formal specification;public domain software,behavioral specification;collective intelligence;specification writing;core libraries;program specification;specification inference;specification synthesis;software artifacts;open source projects","keywords":"","startPage":"579","endPage":"582","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203017","citationCount":2,"referenceCount":39,"year":2015,"authors":"H. Rajan; T. N. Nguyen; G. T. Leavens; R. Dyer","affiliations":"Iowa State Univ., Ames, IA, USA; Iowa State Univ., Ames, IA, USA; Univ. of Central Florida, Orlando, FL, USA; Bowling Green State Univ., Bowling Green, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d356e"},"title":"On Architectural Diversity of Dynamic Adaptive Systems","abstract":"We introduce a novel concept of ``architecture diversity'' for adaptive systems and posit that increased diversity has an inverse correlation with adaptation costs. We propose an index to quantify diversity and a static method to estimate the adaptation cost, and conduct an initial experiment on an exemplar cloud-based system which reveals the posited correlation.","conference":"IEEE","terms":"Computer architecture;Diversity reception;Context;Indexes;Routing;Software;Adaptive systems,cloud computing;software architecture,dynamic adaptive systems;architectural diversity;adaptation cost estimation;exemplar cloud-based system","keywords":"software architecture;software diversity;self-adaptive systems;constraint solving","startPage":"595","endPage":"598","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203021","citationCount":1,"referenceCount":8,"year":2015,"authors":"H. Song; A. Elgammal; V. Nallur; F. Chauvel; F. Fleurey; S. Clarke","affiliations":"SINTEF ICT, Oslo, Norway; Distrib. Syst. Group, Trinity Coll. Dublin, Dublin, Ireland; Distrib. Syst. Group, Trinity Coll. Dublin, Dublin, Ireland; SINTEF ICT, Oslo, Norway; SINTEF ICT, Oslo, Norway; Distrib. Syst. Group, Trinity Coll. Dublin, Dublin, Ireland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d356f"},"title":"A Comparative Study of Programming Languages in Rosetta Code","abstract":"Sometimes debates on programming languages are more religious than scientific. Questions about which language is more succinct or efficient, or makes developers more productive are discussed with fervor, and their answers are too often based on anecdotes and unsubstantiated beliefs. In this study, we use the largely untapped research potential of Rosetta Code, a code repository of solutions to common programming tasks in various languages, which offers a large data set for analysis. Our study is based on 7'087 solution programs corresponding to 745 tasks in 8 widely used languages representing the major programming paradigms (procedural: C and Go, object-oriented: C# and Java, functional: F# and Haskell, scripting: Python and Ruby). Our statistical analysis reveals, most notably, that: functional and scripting languages are more concise than procedural and object-oriented languages, C is hard to beat when it comes to raw speed on large inputs, but performance differences over inputs of moderate size are less pronounced and allow even interpreted languages to be competitive, compiled strongly-typed languages, where more defects can be caught at compile time, are less prone to runtime failures than interpreted or weakly-typed languages. We discuss implications of these results for developers, language designers, and educators.","conference":"IEEE","terms":"Java;Programming;Indexes;Statistical analysis;Runtime;Standards,authoring languages;functional languages;Java;object-oriented programming;program compilers;program interpreters;statistical analysis,programming languages;Rosetta Code;code repository;Go;C#;Java;F#;Haskell;Python;Ruby;statistical analysis;functional language;scripting language;procedural language;object-oriented language;language interpretation;strongly-typed language compilation","keywords":"","startPage":"778","endPage":"788","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194625","citationCount":25,"referenceCount":32,"year":2015,"authors":"S. Nanz; C. A. Furia","affiliations":"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3570"},"title":"Build It Yourself! Homegrown Tools in a Large Software Company","abstract":"Developers sometimes take the initiative to build toolsto solve problems they face. What motivates developers to buildthese tools? What is the value for a company? Are the tools builtuseful for anyone besides their creator? We conducted a qualitativestudy of tool building, adoption, and impact within Microsoft. Thispaper presents our findings on the extrinsic and intrinsic factorslinked to toolbuilding, the value of building tools, and the factorsassociated with tool spread. We find that the majority of developersbuild tools. While most tools never spread beyond their creator'steam, most have more than one user, and many have more than onecollaborator. Organizational cultures that are receptive towardstoolbuilding produce more tools, and more collaboration on tools.When nurtured and spread, homegrown tools have the potential tocreate significant impact on organizations.","conference":"IEEE","terms":"Interviews;Testing;Software;Buildings;Automation;Visualization;Monitoring,project management;software development management;software tools,software development;software company;software tools;tool building;tool adoption;tool impact;organizational cultures","keywords":"Homegrown Tools","startPage":"369","endPage":"379","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194589","citationCount":6,"referenceCount":27,"year":2015,"authors":"E. K. Smith; C. Bird; T. Zimmermann","affiliations":"Sch. of Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3571"},"title":"Learning to Log: Helping Developers Make Informed Logging Decisions","abstract":"Logging is a common programming practice of practical importance to collect system runtime information for postmortem analysis. Strategic logging placement is desired to cover necessary runtime information without incurring unintended consequences (e.g., Performance overhead, trivial logs). However, in current practice, there is a lack of rigorous specifications for developers to govern their logging behaviours. Logging has become an important yet tough decision which mostly depends on the domain knowledge of developers. To reduce the effort on making logging decisions, in this paper, we propose a \"learning to log\" framework, which aims to provide informative guidance on logging during development. As a proof of concept, we provide the design and implementation of a logging suggestion tool, Log Advisor, which automatically learns the common logging practices on where to log from existing logging instances and further leverages them for actionable suggestions to developers. Specifically, we identify the important factors for determining where to log and extract them as structural features, textual features, and syntactic features. Then, by applying machine learning techniques (e.g., Feature selection and classifier learning) and noise handling techniques, we achieve high accuracy of logging suggestions. We evaluate Log Advisor on two industrial software systems from Microsoft and two open-source software systems from Git Hub (totally 19.1M LOC and 100.6K logging statements). The encouraging experimental results, as well as a user study, demonstrate the feasibility and effectiveness of our logging suggestion tool. We believe our work can serve as an important first step towards the goal of \"learning to log\".","conference":"IEEE","terms":"Feature extraction;Software systems;Context;Data mining;Runtime;Syntactics,learning (artificial intelligence);programming;public domain software,logging programming practice;logging decisions;system runtime information;postmortem analysis;strategic logging placement;learning-to-log framework;Log Advisor logging suggestion tool;machine learning techniques;noise handling techniques;logging suggestions;industrial software systems;Microsoft;open-source software systems;Git Hub","keywords":"","startPage":"415","endPage":"425","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194593","citationCount":40,"referenceCount":48,"year":2015,"authors":"J. Zhu; P. He; Q. Fu; H. Zhang; M. R. Lyu; D. Zhang","affiliations":"Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Microsoft, Washington, DC, USA; Microsoft Res., Beijing, China; Shenzhen Res. Inst., Chinese Univ. of Hong Kong, Shenzhen, China; Microsoft Res., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3572"},"title":"Automated Modularization of GUI Test Cases","abstract":"Test cases that drive an application under test via its graphical user interface (GUI) consist of sequences of steps that perform actions on, or verify the state of, the application user interface. Such tests can be hard to maintain, especially if they are not properly modularized - that is, common steps occur in many test cases, which can make test maintenance cumbersome and expensive. Performing modularization manually can take up considerable human effort. To address this, we present an automated approach for modularizing GUI test cases. Our approach consists of multiple phases. In the first phase, it analyzes individual test cases to partition test steps into candidate subroutines, based on how user-interface elements are accessed in the steps. This phase can analyze the test cases only or also leverage execution traces of the tests, which involves a cost-accuracy tradeoff. In the second phase, the technique compares candidate subroutines across test cases, and refines them to compute the final set of subroutines. In the last phase, it creates callable subroutines, with parameterized data and control flow, and refactors the original tests to call the subroutines with context-specific data and control parameters. Our empirical results, collected using open-source applications, illustrate the effectiveness of the approach.","conference":"IEEE","terms":"Graphical user interfaces;Navigation;HTML;Maintenance engineering;Registers;Partitioning algorithms,graphical user interfaces;program testing,GUI test case modularization;graphical user interface;application user interface;test case analysis;candidate subroutines;test steps partitioning;user-interface elements;callable subroutines;context-specific data;control parameters;open-source applications","keywords":"","startPage":"44","endPage":"54","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194560","citationCount":1,"referenceCount":33,"year":2015,"authors":"R. Yandrapally; G. Sridhara; S. Sinha","affiliations":"IBM Res., Bangalore, India; IBM Res., Bangalore, India; IBM Res., Bangalore, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3573"},"title":"LACE2: Better Privacy-Preserving Data Sharing for Cross Project Defect Prediction","abstract":"Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute \"interesting\" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).","conference":"IEEE","terms":"Data privacy;Privacy;Software;Minimization;Organizations;Measurement;Clustering algorithms,cache storage;data encapsulation;data privacy;project management;security of data;software development management,LACE2;privacy-preserving data sharing;cross project defect prediction;secure data sharing;data minimization;multiparty data sharing;cache;interesting data;obfuscation algorithm;project detail hiding;single-party approach;multiparty approach","keywords":"privacy-preserving data sharing;cross project defect prediction","startPage":"801","endPage":"811","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194627","citationCount":25,"referenceCount":44,"year":2015,"authors":"F. Peters; T. Menzies; L. Layman","affiliations":"Lero - The Irish Software Res. Centre, Univ. of Limerick, Limerick, Ireland; Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Fraunhofer Center for Exp. SE, College Park, MD, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3574"},"title":"GPredict: Generic Predictive Concurrency Analysis","abstract":"Predictive trace analysis (PTA) is an effective approach for detecting subtle bugs in concurrent programs. Existing PTA techniques, however, are typically based on adhoc algorithms tailored to low-level errors such as data races or atomicity violations, and are not applicable to high-level properties such as \"a resource must be authenticated before use\" and \"a collection cannot be modified when being iterated over\". In addition, most techniques assume as input a globally ordered trace of events, which is expensive to collect in practice as it requires synchronizing all threads. In this paper, we present GPredict: a new technique that realizes PTA for generic concurrency properties. Moreover, GPredict does not require a global trace but only the local traces of each thread, which incurs much less runtime overhead than existing techniques. Our key idea is to uniformly model violations of concurrency properties and the thread causality as constraints over events. With an existing SMT solver, GPredict is able to precisely predict property violations allowed by the causal model. Through our evaluation using both benchmarks and real world applications, we show that GPredict is effective in expressing and predicting generic property violations. Moreover, it reduces the runtime overhead of existing techniques by 54% on DaCapo benchmarks on average.","conference":"IEEE","terms":"Concurrent computing;Runtime;Schedules;Predictive models;Syntactics;Java;Prediction algorithms,concurrency control;program debugging;program diagnostics,GPredict;generic predictive concurrency analysis;predictive trace analysis;PTA;subtle bug detection;concurrent programs;local traces;SMT solver;DaCapo benchmarks","keywords":"","startPage":"847","endPage":"857","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194631","citationCount":12,"referenceCount":40,"year":2015,"authors":"J. Huang; Q. Luo; G. Rosu","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3575"},"title":"Tracking Static Analysis Violations over Time to Capture Developer Characteristics","abstract":"Many interesting questions about the software quality of a code base can only be answered adequately if fine-grained information about the evolution of quality metrics over time and the contributions of individual developers is known. We present an approach for tracking static analysis violations (which are often indicative of defects) over the revision history of a program, and for precisely attributing the introduction and elimination of these violations to individual developers. As one application, we demonstrate how this information can be used to compute ``fingerprints'' of developers that reflect which kinds of violations they tend to introduce or to fix. We have performed an experimental study on several large open-source projects written in different languages, providing evidence that these fingerprints are well-defined and capture characteristic information about the coding habits of individual developers.","conference":"IEEE","terms":"Position measurement;Software quality;Java;Libraries;History;Open source software,program diagnostics;public domain software;software quality,static analysis violations tracking;fine-grained information;quality metrics;program revision history;open-source projects;coding habits","keywords":"","startPage":"437","endPage":"447","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194595","citationCount":6,"referenceCount":38,"year":2015,"authors":"P. Avgustinov; A. I. Baars; A. S. Henriksen; G. Lavender; G. Menzel; O. d. Moor; M. Schäfer; J. Tibble","affiliations":"Semmle Ltd., Oxford, UK; Semmle Ltd., Oxford, UK; Semmle Ltd., Oxford, UK; Semmle Ltd., Oxford, UK; Semmle Ltd., Oxford, UK; NA; Semmle Ltd., Oxford, UK; Semmle Ltd., Oxford, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3576"},"title":"Learning Global Agile Software Engineering Using Same-Site and Cross-Site Teams","abstract":"We describe an experience in teaching global software engineering (GSE) using distributed Scrum augmented with industrial best practices. Our unique instructional technique had students work in both same-site and cross-site teams to contrast the two modes of working. The course was a collaboration between Aalto University, Finland and University of Victoria, Canada. Fifteen Canadian and eight Finnish students worked on a single large project, divided into four teams, working on interdependent user stories as negotiated with the industrial product owner located in Finland. Half way through the course, we changed the teams so each student worked in both a local and a distributed team. We studied student learning using a mixed-method approach including 14 post-course interviews, pre-course and Sprint questionnaires, observations, meeting recordings, and repository data from git and Flow dock, the primary communication tool. Our results show no significant differences between working in distributed vs. Non-distributed teams, suggesting that Scrum helps alleviate many GSE problems. Our post-course interviews and survey data allows us to explain this effect, we found that students over time learned to better self-select tasks with less inter-team dependencies, to communicate more, and to work better in teams.","conference":"IEEE","terms":"Interviews;Teamwork;Software;Training;Planning,computer science education;educational courses;educational institutions;software prototyping;teaching;team working,global agile software engineering learning;same-site teams;cross-site teams;GSE;distributed Scrum;instructional technique;Aalto University;Finland;University of Victoria;Canadian students;Finnish students;mixed-method approach;student learning;communication tool;interteam dependencies","keywords":"scrum;global software engineering;distributed scrum;teaching;project","startPage":"285","endPage":"294","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202974","citationCount":11,"referenceCount":28,"year":2015,"authors":"M. Paasivaara; K. Blincoe; C. Lassenius; D. Damian; J. Sheoran; F. Harrison; P. Chhabra; A. Yussuf; V. Isotalo","affiliations":"Software Process Res. Group, Aalto Univ., Aalto, Finland; SEGAL, Univ. of Victoria, Victoria, BC, Canada; Software Process Res. Group, Aalto Univ., Aalto, Finland; SEGAL, Univ. of Victoria, Victoria, BC, Canada; SEGAL, Univ. of Victoria, Victoria, BC, Canada; SEGAL, Univ. of Victoria, Victoria, BC, Canada; SEGAL, Univ. of Victoria, Victoria, BC, Canada; SEGAL, Univ. of Victoria, Victoria, BC, Canada; Software Process Res. Group, Aalto Univ., Aalto, Finland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3577"},"title":"Developing and Evaluating Software Engineering Process Theories","abstract":"A process theory is an explanation of how an entity changes and develops. While software engineering is fundamentally concerned with how software artifacts change and develop, little research explicitly builds and empirically evaluates software engineering process theories. This lack of theory obstructs scientific consensus by focusing the academic community on methods. Methods inevitably oversimplify and over-rationalize reality, obfuscating crucial phenomena including uncertainty, problem framing and illusory requirements. Better process theories are therefore needed to ground software engineering in empirical reality. However, poor understanding of process theory issues impedes research and publication. This paper therefore attempts to clarify the nature and types of process theories, explore their development and provide specific guidance for their empirically evaluation.","conference":"IEEE","terms":"Context;Software engineering;Sociology;Statistics;Encoding;Software design,software engineering,software engineering process theories;software artifacts;empirical software evaluation","keywords":"Research methodology;process theory;questionnaire;case study;field study","startPage":"20","endPage":"31","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194558","citationCount":5,"referenceCount":74,"year":2015,"authors":"P. Ralph","affiliations":"Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3578"},"title":"ZoomIn: Discovering Failures by Detecting Wrong Assertions","abstract":"Automatic testing, although useful, is still quite ineffective against faults that do not cause crashes or uncaught exceptions. In the majority of the cases automatic tests do not include oracles, and only in some cases they incorporate assertions that encode the observed behavior instead of the intended behavior, that is if the application under test produces a wrong result, the synthesized assertions will encode wrong expectations that match the actual behavior of the application. In this paper we present Zoom In, a technique that extends the fault-revealing capability of test case generation techniques from crash-only faults to faults that require non-trivial oracles to be detected. Zoom In exploits the knowledge encoded in the manual tests written by developers and the similarity between executions to automatically determine an extremely small set of suspicious assertions that are likely wrong and thus worth manual inspection. Early empirical results show that Zoom In has been able to detect 50% of the analyzed non-crashing faults in the Apache Commons Math library requiring the inspection of less than 1.5% of the assertions automatically generated by EvoSuite.","conference":"IEEE","terms":"Manuals;Generators;Inspection;Data mining;Computer crashes;Encoding;Law,program testing;software fault tolerance,ZoomIn;failure discovery;wrong assertion detection;automatic testing;synthesized assertions;fault-revealing capability;test case generation;Apache Commons Math library;EvoSuite","keywords":"oracle problem;oracle generation;anomaly detection","startPage":"66","endPage":"76","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194562","citationCount":2,"referenceCount":35,"year":2015,"authors":"F. Pastore; L. Mariani","affiliations":"Centre For Security, Reliability \u0026 Trust, Univ. of Luxembourg, Luxembourg, Luxembourg; Dept. of Inf., Syst. \u0026 Commun., Univ. of Milano - Bicocca, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3579"},"title":"ReCBuLC: Reproducing Concurrency Bugs Using Local Clocks","abstract":"Multi-threaded programs play an increasingly important role in current multi-core environments. Exposing concurrency bugs and debugging such multi-threaded programs have become quite challenging due to their inherent non-determinism. In order to eliminate such non-determinism, many approaches such as record-and-replay and other similar bug reproducing systems have been proposed. However, those approaches often suffer significant performance degradation because they require a large amount of recorded information and/or long analysis and replay time. In this paper, we propose an effective approach, ReCBuLC, to take advantage of the hardware clocks available on modern processors. The key idea is to reduce the recording overhead and analyzing events' global order by using time stamps recorded in each thread. Those timestamps are used to determine the global orders of shared accesses. To avoid the large overhead incurred in accessing system-wide global clock, we opt to use local per-core clocks that incur much less access overhead. We then propose techniques to resolve differences among local clocks and obtain an accurate global event order. By using per-core clocks, state-of-the-art bug reproducing systems such as PRES and CLAP can reduce the recording overheads by 1% ~ 85%, and the analysis time by 84.66% ~ 99.99%, respectively.","conference":"IEEE","terms":"Clocks;Program processors;Computer bugs;Hardware;Coherence;Concurrent computing;Debugging,clocks;concurrency control;multiprocessing systems;multi-threading;program debugging,ReCBuLC;concurrency bug reproduction;multicore environment;concurrency bugs;multithreaded program debugging;nondeterminism elimination;record-and-replay approach;bug reproducing system;hardware clock;recording overhead reduction;event global order analysis;time stamp;shared access;local per-core clock;PRES;CLAP","keywords":"concurrency;bug reproducing;local clock","startPage":"824","endPage":"834","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194629","citationCount":6,"referenceCount":30,"year":2015,"authors":"X. Yuan; C. Wu; Z. Wang; J. Li; P. Yew; J. Huang; X. Feng; Y. Lan; Y. Chen; Y. Guan","affiliations":"State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; Dept. of Comput. Sci. \u0026 Eng., Univ. of Minnesota at Twin-Cities, Minneapolis, MN, USA; Dept. of Comput. Sci. \u0026 Eng., Texas A\u0026M Univ., College Station, TX, USA; State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; Key Lab. of Network Data Sci. \u0026 Technol., Inst. of Comput. Technol., Beijing, China; State Key Lab. of Comput. Archit., Inst. of Comput. Technol., Beijing, China; Coll. of Inf. Eng., Capital Normal Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d357a"},"title":"Discovering Information Explaining API Types Using Text Classification","abstract":"Many software development tasks require developers to quickly learn a subset of an Application Programming Interface (API). API learning resources are crucial for helping developers learn an API, but the knowledge relevant to a particular topic of interest may easily be scattered across different documents, which makes finding the necessary information more challenging. This paper proposes an approach to discovering tutorial sections that explain a given API type. At the core of our approach, we classify fragmented tutorial sections using supervised text classification based on linguistic and structural features. Experiments conducted on five tutorials show that our approach is able to discover sections explaining an API type with precision between 0.69 and 0.87 (depending on the tutorial) when trained and tested on the same tutorial. When trained and tested across tutorials, we obtained a precision between 0.74 and 0.94 and lower recall values.","conference":"IEEE","terms":"Tutorials;Feature extraction;HTML;Libraries;Java;Documentation;Programming,application program interfaces;learning (artificial intelligence);pattern classification;software engineering;text analysis,application programming interface;API;supervised text classification;software development;linguistic feature;structural feature","keywords":"","startPage":"869","endPage":"879","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194633","citationCount":26,"referenceCount":31,"year":2015,"authors":"G. Petrosyan; M. P. Robillard; R. De Mori","affiliations":"Sch. of Comput. Sci., McGill Univ., Montreal, QC, Canada; Sch. of Comput. Sci., McGill Univ., Montreal, QC, Canada; Sch. of Comput. Sci., McGill Univ., Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d357b"},"title":"Safe Memory-Leak Fixing for C Programs","abstract":"Automatic bug fixing has become a promising direction for reducing manual effort in debugging. However, general approaches to automatic bug fixing may face some fundamental difficulties. In this paper, we argue that automatic fixing of specific types of bugs can be a useful complement. This paper reports our first attempt towards automatically fixing memory leaks in C programs. Our approach generates only safe fixes, which are guaranteed not to interrupt normal execution of the program. To design such an approach, we have to deal with several challenging problems such as inter-procedural leaks, global variables, loops, and leaks from multiple allocations. We propose solutions to all the problems and integrate the solutions into a coherent approach. We implemented our inter-procedural memory leak fixing into a tool named Leak Fix and evaluated Leak Fix on 15 programs with 522k lines of code. Our evaluation shows that Leak Fix is able to successfully fix a substantial number of memory leaks, and Leak Fix is scalable for large applications.","conference":"IEEE","terms":"Resource management;Algorithm design and analysis;Safety;Software;Computer bugs;Leak detection;Face,C language;object-oriented programming;program control structures;program debugging;storage management,safe memory-leak fixing;C program;automatic bug fixing;debugging;interprocedural leak;global variables;loops;interprocedural memory leak fixing;Leak Fix tool","keywords":"","startPage":"459","endPage":"470","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194597","citationCount":21,"referenceCount":48,"year":2015,"authors":"Q. Gao; Y. Xiong; Y. Mi; L. Zhang; W. Yang; Z. Zhou; B. Xie; H. Mei","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d357c"},"title":"Remote Development and Distance Delivery of Innovative Courses: Challenges and Opportunities","abstract":"The Rochester Institute of Technology (RIT) offers programs of study at several of RIT's international campuses: Dubrovnik and Zagreb (Croatia), Dubai (United Arab Emirates) and Priatina (Kosovo). At RIT Croatia, some courses are delivered as distance education courses using Polycom, a video conferencing system, supported by other online education tools. Although distance learning methods and tools provide an effective way to offer instructions remotely, delivering a course that emphasizes team-based software development, with laboratory exercises and in-class team activities, creates new challenges that need to be addressed. This paper discusses the authors' experiences with the remote development and delivery of one of those courses - the SWEN-383 Software Design Principles and Patterns course in the Information Technology program at RIT Croatia. The paper first explains the role and need for offering this particular course. It then discusses how the collaborative development of this new course was conducted between the U.S. And the Croatian campuses, including remote delivery from Zagreb to Dubrovnik. The paper concludes with observations and suggestions for those who may engage in such a project in the future.","conference":"IEEE","terms":"Education;Collaboration;Software;Google;Computers;Software engineering;Context,computer science education;distance learning;educational courses;software engineering,Rochester Institute of Technology;RIT;course development;course delivery;Croatia;United Arab Emirates;Kosovo;Dubrovnik;Zagreb;Dubai;Priatina;distance education courses;Polycom system;video conferencing system;online education tools;team-based software development;laboratory exercises;in-class team activities;SWEN-383 software design principles and patterns course;information technology program","keywords":"remote collaboration;distance education;software engineering;information technology education;team projects","startPage":"299","endPage":"302","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202976","citationCount":1,"referenceCount":11,"year":2015,"authors":"K. Marasovic; M. Lutz","affiliations":"Dept. of Inf. Technol., Rochester Inst. of Technol., Zagreb, Croatia; Dept. of Software Eng., Rochester Inst. of Technol., Rochester, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d357d"},"title":"Towards Explicitly Elastic Programming Frameworks","abstract":"It is a widely held view that software engineers should not be \"burdened\" with the responsibility of making their application components elastic, and that elasticity should be either be implicit and automatic in the programming framework; or that it is the responsibility of the cloud provider's operational staff (DevOps) to make distributed applications written for dedicated clusters elastic and execute them on cloud environments. In this paper, we argue the opposite - we present a case for explicit elasticity, where software engineers are given the flexibility to explicitly engineer elasticity into their distributed applications. We present several scenarios where elasticity retrofitted to applications by DevOps is ineffective, present preliminary empirical evidence that explicit elasticity improves efficiency, and argue for elastic programming languages and frameworks to reduce programmer effort in engineering elastic distributed applications. We also present a bird's eye view of ongoing work on two explicitly elastic programming frameworks - Elastic Thrift (based on Apache Thrift) and Elastic Java, an extension of Java with support for explicit elasticity.","conference":"IEEE","terms":"Elasticity;Programming;Runtime;Java;Servers;Software;Measurement,cloud computing;Java;software engineering,explicit elastic programming frameworks;software engineers;cloud provider operational staff;DevOps;cloud environments;elastic programming languages;elastic thrift programming frameworks;elastic Java","keywords":"","startPage":"619","endPage":"622","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203027","citationCount":2,"referenceCount":16,"year":2015,"authors":"K. R. Jayaram","affiliations":"IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d357e"},"title":"Rapid Multi-Purpose, Multi-Commit Code Analysis","abstract":"Existing code- and software evolution studies typically operate on the scale of a few revisions of a small number of projects, mostly because existing tools are unsuited for performing large-scale studies. We present a novel approach, which can be used to analyze an arbitrary number of revisions of a software project simultaneously and which can be adapted for the analysis of mixed-language projects. It lays the foundation for building high-performance code analyzers for a variety of scenarios. We show that for one particular scenario, namely code metric computation, our prototype outperforms existing tools by multiple orders of magnitude when analyzing thousands of revisions.","conference":"IEEE","terms":"Software;Conferences;Measurement;Computer languages;Data mining;Prototypes;Software engineering,program diagnostics;software metrics,rapid multipurpose multicommit code analysis;code evolution;software evolution;software project revisions;mixed-language projects;high-performance code analyzers;code metric computation","keywords":"code analysis;graph;abstract syntax tree;software evolution","startPage":"635","endPage":"638","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203031","citationCount":6,"referenceCount":16,"year":2015,"authors":"C. V. Alexandru; H. C. Gall","affiliations":"Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d357f"},"title":"No PAIN, No Gain? The Utility of PArallel Fault INjections","abstract":"Software Fault Injection (SFI) is an established technique for assessing the robustness of a software under test by exposing it to faults in its operational environment. Depending on the complexity of this operational environment, the complexity of the software under test, and the number and type of faults, a thorough SFI assessment can entail (a) numerous experiments and (b) long experiment run times, which both contribute to a considerable execution time for the tests. In order to counteract this increase when dealing with complex systems, recent works propose to exploit parallel hardware to execute multiple experiments at the same time. While Parallel fault Injections (PAIN) yield higher experiment throughput, they are based on an implicit assumption of non-interference among the simultaneously executing experiments. In this paper we investigate the validity of this assumption and determine the trade-off between increased throughput and the accuracy of experimental results obtained from PAIN experiments.","conference":"IEEE","terms":"Hardware;Throughput;Parallel processing;Detectors;Testing;Kernel,parallel processing;software fault tolerance,PAIN;parallel fault injection;software fault injection;SFI","keywords":"Software Fault Injection;Robustness Testing;Test Automation;Test Parallelization","startPage":"494","endPage":"505","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194600","citationCount":10,"referenceCount":77,"year":2015,"authors":"S. Winter; O. Schwahn; R. Natella; N. Suri; D. Cotroneo","affiliations":"DEEDS Group, Tech. Univ. Darmstadt, Darmstadt, Germany; DEEDS Group, Tech. Univ. Darmstadt, Darmstadt, Germany; DIETI, Federico II Univ. of Naples, Naples, Italy; DEEDS Group, Tech. Univ. Darmstadt, Darmstadt, Germany; DIETI, Federico II Univ. of Naples, Naples, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3580"},"title":"Static Control-Flow Analysis of User-Driven Callbacks in Android Applications","abstract":"Android software presents many challenges for static program analysis. In this work we focus on the fundamental problem of static control-flow analysis. Traditional analyses cannot be directly applied to Android because the applications are framework-based and event-driven. We consider user-event-driven components and the related sequences of callbacks from the Android framework to the application code, both for lifecycle callbacks and for event handler callbacks. We propose a program representation that captures such callback sequences. This representation is built using context-sensitive static analysis of callback methods. The analysis performs graph reachability by traversing context-compatible interprocedural control-flow paths and identifying statements that may trigger callbacks, as well as paths that avoid such statements. We also develop a client analysis that builds a static model of the application's GUI. Experimental evaluation shows that this context-sensitive approach leads to substantial precision improvements, while having practical cost.","conference":"IEEE","terms":"Context;Graphical user interfaces;Smart phones;Algorithm design and analysis;Androids;Humanoid robots;Analytical models,data flow analysis;graphical user interfaces;mobile computing;smart phones,static control-flow analysis;user-driven callback sequence;Android application;static program analysis;program representation;context-sensitive static analysis;client analysis;GUI","keywords":"","startPage":"89","endPage":"99","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194564","citationCount":49,"referenceCount":45,"year":2015,"authors":"S. Yang; D. Yan; H. Wu; Y. Wang; A. Rountev","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3581"},"title":"Hercules: Reproducing Crashes in Real-World Application Binaries","abstract":"Binary analysis is a well-investigated area in software engineering and security. Given real-world program binaries, generating test inputs which cause the binaries to crash is crucial. Generation of crashing inputs has many applications including off-line analysis of software prior to deployment, or online analysis of software patches as they are inserted. In this work, we present a method for generating inputs which reach a given \"potentially crashing\" location. Such potentially crashing locations can be found by a separate static analysis (or by gleaning crash reports submitted by internal / external users) and serve as the input to our method. The test input generated by our method serves as a witness of the crash. Our method is particularly suited for binaries of programs which take in complex structured inputs. Experiments on real-life applications such as the Adobe Reader and the Windows Media Player demonstrate that our Hercules tool built on selective symbolic execution engine S2E can generate crashing inputs within few hours, where symbolic approaches (as embodied by S2E) or blackbox fuzzing approaches (as embodied by the commercial tool PeachFuzzer) failed.","conference":"IEEE","terms":"Computer crashes;Concrete;Registers;Hybrid power systems;Heuristic algorithms;Search problems;Ash,security of data;software tools,Hercules tool;crash reproduction;binary analysis;software engineering;software security;software off-line analysis;Adobe Reader;Windows Media Player;selective symbolic execution engine;S2E","keywords":"binary analysis;test generation;symbolic execution","startPage":"891","endPage":"901","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194635","citationCount":6,"referenceCount":31,"year":2015,"authors":"V. Pham; W. B. Ng; K. Rubinov; A. Roychoudhury","affiliations":"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3582"},"title":"The Art of Testing Less without Sacrificing Quality","abstract":"Testing is a key element of software development processes for the management and assessment of product quality. In most development environments, the software engineers are responsible for ensuring the functional correctness of code. However, for large complex software products, there is an additional need to check that changes do not negatively impact other parts of the software and they comply with system constraints such as backward compatibility, performance, security etc. Ensuring these system constraints may require complex verification infrastructure and test procedures. Although such tests are time consuming and expensive and rarely find defects they act as an insurance process to ensure the software is compliant. However, long lasting tests increasingly conflict with strategic aims to shorten release cycles. To decrease production costs and to improve development agility, we created a generic test selection strategy called THEO that accelerates test processes without sacrificing product quality. THEO is based on a cost model, which dynamically skips tests when the expected cost of running the test exceeds the expected cost of removing it. We replayed past development periods of three major Microsoft products resulting in a reduction of 50% of test executions, saving millions of dollars per year, while maintaining product quality.","conference":"IEEE","terms":"Context;Software;Inspection;Testing;Quality assessment;Product design;Reliability,product quality;program testing;program verification;software quality,software development processes;product quality assessment;product quality management;large complex software products;complex verification infrastructure;test procedures;insurance process;production cost model;THEO generic test selection strategy","keywords":"","startPage":"483","endPage":"493","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194599","citationCount":25,"referenceCount":31,"year":2015,"authors":"K. Herzig; M. Greiler; J. Czerwonka; B. Murphy","affiliations":"Microsoft Res., Cambridge, UK; Microsoft Corp., Redmond, WA, USA; Microsoft Corp., Redmond, WA, USA; Microsoft Res., Cambridge, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3583"},"title":"Teaching Software Systems Thinking at The Open University","abstract":"The Open University is a distance-based higher education institution. Most of our students are in employment and study from home, contacting their tutor and fellow students via e-mail and discussion forums. In this paper, we describe our undergraduate and postgraduate modules in the software systems area, how we teach them at a distance, and our focus on shifting our students' minds into a reflective, critical, holistic socio-technical view of software systems that is relevant to their particular professional contexts.","conference":"IEEE","terms":"Software engineering;Context;Software systems;Training;Information systems,computer aided instruction;computer science education;distance learning;educational institutions;further education;professional aspects;software engineering;teaching,software system thinking teaching;Open University;distance-based higher education institution;student employment;e-mail;discussion forums;undergraduate modules;postgraduate modules;reflective view;critical view;holistic socio-technical view;professional contexts","keywords":"","startPage":"307","endPage":"310","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202978","citationCount":1,"referenceCount":14,"year":2015,"authors":"M. Wermelinger; J. G. Hall; L. Rapanotti; L. Barroca; M. Ramage; A. Bandara","affiliations":"Comput. \u0026 Commun. Dept., Open Univ., Milton Keynes, UK; Comput. \u0026 Commun. Dept., Open Univ., Milton Keynes, UK; Comput. \u0026 Commun. Dept., Open Univ., Milton Keynes, UK; Comput. \u0026 Commun. Dept., Open Univ., Milton Keynes, UK; Comput. \u0026 Commun. Dept., Open Univ., Milton Keynes, UK; Comput. \u0026 Commun. Dept., Open Univ., Milton Keynes, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3584"},"title":"Using GSwE2009 for the Evaluation of a Master Degree in Software Engineering in the Universidad de la República","abstract":"This paper presents an adoption and adaptation of the Curriculum Guidelines for Graduate Degree Programs in Software Engineering (GSwE2009) proposed by the IEEE-CS and the ACM for the creation of a curriculum for a Master's degree in software engineering at the Universidad de la República (Uruguay). A method for evaluating contents and its application is also presented. This evaluation allows us to know the obtained thematic coverage, effort and balance. It also provides information that enables the detection of numerous opportunities for the improvement in the implementation of the program.","conference":"IEEE","terms":"Software engineering;Software;Guidelines;Computer architecture;Joints;Training,computer science education;educational administrative data processing;educational institutions;further education;software engineering,GSwE2009;master degree;software engineering;Universidad de la República;curriculum guidelines adoption;curriculum guidelines adaptation;graduate degree programs;IEEE-CS;ACM;curriculum creation;Uruguay;contents evaluation","keywords":"","startPage":"323","endPage":"332","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202982","citationCount":2,"referenceCount":19,"year":2015,"authors":"L. Camilloni; D. Vallespir; M. Ardis","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3585"},"title":"Combining Multi-Objective Search and Constraint Solving for Configuring Large Software Product Lines","abstract":"Software Product Line (SPL) feature selection involves the optimization of multiple objectives in a large and highly constrained search space. We introduce SATIBEA, that augments multi-objective search-based optimization with constraint solving to address this problem, evaluating it on five large real-world SPLs, ranging from 1,244 to 6,888 features with respect to three different solution quality indicators and two diversity metrics. The results indicate that SATIBEA statistically significantly outperforms the current state-of-the-art (p \u003c; 0.01) for all five SPLs on all three quality indicators and with maximal effect size (Ȃ12 = 1.0). We also present results that demonstrate the importance of combining constraint solving with search-based optimization and the significant improvement SATIBEA produces over pure constraint solving. Finally, we demonstrate the scalability of SATIBEA: within less than half an hour, it finds thousands of constraint-satisfying optimized software products, even for the largest SPL considered in the literature to date.","conference":"IEEE","terms":"Optimization;Software;Frequency modulation;Search problems;Measurement;Software product lines;Filtering algorithms,configuration management;optimisation;search problems;software metrics;software product lines,constraint solving;software product lines configuration;SPL feature selection;SATIBEA framework;multiobjective search-based optimization;diversity metrics","keywords":"","startPage":"517","endPage":"528","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194602","citationCount":35,"referenceCount":55,"year":2015,"authors":"C. Henard; M. Papadakis; M. Harman; Y. Le Traon","affiliations":"Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg, Luxembourg; Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg, Luxembourg; Univ. Coll. London, London, UK; Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911beee8435e8e7d3586"},"title":"Interactive Code Review for Systematic Changes","abstract":"Developers often inspect a diff patch during peer code reviews. Diff patches show low-level program differences per file without summarizing systematic changes -- similar, related changes to multiple contexts. We present Critics, an interactive approach for inspecting systematic changes. When a developer specifies code change within a diff patch, Critics allows developers to customize the change template by iteratively generalizing change content and context. By matching a generalized template against the codebase, it summarizes similar changes and detects potential mistakes. We evaluated Critics using two methods. First, we conducted a user study at Salesforce.com, where professional engineers used Critics to investigate diff patches authored by their own team. After using Critics, all six participants indicated that they would like Critics to be integrated into their current code review environment. This also attests to the fact that Critics scales to an industry-scale project and can be easily adopted by professional engineers. Second, we conducted a user study where twelve participants reviewed diff patches using Critics and Eclipse diff. The results show that human subjects using Critics answer questions about systematic changes 47.3% more correctly with 31.9% saving in time during code review tasks, in comparison to the baseline use of Eclipse diff. These results show that Critics should improve developer productivity in inspecting systematic changes during peer code reviews.","conference":"IEEE","terms":"Systematics;Context;Switches;Interviews;Software;Syntactics;Data mining,software engineering;software reviews;source code (software),peer code review;software development;systematic change;interactive code review;Critics","keywords":"","startPage":"111","endPage":"122","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194566","citationCount":23,"referenceCount":44,"year":2015,"authors":"T. Zhang; M. Song; J. Pinedo; M. Kim","affiliations":"Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of Texas at Austin, Austin, TX, USA; Univ. of Texas at Austin, Austin, TX, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3587"},"title":"Measuring Software Redundancy","abstract":"Redundancy is the presence of different elements with the same functionality. In software, redundancy is useful (and used) in many ways, for example for fault tolerance and reliability engineering, and in self-adaptive and self-checking programs. However, despite the many uses, we still do not know how to measure software redundancy to support a proper and effective design. If, for instance, the goal is to improve reliability, one might want to measure the redundancy of a solution to then estimate the reliability gained with that solution. Or one might compare alternative solutions to choose the one that expresses more redundancy and therefore, presumably, more reliability. We first formalize a notion of redundancy whereby two code fragments are considered redundant when they achieve the same functionality with different executions. On the basis of this abstract and general notion, we then develop a concrete method to obtain a meaningful quantitative measure of software redundancy. The results we obtain are very positive: we show, through an extensive experimental analysis, that it is possible to distinguish code that is only minimally different, from truly redundant code, and that it is even possible to distinguish low-level code redundancy from high-level algorithmic redundancy. We also show that the measurement is significant and useful for the designer, as it can help predict the effectiveness of techniques that exploit redundancy.","conference":"IEEE","terms":"Redundancy;Software;Software measurement;Semantics;Atmospheric measurements;Particle measurements,software reliability,software redundancy measurement;software reliability;low-level code redundancy;high-level algorithmic redundancy","keywords":"Software redundancy;functional equivalence;execution diversity","startPage":"156","endPage":"166","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194570","citationCount":12,"referenceCount":36,"year":2015,"authors":"A. Carzaniga; A. Mattavelli; M. Pezzè","affiliations":"Univ. della Svizzera italiana (USI), Lugano, Switzerland; Univ. della Svizzera italiana (USI), Lugano, Switzerland; Univ. della Svizzera italiana (USI), Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3588"},"title":"An Empirical Study on Quality Issues of Production Big Data Platform","abstract":"Big Data computing platform has evolved to be a multi-tenant service. The service quality matters because system failure or performance slowdown could adversely affect business and user experience. There is few study in literature on service quality issues of production Big Data computing platform. In this paper, we present an empirical study on the service quality issues of Microsoft ProductA, which is a company-wide multi-tenant Big Data computing platform, serving thousands of customers from hundreds of teams. ProductA has a well-defined incident management process, which helps customers report and mitigate service quality issues on 24/7 basis. This paper explores the common symptom, causes and mitigation of service quality issues in Big Data computing. We conduct an empirical study on 210 real service quality issues in ProductA. Our major findings include (1) 21.0% of escalations are caused by hardware faults; (2) 36.2% are caused by system side defects; (3) 37.2% are due to customer side faults. We also studied the general diagnosis process and the commonly adopted mitigation solutions. Our findings can help improve current development and maintenance practice of Big Data computing platform, and motivate tool support.","conference":"IEEE","terms":"Big data;Hardware;Electronic mail;Iron;Software engineering;Business;Programming,Big Data;software quality,production Big Data computing platform;service quality;Microsoft ProductA;multitenant Big Data computing platform;incident management process;hardware faults;system side defects;customer side faults","keywords":"","startPage":"17","endPage":"26","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202945","citationCount":7,"referenceCount":30,"year":2015,"authors":"H. Zhou; J. Lou; H. Zhang; H. Lin; H. Lin; T. Qin","affiliations":"Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Microsoft, Beijing, China; Microsoft Res., Beijing, China; Microsoft Res., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3589"},"title":"An Empirical Study on Real Bug Fixes","abstract":"Software bugs can cause significant financial loss and even the loss of human lives. To reduce such loss, developers devote substantial efforts to fixing bugs, which generally requires much expertise and experience. Various approaches have been proposed to aid debugging. An interesting recent research direction is automatic program repair, which achieves promising results, and attracts much academic and industrial attention. However, people also cast doubt on the effectiveness and promise of this direction. A key criticism is to what extent such approaches can fix real bugs. As only research prototypes for these approaches are available, it is infeasible to address the criticism by evaluating them directly on real bugs. Instead, in this paper, we design and develop BUGSTAT, a tool that extracts and analyzes bug fixes. With BUGSTAT's support, we conduct an empirical study on more than 9,000 real-world bug fixes from six popular Java projects. Comparing the nature of manual fixes with automatic program repair, we distill 15 findings, which are further summarized into four insights on the two key ingredients of automatic program repair: fault localization and faulty code fix. In addition, we provide indirect evidence on the size of the search space to fix real bugs and find that bugs may also reside in non-source files. Our results provide useful guidance and insights for improving the state-of-the-art of automatic program repair.","conference":"IEEE","terms":"Computer bugs;Maintenance engineering;Java;Software;Shape;Manuals;Interference,Java;program debugging;search problems;software fault tolerance,bug fixing;software bugs;financial loss;debugging;automatic program repair;BUGSTAT;Java projects;fault localization;faulty code fix;search space","keywords":"","startPage":"913","endPage":"923","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194637","citationCount":35,"referenceCount":45,"year":2015,"authors":"H. Zhong; Z. Su","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Shanghai Jiao Tong Univ., Shanghai, China; Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d358a"},"title":"On the Role of Value Sensitive Concerns in Software Engineering Practice","abstract":"The role of software systems on societal sustainability has generally not been the subject of substantive research activity. In this paper we examine the role of software engineering practice as an agent of change/impact for societal sustainability through the manifestation of value sensitive concerns. These concerns remain relatively neglected by software design processes except at early stages of user interface design. Here, we propose a conceptual model that can contribute to a translation of value sensitive design from its current focus in participatory design to one located in mainstream software engineering processes. Addressing this need will have an impact of societal sustainability and we outline some of the key research challenges for that journey.","conference":"IEEE","terms":"Software engineering;Stakeholders;Unified modeling language;Privacy;Conferences;Computers;Computational modeling,social aspects of automation;software engineering;user interfaces,value sensitive concerns;software systems;societal sustainability;user interface design;value sensitive design;participatory design;mainstream software engineering processes","keywords":"Value Sensitive Design;Values;Co-Design;Requirements elicitation","startPage":"497","endPage":"500","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203000","citationCount":9,"referenceCount":22,"year":2015,"authors":"B. Barn; R. Barn; F. Raimondi","affiliations":"Sch. of Sci. \u0026 Technol., Middlesex Univ., London, UK; R. Holloway Univ. of London, Egham, UK; Sch. of Sci. \u0026 Technol., Middlesex Univ., London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d358b"},"title":"Transparently Teaching in the Context of Game-based Learning: the Case of SimulES-W","abstract":"This work presents a pedagogical proposal, in the context of game-based learning (GBL), that uses the concept of Transparency Pedagogy. As such, it aims to improve the quality of teaching, and the relationship between student, teacher and teaching methods. Transparency is anchored in the principle of information disclosure. In pedagogy, transparency emerges as an important issue that proposes to raise student awareness about the educational processes. Using GBL as an educational strategy we managed to make the game, a software, transparent. That is we made the inner processes of the game known to the students. As such, besides learning by playing, students had access to the game design, through intentional modeling. We collected evidence that, by disclosure of the information about the design, students better performed on learning software engineering.","conference":"IEEE","terms":",computer aided instruction;computer games;software engineering,game-based learning;SimulES-W;GBL;transparency pedagogy;educational process;educational strategy;learning software engineering","keywords":"Transparency;Games-based Learning;SimulES-W;Pedagogy","startPage":"343","endPage":"352","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202984","citationCount":4,"referenceCount":40,"year":2015,"authors":"E. S. Monsalve; J. C. S. d. P. Leite; V. M. B. Werneck","affiliations":"Dept. de Informdtica, Pontiticia Univ. Catdlica do Rio de Janeiro, Rio de Janeiro, Brazil; NA; Dept. de Inf. e Cienc. da Computacau, Univ. do Estado do Rio de Janeiro, Rio de Janeiro, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d358c"},"title":"Learning Combinatorial Interaction Test Generation Strategies Using Hyperheuristic Search","abstract":"The surge of search based software engineering research has been hampered by the need to develop customized search algorithms for different classes of the same problem. For instance, two decades of bespoke Combinatorial Interaction Testing (CIT) algorithm development, our exemplar problem, has left software engineers with a bewildering choice of CIT techniques, each specialized for a particular task. This paper proposes the use of a single hyperheuristic algorithm that learns search strategies across a broad range of problem instances, providing a single generalist approach. We have developed a Hyperheuristic algorithm for CIT, and report experiments that show that our algorithm competes with known best solutions across constrained and unconstrained problems: For all 26 real-world subjects, it equals or outperforms the best result previously reported in the literature. We also present evidence that our algorithm's strong generic performance results from its unsupervised learning. Hyperheuristic search is thus a promising way to relocate CIT design intelligence from human to machine.","conference":"IEEE","terms":"Arrays;Search problems;Heuristic algorithms;Simulated annealing;Software algorithms;Algorithm design and analysis;Testing,program testing;software engineering;unsupervised learning,combinatorial interaction test generation strategies;hyperheuristic search strategies;search based software engineering;CIT algorithm development;unsupervised learning","keywords":"Hyperheuristic;CIT;SBSE","startPage":"540","endPage":"550","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194604","citationCount":28,"referenceCount":40,"year":2015,"authors":"Y. Jia; M. B. Cohen; M. Harman; J. Petke","affiliations":"Univ. Coll. London, London, UK; Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Univ. Coll. London, London, UK; Univ. Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d358d"},"title":"Helping Developers Help Themselves: Automatic Decomposition of Code Review Changesets","abstract":"Code Reviews, an important and popular mechanism for quality assurance, are often performed on a change set, a set of modified files that are meant to be committed to a source repository as an atomic action. Understanding a code review is more difficult when the change set consists of multiple, independent, code differences. We introduce CLUSTERCHANGES, an automatic technique for decomposing change sets and evaluate its effectiveness through both a quantitative analysis and a qualitative user study.","conference":"IEEE","terms":"Prototypes;Histograms;Quality assurance;Manuals;Software;Computer bugs;Standards,software quality;software reviews;source code (software),software development;code review changeset;quality assurance;CLUSTERCHANGES;automatic decomposition technique","keywords":"","startPage":"134","endPage":"144","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194568","citationCount":34,"referenceCount":28,"year":2015,"authors":"M. Barnett; C. Bird; J. Brunet; S. K. Lahiri","affiliations":"Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Fed. Univ. of Campina Grande, Campina Grande, Brazil; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d358e"},"title":"Automatic Documentation Generation via Source Code Summarization","abstract":"Programmers need software documentation. However, documentation is expensive to produce and maintain, and often becomes outdated over time. Programmers often lack the time and resources to write documentation. Therefore, automated solutions are desirable. Designers of automatic documentation tools are limited because there is not yet a clear understanding of what characteristics are important to generating high quality summaries. I propose three specific research objectives to improving automatic documentation generation. I propose to study the similarity between source code and summary. Second, I propose studying whether or not including contextual information about source code improves summary quality. Finally, I propose to study the problem of similarity in source code structure and source code documentation. This paper discusses my work on these three objectives towards my Ph.D. dissertation, including my preliminary and proposed work.","conference":"IEEE","terms":"Documentation;Software;Java;Context;Natural languages;Semantics;Measurement,software quality;source code (software);system documentation,source code summarization;software documentation;programmers;automatic documentation tools;automatic documentation generation;summary quality;source code structure similarity;source code documentation","keywords":"source code summarization;automatic documentation;software engineering","startPage":"903","endPage":"906","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203110","citationCount":1,"referenceCount":31,"year":2015,"authors":"P. W. McBurney","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Univ. of Notre Dame, Notre Dame, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d358f"},"title":"Presence-Condition Simplification in Highly Configurable Systems","abstract":"For the analysis of highly configurable systems, analysis approaches need to take the inherent variability of these systems into account. The notion of presence conditions is central to such approaches. A presence condition specifies a subset of system configurations in which a certain artifact or a concern of interest is present (e.g., a defect associated with this subset). In this paper, we introduce and analyze the problem of presence-condition simplification. A key observation is that presence conditions often contain redundant information, which can be safely removed in the interest of simplicity and efficiency. We present a formalization of the problem, discuss application scenarios, compare different algorithms for solving the problem, and empirically evaluate the algorithms by means of a set of substantial case studies.","conference":"IEEE","terms":"Electronic mail;Context;Data structures;Boolean functions;Cryptography;Size measurement;Algorithm design and analysis,program diagnostics,presence-condition simplification;highly configurable systems analysis;systems variability;presence conditions notion;system configuration","keywords":"","startPage":"178","endPage":"188","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194572","citationCount":10,"referenceCount":48,"year":2015,"authors":"A. v. Rhein; A. Grebhahn; S. Apel; N. Siegmund; D. Beyer; T. Berger","affiliations":"NA; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3590"},"title":"Systematic Testing of Reactive Software with Non-Deterministic Events: A Case Study on LG Electric Oven","abstract":"Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.","conference":"IEEE","terms":"Ovens;Probes;Software;Testing;Light emitting diodes;Systematics;Computer bugs,electrical engineering computing;ovens;program debugging;program testing,systematic reactive software testing;nondeterministic events;LG electric oven;home appliance devices;reactive systems;user input-event;event handler;nondeterministic events sequence;asynchronous event handlers;main loop execution;systematic event generation framework;concolic testing technique;critical bugs;illegal state transition","keywords":"","startPage":"29","endPage":"38","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202947","citationCount":1,"referenceCount":17,"year":2015,"authors":"Y. Park; S. Hong; M. Kim; D. Lee; J. Cho","affiliations":"CS Dept., KAIST, Daejeon, South Korea; CS Dept., KAIST, Daejeon, South Korea; CS Dept., KAIST, Daejeon, South Korea; LG Electron., Seoul, South Korea; LG Electron., Seoul, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3591"},"title":"Comparing Software Architecture Recovery Techniques Using Accurate Dependencies","abstract":"Many techniques have been proposed to automatically recover software architectures from software implementations. A thorough comparison among the recovery techniques is needed to understand their effectiveness and applicability. This study improves on previous studies in two ways. First, we study the impact of leveraging more accurate symbol dependencies on the accuracy of architecture recovery techniques. Previous studies have not seriously considered how the quality of the input might affect the quality of the output for architecture recovery techniques. Second, we study a system (Chromium) that is substantially larger (9.7 million lines of code) than those included in previous studies. Obtaining the ground-truth architecture of Chromium involved two years of collaboration with its developers. As part of this work we developed a new sub module-based technique to recover preliminary versions of ground-truth architectures. The other systems that we study have been examined previously. In some cases, we have updated the ground-truth architectures to newer versions, and in other cases we have corrected newly discovered inconsistencies. Our evaluation of nine variants of six state-of-the-art architecture recovery techniques shows that symbol dependencies generally produce architectures with higher accuracies than include dependencies. Despite this improvement, the overall accuracy is low for all recovery techniques. The results suggest that (1) in addition to architecture recovery techniques, the accuracy of dependencies used as their inputs is another factor to consider for high recovery accuracy, and (2) more accurate recovery techniques are needed. Our results show that some of the studied architecture recovery techniques scale to the 10M lines-of-code range (the size of Chromium), whereas others do not.","conference":"IEEE","terms":"Computer architecture;Accuracy;Chromium;Software;Clustering algorithms;Java;Software architecture,software architecture;software quality,software architecture recovery techniques;accurate symbol dependency;chromium ground-truth architecture;submodule-based technique;software quality","keywords":"","startPage":"69","endPage":"78","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202951","citationCount":21,"referenceCount":33,"year":2015,"authors":"T. Lutellier; D. Chollak; J. Garcia; L. Tan; D. Rayside; N. Medvidovic; R. Kroeger","affiliations":"Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; George Mason Univ., Fairfax, VA, USA; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Southern California, Los Angeles, CA, USA; Google, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3592"},"title":"Trivial Compiler Equivalence: A Large Scale Empirical Study of a Simple, Fast and Effective Equivalent Mutant Detection Technique","abstract":"Identifying equivalent mutants remains the largest impediment to the widespread uptake of mutation testing. Despite being researched for more than three decades, the problem remains. We propose Trivial Compiler Equivalence (TCE) a technique that exploits the use of readily available compiler technology to address this long-standing challenge. TCE is directly applicable to real-world programs and can imbue existing tools with the ability to detect equivalent mutants and a special form of useless mutants called duplicated mutants. We present a thorough empirical study using 6 large open source programs, several orders of magnitude larger than those used in previous work, and 18 benchmark programs with hand-analysis equivalent mutants. Our results reveal that, on large real-world programs, TCE can discard more than 7% and 21% of all the mutants as being equivalent and duplicated mutants respectively. A human- based equivalence verification reveals that TCE has the ability to detect approximately 30% of all the existing equivalent mutants.","conference":"IEEE","terms":"Optimization;Java;Scalability;Benchmark testing;Syntactics,formal verification;program compilers;program testing,trivial compiler equivalence technology;mutant detection technique;mutation testing;TCE technique;duplicated mutants;human-based equivalence verification","keywords":"","startPage":"936","endPage":"946","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194639","citationCount":55,"referenceCount":58,"year":2015,"authors":"M. Papadakis; Y. Jia; M. Harman; Y. Le Traon","affiliations":"Interdiscipl. Centre for Security, Reliability \u0026 Trust, Univ. of Luxembourg, Luxembourg, Luxembourg; CREST Centre, Univ. Coll. London, London, UK; CREST Centre, Univ. Coll. London, London, UK; Interdiscipl. Centre for Security, Reliability \u0026 Trust, Univ. of Luxembourg, Luxembourg, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3593"},"title":"Enabling the Definition and Enforcement of Governance Rules in Open Source Systems","abstract":"Governance rules in software development projects help to prioritize and manage their development tasks, and contribute to the long-term sustainability of the project by clarifying how core and external contributors should collaborate in order to advance the project during its whole lifespan. Despite their importance, specially in Open Source Software (OSS) projects, these rules are usually implicit or scattered in the project documentation/tools (e.g., Tracking-systems or forums), hampering the correct understanding of the development process. We propose to enable the explicit definition and enforcement of governance rules for OSS projects. We believe this brings several important benefits, including improvements in the transparency of the process, its traceability and the semi-automation of the governance itself. Our approach has been implemented on top of My Lyn, a project-management Eclipse plug-in supporting most popular tracking-systems.","conference":"IEEE","terms":"Documentation;Software;Computer bugs;DSL;Software engineering;Syntactics;Organizations,public domain software;software engineering,governance rules;software development projects;long-term sustainability;open source software projects;OSS projects;My Lyn;project-management;Eclipse plug-in","keywords":"governance;open source systems;sustainability","startPage":"505","endPage":"514","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203002","citationCount":0,"referenceCount":29,"year":2015,"authors":"J. L. Cánovas Izquierdo; J. Cabot","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3594"},"title":"Contest Based Learning with Blending Software Engineering and Business Management: For Students' High Motivation and High Practice Ability","abstract":"We began implementing contest-based learning with a blend of software engineering and business management 10 years ago. At first, a project subject was assigned. However, several problems occurred: For example, the students became absorbed in programming rather than design and analysis activities. Therefore, the curriculum changed from project subjects to contest-based learning. Business management, marketing, and accounting subjects were added to the new curriculum, and students made information technology (IT) business plans using their knowledge of software engineering and business management. The IT business plans were submitted to various contests held by public newspaper companies and the federation of economic organizations in Japan. As a result, in the 10 years of the contest-based learning implementation, 20 teams have received awards in various IT business plan contests. We investigated 10 persons who had experience submitting business plans. We confirmed that contest-based learning had clearer goals, such as to win the contest prize, compared to project-based learning. Further, the abilities to solve problems and to investigate increased more in comparison with lecture-style and project-style education.","conference":"IEEE","terms":"Software;Education;Software engineering;Programming profession;Companies,computer science education;human factors;management accounting;management education;marketing;software engineering,contest based learning;blending software engineering;business management;student high motivation;high practice ability;accounting subjects;marketing;information technology;IT business plans;project-based learning;project-style education;lecture-style education","keywords":"Contest;IT business plan;blending education;marketing;accounting;business management","startPage":"360","endPage":"369","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202986","citationCount":1,"referenceCount":10,"year":2015,"authors":"N. Hanakawa","affiliations":"Fac. of Inf. Manage., Hannan Univ., Matsubara, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3595"},"title":"Code Hunt: Experience with Coding Contests at Scale","abstract":"Mastering a complex skill like programming takes many hours. In order to encourage students to put in these hours, we built Code Hunt, a game that enables players to program against the computer with clues provided as unit tests. The game has become very popular and we are now running worldwide contests where students have a fixed amount of time to solve a set of puzzles. This paper describes Code Hunt and the contest experience it offers. We then show some early results that demonstrate how Code Hunt can accurately discriminate between good and bad coders. The challenges of creating and selecting puzzles for contests are covered. We end up with a short description of our course experience, and some figures that show that Code Hunt is enjoyed by women and men alike.","conference":"IEEE","terms":"Games;Encoding;Joints;Training;Algorithm design and analysis;Programming profession,computer aided instruction;computer games;computer science education;programming,programming skill;Code Hunt game;contest experience;coding contest","keywords":"Programming contests;unit tests;symbolic execution;Code Hunt game","startPage":"398","endPage":"407","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202990","citationCount":19,"referenceCount":20,"year":2015,"authors":"J. Bishop; R. N. Horspool; T. Xie; N. Tillmann; J. d. Halleux","affiliations":"Microsoft Res., Redmond, WA, USA; Univ. of Victoria, Victoria, BC, Canada; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Jonathan de Halleux Microsoft Res., Redmond, WA, USA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3596"},"title":"Chiminey: Reliable Computing and Data Management Platform in the Cloud","abstract":"The enabling of scientific experiments that are embarrassingly parallel, long running and data-intensive into a cloud-based execution environment is a desirable, though complex undertaking for many researchers. The management of such virtual environments is cumbersome and not necessarily within the core skill set for scientists and engineers. We present here Chiminey, a software platform that enables researchers to (i) run applications on both traditional high-performance computing and cloud-based computing infrastructures, (ii) handle failure during execution, (iii) curate and visualise execution outputs, (iv) share such data with collaborators or the public, and (v) search for publicly available data.","conference":"IEEE","terms":"Data visualization;Connectors;Cloud computing;Fault tolerance;Fault tolerant systems;Physics,cloud computing;data handling;parallel processing;software reliability;virtualisation,Chiminey;software platform;software reliability;data management platform;cloud-based computing infrastructure;high-performance computing;virtual environment","keywords":"Cloud computing;HPC;reliability;fault tolerance;data management;data curation;data publication;data discovery;visualisation","startPage":"677","endPage":"680","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203041","citationCount":12,"referenceCount":13,"year":2015,"authors":"I. I. Yusuf; I. E. Thomas; M. Spichkova; S. Androulakis; G. R. Meyer; D. W. Drumm; G. Opletal; S. P. Russo; A. M. Buckle; H. W. Schmidt","affiliations":"Appl. Data Sci., Australia; RMIT Univ., Melbourne, VIC, Australia; RMIT Univ., Melbourne, VIC, Australia; Monash Univ., Melbourne, VIC, Australia; Monash Univ., Melbourne, VIC, Australia; RMIT Univ., Melbourne, VIC, Australia; RMIT Univ., Melbourne, VIC, Australia; RMIT Univ., Melbourne, VIC, Australia; Monash Univ., Melbourne, VIC, Australia; RMIT Univ., Melbourne, VIC, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3597"},"title":"From Developer Networks to Verified Communities: A Fine-Grained Approach","abstract":"Effective software engineering demands a coordinated effort. Unfortunately, a comprehensive view on developer coordination is rarely available to support software-engineering decisions, despite the significant implications on software quality, software architecture, and developer productivity. We present a fine-grained, verifiable, and fully automated approach to capture a view on developer coordination, based on commit information and source-code structure, mined from version-control systems. We apply methodology from network analysis and machine learning to identify developer communities automatically. Compared to previous work, our approach is fine-grained, and identifies statistically significant communities using order-statistics and a community-verification technique based on graph conductance. To demonstrate the scalability and generality of our approach, we analyze ten open-source projects with complex and active histories, written in various programming languages. By surveying 53 open-source developers from the ten projects, we validate the authenticity of inferred community structure with respect to reality. Our results indicate that developers of open-source projects form statistically significant community structures and this particular view on collaboration largely coincides with developers' perceptions of real-world collaboration.","conference":"IEEE","terms":"Collaboration;Open source software;Measurement;Computer languages;Standards;Software systems,configuration management;graph theory;learning (artificial intelligence);software architecture;software quality;source code (software);statistics,software developer networks;software engineering;software developer coordination;software quality;software architecture;software developer productivity;source-code structure;version-control systems;network analysis;machine learning;order-statistics;community-verification technique;graph conductance","keywords":"","startPage":"563","endPage":"573","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194606","citationCount":19,"referenceCount":34,"year":2015,"authors":"M. Joblin; W. Mauerer; S. Apel; J. Siegmund; D. Riehle","affiliations":"Siemens AG, Erlangen, Germany; OTH Regensburg, Siemens AG, Regensburg, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Friedrich-Alexander-Univ., Erlangen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3598"},"title":"Alloy*: A General-Purpose Higher-Order Relational Constraint Solver","abstract":"The last decade has seen a dramatic growth in the use of constraint solvers as a computational mechanism, not only for analysis of software, but also at runtime. Solvers are available for a variety of logics but are generally restricted to first-order formulas. Some tasks, however, most notably those involving synthesis, are inherently higher order; these are typically handled by embedding a first-order solver (such as a SAT or SMT solver) in a domain-specific algorithm. Using strategies similar to those used in such algorithms, we show how to extend a first-order solver (in this case Kodkod, a model finder for relational logic used as the engine of the Alloy Analyzer) so that it can handle quantifications over higher-order structures. The resulting solver is sufficiently general that it can be applied to a range of problems; it is higher order, so that it can be applied directly, without embedding in another algorithm; and it performs well enough to be competitive with specialized tools. Just as the identification of first-order solvers as reusable backends advanced the performance of specialized tools and simplified their architecture, factoring out higher-order solvers may bring similar benefits to a new class of tools.","conference":"IEEE","terms":"Metals;Syntactics;Semantics;Data structures;Boolean functions;Concrete;Engines,constraint handling,general-purpose higher-order relational constraint solver;software analysis;runtime analysis;first-order solver;domain-specific algorithm;Kodkod model;relational logic;Alloy Analyzer engine","keywords":"alloy;constraint solving;higher-order","startPage":"609","endPage":"619","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194610","citationCount":31,"referenceCount":43,"year":2015,"authors":"A. Milicevic; J. P. Near; E. Kang; D. Jackson","affiliations":"Massachusetts Inst. of Technol., Cambridge, MA, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d3599"},"title":"Lightweight Adaptive Filtering for Efficient Learning and Updating of Probabilistic Models","abstract":"Adaptive software systems are designed to cope with unpredictable and evolving usage behaviors and environmental conditions. For these systems reasoning mechanisms are needed to drive evolution, which are usually based on models capturing relevant aspects of the running software. The continuous update of these models in evolving environments requires efficient learning procedures, having low overhead and being robust to changes. Most of the available approaches achieve one of these goals at the price of the other. In this paper we propose a lightweight adaptive filter to accurately learn time-varying transition probabilities of discrete time Markov models, which provides robustness to noise and fast adaptation to changes with a very low overhead. A formal stability, unbiasedness and consistency assessment of the learning approach is provided, as well as an experimental comparison with state-of-the-art alternatives.","conference":"IEEE","terms":"Estimation;Noise;Asymptotic stability;Markov processes;Current measurement;Probabilistic logic;Robustness,adaptive filters;learning (artificial intelligence);Markov processes;mathematics computing;signal denoising,lightweight adaptive filtering;probabilistic model learning;probabilistic model updating;adaptive software systems;reasoning mechanisms;learning procedures;discrete time Markov models;time-varying transition probabilities;learning approach","keywords":"","startPage":"200","endPage":"211","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194574","citationCount":20,"referenceCount":48,"year":2015,"authors":"A. Filieri; L. Grunske; A. Leva","affiliations":"Univ. of Stuttgart, Stuttgart, Germany; Univ. of Stuttgart, Stuttgart, Germany; Politec. di Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d359a"},"title":"Striving for Failure: An Industrial Case Study about Test Failure Prediction","abstract":"Software regression testing is an important, yet very costly, part of most major software projects. When regression tests run, any failures that are found help catch bugs early and smooth the future development work. The act of executing large numbers of tests takes significant resources that could, otherwise, be applied elsewhere. If tests could be accurately classified as likely to pass or fail prior to the run, it could save significant time while maintaining the benefits of early bug detection. In this paper, we present a case study to build a classifier for regression tests based on industrial software, Microsoft Dynamics AX. In this study, we examine the effectiveness of this classification as well as which aspects of the software are the most important in predicting regression test failures.","conference":"IEEE","terms":"Software;Predictive models;Complexity theory;History;Prediction algorithms;Testing;Software engineering,pattern classification;program testing;regression analysis;software reliability,regression test failure prediction;software regression testing;software projects;early bug detection;Microsoft Dynamics AX industrial software;software classification","keywords":"Test failure prediction;data-mining software repositories;regression testing;case study","startPage":"49","endPage":"58","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202949","citationCount":5,"referenceCount":26,"year":2015,"authors":"J. Anderson; S. Salem; H. Do","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d359b"},"title":"Merits of Organizational Metrics in Defect Prediction: An Industrial Replication","abstract":"Defect prediction models presented in the literature lack generalization unless the original study can be replicated using new datasets and in different organizational settings. Practitioners can also benefit from replicating studies in their own environment by gaining insights and comparing their findings with those reported. In this work, we replicated an earlier study in order to investigate the merits of organizational metrics in building defect prediction models for large-scale enterprise software. We mined the organizational, code complexity, code churn and pre-release bug metrics of that large scale software and built defect prediction models for each metric set. In the original study, organizational metrics were found to achieve the highest performance. In our case, models based on organizational metrics performed better than models based on churn metrics but were outperformed by pre-release metric models. Further, we verified four individual organizational metrics as indicators for defects. We conclude that the performance of different metric sets in building defect prediction models depends on the project's characteristics and the targeted prediction level. Our replication of earlier research enabled assessing the validity and limitations of organizational metrics in a different context.","conference":"IEEE","terms":"Measurement;Software;Organizations;Software engineering;Predictive models;Context;Principal component analysis,software metrics;software reliability,organizational metrics;defect prediction models;large-scale enterprise software;organizational metric;code complexity metric;code churn metric;pre-release bug metric","keywords":"software engineering;replication;organizational metrics;defect prediction;model comparison","startPage":"89","endPage":"98","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202953","citationCount":8,"referenceCount":25,"year":2015,"authors":"B. Caglayan; B. Turhan; A. Bener; M. Habayeb; A. Miransky; E. Cialini","affiliations":"Dept. of Math., Ryerson Univ., Toronto, ON, Canada; Dept. of Inf. Process. Sci., Univ. of Oulu, Oulu, Finland; Mech. \u0026 Ind. Eng., Ryerson Univ., Toronto, ON, Canada; Mech. \u0026 Ind. Eng., Ryerson Univ., Toronto, ON, Canada; Dept. of Comput. Sci., Ryerson Univ., Toronto, ON, Canada; IBM Toronto Software Lab., Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d359c"},"title":"SOA4DM: Applying an SOA Paradigm to Coordination in Humanitarian Disaster Response","abstract":"Despite efforts to achieve a sustainable state of control over the management of global crises, disasters are occurring with greater frequency, intensity, and affecting many more people than ever before while the resources to deal with them do not grow apace. As we enter 2015, with continued concerns that mega-crises may become the new normal, we need to develop novel methods to improve the efficiency and effectiveness of our management of disasters. Software engineering as a discipline has long had an impact on society beyond its role in the development of software systems. In fact, software engineers have been described as the developers of prototypes for future knowledge workers; tools such as Github and Stack Overflow have demonstrated applications beyond the domain of software engineering. In this paper, we take the potential influence of software engineering one-step further and propose using the software service engineering paradigm as a new approach to managing disasters. Specifically, we show how the underlying principles of service-oriented architectures (SOA) can be applied to the coordination of disaster response operations. We describe key challenges in coordinating disaster response and discuss how an SOA approach can address those challenges.","conference":"IEEE","terms":"Societies;Software engineering;Communities;Conferences;Indexes;Economics;Semiconductor optical amplifiers,emergency management;service-oriented architecture,SOA4DM;SOA paradigm;humanitarian disaster response;sustainable state;global crises management;disaster management;efficiency improvement;effectiveness improvement;software system development;knowledge workers;github overflow;stack overflow;software service engineering paradigm;service-oriented architectures;disaster response operation coordination","keywords":"SOA;Disaster Response","startPage":"519","endPage":"522","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203004","citationCount":1,"referenceCount":34,"year":2015,"authors":"K. Lyons; C. Oh","affiliations":"Fac. of Inf., Univ. of Toronto, Toronto, ON, Canada; Fac. of Inf., Univ. of Toronto, Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d359d"},"title":"The Development of a Dashboard Tool for Visualising Online Teamwork Discussions","abstract":"Many software development organisations today adopt global software engineering (GSE) and agile models, requiring software engineers to collaborate and develop software in flexible, distributed, online teams. However, many employers have expressed concern that graduates lack teamwork skills and one of the most commonly occurring problems with GSE models are issues with project management. Team managers and educators often oversee a number of teams and the large corpus of data, in combination with agile models, make it difficult to efficiently assess factors such as team role distribution and emotional climate. Current methods and tools for monitoring software engineering (SE) teamwork in both industry and education settings typically focus on member contributions, reflection, or product outcomes, which are limited in terms of real-time feedback and accurate behavioural analysis. We have created a dashboard that extracts and communicates team role distribution and team emotion information in real-time. Our proof of concept provides a real-time analysis of teamwork discussions and visualises team member emotions, the roles they have adopted and overall team sentiment during the course of a collaborative problem-solving project. We demonstrate and discuss how such a tool could be useful for SE team management and training and the development of teamwork skills in SE university courses.","conference":"IEEE","terms":"Teamwork;Software;Monitoring;Training;Industries,computer science education;data visualisation;groupware;project management;software development management;software prototyping;software tools,SE university courses;teamwork skill development;training;SE team management;collaborative problem-solving project;real-time analysis;behavioural analysis;real-time feedback;software engineering teamwork monitoring;emotional climate;team role distribution;project management;agile models;GSE;global software engineering;software development organisations;online teamwork discussion visualisation;dashboard tool development","keywords":"","startPage":"380","endPage":"388","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202988","citationCount":7,"referenceCount":41,"year":2015,"authors":"R. Vivian; H. Tarmazdi; K. Falkner; N. Falkner; C. Szabo","affiliations":"Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d359e"},"title":"Active and Inductive Learning in Software Engineering Education","abstract":"If software engineering education is done in a traditional lecture-oriented style students have no other choice than believing that the solutions they are told actually work for a problem that they never encountered themselves. In order to overcome this problem, this paper describes an approach which allows students to better understand why software engineering and several of its core methods and techniques are needed, thus preparing them better for their professional life. This approach builds on active and inductive learning. Exercises that make students actively discover relevant software engineering issues are described in detail together with their pedagogical underpinning.","conference":"IEEE","terms":"Software;Education;Programming profession;Vehicles;Capability maturity model,computer science education;software engineering,inductive learning;active learning;software engineering education;lecture-oriented style students","keywords":"software engineering education;didactical approach;inductive learning;active learning;higher education","startPage":"418","endPage":"427","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202992","citationCount":10,"referenceCount":20,"year":2015,"authors":"Y. Sedelmaier; D. Landes","affiliations":"Fac. of Electr. Eng. \u0026 Inf., Univ. of Appl. Sci. \u0026 Arts, Coburg, Germany; Fac. of Electr. Eng. \u0026 Inf., Univ. of Appl. Sci. \u0026 Arts, Coburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d359f"},"title":"Database-Backed Program Analysis for Scalable Error Propagation","abstract":"Software is rapidly increasing in size and complexity. Static analyses must be designed to scale well if they are to be usable with realistic applications, but prior efforts have often been limited by available memory. We propose a database-backed strategy for large program analysis based on graph algorithms, using a Semantic Web database to manage representations of the program under analysis. Our approach is applicable to a variety of interprocedural finite distributive subset (IFDS) dataflow problems; we focus on error propagation as a motivating example. Our implementation analyzes multi-million-line programs quickly and in just a fraction of the memory required by prior approaches. When memory alone is insufficient, our approach falls back on disk using several hybrid configurations tuned to put all available resources to good use.","conference":"IEEE","terms":"Databases;Scalability;Linux;Pattern matching;Memory management;Kernel,data flow computing;graph theory;program diagnostics;semantic Web,database-backed program analysis;error propagation;static analyses;database-backed strategy;graph algorithms;semantic Web database;program representation management;interprocedural finite distributive subset dataflow problems;IFDS dataflow problems;multimillion-line programs","keywords":"","startPage":"586","endPage":"597","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194608","citationCount":6,"referenceCount":75,"year":2015,"authors":"C. Weiss; C. Rubio-González; B. Liblit","affiliations":"NA; Univ. of California, Davis, Davis, CA, USA; Univ. of Wisconsin - Madison, Madison, WI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a0"},"title":"Compositional Symbolic Execution with Memoized Replay","abstract":"Symbolic execution is a powerful, systematic analysis that has received much visibility in the last decade. Scalability however remains a major challenge for symbolic execution. Compositional analysis is a well-known general purpose methodology for increasing scalability. This paper introduces a new approach for compositional symbolic execution. Our key insight is that we can summarize each analyzed method as a memoization tree that captures the crucial elements of symbolic execution, and leverage these memoization trees to efficiently replay the symbolic execution of the corresponding methods with respect to their calling contexts. Memoization trees offer a natural way to compose in the presence of heap operations, which cannot be dealt with by previous work that uses logical formulas as summaries for compositional symbolic execution. Our approach also enables efficient target oriented symbolic execution for error detection or program coverage. Initial experimental evaluation based on a prototype implementation in Symbolic Path Finder shows that our approach can be up to an order of magnitude faster than traditional non-compositional symbolic execution.","conference":"IEEE","terms":"Context;Concrete;Scalability;Systematics;Data structures;Prototypes,error detection;program diagnostics;trees (mathematics),compositional symbolic execution;memoized replay;memoization trees;heap operations;target oriented symbolic execution;error detection;program coverage;symbolic path finder","keywords":"","startPage":"632","endPage":"642","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194612","citationCount":12,"referenceCount":25,"year":2015,"authors":"R. Qiu; G. Yang; C. S. Pasareanu; S. Khurshid","affiliations":"Univ. of Texas, Austin, TX, USA; Texas State Univ., San Marcos, TX, USA; NASA Ames Res. Center, Carnegie Mellon Univ., Moffett Field, CA, USA; Univ. of Texas, Austin, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a1"},"title":"Supporting Selective Undo in a Code Editor","abstract":"Programmers often need to revert some code to an earlier state, or restore a block of code that was deleted a while ago. However, support for this backtracking in modern programming environments is limited. Many of the backtracking tasks can be accomplished by having a selective undo feature in code editors, but this has major challenges: there can be conflicts among edit operations, and it is difficult to provide usable interfaces for selective undo. In this paper, we present AZURITE, an Eclipse plug-in that allows programmers to selectively undo fine-grained code changes made in the code editor. With AZURITE, programmers can easily perform backtracking tasks, even when the desired code is not in the undo stack or a version control system. AZURITE also provides novel user interfaces specifically designed for selective undo, which were iteratively improved through user feedback gathered from actual users in a preliminary field trial. A formal lab study showed that programmers can successfully use AZURITE, and were twice as fast as when limited to conventional features.","conference":"IEEE","terms":"History;Layout;Encoding;Control systems;Graphical user interfaces,programming environments;text editing;user interfaces,selective undo;code editor;programming environments;backtracking tasks;AZURITE;Eclipse plug-in;user interfaces;user feedback","keywords":"selective undo;backtracking","startPage":"223","endPage":"233","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194576","citationCount":19,"referenceCount":39,"year":2015,"authors":"Y. Yoon; B. A. Myers","affiliations":"Inst. for Software Res., Carnegie Mellon Univ., Pittsburgh, PA, USA; Human-Comput. Interaction Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a2"},"title":"An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes","abstract":"Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.","conference":"IEEE","terms":"Software engineering;Information retrieval;Testing;Standards;Open source software;Natural languages,information retrieval;program debugging;program testing;public domain software;system monitoring,information retrieval approach;regression test prioritization;program changes;regression testing;regression suites;bugs;dynamic analysis;REPiR;open-source IR toolkit Indri;open-source Java projects","keywords":"Regression Testing;Test Prioritization;Information Retrieval","startPage":"268","endPage":"279","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194580","citationCount":25,"referenceCount":67,"year":2015,"authors":"R. K. Saha; L. Zhang; S. Khurshid; D. E. Perry","affiliations":"Electr. \u0026 Comput. Eng., Univ. of Texas at Austin, Austin, TX, USA; Dept. of Comput. Sci., Univ. of Texas at Dallas, Dallas, TX, USA; Electr. \u0026 Comput. Eng., Univ. of Texas at Austin, Austin, TX, USA; Electr. \u0026 Comput. Eng., Univ. of Texas at Austin, Austin, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a3"},"title":"Measuring Dependency Freshness in Software Systems","abstract":"Modern software systems often make use of third-party components to speed-up development and reduce maintenance costs. In return, developers need to update to new releases of these dependencies to avoid, for example, security and compatibility risks. In practice, prioritizing these updates is difficult because the use of outdated dependencies is often opaque. In this paper we aim to make this concept more transparent by introducing metrics to quantify the use of recent versions of dependencies, i.e. The system's \"dependency freshness\". We propose and investigate a system-level metric based on an industry benchmark. We validate the usefulness of the metric using interviews, analyze the variance of the metric through time, and investigate the relationship between outdated dependencies and security vulnerabilities. The results show that the measurements are considered useful, and that systems using outdated dependencies four times as likely to have security issues as opposed to systems that are up-to-date.","conference":"IEEE","terms":"Software engineering;Software systems;Security;Context;Industries;Software measurement,cost reduction;object-oriented programming;security of data;software maintenance;software metrics,dependency freshness measurement;software systems;third-party components;maintenance cost reduction;system-level metric;industry benchmark;outdated dependencies;security vulnerabilities","keywords":"software metrics;software maintenance","startPage":"109","endPage":"118","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202955","citationCount":12,"referenceCount":15,"year":2015,"authors":"J. Cox; E. Bouwers; M. v. Eekelen; J. Visser","affiliations":"Inst. for Comput. \u0026 Inf. Sci., Radboud Univ. Nijmegen, Nijmegen, Netherlands; Software Improvement Group, Amsterdam, Netherlands; NA; Inst. for Comput. \u0026 Inf. Sci., Radboud Univ. Nijmegen, Nijmegen, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a4"},"title":"Dementia and Social Sustainability: Challenges for Software Engineering","abstract":"Dementia is a serious threat to social sustainability. As life expectancy increases, more people are developing dementia. At the same time, demographic change is reducing the economically active part of the population. Care of people with dementia imposes great emotional and financial strain on sufferers, their families and society at large. In response, significant research resources are being focused on dementia. One research thread is focused on using computer technology to monitor people in at-risk groups to improve rates of early diagnosis. In this paper we provide an overview of dementia monitoring research and identify a set of scientific challenges for the engineering of dementia-monitoring software, with implications for other mental health self-management systems.","conference":"IEEE","terms":"Dementia;Monitoring;Software;Games;Computers;Software engineering,medical computing;social sciences computing;software engineering,software engineering;social sustainability;demographic change;computer technology;dementia monitoring research;mental health self-management systems","keywords":"Software engineering;Dementia;Social sustainability","startPage":"527","endPage":"530","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203006","citationCount":5,"referenceCount":24,"year":2015,"authors":"P. Sawyer; A. Sutcliffe; P. Rayson; C. Bull","affiliations":"Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK; Sch. of Comput. \u0026 Commun., Lancaster Univ., Lancaster, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a5"},"title":"CodeAware: Sensor-Based Fine-Grained Monitoring and Management of Software Artifacts","abstract":"Current continuous integration (CI) tools, although extensible, can be limiting in terms of flexibility. In particular, artifact analysis capabilities available through plug in mechanisms are both coarse-grained and centralized. To address this limitation, this paper introduces a new paradigm, Code Aware, for distributed and fine-grained artifact analysis. Code Aware is an ecosystem inspired by sensor networks, consisting of monitors and actuators, aimed at improving code quality and team productivity. Code ware's vision entails (a) the ability to probe software artifacts of any granularity and localization, from variables to classes or files to entire systems, (b) the ability to perform both static and dynamic analyses on these artifacts, and (c) the ability to describe targeted remediation actions, for example to notify interested developers, through automated actuators. We provide motivational examples for the use of Code Aware that leverage current CI solutions, sketch the architecture of its underlying ecosystem, and outline research challenges.","conference":"IEEE","terms":"Probes;Software;Ecosystems;Software engineering;Monitoring;Electronic mail;DSL,productivity;program diagnostics;software quality,CodeAware;sensor-based fine-grained monitoring;software artifact management;continuous integration tools;CI tools;artifact analysis capabilities;plug in mechanisms;distributed artifact analysis;fine-grained artifact analysis;ecosystem;monitors;code quality improvement;team productivity improvement;static analysis;dynamic analysis;automated actuators","keywords":"","startPage":"551","endPage":"554","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203010","citationCount":5,"referenceCount":9,"year":2015,"authors":"R. Abreu; H. Erdogmus; A. Perez","affiliations":"Palo Alto Res. Center, Palo Alto, CA, USA; Carnegie Mellon Univ. - Silicon Valley, Moffett Field, CA, USA; Palo Alto Res. Center, Palo Alto, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a6"},"title":"Experiences in Developing and Delivering a Programme of Part-Time Education in Software and Systems Security","abstract":"We report upon our experiences in developing and delivering a programme of part-time education in Software and Systems Security at the University of Oxford. The MSc in Software and Systems Security is delivered as part of the Software Engineering Programme at Oxford - a collection of one-week intensive courses aimed at individuals who are responsible for the procurement, development, deployment and maintenance of large-scale software-based systems. We expect that our experiences will be useful to those considering a similar journey.","conference":"IEEE","terms":"Software engineering;Software;Education;Data privacy;Access control;Context,computer science education;educational courses;security of data;software maintenance,large-scale software-based systems;software maintenance;software deployment;software development;one-week intensive courses;software engineering programme;MSc;University of Oxford;systems security;software security;part-time education","keywords":"software engineering education;security education","startPage":"435","endPage":"444","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202994","citationCount":1,"referenceCount":36,"year":2015,"authors":"A. Simpson; A. Martin; C. Cremers; I. Flechais; I. Martinovic; K. Rasmussen","affiliations":"Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a7"},"title":"Combining Symbolic Execution and Model Checking for Data Flow Testing","abstract":"Data flow testing (DFT) focuses on the flow of data through a program. Despite its higher fault-detection ability over other structural testing techniques, practical DFT remains a significant challenge. This paper tackles this challenge by introducing a hybrid DFT framework: (1) The core of our framework is based on dynamic symbolic execution (DSE), enhanced with a novel guided path search to improve testing performance, and (2) we systematically cast the DFT problem as reach ability checking in software model checking to complement our DSE-based approach, yielding a practical hybrid DFT technique that combines the two approaches' respective strengths. Evaluated on both open source and industrial programs, our DSE-based approach improves DFT performance by 60~80% in terms of testing time compared with state-of-the-art search strategies, while our combined technique further reduces 40% testing time and improves data-flow coverage by 20% by eliminating infeasible test objectives. This combined approach also enables the cross-checking of each component for reliable and robust testing results.","conference":"IEEE","terms":"Search problems;Model checking;Discrete Fourier transforms;Software;Engines;Safety,data flow computing;formal verification;program testing;public domain software;software fault tolerance;software performance evaluation,data flow testing;fault-detection ability;structural testing;hybrid DFT framework;dynamic symbolic execution;DSE;guided path search;performance testing;software model checking;open source program;industrial program","keywords":"symbolic execution;model checking;data flow coverage;data flow testing;coverage criteria","startPage":"654","endPage":"665","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194614","citationCount":7,"referenceCount":61,"year":2015,"authors":"T. Su; Z. Fu; G. Pu; J. He; Z. Su","affiliations":"Shanghai Key Lab. of Trustworthy Comput., East China Normal Univ., Shanghai, China; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Shanghai Key Lab. of Trustworthy Comput., East China Normal Univ., Shanghai, China; Shanghai Key Lab. of Trustworthy Comput., East China Normal Univ., Shanghai, China; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a8"},"title":"A Comprehensive Framework for the Development of Dynamic Smart Spaces","abstract":"The conception of reliable smart spaces requires a suitable and comprehensive framework for their design, implementation, testing, and deployment. Numerous solutions have been proposed to solve different aspects related to smart spaces, but we still lack a concrete framework that provides solutions suitable for the whole development life-cycle. This work aims to fill the gap and proposes a framework that provides: (i) well-defined abstractions for designing smart spaces, (ii) a middleware infrastructure to implement them and plug physical objects, (iii) a semantic layer to support heterogeneous elements, and (iv) plugs to integrate external simulators and be able to always work on \"complete'' systems in the different phases of the development.","conference":"IEEE","terms":"Buildings;Middleware;Conferences;Semantics;Intelligent sensors;Context,middleware,dynamic smart space development;development life-cycle;smart space designing;middleware infrastructure;semantic layer;heterogeneous elements;external simulators","keywords":"","startPage":"927","endPage":"930","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203116","citationCount":1,"referenceCount":11,"year":2015,"authors":"A. Shahzada","affiliations":"Dipt. di Elettron., Inf. e Bioingegneria, Politec. di Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35a9"},"title":"RECONTEST: Effective Regression Testing of Concurrent Programs","abstract":"Concurrent programs proliferate as multi-core technologies advance. The regression testing of concurrent programs often requires running a failing test for weeks before catching a faulty interleaving, due to the myriad of possible interleavings of memory accesses arising from concurrent program executions. As a result, the conventional approach that selects a sub-set of test cases for regression testing without considering interleavings is insufficient. In this paper we present RECONTEST to address the problem by selecting the new interleavings that arise due to code changes. These interleavings must be explored in order to uncover regression bugs. RECONTEST efficiently selects new interleavings by first identifying shared memory accesses that are affected by the changes, and then exploring only those problematic interleavings that contain at least one of these accesses. We have implemented RECONTEST as an automated tool and evaluated it using 13 real-world concurrent program subjects. Our results show that RECONTEST can significantly reduce the regression testing cost without missing any faulty interleavings induced by code changes.","conference":"IEEE","terms":"Concurrent computing;Testing;Programming;Message systems;Synchronization;Computer bugs;Integrated circuits,concurrency control;program testing,RECONTEST framework;regression concurrency testing framework;concurrent programs;multicore technologies;memory access;regression bugs;shared memory access identification;code changes","keywords":"","startPage":"246","endPage":"256","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194578","citationCount":10,"referenceCount":49,"year":2015,"authors":"V. Terragni; S. Cheung; C. Zhang","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35aa"},"title":"Do Security Patterns Really Help Designers?","abstract":"Security patterns are well-known solutions to security-specific problems. They are often claimed to benefit designers without much security expertise. We have performed an empirical study to investigate whether the usage of security patterns by such an audience leads to a more secure design, or to an increased productivity of the designers. Our study involved 32 teams of master students enrolled in a course on software architecture, working on the design of a realistically-sized banking system. Irrespective of whether the teams were using security patterns, we have not been able to detect a difference between the two treatment groups. However, the teams prefer to work with the support of security patterns.","conference":"IEEE","terms":"Security;Banking;Training;IEEE catalogs;Software;Context;Productivity,security of data;software architecture,security pattern;software design;software architecture;realistically-sized banking system","keywords":"","startPage":"292","endPage":"302","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194582","citationCount":16,"referenceCount":20,"year":2015,"authors":"K. Yskout; R. Scandariato; W. Joosen","affiliations":"iMinds-DistriNet, KU Leuven, Leuven, Belgium; iMinds-DistriNet, KU Leuven, Leuven, Belgium; iMinds-DistriNet, KU Leuven, Leuven, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35ab"},"title":"Metamorphic Model-Based Testing Applied on NASA DAT -- An Experience Report","abstract":"Testing is necessary for all types of systems, but becomes difficult when the tester cannot easily determine whether the system delivers the correct result or not. NASA's Data Access Toolkit allows NASA analysts to query a large database of telemetry data. Since the user is unfamiliar with the data and several data transformations can occur, it is impossible to determine whether the system behaves correctly or not in full scale production situations. Small scale testing was already conducted manually by other teams and unit testing was conducted on individual functions. However, there was still a need for full scale acceptance testing on a broad scale. We describe how we addressed this testing problem by applying the idea of metamorphic testing [1]. Specifically, we base it on equivalence of queries and by using the system itself for testing. The approach is implemented using a model-based testing approach in combination with a test data generation and test case outcome analysis strategy. We also discuss some of the issues that were detected using this approach.","conference":"IEEE","terms":"Testing;Databases;Grammar;Telemetry;NASA;Software engineering;Computational modeling,aerospace computing;program testing;query processing,metamorphic model-based testing;NASA DAT;systems testing;NASA data access toolkit;query;large database;telemetry data;data transformations;small scale testing;unit testing;full scale acceptance testing;test data generation;test case outcome analysis strategy","keywords":"","startPage":"129","endPage":"138","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202957","citationCount":27,"referenceCount":12,"year":2015,"authors":"M. Lindvall; D. Ganesan; R. Árdal; R. E. Wiegand","affiliations":"Fraunhofer USA Center for Exp. Software Eng. (CESE), MD, USA; Fraunhofer USA Center for Exp. Software Eng. (CESE), MD, USA; Fraunhofer USA Center for Exp. Software Eng. (CESE), MD, USA; NASA Goddard Space Flight Center, Greenbelt, MD, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35ac"},"title":"Industry Practices and Event Logging: Assessment of a Critical Software Development Process","abstract":"Practitioners widely recognize the importance of event logging for a variety of tasks, such as accounting, system measurements and troubleshooting. Nevertheless, in spite of the importance of the tasks based on the logs collected under real workload conditions, event logging lacks systematic design and implementation practices. The implementation of the logging mechanism strongly relies on the human expertise. This paper proposes a measurement study of event logging practices in a critical industrial domain. We assess a software development process at Selex ES, a leading Finmeccanica company in electronic and information solutions for critical systems. Our study combines source code analysis, inspection of around 2.3 millions log entries, and direct feedback from the development team to gain process-wide insights ranging from programming practices, logging objectives and issues impacting log analysis. The findings of our study were extremely valuable to prioritize event logging reengineering tasks at Selex ES.","conference":"IEEE","terms":"Software;Inspection;Programming;Encoding;Industries;Runtime,program diagnostics;software engineering;system monitoring,critical software development process assessment;event logging mechanism;measurement study;event logging practices;critical industrial domain;Selex ES;electronic and information solutions;source code analysis;log entry inspection;log analysis;event logging reengineering tasks","keywords":"Source code analysis;Event logging;Development process;Coding practices;Industry domain","startPage":"169","endPage":"178","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202961","citationCount":22,"referenceCount":31,"year":2015,"authors":"A. Pecchia; M. Cinque; G. Carrozza; D. Cotroneo","affiliations":"Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy; Selex ES S.p.A., Finmeccanica Co., Rome, Italy; Dipt. di Ing. Elettr. e Tecnol. dell'Inf., Univ. degli Studi di Napoli Federico II, Naples, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35ad"},"title":"New Initiative: The Naturalness of Software","abstract":"This paper describes a new research consortium, studying the Naturalness of Software. This initiative is supported by a pair of grants by the US National Science Foundation, totaling $2,600,000: the first, exploratory (\"EAGER\") grant of $600,000 helped kickstart an inter-disciplinary effort, and demonstrate feasibility; a follow-on full grant of $2,000,000 was recently awarded. The initiative is led by the author, who is at UC Davis, and includes investigators from Iowa State University and Carnegie-Mellon University (Language Technologies Institute).","conference":"IEEE","terms":"Software;Pragmatics;Java;Software engineering;Programming;Predictive models;Computers,computational linguistics;software engineering,software naturalness;US National Science Foundation;EAGER;UC Davis;Iowa State University;Carnegie-Mellon University;Language Technologies Institute","keywords":"language modeling;NLP;software engineering;big code","startPage":"543","endPage":"546","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203008","citationCount":3,"referenceCount":24,"year":2015,"authors":"P. Devanbu","affiliations":"Dept. of Comput. Sci., UC Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35ae"},"title":"How (Much) Do Developers Test?","abstract":"What do we know about software testing in the real world? It seems we know from Fred Brooks' seminal work \"The Mythical Man-Month\" that 50% of project effort is spent on testing. However, due to the enormous advances in software engineering in the past 40 years, the question stands: Is this observation still true? In fact, was it ever true? The vision for our research is to settle the discussion about Brooks' estimation once and for all: How much do developers test? Does developers' estimation on how much they test match reality? How frequently do they execute their tests, and is there a relationship between test runtime and execution frequency? What are the typical reactions to failing tests? Do developers solve actual defects in the production code, or do they merely relax their test assertions? Emerging results from 40 software engineering students show that students overestimate their testing time threefold, and 50% of them test as little as 4% of their time, or less. Having proven the scalability of our infrastructure, we are now extending our case study with professional software engineers from open-source and industrial organizations.","conference":"IEEE","terms":"Testing;Production;Software engineering;Java;Estimation;Open source software,program testing,software testing;test runtime;execution frequency;production code;professional software engineers;open-source organizations;industrial organizations","keywords":"","startPage":"559","endPage":"562","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203012","citationCount":20,"referenceCount":14,"year":2015,"authors":"M. Beller; G. Gousios; A. Zaidman","affiliations":"Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35af"},"title":"CS/SE Instructors Can Improve Student Writing without Reducing Class Time Devoted to Technical Content: Experimental Results","abstract":"The Computer Science and Software Engineering (CS/SE) profession reports that new college graduates lack the communication skills needed for personal and organizational success. Many CS/SE faculty may omit communication instruction from their courses because they do not want to reduce technical content. We experimented in a software-engineering-intensive second-semester programming course with strategies for improving students' writing of black box test plans that included no instruction on writing the plans beyond the standard lecture on testing. The treatment version of the course used 1) a modified assignment that focused on the plan's readers, 2) a model plan students could consult online, and 3) a modified grading rubric that identified the readers' needs. Three external raters found that students in the treatment sections outperformed students in the control sections on writing for five of nine criteria on rubrics for evaluating the plans and on the raters' holistic impression of the students' technical and communication abilities from the perspectives of a manager and a tester.","conference":"IEEE","terms":"Writing;Software testing;Software engineering;Employment;Programming;Education,computer science education;educational courses;software engineering,CS-SE instructors;student writing improvement;computer science and software engineering profession;communication instruction;software-engineering-intensive second-semester programming course;black box test plans","keywords":"communication across the curriculum;software engineering education;black box test plans","startPage":"455","endPage":"464","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202996","citationCount":1,"referenceCount":24,"year":2015,"authors":"P. V. Anderson; S. Heckman; M. Vouk; D. Wright; M. Carter; J. E. Burge; G. C. Gannod","affiliations":"Elon Univ., Elon, NC, USA; North Carolina State Univ., Raleigh, NC, USA; North Carolina State Univ., Raleigh, NC, USA; North Carolina State Univ., Raleigh, NC, USA; North Carolina State Univ., Raleigh, NC, USA; Wesleyan Univ., Middletown, CT, USA; Miami Univ., Oxford, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b0"},"title":"Why Good Developers Write Bad Code: An Observational Case Study of the Impacts of Organizational Factors on Software Quality","abstract":"How can organizational factors such as structure and culture have an impact on the working conditions of developers? This study is based on ten months of observation of an in-house software development project within a large telecommunications company. The observation was conducted during mandatory weekly status meetings, where technical and managerial issues were raised and discussed. Preliminary results show that many decisions made under the pressure of certain organizational factors negatively affected software quality. This paper describes cases depicting the complexity of organizational factors and reports on ten issues that have had a negative impact on quality, followed by suggested avenues for corrective action.","conference":"IEEE","terms":"Software;Companies;Testing;Contracts;Documentation;Software engineering,organisational aspects;software development management;software quality,organizational factors;software quality;in-house software development project;technical issues;managerial issues;corrective action","keywords":"Organizational factors;software quality;observational case study","startPage":"677","endPage":"687","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194616","citationCount":11,"referenceCount":26,"year":2015,"authors":"M. Lavallée; P. N. Robillard","affiliations":"Dept. de Genie Inf. et Genie Logiciel, Polytech. Montreal, Montréal, QC, Canada; Dept. de Genie Inf. et Genie Logiciel, Polytech. Montreal, Montréal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b1"},"title":"Empirical Study Towards a Leading Indicator for Cost of Formal Software Verification","abstract":"Formal verification can provide the highest degree of software assurance. Demand for it is growing, but there are still few projects that have successfully applied it to sizeable, real-world systems. This lack of experience makes it hard to predict the size, effort and duration of verification projects. In this paper, we aim to better understand possible leading indicators of proof size. We present an empirical analysis of proofs from the landmark formal verification of the seL4 microkernel and the two largest software verification proof developments in the Archive of Formal Proofs. Together, these comprise 15,018 individual lemmas and approximately 215,000 lines of proof script. We find a consistent quadratic relationship between the size of the formal statement of a property, and the final size of its formal proof in the interactive theorem prover Isabelle. Combined with our prior work, which has indicated that there is a strong linear relationship between proof effort and proof size, these results pave the way for effort estimation models to support the management of large-scale formal verification projects.","conference":"IEEE","terms":"Size measurement;Software;Complexity theory;Estimation;Approximation methods;Mathematical model,formal verification;theorem proving,cost indicator;formal software verification;software assurance;formal proof;interactive theorem prover;Isabelle","keywords":"seL4;Isabelle;proof engineering;formal verification","startPage":"722","endPage":"732","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194620","citationCount":11,"referenceCount":36,"year":2015,"authors":"D. Matichuk; T. Murray; J. Andronick; R. Jeffery; G. Klein; M. Staples","affiliations":"NICTA, Sydney, NSW, Australia; NICTA, Sydney, NSW, Australia; NICTA, Sydney, NSW, Australia; NICTA, Sydney, NSW, Australia; NICTA, Sydney, NSW, Australia; NICTA, Sydney, NSW, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b2"},"title":"Multi-objective Integer Programming Approaches for Solving Optimal Feature Selection Problem: A New Perspective on Multi-objective Optimization Problems in SBSE","abstract":"The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study, we first expose the mathematical nature of this problem - multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly, less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).","conference":"IEEE","terms":"Feature extraction;Linear programming;IP networks;Optimization;Software;Encryption;Graphical user interfaces,computational complexity;evolutionary computation;integer programming;Pareto optimisation,optimal feature selection problem;multiobjective optimization problems;software product line;IBEA;multiobjective binary integer linear programming;mathematical programming approaches;small-scale problems;medium-to-large problems;indicator-based evolutionary algorithm;SBSE;Pareto front;linear time complexity","keywords":"Optimal Feature Selection Problem;Multi Objective Optimization(MOO);Multi Objective Integer Programming (MOIP);Indicator Based Evolutionary Algorithm (IBEA);IBED","startPage":"1231","endPage":"1242","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453205","citationCount":1,"referenceCount":55,"year":2018,"authors":"Y. Xue; Y. Li","affiliations":"Univ. of Sci. \u0026 Technol. of China, Hefei, China; Dept. of Ind. Eng., Tsinghua Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b3"},"title":"Nomen est Omen: Exploring and Exploiting Similarities between Argument and Parameter Names","abstract":"Programmer-provided identifier names convey information about the semantics of a program. This information can complement traditional program analyses in various software engineering tasks, such as bug finding, code completion, and documentation. Even though identifier names appear to be a rich source of information, little is known about their properties and their potential usefulness. This paper presents an empirical study of the lexical similarity between arguments and parameters of methods, which is one prominent situation where names can provide otherwise missing information. The study involves 60 real-world Java programs. We find that, for most arguments, the similarity is either very high or very low, and that short and generic names often cause low similarities. Furthermore, we show that inferring a set of low-similarity parameter names from one set of programs allows for pruning such names in another set of programs. Finally, the study shows that many arguments are more similar to thecorresponding parameter than any alternative argument available in the call site's scope. As applications of our findings, we present an anomaly detection technique that identifies 144 renaming opportunities and incorrect arguments in 14 programs, and a code recommendation system that suggests correct arguments with a precision of 83%.","conference":"IEEE","terms":"Semantics;Java;Documentation;Open source software;Data collection,Java;program diagnostics;software engineering;text analysis,software engineering;lexical similarity;real-world Java programs;low-similarity parameter names;anomaly detection technique;code recommendation system;name-based program analysis","keywords":"","startPage":"1063","endPage":"1073","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886980","citationCount":8,"referenceCount":33,"year":2016,"authors":"H. Liu; Q. Liu; C. Staicu; M. Pradel; Y. Luo","affiliations":"Sch. of Comput. Sci. \u0026 Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. \u0026 Technol., Beijing Inst. of Technol., Beijing, China; Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany; Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany; Sch. of Comput. Sci. \u0026 Technol., Beijing Inst. of Technol., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b4"},"title":"Finding and Analyzing Compiler Warning Defects","abstract":"Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers.At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting?Our technique is very effective - we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers.","conference":"IEEE","terms":"Computer bugs;Program processors;Testing;Security;Production;Maintenance engineering,program compilers;program debugging,randomized differential testing technique;compiler warning defects;C compilers;bugs;random programs;textual warning alignment;GCC;Clang","keywords":"compiler warnings;compiler testing;differential testing","startPage":"203","endPage":"213","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886904","citationCount":5,"referenceCount":39,"year":2016,"authors":"C. Sun; V. Le; Z. Su","affiliations":"Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b5"},"title":"A Combinatorial Approach for Exposing Off-Nominal Behaviors","abstract":"Off-nominal behaviors (ONBs) have been a major concern in the areas of embedded systems and safety-critical systems. To address ONB problems, some researchers have proposed model-based approaches that can expose ONBs by analyzing natural language requirements documents. While these approaches produced promising results, they require a lot of human effort and time. In this paper, to reduce human effort and time, we propose a combinatorial-based approach, Combinatorial Causal Component Model (Combi-CCM), which uses structured requirements patterns and combinations generated using the IPOG algorithm. We conducted an empirical study using several requirements documents to evaluate our approach, and our results indicate that the proposed approach can reduce human effort and time while maintaining the same ONB exposure ability obtained by the control techniques.","conference":"IEEE","terms":"Ear;Electronic countermeasures;Robot sensing systems;Natural languages;Analytical models;Writing,combinatorial mathematics;embedded systems;formal specification;formal verification;natural language processing;safety-critical software,off-nominal behaviors;embedded systems;safety-critical systems;ONB problems;natural language requirements documents;combinatorial-based approach;requirements patterns;ONB exposure ability;combinatorial approach;combinatorial causal component model;IPOG algorithm","keywords":"Off-Nominal Behaviors;Requirements Verification;Combinatorial Approach;Model-based Approach","startPage":"910","endPage":"920","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453169","citationCount":2,"referenceCount":47,"year":2018,"authors":"K. Madala; H. Do; D. Aceituna","affiliations":"Univ. of North Texas, Denton, TX, USA; Univ. of North Texas, Denton, TX, USA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b6"},"title":"Multi-objective Software Effort Estimation","abstract":"We introduce a bi-objective effort estimation algorithm that combines Confidence Interval Analysis and assessment of Mean Absolute Error. We evaluate our proposed algorithm on three different alternative formulations, baseline comparators and current state-of-the-art effort estimators applied to five real-world datasets from the PROMISE repository, involving 724 different software projects in total. The results reveal that our algorithm outperforms the baseline, state-of-the-art and all three alternative formulations, statistically significantly (p \u003c; 0.001) and with large effect size (A12 ≥ 0.9) over all five datasets. We also provide evidence that our algorithm creates a new state-of-the-art, which lies within currently claimed industrial human-expert-based thresholds, thereby demonstrating that our findings have actionable conclusions for practicing software engineers.","conference":"IEEE","terms":"Estimation;Software;Uncertainty;Predictive models;Mathematical model;Software algorithms;Software engineering,software engineering,multi objective software effort estimation;confidence interval analysis;mean absolute error;software projects;software engineers","keywords":"Software effort estimation;multi-objective evolutionary algorithm;confidence interval;estimates uncertainty","startPage":"619","endPage":"630","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886939","citationCount":17,"referenceCount":92,"year":2016,"authors":"F. Sarro; A. Petrozziello; M. Harman","affiliations":"Univ. Coll. London, London, UK; Univ. of Portsmouth, Portsmouth, UK; Univ. Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b7"},"title":"Feature-Model Interfaces: The Highway to Compositional Analyses of Highly-Configurable Systems","abstract":"Today's software systems are often customizable by means of load-time or compile-time configuration options. These options are typically not independent and their dependencies can be specified by means of feature models. As many industrial systems contain thousands of options, the maintenance and utilization of feature models is a challenge for all stakeholders. In the last two decades, numerous approaches have been presented to support stakeholders in analyzing feature models. Such analyses are commonly reduced to satisfiability problems, which suffer from the growing number of options. While first attempts have been made to decompose feature models into smaller parts, they still require to compose all parts for analysis. We propose the concept of a feature-model interface that only consists of a subset of features, typically selected by experts, and hides all other features and dependencies. Based on a formalization of feature-model interfaces, we prove compositionality properties. We evaluate feature-model interfaces using a three-month history of an industrial feature model from the automotive domain with 18,616 features. Our results indicate performance benefits especially under evolution as often only parts of the feature model need to be analyzed again.","conference":"IEEE","terms":"Analytical models;Indexes;Load modeling;Stakeholders;Software systems;Automotive engineering,program diagnostics,feature-model interface;compositional analysis;highly-configurable systems;load-time configuration option;compile-time configuration option;satisfiability problems;compositionality properties","keywords":"Configurable Software;Software Product Line;Variability Modeling;Feature Model;Modularity;Compositionality","startPage":"667","endPage":"678","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886943","citationCount":1,"referenceCount":53,"year":2016,"authors":"R. Schröter; S. Krieter; T. Thüm; F. Benduhn; G. Saake","affiliations":"Univ. of Magdeburg, Magdeburg, Germany; Univ. of Magdeburg, Magdeburg, Germany; Tech. Univ. Braunschweig, Braunschweig, Germany; Univ. of Magdeburg, Magdeburg, Germany; Univ. of Magdeburg, Magdeburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b8"},"title":"When Testing Meets Code Review: Why and How Developers Review Tests","abstract":"Automated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research.","conference":"IEEE","terms":"Production;Computer bugs;Testing;Tools;Software quality;Measurement,program testing;software quality,developer review tests;code review tools;current code reviewing practices;review test files;software quality practice;production code;high-quality test code;automated testing","keywords":"software testing;automated testing;code review;Gerrit","startPage":"677","endPage":"687","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453136","citationCount":3,"referenceCount":0,"year":2018,"authors":"D. Spadini; M. Aniche; M. Storey; M. Bruntink; A. Bacchelli","affiliations":"Software Improvement Group, Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Univ. of Victoria, Victoria, BC, Canada; Software Improvement Group, Amsterdam, Netherlands; Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35b9"},"title":"Revisiting Code Ownership and Its Relationship with Software Quality in the Scope of Modern Code Review","abstract":"Code ownership establishes a chain of responsibility for modules in large software systems. Although prior work uncovers a link between code ownership heuristics and software quality, these heuristics rely solely on the authorship of code changes. In addition to authoring code changes, developers also make important contributions to a module by reviewing code changes. Indeed, recent work shows that reviewers are highly active in modern code review processes, often suggesting alternative solutions or providing updates to the code changes. In this paper, we complement traditional code ownership heuristics using code review activity. Through a case study of six releases of the large Qt and OpenStack systems, we find that: (1) 67%-86% of developers did not author any code changes for a module, but still actively contributed by reviewing 21%-39% of the code changes, (2) code ownership heuristics that are aware of reviewing activity share a relationship with software quality, and (3) the proportion of reviewers without expertise shares a strong, increasing relationship with the likelihood of having post-release defects. Our results suggest that reviewing activity captures an important aspect of code ownership, and should be included in approximations of it in future studies.","conference":"IEEE","terms":"Software quality;Birds;Software systems;Software design;Organizations;Conferences,software quality;source code (software),large software systems;code ownership heuristics;code change authorship;modern code review processes;code review activity;large Qt systems;OpenStack systems;software quality;post-release defects","keywords":"Ownership;Expertise;Software Quality","startPage":"1039","endPage":"1050","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886978","citationCount":11,"referenceCount":55,"year":2016,"authors":"P. Thongtanunam; S. McIntosh; A. E. Hassan; H. Iida","affiliations":"Nara Inst. of Sci. \u0026 Technol., Nara, Japan; McGill Univ., Montreal, QC, Canada; Queen's Univ., Kingston, ON, Canada; Nara Inst. of Sci. \u0026 Technol., Nara, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35ba"},"title":"Automatically Generating Search Heuristics for Concolic Testing","abstract":"We present a technique to automatically generate search heuristics for concolic testing. A key challenge in concolic testing is how to effectively explore the program's execution paths to achieve high code coverage in a limited time budget. Concolic testing employs a search heuristic to address this challenge, which favors exploring particular types of paths that are most likely to maximize the final coverage. However, manually designing a good search heuristic is nontrivial and typically ends up with suboptimal and unstable outcomes. The goal of this paper is to overcome this shortcoming of concolic testing by automatically generating search heuristics. We define a class of search heuristics, namely a parameterized heuristic, and present an algorithm that efficiently finds an optimal heuristic for each subject program. Experimental results with open-source C programs show that our technique successfully generates search heuristics that significantly outperform existing manually-crafted heuristics in terms of branch coverage and bug-finding.","conference":"IEEE","terms":"Heuristic algorithms;Manuals;Software engineering;Software algorithms;Software;Software testing,program debugging;program testing;search problems,concolic testing;good search heuristic;manually-crafted heuristics;program execution path","keywords":"Software Testing;Concolic Testing;Search Heuristics","startPage":"1244","endPage":"1254","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453207","citationCount":0,"referenceCount":33,"year":2018,"authors":"S. Cha; S. Hong; J. Lee; H. Oh","affiliations":"Korea Univ., Seoul, South Korea; Korea Univ., Seoul, South Korea; Korea Univ., Seoul, South Korea; Korea Univ., Seoul, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35bb"},"title":"Crowdsourcing Program Preconditions via a Classification Game","abstract":"Invariant discovery is one of the central problems in software verification. This paper reports on an approach that addresses this problem in a novel way; it crowdsources logical expressions for likely invariants by turning invariant discovery into a computer game. The game, called Binary Fission, employs a classification model. In it, players compose preconditions by separating program states that preserve or violate program assertions. The players have no special expertise in formal methods or programming, and are not specifically aware they are solving verification tasks. We show that Binary Fission players discover concise, general, novel, and human readable program preconditions. Our proof of concept suggests that crowdsourcing offers a feasible and promising path towards the practical application of verification technology.","conference":"IEEE","terms":"Games;Crowdsourcing;Software;Technological innovation;Computers;Gold;Software engineering,computer games;crowdsourcing;pattern classification;program verification,program precondition crowdsourcing;classification game;invariant discovery;computer game;Binary Fission;program assertions;verification technology","keywords":"crowdsourcing;software verification;invariant discovery;game with a purpose","startPage":"1086","endPage":"1096","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886982","citationCount":1,"referenceCount":30,"year":2016,"authors":"D. Fava; D. Shapiro; J. Osborn; M. Schaef; E. J. Whitehead","affiliations":"Univ. of California Santa Cruz, Santa Cruz, CA, USA; Univ. of California Santa Cruz, Santa Cruz, CA, USA; Univ. of California Santa Cruz, Santa Cruz, CA, USA; SRI Int., Menlo Park, CA, USA; Univ. of California Santa Cruz, Santa Cruz, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35bc"},"title":"Energy Profiles of Java Collections Classes","abstract":"We created detailed profiles of the energy consumed by common operations done on Java List, Map, and Set abstractions. The results show that the alternative data types for these abstractions differ significantly in terms of energy consumption depending on the operations. For example, an ArrayList consumes less energy than a LinkedList if items are inserted at the middle or at the end, but consumes more energy than a LinkedList if items are inserted at the start of the list. To explain the results, we explored the memory usage and the bytecode executed during an operation. Expensive computation tasks in the analyzed bytecode traces appeared to have an energy impact, but memory usage did not contribute. We evaluated our profiles by using them to selectively replace Collections types used in six applications and libraries. We found that choosing the wrong Collections type, as indicated by our profiles, can cost even 300% more energy than the most efficient choice. Our work shows that the usage context of a data structure and our measured energy profiles can be used to decide between alternative Collections implementations.","conference":"IEEE","terms":"Energy consumption;Java;Energy measurement;Encoding;Software engineering;Semiconductor device measurement;Software,Java,Java collections classes;energy profiles;ArrayList;Java List;Set abstractions;LinkedList","keywords":"Energy Profile;Collections;API;Java","startPage":"225","endPage":"236","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886906","citationCount":22,"referenceCount":54,"year":2016,"authors":"S. Hasan; Z. King; M. Hafiz; M. Sayagh; B. Adams; A. Hindle","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35bd"},"title":"Overcoming Open Source Project Entry Barriers with a Portal for Newcomers","abstract":"Community-based Open Source Software (OSS) projects are usually self-organized and dynamic, receiving contributions from distributed volunteers. Newcomer are important to the survival, long-term success, and continuity of these communities. However, newcomers face many barriers when making their first contribution to an OSS project, leading in many cases to dropouts. Therefore, a major challenge for OSS projects is to provide ways to support newcomers during their first contribution. In this paper, we propose and evaluate FLOSScoach, a portal created to support newcomers to OSS projects. FLOSScoach was designed based on a conceptual model of barriers created in our previous work. To evaluate the portal, we conducted a study with 65 students, relying on qualitative data from diaries, self-efficacy questionnaires, and the Technology Acceptance Model. The results indicate that FLOSScoach played an important role in guiding newcomers and in lowering barriers related to the orientation and contribution process, whereas it was not effective in lowering technical barriers. We also found that FLOSScoach is useful, easy to use, and increased newcomers' confidence to contribute. Our results can help project maintainers on deciding the points that need more attention in order to help OSS project newcomers overcome entry barriers.","conference":"IEEE","terms":"Portals;Documentation;Computer bugs;Joining processes;Industries;Open source software,portals;public domain software,open source project entry barriers;community-based open source software projects;OSS projects;FLOSScoach;technology acceptance model;Web portal","keywords":"Newcomers;Newbies;Novices;Beginners;Open Source Software;Barriers;Obstacles;Onboarding;Joining Process","startPage":"273","endPage":"284","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886910","citationCount":11,"referenceCount":52,"year":2016,"authors":"I. Steinmacher; T. U. Conte; C. Treude; M. A. Gerosa","affiliations":"Dept. of Comput., Fed. Univ. of Technol., Paraná, Brazil; Inst. of Comput., Fed. Univ. of Amazonas, Manaus, Brazil; Inst. of Math. \u0026 Stat., Univ. of Sao Paulo, Sao Paulo, Brazil; Inst. of Math. \u0026 Stat., Univ. of Sao Paulo, Sao Paulo, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35be"},"title":"Generalized Data Structure Synthesis","abstract":"Data structure synthesis is the task of generating data structure implementations from high-level specifications. Recent work in this area has shown potential to save programmer time and reduce the risk of defects. Existing techniques focus on data structures for manipulating subsets of a single collection, but real-world programs often track multiple related collections and aggregate properties such as sums, counts, minimums, and maximums. This paper shows how to synthesize data structures that track subsets and aggregations of multiple related collections. Our technique decomposes the synthesis task into alternating steps of query synthesis and incrementalization. The query synthesis step implements pure operations over the data structure state by leveraging existing enumerative synthesis techniques, specialized to the data structures domain. The incrementalization step implements imperative state modifications by re-framing them as fresh queries that determine what to change, coupled with a small amount of code to apply the change. As an added benefit of this approach over previous work, the synthesized data structure is optimized for not only the queries in the specification but also the required update operations. We have evaluated our approach in four large case studies, demonstrating that these extensions are broadly applicable.","conference":"IEEE","terms":"Data structures;Synthesizers;Task analysis;Software;Servers;Tools,data structures;query processing;reasoning about programs;set theory,data structure implementations;track subsets;incrementalization;data structure state;data structures domain;data structure synthesis;query synthesis;aggregations;re-framing;code","keywords":"Program synthesis;automatic programming;data structures","startPage":"958","endPage":"968","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453175","citationCount":1,"referenceCount":0,"year":2018,"authors":"C. Loncaric; M. D. Ernst; E. Torlak","affiliations":"Paul G. Allen Sch. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA; Paul G. Allen Sch. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA; Paul G. Allen Sch. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35bf"},"title":"Sentiment Analysis for Software Engineering: How Far Can We Go?","abstract":"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context. We describe our experience in building a software library recommender exploiting crowdsourced opinions mined from Stack Overflow (e.g., what is the sentiment of developers about the usability of a library). To reach our goal, we retrained-on a set of 40k manually labeled sentences/words extracted from Stack Overflow-a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of these tools on a variety of SE datasets. Our results should warn the research community about the strong limitations of current sentiment analysis tools.","conference":"IEEE","terms":"Sentiment analysis;Tools;Software engineering;Software;Task analysis;Training;Motion pictures,data analysis;data mining;learning (artificial intelligence);sentiment analysis;social networking (online);software engineering;software libraries,sentiment analysis tools;app reviews evaluation;commit messages;out-of-the-box;silver bullet;crowdsourced opinions mined;stack overflow;deep learning;time-consuming training process;effort training process;research community;developers emotions analyzing;software engineering tasks;SE datasets;software library recommender","keywords":"sentiment analysis;software engineering;NLP","startPage":"94","endPage":"104","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453067","citationCount":10,"referenceCount":0,"year":2018,"authors":"B. Lin; F. Zampetti; G. Bavota; M. Di Penta; M. Lanza; R. Oliveto","affiliations":"Software Inst., Univ. della Svizzera italiana, Lugano, Switzerland; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Software Inst., Univ. della Svizzera italiana, Lugano, Switzerland; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Software Inst., Univ. della Svizzera italiana, Lugano, Switzerland; STAKE Lab., Univ. of Molise, Pesche, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c0"},"title":"Semantic Program Repair Using a Reference Implementation","abstract":"Automated program repair has been studied via the use of techniques involving search, semantic analysis and artificial intelligence. Most of these techniques rely on tests as the correctness criteria, which causes the test overfitting problem. Although various approaches such as learning from code corpus have been proposed to address this problem, they are unable to guarantee that the generated patches generalize beyond the given tests. This work studies automated repair of errors using a reference implementation. The reference implementation is symbolically analyzed to automatically infer a specification of the intended behavior. This specification is then used to synthesize a patch that enforces conditional equivalence of the patched and the reference programs. The use of the reference implementation as an implicit correctness criterion alleviates overfitting in test-based repair. Besides, since we generate patches by semantic analysis, the reference program may have a substantially different implementation from the patched program, which distinguishes our approach from existing techniques for regression repair like Relifix. Our experiments in repairing the embedded Linux Busybox with GNU Coreutils as reference (and vice-versa) revealed that the proposed approach scales to real-world programs and enables the generation of more correct patches.","conference":"IEEE","terms":"Maintenance engineering;Software;Semantics;Forestry;Instruments;Signal processing algorithms;Scalability,embedded systems;learning (artificial intelligence);Linux;program diagnostics;program testing;public domain software;regression analysis;software maintenance,semantic program repair;reference implementation;automated program repair;semantic analysis;test overfitting problem;reference program;test-based repair;patched program;regression repair;GNU Coreutils;Linux Busybox;code corpus learning","keywords":"Debugging;Program repair;Verification","startPage":"129","endPage":"139","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453071","citationCount":8,"referenceCount":49,"year":2018,"authors":"S. Mechtaev; M. Nguyen; Y. Noller; L. Grunske; A. Roychoudhury","affiliations":"Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; Humboldt Univ. of Berlin, Berlin, Germany; Humboldt Univ. of Berlin, Berlin, Germany; Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c1"},"title":"Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis","abstract":"Since debugging is a time-consuming activity, automated program repair tools such as GenProg have garnered interest. A recent study revealed that the majority of GenProg repairs avoid bugs simply by deleting functionality. We found that SPR, a state-of-the-art repair tool proposed in 2015, still deletes functionality in their many \"plausible\" repairs. Unlike generate-and-validate systems such as GenProg and SPR, semantic analysis based repair techniques synthesize a repair based on semantic information of the program. While such semantics-based repair methods show promise in terms of quality of generated repairs, their scalability has been a concern so far. In this paper, we present Angelix, a novel semantics-based repair method that scales up to programs of similar size as are handled by search-based repair tools such as GenProg and SPR. This shows that Angelix is more scalable than previously proposed semantics based repair methods such as SemFix and DirectFix. Furthermore, our repair method can repair multiple buggy locations that are dependent on each other. Such repairs are hard to achieve using SPR and GenProg. In our experiments, Angelix generated repairs from large-scale real-world software such as wireshark and php, and these generated repairs include multi-location repairs. We also report our experience in automatically repairing the well-known Heartbleed vulnerability.","conference":"IEEE","terms":"Maintenance engineering;Semantics;Computer bugs;Software;Testing;Scalability;Software engineering,program debugging;program diagnostics,Angelix method;scalable multiline program patch synthesis;symbolic analysis;program repair tools;program debugging;SPR tool;generate-and-validate systems;semantic analysis;semantic information;multilocation repairs;Heartbleed vulnerability","keywords":"Automated program repair;Semantic analysis","startPage":"691","endPage":"701","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886945","citationCount":54,"referenceCount":40,"year":2016,"authors":"S. Mechtaev; J. Yi; A. Roychoudhury","affiliations":"Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c2"},"title":"Fixing Deadlocks via Lock Pre-Acquisitions","abstract":"Manual deadlock fixing is error-prone and time-consuming. Exist-ing generic approach (GA) simply inserts gate locks to fix dead-locks by serializing executions, which could introduce various new deadlocks and incur high runtime overhead. We propose a novel approach DFixer to fix deadlocks without introducing any new deadlocks by design. DFixer only selects one thread of a deadlock to pre-acquire a lock w together with another lock h, where before fixing, the deadlock occurs when the thread holds lock h and waits for lock w. As such, DFixer eliminates a hold-and-wait necessary condition, preventing the deadlock from occurring. The thread per-forming pre-acquisition is carefully selected such that no other syn-chronization exists in between the two original acquisitions. Other-wise, DFixer further introduces a context-aware conditional protect-ed by above lock w to guarantee the correctness of DFixer. The evaluation is on 20 deadlocks, including 17 from widely-used real-world C/C++ programs. It shows that DFixer successfully fixed all deadlocks. Whereas GA introduced 9 new deadlocks; a latest work Grail failed to fix 8 deadlocks and introduced 3 new deadlocks on others. On average, DFixer incurred only 2.1% overhead, where GA and Grail incurred 15.8% and 11.5% overhead, respectively.","conference":"IEEE","terms":"System recovery;Message systems;Computer bugs;Logic gates;Concurrent computing;Synchronization;Software,concurrency control;system recovery,manual deadlock fixing;lock pre-acquisitions;DFixer;C++ programs;C programs","keywords":"Deadlock;fixing;multithreaded program;lock order","startPage":"1109","endPage":"1120","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886984","citationCount":5,"referenceCount":64,"year":2016,"authors":"Y. Cai; L. Cao","affiliations":"State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c3"},"title":"Automated Energy Optimization of HTTP Requests for Mobile Applications","abstract":"Energy is a critical resource for apps that run on mobile devices. Among all operations, making HTTP requests is one of the most energy consuming. Previous studies have shown that bundling smaller HTTP requests into a single larger HTTP request can be an effective way to improve energy efficiency of network communication, but have not defined an automated way to detect when apps can be bundled nor to transform the apps to do this bundling. In this paper we propose an approach to reduce the energy consumption of HTTP requests in Android apps by automatically detecting and then bundling multiple HTTP requests. Our approach first detects HTTP requests that can be bundled using static analysis, then uses a proxy based technique to bundle HTTP requests at runtime. We evaluated our approach on a set of real world marketplace Android apps. In this evaluation, our approach achieved an average energy reduction of 15% for the subject apps and did not impose a significant runtime overhead on the optimized apps.","conference":"IEEE","terms":"Mobile communication;Uniform resource locators;Runtime;Energy consumption;Optimization;Protocols;Servers,energy conservation;mobile computing;power aware computing;program diagnostics,energy optimization;HTTP requests;mobile applications;mobile devices;network communication;energy efficiency;Android applications;static analysis;proxy based technique","keywords":"Energy optimization;Mobile apps;HTTP requests","startPage":"249","endPage":"260","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886908","citationCount":29,"referenceCount":54,"year":2016,"authors":"D. Li; Y. Lyu; J. Gui; W. G. J. Halfond","affiliations":"Univ. of Southern California, Los Angeles, CA, USA; Univ. of Southern California, Los Angeles, CA, USA; Univ. of Southern California, Los Angeles, CA, USA; Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c4"},"title":"Automatically Learning Semantic Features for Defect Prediction","abstract":"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.","conference":"IEEE","terms":"Semantics;Feature extraction;Training;Predictive models;Buildings;Syntactics;Data models,belief networks;neural nets;program diagnostics;programming language semantics;public domain software;source code (software),semantic feature learning;software defect prediction;representation-learning algorithm;deep learning;programs semantic representation;source code;deep belief network;token vectors;abstract syntax trees;DBN;AST;open source projects;within-project defect prediction;cross-project defect prediction;WPDP;CPDP","keywords":"defect prediction;feature generation;deep learning","startPage":"297","endPage":"308","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886912","citationCount":60,"referenceCount":70,"year":2016,"authors":"S. Wang; T. Liu; L. Tan","affiliations":"Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada; Electr. \u0026 Comput. Eng., Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c5"},"title":"Automatically Finding Bugs in a Commercial Cyber-Physical System Development Tool Chain With SLforge","abstract":"Cyber-physical system (CPS) development tool chains are widely used in the design, simulation, and verification of CPS data-flow models. Commercial CPS tool chains such as MathWorks' Simulink generate artifacts such as code binaries that are widely deployed in embedded systems. Hardening such tool chains by testing is crucial since formally verifying them is currently infeasible. Existing differential testing frameworks such as CyFuzz can not generate models rich in language features, partly because these tool chains do not leverage the available informal Simulink specifications. Furthermore, no study of existing Simulink models is available, which could guide CyFuzz to generate realistic models. To address these shortcomings, we created the first large collection of public Simulink models and used the collected models' properties to guide random model generation. To further guide model generation we systematically collected semi-formal Simulink specifications. In our experiments on several hundred models, the resulting SLforge generator was more effective and efficient than the state-of-the-art tool CyFuzz. SLforge also found 8 new confirmed bugs in Simulink.","conference":"IEEE","terms":"Software packages;Tools;Testing;Computer bugs;Object oriented modeling;Numerical models;Cyber-physical systems,embedded systems;formal verification;mathematics computing;program debugging;program testing,CPS data-flow models;commercial CPS tool chains;embedded systems;differential testing frameworks;available informal Simulink specifications;realistic models;public Simulink models;random model generation;semiformal Simulink specifications;finding bugs;commercial cyber-physical system development tool chain;cyber-physical system development tool chains;MathWork Simulink;CyFuzz;SLforge generator","keywords":"cyber-physical system;Simulink;differential testing;tool chain bug","startPage":"981","endPage":"992","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453177","citationCount":1,"referenceCount":60,"year":2018,"authors":"S. A. Chowdhury; S. Mohian; S. Mehra; S. Gawsane; T. T. Johnson; C. Csallner","affiliations":"Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA; EECS Dept., Vanderbilt Univ., Nashville, TN, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c6"},"title":"To Preserve or Not to Preserve Invalid Solutions in Search-Based Software Engineering: A Case Study in Software Product Lines","abstract":"Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.","conference":"IEEE","terms":"Optimization;Sociology;Statistics;Software;Search problems;Software product lines,evolutionary computation;search problems;software engineering;software product lines,near-optimal solutions;search-based software engineering;software product lines;multiobjective evolutionary algorithms;MOEA;SPL","keywords":"Search-based software engineering;software product lines;multi objective evolutionary algorithms;constraint solving;validity","startPage":"1027","endPage":"1038","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453181","citationCount":0,"referenceCount":78,"year":2018,"authors":"J. Guo; K. Shi","affiliations":"Alibaba Group, Hangzhou, China; East China Univ. of Sci. \u0026 Technol., Shanghai, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c7"},"title":"Static Automated Program Repair for Heap Properties","abstract":"Static analysis tools have demonstrated effectiveness at finding bugs in real world code. Such tools are increasingly widely adopted to improve software quality in practice. Automated Program Repair (APR) has the potential to further cut down on the cost of improving software quality. However, there is a disconnect between these effective bug-finding tools and APR. Recent advances in APR rely on test cases, making them inapplicable to newly discovered bugs or bugs difficult to test for deterministically (like memory leaks). Additionally, the quality of patches generated to satisfy a test suite is a key challenge. We address these challenges by adapting advances in practical static analysis and verification techniques to enable a new technique that finds and then accurately fixes real bugs without test cases. We present a new automated program repair technique using Separation Logic. At a high-level, our technique reasons over semantic effects of existing program fragments to fix faults related to general pointer safety properties: resource leaks, memory leaks, and null dereferences. The procedure automatically translates identified fragments into source-level patches, and verifies patch correctness with respect to reported faults. In this work we conduct the largest study of automatically fixing undiscovered bugs in real-world code to date. We demonstrate our approach by correctly fixing 55 bugs, including 11 previously undiscovered bugs, in 11 real-world projects.","conference":"IEEE","terms":"Computer bugs;Maintenance engineering;Tools;Static analysis;Semantics;Software;Safety,formal verification;program debugging;program diagnostics;program testing;program verification;software quality;software tools,bug-finding tools;static analysis;real-world code;undiscovered bugs;source-level patches;resource leaks;general pointer safety properties;program fragments;semantic effects;automated program repair technique;verification techniques;test suite;memory leaks;software quality;static analysis tools;heap properties;static automated Program Repair","keywords":"Automated Program Repair;Separation Logic","startPage":"151","endPage":"162","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453073","citationCount":3,"referenceCount":0,"year":2018,"authors":"R. van Tonder; C. Le Goues","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c8"},"title":"PAC Learning-Based Verification and Model Synthesis","abstract":"We introduce a novel technique for verification and model synthesis of sequential programs. Our technique is based on learning an approximate regular model of the set of feasible paths in a program, and testing whether this model contains an incorrect behavior. Exact learning algorithms require checking equivalence between the model and the program, which is a difficult problem, in general undecidable. Our learning procedure is therefore based on the framework of probably approximately correct (PAC) learning, which uses sampling instead, and provides correctness guarantees expressed using the terms error probability and confidence. Besides the verification result, our procedure also outputs the model with the said correctness guarantees. Obtained preliminary experiments show encouraging results, in some cases even outperforming mature software verifiers.","conference":"IEEE","terms":"Picture archiving and communication systems;Software;Testing;Approximation algorithms;Syntactics;Frequency modulation;Software engineering,error statistics;learning (artificial intelligence);program verification,PAC learning-based verification;model synthesis;sequential programs;probably approximately correct learning;sampling;error probability;confidence;software verification","keywords":"model synthesis;PAC learning;finite automata;program verification","startPage":"714","endPage":"724","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886947","citationCount":1,"referenceCount":23,"year":2016,"authors":"Y. Chen; C. Hsieh; O. Lengál; T. Lii; M. Tsai; B. Wang; F. Wang","affiliations":"NA; Nat. Taiwan Univ., Taipei, Taiwan; NA; Nat. Taiwan Univ., Taipei, Taiwan; NA; NA; Nat. Taiwan Univ., Taipei, Taiwan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35c9"},"title":"Multi-granular Conflict and Dependency Analysis in Software Engineering Based on Graph Transformation","abstract":"Conflict and dependency analysis (CDA) of graph transformation has been shown to be a versatile foundation for understanding interactions in many software engineering domains, including software analysis and design, model-driven engineering, and testing. In this paper, we propose a novel static CDA technique that is multi-granular in the sense that it can detect all conflicts and dependencies on multiple granularity levels. Specifically, we provide an efficient algorithm suite for computing binary, coarse-grained, and fine-grained conflicts and dependencies: Binary granularity indicates the presence or absence of conflicts and dependencies, coarse granularity focuses on root causes for conflicts and dependencies, and fine granularity shows each conflict and dependency in full detail. Doing so, we can address specific performance and usability requirements that we identified in a literature survey of CDA usage scenarios. In an experimental evaluation, our algorithm suite computes conflicts and dependencies rapidly. Finally, we present a user study, in which the participants found our coarse-grained results more understandable than the fine-grained ones reported in a state-of-the-art tool. Our overall contribution is twofold: (i) we significantly speed up the computation of fine-grained and binary CDA results and, (ii) complement them with coarse-grained ones, which offer usability benefits for numerous use cases.","conference":"IEEE","terms":"Software engineering;Data mining;Usability;Task analysis;Model driven engineering,graph theory;program testing;software engineering,graph transformation;software engineering domains;model-driven engineering;multiple granularity levels;coarse granularity;CDA usage scenarios;fine granularity;multigranular conflict and dependency analysis;software testing;static CDA technique;binary granularity","keywords":"automated static analysis","startPage":"716","endPage":"727","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453144","citationCount":0,"referenceCount":0,"year":2018,"authors":"L. Lambers; D. Strüber; G. Taentzer; K. Born; J. Huebert","affiliations":"Hasso-Plattner-Inst. Potsdam, Potsdam, Germany; Univ. Koblenz-Landau, Koblenz, Germany; Univ. Marburg, Marburg, Germany; Univ. Marburg, Marburg, Germany; Univ. Marburg, Marburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35ca"},"title":"Locking Discipline Inference and Checking","abstract":"Concurrency is a requirement for much modern software, but the implementation of multithreaded algorithms comes at the risk of errors such as data races.Programmers can prevent data races by documenting and obeying a locking discipline, which indicates which locks must be held in order to access which data.This paper introduces a formal semantics for locking specifications that gives a guarantee of race freedom.A notable difference from most other semantics is that it is in terms of values (which is what the runtime system locks) rather than variables.The paper also shows how to express the formal semantics in two different styles of analysis:abstract interpretation and type theory.We have implemented both analyses, in tools that operate on Java.To the best of our knowledge, these are the first tools that can soundly infer and check a locking discipline for Java.Our experiments compare the implementations with one another and with annotations written by programmers, showing that the ambiguities and unsoundness of previous formulations are a problem in practice.","conference":"IEEE","terms":"Synchronization;Semantics;Java;Concurrent computing;Monitoring;Software;Runtime,concurrency control;formal specification;Java;multi-threading,software concurrency;multithreaded algorithms;locking discipline;formal semantics;locking specifications;abstract interpretation;type theory;Java","keywords":"","startPage":"1133","endPage":"1144","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886986","citationCount":0,"referenceCount":49,"year":2016,"authors":"M. D. Ernst; A. Lovato; D. Macedonio; F. Spoto; J. Thaine","affiliations":"Univ. of Washington, Seattle, WA, USA; Univ. di Verona, Verona, Italy; Julia Srl, Italy; Univ. di Verona, Verona, Italy; Univ. of Washington, Seattle, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35cb"},"title":"Shadow of a Doubt: Testing for Divergences between Software Versions","abstract":"While developers are aware of the importance of comprehensively testing patches, the large effort involved in coming up with relevant test cases means that such testing rarely happens in practice. Furthermore, even when test cases are written to cover the patch, they often exercise the same behaviour in the old and the new version of the code. In this paper, we present a symbolic execution-based technique that is designed to generate test inputs that cover the new program behaviours introduced by a patch. The technique works by executing both the old and the new version in the same symbolic execution instance, with the old version shadowing the new one. During this combined shadow execution, whenever a branch point is reached where the old and the new version diverge, we generate a test case exercising the divergence and comprehensively test the new behaviours of the new version. We evaluate our technique on the Coreutils patches from the CoREBench suite of regression bugs, and show that it is able to generate test inputs that exercise newly added behaviours and expose some of the regression bugs.","conference":"IEEE","terms":"Testing;Software;Computer bugs;Concrete;Random access memory;Shadow mapping;Performance analysis,program debugging;program testing,divergence testing;software versions;symbolic execution-based technique;shadow execution;Coreutils patches;CoREBench;regression bugs","keywords":"","startPage":"1181","endPage":"1192","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886990","citationCount":4,"referenceCount":31,"year":2016,"authors":"H. Palikareva; T. Kuchta; C. Cadar","affiliations":"Dept. of Comput., Imperial Coll. London, London, UK; Dept. of Comput., Imperial Coll. London, London, UK; Dept. of Comput., Imperial Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911ceee8435e8e7d35cc"},"title":"Automated Parameter Optimization of Classification Techniques for Defect Prediction Models","abstract":"Defect prediction models are classifiers that are trained to identify defect-prone software modules. Such classifiers have configurable parameters that control their characteristics (e.g., the number of trees in a random forest classifier). Recent studies show that these classifiers may underperform due to the use of suboptimal default parameter settings. However, it is impractical to assess all of the possible settings in the parameter spaces. In this paper, we investigate the performance of defect prediction models where Caret - an automated parameter optimization technique - has been applied. Through a case study of 18 datasets from systems that span both proprietary and open source domains, we find that (1) Caret improves the AUC performance of defect prediction models by as much as 40 percentage points; (2) Caret-optimized classifiers are at least as stable as (with 35% of them being more stable than) classifiers that are trained using the default settings; and (3) Caret increases the likelihood of producing a top-performing classifier by as much as 83%. Hence, we conclude that parameter settings can indeed have a large impact on the performance of defect prediction models, suggesting that researchers should experiment with the parameters of the classification techniques. Since automated parameter optimization techniques like Caret yield substantially benefits in terms of performance improvement and stability, while incurring a manageable additional computational cost, they should be included in future defect prediction studies.","conference":"IEEE","terms":"Predictive models;Vegetation;Optimization;Boosting;Decision trees;Kernel,pattern classification;public domain software;software fault tolerance;software quality,parameter optimization;classification techniques;defect prediction models;configurable parameters;Caret technique;proprietary domain;open source domain;Caret-optimized classifiers","keywords":"software defect prediction;experimental design;classification techniques;parameter optimization","startPage":"321","endPage":"332","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886914","citationCount":38,"referenceCount":64,"year":2016,"authors":"C. Tantithamthavorn; S. McIntosh; A. E. Hassan; K. Matsumoto","affiliations":"Nara Inst. of Sci. \u0026 Technol., Nara, Japan; McGill Univ., Montreal, QC, Canada; Queen's Univ., Kingston, ON, Canada; Nara Inst. of Sci. \u0026 Technol., Nara, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35cd"},"title":"Is \"Better Data\" Better Than \"Better Data Miners\"?","abstract":"We report and fix an important systematic error in prior studies that ranked classifiers for software analytics. Those studies did not (a) assess classifiers on multiple criteria and they did not (b) study how variations in the data affect the results. Hence, this paper applies (a) multi-performance criteria while (b) fixing the weaker regions of the training data (using SMOTUNED, which is an auto-tuning version of SMOTE). This approach leads to dramatically large increases in software defect predictions when applied in a 5*5 cross-validation study for 3,681 JAVA classes (containing over a million lines of code) from open source systems, SMOTUNED increased AUC and recall by 60% and 20% respectively. These improvements are independent of the classifier used to predict for defects. Same kind of pattern (improvement) was observed when a comparative analysis of SMOTE and SMOTUNED was done against the most recent class imbalance technique. In conclusion, for software analytic tasks like defect prediction, (1) data pre-processing can be more important than classifier choice, (2) ranking studies are incomplete without such pre-processing, and (3) SMOTUNED is a promising candidate for pre-processing.","conference":"IEEE","terms":"Software;Measurement;Couplings;Tuning;Task analysis;Complexity theory;Software engineering,data mining;Java;learning (artificial intelligence);pattern classification,dramatically large increases;software defect predictions;open source systems;SMOTE;recent class imbalance technique;software analytic tasks;defect prediction;classifier choice;data miners;important systematic error;software analytics;multiperformance criteria;training data;JAVA classes;SMOTUNED","keywords":"Search based SE;defect prediction;classification;data analytics for software engineering;SMOTE;imbalanced data;preprocessing","startPage":"1050","endPage":"1061","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453183","citationCount":2,"referenceCount":71,"year":2018,"authors":"A. Agrawal; T. Menzies","affiliations":"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ce"},"title":"Accurate and Efficient Refactoring Detection in Commit History","abstract":"Refactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios. To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art.","conference":"IEEE","terms":"Tools;History;Open source software;Software systems;Java;Software engineering,application program interfaces;data mining;Java;public domain software;software libraries;software maintenance;software quality,comprehensive oracle;RMiner;commit history;detection algorithms;empirical studies;library API migration;code reviews;user-provided similarity thresholds;AST-based statement matching algorithm;user-defined thresholds;refactoring detection tools","keywords":"Refactoring;Commit;Git;Abstract Syntax Tree;Oracle;Accuracy","startPage":"483","endPage":"494","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453111","citationCount":16,"referenceCount":80,"year":2018,"authors":"N. Tsantalis; M. Mansouri; L. Eshkevari; D. Mazinanian; D. Dig","affiliations":"Concordia Univ., Montreal, QC, Canada; Concordia Univ., Montreal, QC, Canada; Concordia Univ., Montreal, QC, Canada; Concordia Univ., Montreal, QC, Canada; Oregon State Univ., Corvallis, OR, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35cf"},"title":"The Evolution of C Programming Practices: A Study of the Unix Operating System 1973-2015","abstract":"Tracking long-term progress in engineering and applied science allows us to take stock of things we have achieved, appreciate the factors that led to them, and set realistic goals for where we want to go. We formulate seven hypotheses associated with the long term evolution of C programming in the Unix operating system, and examine them by extracting, aggregating, and synthesising metrics from 66 snapshots obtained from a synthetic software configuration management repository covering a period of four decades. We found that over the years developers of the Unix operating system appear to have evolved their coding style in tandem with advancements in hardware technology, promoted modularity to tame rising complexity, adopted valuable new language features, allowed compilers to allocate registers on their behalf, and reached broad agreement regarding code formatting. The progress we have observed appears to be slowing or even reversing prompting the need for new sources of innovation to be discovered and followed.","conference":"IEEE","terms":"Software;Measurement;Complexity theory;Registers;Programming profession;Encoding,C language;configuration management;Unix,C programming;Unix operating system;synthetic software configuration management repository","keywords":"C;coding style;coding practices;Unix;BSD;FreeBSD","startPage":"748","endPage":"759","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886953","citationCount":4,"referenceCount":63,"year":2016,"authors":"D. Spinellis; P. Louridas; M. Kechagia","affiliations":"Dept. of Manage. Sci. \u0026 Technol., Athens Univ. of Econ. \u0026 Bus., Athens, Greece; Dept. of Manage. Sci. \u0026 Technol., Athens Univ. of Econ. \u0026 Bus., Athens, Greece; Dept. of Manage. Sci. \u0026 Technol., Athens Univ. of Econ. \u0026 Bus., Athens, Greece","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d0"},"title":"SourcererCC: Scaling Code Clone Detection to Big-Code","abstract":"Despite a decade of active research, there has been a marked lack in clone detection techniques that scale to large repositories for detecting near-miss clones. In this paper, we present a token-based clone detector, SourcererCC, that can detect both exact and near-miss clones from large inter-project repositories using a standard workstation. It exploits an optimized inverted-index to quickly query the potential clones of a given code block. Filtering heuristics based on token ordering are used to significantly reduce the size of the index, the number of code-block comparisons needed to detect the clones, as well as the number of required token-comparisons needed to judge a potential clone. We evaluate the scalability, execution time, recall and precision of SourcererCC, and compare it to four publicly available and state-of-the-art tools. To measure recall, we use two recent benchmarks: (1) a big benchmark of real clones, BigCloneBench, and (2) a Mutation/Injection-based framework of thousands of fine-grained artificial clones. We find SourcererCC has both high recall and precision, and is able to scale to a large inter-project repository (25K projects, 250MLOC) using a standard workstation.","conference":"IEEE","terms":"Cloning;Indexes;Detectors;Benchmark testing;Scalability;Software;Standards,Java;software engineering,SourcererCC;code clone detection technique;big-code;code-block comparisons;token-comparisons;BigCloneBench;mutation-injection-based framework;open-source Java systems","keywords":"clone detection;large software repositories;software repositories;code clones;recall;precision;scalability;large-scale;big-code","startPage":"1157","endPage":"1168","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886988","citationCount":63,"referenceCount":42,"year":2016,"authors":"H. Sajnani; V. Saini; J. Svajlenko; C. K. Roy; C. V. Lopes","affiliations":"Sch. of Inf. \u0026 Comput. Sci., UC Irvine, Irvine, CA, USA; Sch. of Inf. \u0026 Comput. Sci., UC Irvine, Irvine, CA, USA; Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada; Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada; Sch. of Inf. \u0026 Comput. Sci., UC Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d1"},"title":"Program Synthesis Using Natural Language","abstract":"Interacting with computers is a ubiquitous activity for millions of people. Repetitive or specialized tasks often require creation of small, often one-off, programs. End-users struggle with learning and using the myriad of domain-specific languages (DSLs) to effectively accomplish these tasks. We present a general framework for constructing program synthesizers that take natural language (NL) inputs and produce expressions in a target DSL. The framework takes as input a DSL definition and training data consisting of NL/DSL pairs. From these it constructs a synthesizer by learning optimal weights and classifiers (using NLP features) that rank the outputs of a keyword-programming based translation. We applied our framework to three domains: repetitive text editing, an intelligent tutoring system, and flight information queries. On 1200+ English descriptions, the respective synthesizers rank the desired program as the top-1 and top-3 for 80% and 90% descriptions respectively.","conference":"IEEE","terms":"DSL;Benchmark testing;Synthesizers;Automata;Training data;Training;Natural languages,formal specification;intelligent tutoring systems;natural language processing;query processing,program synthesis;natural language;domain-specific languages;DSL;keyword-programming based translation;repetitive text editing;intelligent tutoring system;flight information queries","keywords":"Program Synthesis;Natrual Language Programming;Domain Specific Languages;End-user Programming;User Intent","startPage":"345","endPage":"356","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886916","citationCount":5,"referenceCount":57,"year":2016,"authors":"A. Desai; S. Gulwani; V. Hingorani; N. Jain; A. Karkare; M. Marron; S. R.; S. Roy","affiliations":"IIT Kanpur; MSR Redmond; IIT Kanpur; IIT Kanpur; IIT Kanpur; MSR Redmond; IIT Kanpur; IIT Kanpur","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d2"},"title":"Augmenting API Documentation with Insights from Stack Overflow","abstract":"Software developers need access to different kinds of information which is often dispersed among different documentation sources, such as API documentation or Stack Overflow. We present an approach to automatically augment API documentation with \"insight sentences\" from Stack Overflow -- sentences that are related to a particular API type and that provide insight not contained in the API documentation of that type. Based on a development set of 1,574 sentences, we compare the performance of two state-of-the-art summarization techniques as well as a pattern-based approach for insight sentence extraction. We then present SISE, a novel machine learning based approach that uses as features the sentences themselves, their formatting, their question, their answer, and their authors as well as part-of-speech tags and the similarity of a sentence to the corresponding API documentation. With SISE, we were able to achieve a precision of 0.64 and a coverage of 0.7 on the development set. In a comparative study with eight software developers, we found that SISE resulted in the highest number of sentences that were considered to add useful information not found in the API documentation. These results indicate that taking into account the meta data available on Stack Overflow as well as part-of-speech tags can significantly improve unsupervised extraction approaches when applied to Stack Overflow data.","conference":"IEEE","terms":"Documentation;Java;Software;Data mining;Joining processes;Feature extraction;Uniform resource locators,application program interfaces;learning (artificial intelligence),API documentation augmentation;application program interface;Stack overflow;insight sentences;summarization techniques;pattern-based approach;insight sentence extraction;SISE approach;machine learning based approach","keywords":"API documentation;Stack Overflow;insight sentences","startPage":"392","endPage":"403","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886920","citationCount":36,"referenceCount":41,"year":2016,"authors":"C. Treude; M. P. Robillard","affiliations":"Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; Sch. of Comput. Sci., McGill Univ., Montréal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d3"},"title":"FAST Approaches to Scalable Similarity-Based Test Case Prioritization","abstract":"Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.","conference":"IEEE","terms":"Scalability;History;Fault detection;Big Data;Software testing;Data mining,fault diagnosis;greedy algorithms;Java;program testing,test case prioritization criteria;similarity-based approaches;test case prioritization techniques;FAST technique;scalable similarity-based test case prioritization;black-box fashion;black-box approaches;white-box approaches;FAST approaches","keywords":"locality sensitive hashing;minhashing;scalability;similarity;software testing;test case prioritization","startPage":"222","endPage":"232","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453081","citationCount":3,"referenceCount":34,"year":2018,"authors":"B. Miranda; E. Cruciani; R. Verdecchia; A. Bertolino","affiliations":"Fed. Univ. of Pernambuco, Recife, Brazil; Gran Sasso Sci. Inst., L'Aquila, Italy; Gran Sasso Sci. Inst., L'Aquila, Italy; ISTI, Pisa, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d4"},"title":"Understanding and Fixing Multiple Language Interoperability Issues: The C/Fortran Case","abstract":"We performed an empirical study to understand interoperability issues in C and Fortran programs. C/Fortran interoperability is very common and is representative of general language interoperability issues, such as how interfaces between languages are defined and how data types are shared. Fortran presents an additional challenge, since several ad hoc approaches to C/Fortran interoperability were in use long before a standard mechanism was defined. We explored 20 applications, automatically analyzing over 12 million lines of code. We found that only 3% of interoperability instances follow the ISO standard to describe interfaces; the rest follow a combination of compiler-dependent ad hoc approaches. Several parameters in cross-language functions did not have standards-compliant interoperable types, and about one-fourth of the parameters that were passed by reference could be passed by value. We propose that automated refactoring tools may provide a viable way to migrate programs to use the new interoperability features. We present two refactorings to transform code for this purpose and one refactoring to evolve code thereafter; all of these are instances of multiple language refactorings.","conference":"IEEE","terms":"Interoperability;Java;Security;ISO Standards;Software engineering;Transforms,C language;FORTRAN;ISO standards;open systems;program compilers,multiple language interoperability issues;Fortran programs;C programs;ISO standard;compiler-dependent ad hoc approaches;cross-language functions;multiple language refactorings","keywords":"C;Fortran;language interoperability;polyglot;refactoring","startPage":"772","endPage":"783","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886955","citationCount":0,"referenceCount":40,"year":2016,"authors":"N. Sultana; J. Middleton; J. Overbey; M. Hafiz","affiliations":"Dept. of Comput. Sci. \u0026 Software Eng., Auburn Univ., Auburn, AL, USA; Dept. of Comput. Sci. \u0026 Software Eng., Auburn Univ., Auburn, AL, USA; Dept. of Comput. Sci. \u0026 Software Eng., Auburn Univ., Auburn, AL, USA; Dept. of Comput. Sci. \u0026 Software Eng., Auburn Univ., Auburn, AL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d5"},"title":"Debugging Data Flows in Reactive Programs","abstract":"Reactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console. In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools.","conference":"IEEE","terms":"Debugging;Programming;Tools;Observers;Libraries;Companies;Interviews,program debugging;program visualisation,debugging tool;RxFiddle;data flow;debugging tasks;traditional debugging tools;stream processing;traditional debug tools;rudimentary debug tool;reactive programming","keywords":"reactive programming;debugging;visualization;program comprehension","startPage":"752","endPage":"763","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453148","citationCount":2,"referenceCount":52,"year":2018,"authors":"H. Banken; E. Meijer; G. Gousios","affiliations":"Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d6"},"title":"Identifying Patch Correctness in Test-Based Program Repair","abstract":"Test-based automatic program repair has attracted a lot of attention in recent years However, the test suites in practice are often too weak to guarantee correctness and existing approaches often generate a large number of incorrect patches. To reduce the number of incorrect patches generated, we propose a novel approach that heuristically determines the correctness of the generated patches. The core idea is to exploit the behavior similarity of test case executions. The passing tests on original and patched programs are likely to behave similarly while the failing tests on original and patched programs are likely to behave differently. Also, if two tests exhibit similar runtime behavior, the two tests are likely to have the same test results. Based on these observations, we generate new test inputs to enhance the test suites and use their behavior similarity to determine patch correctness. Our approach is evaluated on a dataset consisting of 139 patches generated from existing program repair systems including jGenProg, Nopol, jKali, ACS, and HDRepair. Our approach successfully prevented 56.3% of the incorrect patches to be generated, without blocking any correct patches.","conference":"IEEE","terms":"Maintenance engineering;Search problems;Software;Computer crashes;Null value;Runtime,automatic programming;program debugging;program diagnostics;program testing;software maintenance,identifying patch correctness;test-based program repair;automatic program repair;incorrect patches;generated patches;behavior similarity;test case executions;passing tests;original programs;patched programs;similar runtime behavior;correct patches;program repair systems;jGenProg","keywords":"Test based program repair;patch correctness;patch classification;test generation","startPage":"789","endPage":"799","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453152","citationCount":10,"referenceCount":0,"year":2018,"authors":"Y. Xiong; X. Liu; M. Zeng; L. Zhang; G. Huang","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d7"},"title":"Cross-Supervised Synthesis of Web-Crawlers","abstract":"A web-crawler is a program that automatically and systematically tracks the links of a website and extracts information from its pages. Due to the different formats of websites, the crawling scheme for different sites can differ dramatically. Manually customizing a crawler for each specific site is time consuming and error-prone. Furthermore, because sites periodically change their format and presentation, crawling schemes have to be manually updated and adjusted. In this paper, we present a technique for automatic synthesis of web-crawlers from examples. The main idea is to use hand-crafted (possibly partial) crawlers for some websites as the basis for crawling other sites that contain the same kind of information. Technically, we use the data on one site to identify data on another site. We then use the identified data to learn the website structure and synthesize an appropriate extraction scheme. We iterate this process, as synthesized extraction schemes result in additional data to be used for re-learning the website structure. We implemented our approach and automatically synthesized 30 crawlers for websites from nine different categories: books, TVs, conferences, universities, cameras, phones, movies, songs, and hotels.","conference":"IEEE","terms":"Crawlers;Data mining;Containers;Layout;Database languages;Glass;Concrete,information retrieval;Internet,cross-supervised synthesis;Web crawler;information extraction;crawler customization;crawling schemes","keywords":"Data extraction;wrapper;synthesis;scarpper","startPage":"368","endPage":"379","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886918","citationCount":1,"referenceCount":41,"year":2016,"authors":"A. Omari; S. Shoham; E. Yahav","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d8"},"title":"Learning API Usages from Bytecode: A Statistical Approach","abstract":"Mobile app developers rely heavily on standard API frameworks and libraries. However, learning API usages is often challenging due to the fast-changing nature of API frameworks for mobile systems and the insufficiency of API documentation and source code examples. In this paper, we propose a novel approach to learn API usages from bytecode of Android mobile apps. Our core contributions include HAPI, a statistical model of API usages and three algorithms to extract method call sequences from apps' bytecode, to train HAPI based on those sequences, and to recommend method calls in code completion using the trained HAPIs. Our empirical evaluation shows that our prototype tool can effectively learn API usages from 200 thousand apps containing 350 million method sequences. It recommends next method calls with top-3 accuracy of 90% and outperforms baseline approaches on average 10-20%.","conference":"IEEE","terms":"Hidden Markov models;Mobile communication;Androids;Humanoid robots;Algorithm design and analysis;Documentation;Probabilistic logic,application program interfaces;mobile computing;statistical analysis,API usages learning;Bytecode;statistical approach;mobile app developers;application program interface;API frameworks;API documentation;HAPI model;code completion","keywords":"Statistical model;API usage;mobile apps","startPage":"416","endPage":"427","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886922","citationCount":10,"referenceCount":45,"year":2016,"authors":"T. T. Nguyen; H. V. Pham; P. M. Vu; T. T. Nguyen","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35d9"},"title":"Assessing the Threat of Untracked Changes in Software Evolution","abstract":"While refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies.","conference":"IEEE","terms":"Blogs;Software;Tracking;Bars;History;Tools;Computer bugs,data mining;Java;public domain software;software maintenance,software evolution;software repository approache mining;source code evolution analysis;MSR studies;extensive empirical study;MSR approaches;untracked changes","keywords":"Mining Software Repositories;Software Evolution;Refactoring","startPage":"1102","endPage":"1113","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453191","citationCount":1,"referenceCount":69,"year":2018,"authors":"A. Hora; D. Silva; M. T. Valente; R. Robbes","affiliations":"FACOM, UFMS, Campo Grande, Brazil; Valente ASERG Group, UFMG, Valente, Brazil; Valente ASERG Group, UFMG, Valente, Brazil; SwSE Group, Free Univ. of Bozen-Bolzano, Bolzano, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35da"},"title":"Mining Sandboxes","abstract":"We present sandbox mining, a technique to confine an application to resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates. The use of test generation makes sandbox mining a fully automatic process that can be run by vendors and end users alike. Our BOXMATE prototype requires less than one hour to extract a sandbox from an Android app, with few to no confirmations required for frequently used functionality.","conference":"IEEE","terms":"Testing;Androids;Humanoid robots;Monitoring;Smart phones;Generators;Graphical user interfaces,data mining;program testing,sandbox mining technique;automatic test generation;resource extraction;test generation;BOXMATE;Android application","keywords":"specification mining;sandboxing;test generation;program analysis;dynamic analysis;android;security;policy enforcement","startPage":"37","endPage":"48","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886890","citationCount":11,"referenceCount":38,"year":2016,"authors":"K. Jamrozik; P. von Styp-Rekowsky; A. Zeller","affiliations":"Center for IT-Security, Privacy \u0026 Accountability, Saarbrucken, Germany; Center for IT-Security, Privacy \u0026 Accountability, Saarbrucken, Germany; Center for IT-Security, Privacy \u0026 Accountability, Saarbrucken, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35db"},"title":"Debugging for Reactive Programming","abstract":"Reactive programming is a recent programming technique that provides dedicated language abstractions for reactive software. Reactive programming relieves developers from manually updating outputs when the inputs of a computation change, it overcomes a number of well-know issues of the Observer design pattern, and it makes programs more comprehensible. Unfortunately, complementing the new paradigm with proper tools is a vastly unexplored area. Hence, as of now, developers can embrace reactive programming only at the cost of a more challenging development process. In this paper, we investigate a primary issue in the field: debugging programs in the reactive style. We analyze the problem of debugging reactive programs, show that the reactive style requires a paradigm shift in the concepts needed for debugging, and propose RP Debugging, a methodology for effectively debugging reactive programs. These ideas are implemented in Reactive Inspector, a debugger for reactive programs integrated with the Eclipse Scala IDE. Evaluation based on a controlled experiment shows that RP Debugging outperforms traditional debugging techniques.","conference":"IEEE","terms":"Debugging;Programming;Reactive power;Runtime;Observers;Libraries;Data models,program debugging;programming environments,reactive programming;RP Debugging;Reactive Inspector;Eclipse Scala IDE","keywords":"Functional-reactive Programming;Debugging","startPage":"796","endPage":"807","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886957","citationCount":4,"referenceCount":49,"year":2016,"authors":"G. Salvaneschi; M. Mezini","affiliations":"Tech. Univ. of Darmstadt, Darmstadt, Germany; Tech. Univ. of Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35dc"},"title":"Probing for Requirements Knowledge to Stimulate Architectural Thinking","abstract":"Software requirements specifications (SRSs) often lack the detail needed to make informed architectural decisions. Architects therefore either make assumptions, which can lead to incorrect decisions, or conduct additional stakeholder interviews, resulting in potential project delays. We previously observed that software architects ask Probing Questions (PQs) to gather information crucial to architectural decision-making. Our goal is to equip Business Analysts with appropriate PQs so that they can ask these questions themselves. We report a new study with over 40 experienced architects to identify reusable PQs for five areas of functionality and organize them into structured flows. These PQflows can be used by Business Analysts to elicit and specify architecturally relevant information. Additionally, we leverage machine learning techniques to determine when a PQ-flow is appropriate for use in a project, and to annotate individual PQs with relevant information extracted from the existing SRS. We trained and evaluated our approach on over 8,000 individual requirements from 114 requirements specifications and also conducted a pilot study to validate its usefulness.","conference":"IEEE","terms":"Interviews;Barium;Software;Stakeholders;Batch production systems;Requirements engineering,formal specification;learning (artificial intelligence);software architecture,requirements knowledge;architecturally significant requirements;probing questions;PQ-flow;machine learning techniques;software requirements specifications;SRS","keywords":"Architecturally significant requirements;automated requirement classification;functional requirements;requirements knowledge;Probing Questions (PQs);PQ-flows","startPage":"843","endPage":"854","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886961","citationCount":3,"referenceCount":52,"year":2016,"authors":"P. R. Anish; B. Balasubramaniam; A. Sainani; J. Cleland-Huang; M. Daneva; R. J. Wieringa; S. Ghaisas","affiliations":"TATA Res. Dev. \u0026 Design Centre, TATA Consultancy Services Ltd., Pune, India; TATA Res. Dev. \u0026 Design Centre, TATA Consultancy Services Ltd., Pune, India; TATA Res. Dev. \u0026 Design Centre, TATA Consultancy Services Ltd., Pune, India; Syst. \u0026 Requirements Eng. Center, DePaul Univ., Chicago, IL, USA; Univ. of Twente, Enschede, Netherlands; Univ. of Twente, Enschede, Netherlands; TATA Res. Dev. \u0026 Design Centre, TATA Consultancy Services Ltd., Pune, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35dd"},"title":"Speedoo: Prioritizing Performance Optimization Opportunities","abstract":"Performance problems widely exist in modern software systems. Existing performance optimization techniques, including profiling-based and pattern-based techniques, usually fail to consider the architectural impacts among methods that easily slow down the overall system performance. This paper contributes a new approach, named Speedoo, to identify groups of methods that should be treated together and deserve high priorities for performance optimization. The uniqueness of Speedoo is to measure and rank the performance optimization opportunities of a method based on 1) the architectural impact and 2) the optimization potential. For each highly ranked method, we locate a respective Optimization Space based on 5 performance patterns generalized from empirical observations. The top ranked optimization spaces are suggested to developers as potential optimization opportunities. Our evaluation on three real-life projects has demonstrated that 18.52% to 42.86% of methods in the top ranked optimization spaces indeed undertook performance optimization in the projects. This outperforms one of the state-of-the-art profiling tools YourKit by 2 to 3 times. An important implication of this study is that developers should treat methods in an optimization space together as a group rather than as individuals in performance optimization. The proposed approach can provide guidelines and reduce developers' manual effort.","conference":"IEEE","terms":"Optimization;Measurement;Software;System performance;Computer architecture;Complexity theory;Manuals,optimisation;program diagnostics;software performance evaluation,Speedoo;performance optimization techniques;pattern-based techniques;architectural impact;system performance;optimization potential;software systems;optimization space;profiling-based techniques","keywords":"Performance;Metrics;Architecture","startPage":"811","endPage":"821","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453154","citationCount":2,"referenceCount":60,"year":2018,"authors":"Z. Chen; B. Chen; L. Xiao; X. Wang; L. Chen; Y. Liu; B. Xu","affiliations":"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Shanghai Key Lab. of Data Sci., Fudan Univ., Shanghai, China; Sch. of Syst. \u0026 Enterprises, Stevens Inst. of Technol., Hoboken, NJ, USA; Sch. of Syst. \u0026 Enterprises, Stevens Inst. of Technol., Hoboken, NJ, USA; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Sch. of Comput. Sci. \u0026 Eng., Nanyang Technol. Univ., Singapore, Singapore; State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35de"},"title":"Code Anomalies Flock Together: Exploring Code Anomaly Agglomerations for Locating Design Problems","abstract":"Design problems affect every software system. Diverse software systems have been discontinued or reengineered due to design problems. As design documentation is often informal or nonexistent, design problems need to be located in the source code. The main difficulty to identify a design problem in the implementation stems from the fact that such problem is often scattered through several program elements. Previous work assumed that code anomalies -- popularly known as code smells -- may provide sufficient hints about the location of a design problem. However, each code anomaly alone may represent only a partial embodiment of a design problem. In this paper, we hypothesize that code anomalies tend to ``flock together'' to realize a design problem. We analyze to what extent groups of inter-related code anomalies, named agglomerations, suffice to locate design problems. We analyze more than 2200 agglomerations found in seven software systems of different sizes and from different domains. Our analysis indicates that certain forms of agglomerations are consistent indicators of both congenital and evolutionary design problems, with accuracy often higher than 80%.","conference":"IEEE","terms":"Fats;Syntactics;Software systems;Surgery;Documentation;Semantics,software engineering,code anomaly agglomerations;software design;diverse software systems;design documentation;code smells;congenital design problems;evolutionary design problems","keywords":"agglomeration;code anomaly;desing problem","startPage":"440","endPage":"451","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886924","citationCount":9,"referenceCount":45,"year":2016,"authors":"W. Oizumi; A. Garcia; L. d. S. Sousa; B. Cafeo; Y. Zhao","affiliations":"Inf. Dept., PUC-Rio Rio de Janeiro, Rio de Janeiro, Brazil; Inf. Dept., PUC-Rio Rio de Janeiro, Rio de Janeiro, Brazil; NA; Inf. Dept., PUC-Rio Rio de Janeiro, Rio de Janeiro, Brazil; Dept. of Comput. Sci., USC, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35df"},"title":"HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts","abstract":"Advancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: History-Driven Repair of Build Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes.","conference":"IEEE","terms":"Maintenance engineering;Software;Tools;Task analysis;Computer bugs;Data mining;Software engineering,software fault tolerance;software maintenance;software tools;source code (software),software build failures;History-Driven Repair;build log similarity;Gradle build scripts;software build tools;build script files;fix patterns;reproducible build failures;HireBuild;Automated program repair techniques;source code;automatic patch generation;build script fixes","keywords":"Patch Generation;Software Build Scripts;Build Logs","startPage":"1078","endPage":"1089","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453189","citationCount":6,"referenceCount":51,"year":2018,"authors":"F. Hassan; X. Wang","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e0"},"title":"Release Planning of Mobile Apps Based on User Reviews","abstract":"Developers have to to constantly improve their apps by fixing critical bugs and implementing the most desired features in order to gain shares in the continuously increasing and competitive market of mobile apps. A precious source of information to plan such activities is represented by reviews left by users on the app store. However, in order to exploit such information developers need to manually analyze such reviews. This is something not doable if, as frequently happens, the app receives hundreds of reviews per day. In this paper we introduce CLAP (Crowd Listener for releAse Planning), a thorough solution to (i) categorize user reviews based on the information they carry out (e.g., bug reporting), (ii) cluster together related reviews (e.g., all reviews reporting the same bug), and (iii) automatically prioritize the clusters of reviews to be implemented when planning the subsequent app release. We evaluated all the steps behind CLAP, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. Also, given the availability of CLAP as a working tool, we assessed its practical applicability in industrial environments.","conference":"IEEE","terms":"Computer bugs;Mobile communication;Planning;Thesauri;Software;Machine learning algorithms;Merging,mobile computing;software maintenance,release planning;mobile applications;user review;app store;information source;CLAP;crowd listener for release planning","keywords":"","startPage":"14","endPage":"24","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886888","citationCount":41,"referenceCount":31,"year":2016,"authors":"L. Villarroel; G. Bavota; B. Russo; R. Oliveto; M. Di Penta","affiliations":"Free Univ. of Bozen-Bolzano, Bolzano, Italy; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Univ. of Molise, Pesche, Italy; Univ. of Sannio, Benevento, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e1"},"title":"Goal-Conflict Likelihood Assessment Based on Model Counting","abstract":"In goal-oriented requirements engineering approaches, conflict analysis has been proposed as an abstraction for risk analysis. Intuitively, given a set of expected goals to be achieved by the system-to-be, a conflict represents a subtle situation that makes goals diverge, i.e., not be satisfiable as a whole. Conflict analysis is typically driven by the identify-assess-control cycle, aimed at identifying, assessing and resolving conflicts that may obstruct the satisfaction of the expected goals. In particular, the assessment step is concerned with evaluating how likely the identified conflicts are, and how likely and severe are their consequences. So far, existing assessment approaches restrict their analysis to obstacles (conflicts that prevent the satisfaction of a single goal), and assume that certain probabilistic information on the domain is provided, that needs to be previously elicited from experienced users, statistical data or simulations. In this paper, we present a novel automated approach to assess how likely a conflict is, that applies to general conflicts (not only obstacles) without requiring probabilistic information on the domain. Intuitively, given the LTL formulation of the domain and of a set of goals to be achieved, we compute goal conflicts, and exploit string model counting techniques to estimate the likelihood of the occurrence of the corresponding conflicting situations and the severity in which these affect the satisfaction of the goals. This information can then be used to prioritize conflicts to be resolved, and suggest which goals to drive attention to for refinements.","conference":"IEEE","terms":"Analytical models;Boundary conditions;Software;Probabilistic logic;Computational modeling;Software engineering;Requirements engineering,formal specification;formal verification;maximum likelihood estimation;risk analysis;systems analysis,goal-conflict likelihood assessment;conflict analysis;risk analysis;goal-oriented requirements engineering","keywords":"Goal Conflicts;Risk Likelihood Assessment;Model Counting","startPage":"1125","endPage":"1135","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453193","citationCount":0,"referenceCount":45,"year":2018,"authors":"R. Degiovanni; P. Castro; M. Arroyo; M. Ruiz; N. Aguirre; M. Frias","affiliations":"Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina; Univ. Nac. de Rio Cuarto, Rio Cuarto, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e2"},"title":"Adding Sparkle to Social Coding: An Empirical Study of Repository Badges in the npm Ecosystem","abstract":"In fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist.","conference":"IEEE","terms":"Reliability;Encoding;Ecosystems;Open source software;Tools;Best practices,,","keywords":"repository badge;signaling theory;regression discontinuity design;mining software repositories;dependency manager","startPage":"511","endPage":"522","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453117","citationCount":2,"referenceCount":65,"year":2018,"authors":"A. Trockman; S. Zhou; C. Kästner; B. Vasilescu","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e3"},"title":"Performance Issues and Optimizations in JavaScript: An Empirical Study","abstract":"As JavaScript is becoming increasingly popular, the performance of JavaScript programs is crucial to ensure the responsiveness and energy-efficiency of thousands of pro- grams. Yet, little is known about performance issues that developers face in practice and they address these issues. This paper presents an empirical study of 98 fixed performance issues from 16 popular client-side and server-side JavaScript projects. We identify eight root causes of issues and show that inefficient usage of APIs is the most prevalent root cause. Furthermore, we find that most is- sues are addressed by optimizations that modify only a few lines of code, without significantly affecting the complexity of the source code. By studying the performance impact of optimizations on several versions of the SpiderMonkey and V8 engines, we find that only 42.68% of all optimizations improve performance consistently across all versions of both engines. Finally, we observe that many optimizations are instances of patterns applicable across projects, as evidenced by 139 previously unknown optimization opportunities that we find based on the patterns identified during the study. The results of the study help application developers to avoid common mistakes, researchers to develop performance-related techniques that address relevant problems, and engine developers to address prevalent bottleneck patterns.","conference":"IEEE","terms":"Optimization;Engines;Servers;Libraries;Computer bugs;Reliability;Computer science,Java;software performance evaluation,JavaScript programs;API;SpiderMonkey;V8 engines;JavaScript performance","keywords":"JavaScript;Empirical study;Performance issue;Optimization","startPage":"61","endPage":"72","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886892","citationCount":9,"referenceCount":44,"year":2016,"authors":"M. Selakovic; M. Pradel","affiliations":"Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany; Dept. of Comput. Sci., Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e4"},"title":"Are Mutation Scores Correlated with Real Fault Detection? A Large Scale Empirical Study on the Relationship Between Mutants and Real Faults","abstract":"Empirical validation of software testing studies is increasingly relying on mutants. This practice is motivated by the strong correlation between mutant scores and real fault detection that is reported in the literature. In contrast, our study shows that correlations are the results of the confounding effects of the test suite size. In particular, we investigate the relation between two independent variables, mutation score and test suite size, with one dependent variable the detection of (real) faults. We use two data sets, CoreBench and De-fects4J, with large C and Java programs and real faults and provide evidence that all correlations between mutation scores and real fault detection are weak when controlling for test suite size. We also found that both independent variables significantly influence the dependent one, with significantly better fits, but overall with relative low prediction power. By measuring the fault detection capability of the top ranked, according to mutation score, test suites (opposed to randomly selected test suites of the same size), we found that achieving higher mutation scores improves significantly the fault detection. Taken together, our data suggest that mutants provide good guidance for improving the fault detection of test suites, but their correlation with fault detection are weak.","conference":"IEEE","terms":"Fault detection;Correlation;Java;Measurement;Software testing;Software engineering,Java;program testing,mutation score;randomly selected test suites;higher mutation scores;mutation scores correlated;real fault detection;scale empirical study;empirical validation;software testing studies;mutant scores;test suite size;dependent variable the detection;fault detection capability","keywords":"mutation testing;real faults;test suite effectiveness","startPage":"537","endPage":"548","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453121","citationCount":4,"referenceCount":0,"year":2018,"authors":"M. Papadakis; D. Shin; S. Yoo; D. Bae","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e5"},"title":"RETracer: Triaging Crashes by Reverse Execution from Partial Memory Dumps","abstract":"Many software providers operate crash reporting services to automatically collect crashes from millions of customers and file bug reports. Precisely triaging crashes is necessary and important for software providers because the millions of crashes that may be reported every day are critical in identifying high impact bugs. However, the triaging accuracy of existing systems is limited, as they rely only on the syntactic information of the stack trace at the moment of a crash without analyzing program semantics. In this paper, we present RETracer, the first system to triage software crashes based on program semantics reconstructed from memory dumps. RETracer was designed to meet the requirements of large-scale crash reporting services. RETracer performs binary-level backward taint analysis without a recorded execution trace to understand how functions on the stack contribute to the crash. The main challenge is that the machine state at an earlier time cannot be recovered completely from a memory dump, since most instructions are information destroying. We have implemented RETracer for x86 and x86-64 native code, and compared it with the existing crash triaging tool used by Microsoft. We found that RETracer eliminates two thirds of triage errors based on a manual analysis of 140 bugs fixed in Microsoft Windows and Office. RETracer has been deployed as the main crash triaging system on Microsoft's crash reporting service.","conference":"IEEE","terms":"Computer bugs;Registers;Semantics;Instruction sets,program debugging;program diagnostics;programming language semantics,RETracer;software crashes triage;program semantics;large-scale crash reporting services;binary-level backward taint analysis;machine state;x86-64 native code;x86 native code;software crash triaging;file bug reports;Microsoft Windows;Microsoft Office;Microsoft crash reporting service;high impact bugs;syntactic information;stack trace;reverse execution;partial memory dumps;software providers","keywords":"triaging;backward taint analysis;reverse execution","startPage":"820","endPage":"831","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886959","citationCount":8,"referenceCount":52,"year":2016,"authors":"W. Cui; M. Peinado; S. K. Cha; Y. Fratantonio; V. P. Kemerlis","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e6"},"title":"Discovering \"Unknown Known\" Security Requirements","abstract":"Security is one of the biggest challenges facing organisations in the modern hyper-connected world. A number of theoretical security models are available that provide best practice security guidelines and are widely utilised as a basis to identify and operationalise security requirements. Such models often capture high-level security concepts (e.g., whitelisting, secure configurations, wireless access control, data recovery, etc.), strategies for operationalising such concepts through specific security controls, and relationships between the various concepts and controls. The threat landscape, however, evolves leading to new tacit knowledge that is embedded in or across a variety of security incidents. These unknown knowns alter, or at least demand reconsideration of the theoretical security models underpinning security requirements. In this paper, we present an approach to discover such unknown knowns through multi-incident analysis. The approach is based on a novel combination of grounded theory and incident fault trees. We demonstrate the effectiveness of the approach through its application to identify revisions to a theoretical security model widely used in industry.","conference":"IEEE","terms":"Security;Fault trees;Biological system modeling;Logic gates;Guidelines;Industries;Adaptation models,fault trees;formal specification;security of data,security requirements;multiincident analysis;grounded theory;incident fault trees;theoretical security model","keywords":"Security requirements;incident analysis;grounded theory","startPage":"866","endPage":"876","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886963","citationCount":2,"referenceCount":23,"year":2016,"authors":"A. Rashid; S. A. A. Naqvi; R. Ramdhany; M. Edwards; R. Chitchyan; M. A. Babar","affiliations":"Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Security Lancaster Res. Centre, Lancaster Univ., Lancaster, UK; Dept. of Comput. Sci., Univ. of Leicester, Leicester, UK; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e7"},"title":"RFC-Directed Differential Testing of Certificate Validation in SSL/TLS Implementations","abstract":"Certificate validation in Secure Socket Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS is correctly implemented. With this motivation, we propose a novel differential testing approach which is directed by the standard Request For Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e. certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations.","conference":"IEEE","terms":"Testing;Computer bugs;Public key;Standards;Servers;Protocols,certification;computer network security;Internet;program testing;protocols;security of data,RFC-directed differential testing;certificate validation;Internet security;novel differential testing approach;low-level test cases;high-level test cases;assembled certificates;RFCcert;traditional differential testing;secure socket layer;transport layer security protocol;SSL-TLS implementations","keywords":"Differential testing;certificate validation;SSL/TLS;dynamic symbolic execution;Request For Comments","startPage":"859","endPage":"870","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453160","citationCount":1,"referenceCount":0,"year":2018,"authors":"C. Chen; C. Tian; Z. Duan; L. Zhao","affiliations":"ICTT \u0026 ISN Lab., Xidian Univ. Xi'an, Xi'an, China; ICTT \u0026 ISN Lab., Xidian Univ. Xi'an, Xi'an, China; ICTT \u0026 ISN Lab., Xidian Univ. Xi'an, Xi'an, China; ICTT \u0026 ISN Lab., Xidian Univ. Xi'an, Xi'an, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e8"},"title":"CUSTODES: Automatic Spreadsheet Cell Clustering and Smell Detection Using Strong and Weak Features","abstract":"Various techniques have been proposed to detect smells in spreadsheets, which are susceptible to errors. These techniques typically detect spreadsheet smells through a mechanism based on a fixed set of patterns or metric thresholds. Unlike conventional programs, tabulation styles vary greatly across spreadsheets. Smell detection based on fixed patterns or metric thresholds, which are insensitive to the varying tabulation styles, can miss many smells in one spreadsheet while reporting many spurious smells in another. In this paper, we propose CUSTODES to effectively cluster spreadsheet cells and detect smells in these clusters. The clustering mechanism can automatically adapt to the tabulation styles of each spreadsheet using strong and weak features. These strong and weak features capture the invariant and variant parts of tabulation styles, respectively. As smelly cells in a spreadsheet normally occur in minority, they can be mechanically detected as clusters' outliers in feature spaces. We implemented and applied CUSTODES to 70 spreadsheets files randomly sampled from the EUSES corpus. These spreadsheets contain 1,610 formula cell clusters. Experimental results confirmed that CUSTODES is effective. It successfully detected harmful smells that can induce computation anomalies in spreadsheets with an F-measure of 0.72, outperforming state-of-the-art techniques.","conference":"IEEE","terms":"Feature extraction;Measurement;Adaptation models;Software;Computers;Computational modeling;Quality assurance,pattern clustering;program diagnostics;spreadsheet programs,automatic spreadsheet cell clustering;smell detection;tabulation styles;CUSTODES;EUSES corpus;F-measure","keywords":"Spreadsheets;cell clustering;smell detection;feature modeling;end-user programming","startPage":"464","endPage":"475","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886926","citationCount":8,"referenceCount":49,"year":2016,"authors":"S. Cheung; W. Chen; Y. Liu; C. Xu","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Tech., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35e9"},"title":"On The Limits of Mutation Reduction Strategies","abstract":"Although mutation analysis is considered the best way to evaluate the effectiveness of a test suite, hefty computational cost often limits its use. To address this problem, various mutation reduction strategies have been proposed, all seeking to reduce the number of mutants while maintaining the representativeness of an exhaustive mutation analysis. While research has focused on the reduction achieved, the effectiveness of these strategies in selecting representative mutants, and the limits in doing so have not been investigated, either theoretically or empirically. We investigate the practical limits to the effectiveness of mutation reduction strategies, and provide a simple theoretical framework for thinking about the absolute limits. Our results show that the limit in improvement of effectiveness over random sampling for real-world open source programs is a mean of only 13.078%. Interestingly, there is no limit to the improvement that can be made by addition of new mutation operators. Given that this is the maximum that can be achieved with perfect advance knowledge of mutation kills, what can be practically achieved may be much worse. We conclude that more effort should be focused on enhancing mutations than removing operators in the name of selective mutation for questionable benefit.","conference":"IEEE","terms":"Sociology;Software engineering;Testing;Computer bugs;Syntactics;Correlation,program diagnostics;program testing;public domain software,software testing;mutation operators;open source programs;random sampling;mutation reduction strategies","keywords":"software testing; statistical analysis; theoretical analysis; mutation analysis","startPage":"511","endPage":"522","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886930","citationCount":4,"referenceCount":54,"year":2016,"authors":"R. Gopinath; M. A. Alipour; I. Ahmed; C. Jensen; A. Groce","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ea"},"title":"A Static Verification Framework for Message Passing in Go Using Behavioural Types","abstract":"The Go programming language has been heavily adopted in industry as a language that efficiently combines systems programming with concurrency. Go's concurrency primitives, inspired by process calculi such as CCS and CSP, feature channel-based communication and lightweight threads, providing a distinct means of structuring concurrent software. Despite its popularity, the Go programming ecosystem offers little to no support for guaranteeing the correctness of message-passing concurrent programs. This work proposes a practical verification framework for message passing concurrency in Go by developing a robust static analysis that infers an abstract model of a program's communication behaviour in the form of a behavioural type, a powerful process calculi typing discipline. We make use of our analysis to deploy a model and termination checking based verification of the inferred behavioural type that is suitable for a range of safety and liveness properties of Go programs, providing several improvements over existing approaches. We evaluate our framework and its implementation on publicly available real-world Go code.","conference":"IEEE","terms":"Concurrent computing;System recovery;Programming;Safety;Message systems;Software;Computer languages,concurrency control;formal verification;message passing;program diagnostics;program verification,static verification framework;Go programming language;concurrency primitives;feature channel-based communication;lightweight threads;concurrent software;Go programming ecosystem;message-passing concurrent programs;practical verification framework;message passing concurrency;robust static analysis;powerful process calculi;termination checking based verification;inferred behavioural type","keywords":"concurrency;static analysis;behavioural types;model checking;go programming language","startPage":"1137","endPage":"1148","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453195","citationCount":1,"referenceCount":45,"year":2018,"authors":"J. Lange; N. Ng; B. Toninho; N. Yoshida","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35eb"},"title":"Optimizing Selection of Competing Services with Probabilistic Hierarchical Refinement","abstract":"Recently, many large enterprises (e.g., Netflix, Amazon) have decomposed their monolithic application into services, and composed them to fulfill their business functionalities. Many hosting services on the cloud, with different Quality of Service (QoS) (e.g., availability, cost), can be used to host the services. This is an example of competing services. QoS is crucial for the satisfaction of users. It is important to choose a set of services that maximize the overall QoS, and satisfy all QoS requirements for the service composition. This problem, known as optimal service selection, is NP-hard. Therefore, an effective method for reducing the search space and guiding the search process is highly desirable. To this end, we introduce a novel technique, called Probabilistic Hierarchical Refinement (PROHR). PROHR effectively reduces the search space by removing competing services that cannot be part of the selection. PROHR provides two methods, probabilistic ranking and hierarchical refinement, that enable smart exploration of the reduced search space. Unlike existing approaches that perform poorly when QoS requirements become stricter, PROHR maintains high performance and accuracy, independent of the strictness of the QoS requirements. PROHR has been evaluated on a publicly available dataset, and has shown significant improvement over existing approaches.","conference":"IEEE","terms":"Quality of service;Concrete;Time factors;Probabilistic logic;Web services;Automobiles,computational complexity;probability;quality of service;Web services,probabilistic hierarchical refinement;business functionalities;hosting services;cloud computing;quality of service;QoS;user satisfaction;optimal service selection problem;NP-hard problem;PROHR technique;probabilistic ranking technique;hierarchical refinement technique","keywords":"Service Selection;Service Composition;Mixed Integer Programming","startPage":"85","endPage":"95","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886894","citationCount":6,"referenceCount":38,"year":2016,"authors":"T. H. Tan; M. Chen; J. Sun; Y. Liu; É. André; Y. Xue; J. S. Dong","affiliations":"NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ec"},"title":"Augusto: Exploiting Popular Functionalities for the Generation of Semantic GUI Tests with Oracles","abstract":"Testing software applications by interacting with their graphical user interface (GUI) is an expensive and complex process. Current automatic test case generation techniques implement explorative approaches that, although producing useful test cases, have a limited capability of covering semantically relevant interactions, thus frequently missing important testing scenarios. These techniques typically interact with the available widgets following the structure of the GUI, without any guess about the functions that are executed. In this paper we propose Augusto, a test case generation technique that exploits a built-in knowledge of the semantics associated with popular and well-known functionalities, such as CRUD operations, to automatically generate effective test cases with automated functional oracles. Empirical results indicate that Augusto can reveal faults that cannot be revealed with state of the art techniques.","conference":"IEEE","terms":"Graphical user interfaces;Semantics;Testing;Software;Authentication;Pattern matching;Adaptation models,graphical user interfaces;program testing,useful test cases;semantically relevant interactions;important testing scenarios;Augusto;test case generation technique;semantics;effective test cases;automated functional oracles;popular functionalities;semantic GUI tests;software applications;graphical user interface;expensive process;complex process;current automatic test case generation techniques","keywords":"GUI testing;automatic test case generation;semantics;oracles","startPage":"280","endPage":"290","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453087","citationCount":2,"referenceCount":0,"year":2018,"authors":"L. Mariani; M. Pezzè; D. Zuddas","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ed"},"title":"Spatio-Temporal Context Reduction: A Pointer-Analysis-Based Static Approach for Detecting Use-After-Free Vulnerabilities","abstract":"Zero-day Use-After-Free (UAF) vulnerabilities are increasingly popular and highly dangerous, but few mitigations exist. We introduce a new pointer-analysis-based static analysis, CRed, for finding UAF bugs in multi-MLOC C source code efficiently and effectively. Cred achieves this by making three advances: (i) a spatio-temporal context reduction technique for scaling down soundly and precisely the exponential number of contexts that would otherwise be considered at a pair of free and use sites, (ii) a multi-stage analysis for filtering out false alarms efficiently, and (iii) a path-sensitive demand-driven approach for finding the points-to information required. We have implemented CRed in LLVM-3.8.0 and compared it with four different state-of-the-art static tools: CBMC (model checking), Clang (abstract interpretation), Coccinelle (pattern matching), and Supa (pointer analysis) using all the C test cases in Juliet Test Suite (JTS) and 10 open-source C applications. For the ground-truth validated with JTS, CRed detects all the 138 known UAF bugs as CBMC and Supa do while Clang and Coccinelle miss some bugs, with no false alarms from any tool. For practicality validated with the 10 applications (totaling 3+ MLOC), CRed reports 132 warnings including 85 bugs in 7.6 hours while the existing tools are either unscalable by terminating within 3 days only for one application (CBMC) or impractical by finding virtually no bugs (Clang and Coccinelle) or issuing an excessive number of false alarms (Supa).","conference":"IEEE","terms":"Computer bugs;Tools;Correlation;Australia;Pattern matching;Software;Static analysis,C language;pattern matching;program compilers;program debugging;program diagnostics;program verification,Coccinelle;false alarms;pointer-analysis-based static approach;zero-day Use-After-Free vulnerabilities;pointer-analysis-based static analysis;finding UAF bugs;multiMLOC C source code;spatio-temporal context reduction technique;multistage analysis;path-sensitive demand-driven approach;CBMC;Supa;Juliet Test Suite;open-source C applications;UAF bugs","keywords":"use after free;program analysis;bug detection","startPage":"327","endPage":"337","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453091","citationCount":3,"referenceCount":0,"year":2018,"authors":"H. Yan; Y. Sui; S. Chen; J. Xue","affiliations":"Sch. of Comput. Sci. \u0026 Eng., Univ. of New South Wales, Sydney, NSW, Australia; Centre for Artificial Intell. \u0026 Sch. of Software, Univ. of Technol., Sydney, NSW, Australia; Data61, CSIRO, Australia; Sch. of Comput. Sci. \u0026 Eng., Univ. of New South Wales, Sydney, NSW, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ee"},"title":"Efficient Large-Scale Trace Checking Using MapReduce","abstract":"The problem of checking a logged event trace against a temporal logic specification arises in many practical cases. Unfortunately, known algorithms for an expressive logic like MTL (Metric Temporal Logic) do not scale with respect to two crucial dimensions: the length of the trace and the size of the time interval of the formula to be checked. The former issue can be addressed by distributed and parallel trace checking algorithms that can take advantage of modern cloud computing and programming frameworks like MapReduce. Still, the latter issue remains open with current state-of-the-art approaches. In this paper we address this memory scalability issue by proposing a new semantics for MTL, called lazy semantics. This semantics can evaluate temporal formulae and boolean combinations of temporal-only formulae at any arbitrary time instant. We prove that lazy semantics is more expressive than point-based semantics and that it can be used as a basis for a correct parametric decomposition of any MTL formula into an equivalent one with smaller, bounded time intervals. We use lazy semantics to extend our previous distributed trace checking algorithm for MTL. The evaluation shows that the proposed algorithm can check formulae with large intervals, on large traces, in a memory-efficient way.","conference":"IEEE","terms":"Semantics;Programming;Scalability;Clustering algorithms;Measurement;Data models;Heuristic algorithms,formal specification;parallel algorithms;temporal logic,large-scale trace checking;MapReduce;temporal logic specification;MTL logic;metric temporal logic;parallel trace checking algorithms;distributed trace checking algorithms;memory scalability issue;lazy semantics;Boolean combination;point-based semantics","keywords":"metric temporal logic;trace checking;MapReduce","startPage":"888","endPage":"898","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886965","citationCount":1,"referenceCount":29,"year":2016,"authors":"M. M. Bersani; D. Bianculli; C. Ghezzi; S. Krstic; P. San Pietro","affiliations":"DEEPSE group - DEIB, Politec. di Milano, Milan, Italy; SnT Centre, Univ. of Luxembourg, Luxembourg, Luxembourg; DEEPSE group - DEIB, Politec. di Milano, Milan, Italy; DEEPSE group - DEIB, Politec. di Milano, Milan, Italy; DEEPSE group - DEIB, Politec. di Milano, Milan, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ef"},"title":"A Temporal Permission Analysis and Enforcement Framework for Android","abstract":"Permission-induced attacks, i.e., security breaches enabled by permission misuse, are among the most critical and frequent issues threatening the security of Android devices. By ignoring the temporal aspects of an attack during the analysis and enforcement, the state-of-the-art approaches aimed at protecting the users against such attacks are prone to have low-coverage in detection and high-disruption in prevention of permission-induced attacks. To address the aforementioned shortcomings, we present TERMINATOR, a temporal permission analysis and enforcement framework for Android. Leveraging temporal logic model checking, TERMINATOR's analyzer identifies permission-induced threats with respect to dynamic permission states of the apps. At runtime, TERMINATOR's enforcer selectively leases (i.e., temporarily grants) permissions to apps when the system is in a safe state, and revokes the permissions when the system moves to an unsafe state realizing the identified threats. The results of our experiments, conducted over thousands of apps, indicate that TERMINATOR is able to provide an effective, yet non-disruptive defense against permission-induced attacks. We also show that our approach, which does not require modification to the Android framework or apps' implementation logic, is highly reliable and widely applicable.","conference":"IEEE","terms":"Androids;Humanoid robots;Security;Informatics;Malware;Analytical models;Tools,Android (operating system);authorisation;data privacy;temporal logic,permission misuse;temporal aspects;permission-induced attacks;temporal permission analysis;enforcement framework;temporal logic model checking;permission-induced threats;dynamic permission states;Android framework;TERMINATOR enforcer","keywords":"Android;Access Control (Permission);Temporal Logic","startPage":"846","endPage":"857","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453158","citationCount":1,"referenceCount":0,"year":2018,"authors":"A. Sadeghi; R. Jabbarvand; N. Ghorbani; H. Bagheri; S. Malek","affiliations":"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska, Lincoln, NE, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f0"},"title":"Identifying and Quantifying Architectural Debt","abstract":"Our prior work showed that the majority of error-prone source files in a software system are architecturally connected. Flawed architectural relations propagate defectsamong these files and accumulate high maintenance costs over time, just like debts accumulate interest. We model groups of architecturally connected files that accumulate high maintenance costs as architectural debts. To quantify such debts, we formally define architectural debt, and show how to automatically identify debts, quantify their maintenance costs, and model these costs over time. We describe a novel history coupling probability matrix for this purpose, and identify architecture debts using 4 patterns of architectural flaws shown to correlate with reduced software quality. We evaluate our approach on 7 large-scale open source projects, and show that a significant portion of total project maintenance effort is consumed by paying interest on architectural debts. The top 5 architectural debts, covering a small portion (8% to 25%) of each project's error-prone files, capture a significant portion (20% to 61%) of each project's maintenance effort. Finally, we show that our approach reveals how architectural issues evolve into debts over time.","conference":"IEEE","terms":"Maintenance engineering;History;Computer architecture;Couplings;Software;Economic indicators;Software architecture,costing;software architecture;software maintenance;software quality,architectural debt identification;architectural debt quantification;error-prone source files;software system;architecturally connected files;maintenance costs;history coupling probability matrix;software quality;total project maintenance effort","keywords":"","startPage":"488","endPage":"498","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886928","citationCount":17,"referenceCount":28,"year":2016,"authors":"L. Xiao; Y. Cai; R. Kazman; R. Mo; Q. Feng","affiliations":"Drexel Univ. Philadelphia, Philadelphia, PA, USA; Drexel Univ. Philadelphia, Philadelphia, PA, USA; SEI, Univ. of Hawaii, Honolulu, HI, USA; Drexel Univ. Philadelphia, Philadelphia, PA, USA; SEI, Univ. of Hawaii, Honolulu, HI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f1"},"title":"How Does Regression Test Prioritization Perform in Real-World Software Evolution?","abstract":"In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.","conference":"IEEE","terms":"Testing;Software systems;Schedules;Time factors;Instruments;Software engineering,program testing;software engineering,regression test prioritization;real-world software evolution;fault detection;prioritization techniques;software development;test suite augmentation;GitHub","keywords":"","startPage":"535","endPage":"546","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886932","citationCount":9,"referenceCount":67,"year":2016,"authors":"Y. Lu; Y. Lou; S. Cheng; L. Zhang; D. Hao; Y. Zhou; L. Zhang","affiliations":"Dept. of Comput. Sci., Univ. of Texas at Dallas, Dallas, TX, USA; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Dept. of Comput. Sci., Univ. of Texas at Dallas, Dallas, TX, USA; Dept. of Comput. Sci., Univ. of Texas at Dallas, Dallas, TX, USA; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Sch. of Comput. Sci., Fudan Univ., Shanghai, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f2"},"title":"DroidStar: Callback Typestates for Android Classes","abstract":"Event-driven programming frameworks, such as Android, are based on components with asynchronous interfaces. The protocols for interacting with these components can often be described by finite-state machines we dub *callback typestates. Callback typestates are akin to classical typestates, with the difference that their outputs (callbacks) are produced asynchronously. While useful, these specifications are not commonly available, because writing them is difficult and error-prone. Our goal is to make the task of producing callback typestates significantly easier. We present a callback typestate assistant tool, DroidStar, that requires only limited user interaction to produce a callback typestate. Our approach is based on an active learning algorithm, L*. We improved the scalability of equivalence queries (a key component of L*), thus making active learning tractable on the Android system. We use DroidStar to learn callback typestates for Android classes both for cases where one is already provided by the documentation, and for cases where the documentation is unclear. The results show that DroidStar learns callback typestates accurately and efficiently. Moreover, in several cases, the synthesized callback typestates uncovered surprising and undocumented behaviors.","conference":"IEEE","terms":"Androids;Humanoid robots;Protocols;Tools;Documentation;Learning automata;Task analysis,learning (artificial intelligence);program diagnostics,DroidStar;callback typestate assistant tool;synthesized callback typestates;dub *callback typestates","keywords":"typestate;specification inference;Android;active learning","startPage":"1160","endPage":"1170","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453197","citationCount":0,"referenceCount":41,"year":2018,"authors":"A. Radhakrishna; N. V. Lewchenko; S. Meier; S. Mover; K. C. Sripada; D. Zufferey; B. E. Chang; P. Cerný","affiliations":"NA; NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f3"},"title":"Belief \u0026 Evidence in Empirical Software Engineering","abstract":"Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.","conference":"IEEE","terms":"Bayes methods;Software engineering;Software;Media;Computer science;Immune system,software engineering,empirical software engineering;Microsoft;empirical research;software practice","keywords":"Practitioner Belief;Empirical Evidence;Empirical Software Engineering.","startPage":"108","endPage":"119","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886896","citationCount":20,"referenceCount":59,"year":2016,"authors":"P. Devanbu; T. Zimmermann; C. Bird","affiliations":"Dept. of Comput. Sci., UC Davis, Davis, CA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f4"},"title":"DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars","abstract":"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.","conference":"IEEE","terms":"Neurons;Autonomous automobiles;Software;Testing;Automobiles;Sensors;Computer architecture,automobiles;intelligent transportation systems;neural nets;program testing;road accidents;road safety,DeepTest;automated testing;deep-neural-network-driven autonomous cars;recent advances;DNN-driven autonomous cars;human intervention;autonomous vehicles;US states;potentially fatal collisions;DNN-driven vehicles;test conditions increases;systematic testing tool;erroneous behaviors;automatically generated test cases;DNN logic;test inputs;potentially fatal crashes;Udacity self-driving car challenge;deep neural networks;driving conditions;realistic driving conditions;Waymo-Google","keywords":"deep learning;testing;self-driving cars;deep neural networks;autonomous vehicle;neuron coverage","startPage":"303","endPage":"314","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453089","citationCount":48,"referenceCount":87,"year":2018,"authors":"Y. Tian; K. Pei; S. Jana; B. Ray","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f5"},"title":"Chopped Symbolic Execution","abstract":"Symbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations. In this paper, we propose chopped symbolic execution, a novel form of symbolic execution that allows users to specify uninter-esting parts of the code to exclude during the analysis, thus only targeting the exploration to paths of importance. However, the excluded parts are not summarily ignored, as this may lead to both false positives and false negatives. Instead, they are executed lazily, when their effect may be observable by code under anal-ysis. Chopped symbolic execution leverages various on-demand static analyses at runtime to automatically exclude code fragments while resolving their side effects, thus avoiding expensive manual annotations and imprecision. Our preliminary results show that the approach can effectively improve the effectiveness of symbolic execution in several different scenarios, including failure reproduction and test suite augmentation.","conference":"IEEE","terms":"Explosions;Engines;Computer bugs;Static analysis;Runtime;Software,program diagnostics;program testing;program verification,multiple program paths;chopped symbolic execution leverages;powerful program analysis technique;constraint solving limitations","keywords":"Symbolic execution;Static analysis;Program slicing","startPage":"350","endPage":"360","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453093","citationCount":1,"referenceCount":45,"year":2018,"authors":"D. Trabish; A. Mattavelli; N. Rinetzky; C. Cadar","affiliations":"Tel Aviv Univ., Tel Aviv, Israel; Imperial Coll. London, London, UK; Tel Aviv Univ., Tel Aviv, Israel; Imperial Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f6"},"title":"DoubleTake: Fast and Precise Error Detection via Evidence-Based Dynamic Analysis","abstract":"Programs written in unsafe languages like C and C++ often suffer from errors like buffer overflows, dangling pointers, and memory leaks. Dynamic analysis tools like Valgrind can detect these errors, but their overhead - primarily due to the cost of instrumenting every memory read and write - makes them too heavyweight for use in deployed applications and makes testing with them painfully slow. The result is that much deployed software remains susceptible to these bugs, which are notoriously difficult to track down.This paper presents evidence-based dynamic analysis, an approach that enables these analyses while imposing minimal overhead (under 5%), making it practical for the first time to perform these analyses in deployed settings. The key insight of evidence-based dynamic analysis is that for a class of errors, it is possible to ensure that evidence that they happened at some point in the past remains for later detection. Evidence-based dynamic analysis allows execution to proceed at nearly full speed until the end of an epoch (e.g., a heavyweight system call). It then examines program state to check for evidence that an error occurred at some time during that epoch. If so, it rolls back execution and re-executes the code with instrumentation activated to pinpoint the error.We present DoubleTake, a prototype evidence-based dynamic analysis framework. DoubleTake is practical and easy to deploy, requiring neither custom hardware, compiler, nor operating system support. We demonstrate DoubleTake's generality and efficiency by building dynamic analyses that find buffer overflows, memory use-after-free errors, and memory leaks. Our evaluation shows that DoubleTake is efficient, imposing under 5% overhead on average, making it the fastest such system to date. It is also precise: DoubleTake pinpoints the location of these errors to the exact line and memory addresses where they occur, providing valuable debugging information to programmers.","conference":"IEEE","terms":"Instruments;Testing;Detectors;Hardware;Operating systems;Resource management;Software engineering,program debugging;program diagnostics,error detection;evidence-based dynamic analysis;Valgrind tool;DoubleTake;buffer overflow;memory use-after-free errors;memory leaks;debugging information","keywords":"Dynamic Analysis;Software Quality;Testing;Debugging;Leak Detection;Buffer Overflow Detection;Use-After-Free Detection","startPage":"911","endPage":"922","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886967","citationCount":8,"referenceCount":43,"year":2016,"authors":"T. Liu; C. Curtsinger; E. D. Berger","affiliations":"Dept. of Comput. Sci., Univ. of Texas at San Antonio, San Antonio, TX, USA; Dept. of Comput. Sci., Grinnell Coll., Grinnell, IA, USA; Coll. of Inf. \u0026 Comput. Sci., Univ. of Massachusetts, Amherst, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f7"},"title":"Reference Hijacking: Patching, Protecting and Analyzing on Unmodified and Non-rooted Android Devices","abstract":"Many efforts have been paid to enhance the security of Android. However, less attention has been given to how to practically adopt the enhancements on off-the-shelf devices. In particular, securing Android devices often requires modifying their write-protected underlying system component files (especially the system libraries) by flashing or rooting devices, which is unacceptable in many realistic cases. In this paper, a novel technique, called reference hijacking, is presented to address the problem. By introducing a specially designed reset procedure, a new execution environment is constructed for the target application, in which the reference to the underlying system libraries will be redirected to the security-enhanced alternatives. The technique can be applicable to both the Dalvik and Android Runtime (ART) environments and to almost all mainstream Android versions (2.x to 5.x). To demonstrate the capability of reference hijacking, we develop three prototype systems, PatchMan, ControlMan, and TaintMan, to enforce specific security enhancements, involving patching vulnerabilities, protecting inter-component communications, and performing dynamic taint analysis for the target application. These three prototypes have been successfully deployed on a number of popular Android devices from different manufacturers, without modifying the underlying system. The evaluation results show that they are effective and do not introduce noticeable overhead. They strongly support that reference hijacking can substantially improve the practicability of many security enhancement efforts for Android.","conference":"IEEE","terms":"Libraries;Androids;Humanoid robots;Prototypes;Malware;Access control,Android (operating system);mobile computing;security of data,reference hijacking technique;Android devices;Android security;reset procedure;security-enhanced alternatives;Dalvik environment;Android Runtime environment;PatchMan system;ControlMan system;TaintMan system;security enhancements","keywords":"Android;Security enhancement;Practicability","startPage":"959","endPage":"970","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886971","citationCount":4,"referenceCount":55,"year":2016,"authors":"W. You; B. Liang; W. Shi; S. Zhu; P. Wang; S. Xie; X. Zhang","affiliations":"Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f8"},"title":"GUILeak: Tracing Privacy Policy Claims on User Input Data for Android Applications","abstract":"The Android mobile platform supports billions of devices across more than 190 countries around the world. This popularity coupled with user data collection by Android apps has made privacy protection a well-known challenge in the Android ecosystem. In practice, app producers provide privacy policies disclosing what information is collected and processed by the app. However, it is difficult to trace such claims to the corresponding app code to verify whether the implementation is consistent with the policy. Existing approaches for privacy policy alignment focus on information directly accessed through the Android platform (e.g., location and device ID), but are unable to handle user input, a major source of private information. In this paper, we propose a novel approach that automatically detects privacy leaks of user-entered data for a given Android app and determines whether such leakage may violate the app's privacy policy claims. For evaluation, we applied our approach to 120 popular apps from three privacy-relevant app categories: finance, health, and dating. The results show that our approach was able to detect 21 strong violations and 18 weak violations from the studied apps.","conference":"IEEE","terms":"Graphical user interfaces;Ontologies;Privacy;Data privacy;Androids;Humanoid robots;Layout,data privacy;mobile computing,privacy protection;Android ecosystem;app producers;privacy policies;private information;privacy-relevant app categories;privacy policy claims;android applications;Android mobile platform;user data collection;Android apps;app code;privacy leaks detection;privacy policy alignment","keywords":"Mobile privacy policy;Android application;User input","startPage":"37","endPage":"47","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453060","citationCount":2,"referenceCount":0,"year":2018,"authors":"X. Wang; X. Qin; M. Bokaei Hosseini; R. Slavin; T. D. Breaux; J. Niu","affiliations":"Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35f9"},"title":"Reducing Combinatorics in GUI Testing of Android Applications","abstract":"The rising popularity of Android and the GUI-driven nature of its apps have motivated the need for applicable automated GUI testing techniques. Although exhaustive testing of all possible combinations is the ideal upper bound in combinatorial testing, it is often infeasible, due to the combinatorial explosion of test cases. This paper presents TrimDroid, a framework for GUI testing of Android apps that uses a novel strategy to generate tests in a combinatorial, yet scalable, fashion. It is backed with automated program analysis and formally rigorous test generation engines. TrimDroid relies on program analysis to extract formal specifications. These specifications express the app's behavior (i.e., control flow between the various app screens) as well as the GUI elements and their dependencies. The dependencies among the GUI elements comprising the app are used to reduce the number of combinations with the help of a solver. Our experiments have corroborated TrimDroid's ability to achieve a comparable coverage as that possible under exhaustive GUI testing using significantly fewer test cases.","conference":"IEEE","terms":"Testing;Graphical user interfaces;Androids;Humanoid robots;Erbium;Smart phones,formal specification;graphical user interfaces;mobile computing;program diagnostics;program testing,GUI testing;graphical user interface;Android applications;combinatorial testing;TrimDroid framework;test generation;program analysis;formal specifications","keywords":"Android;Software Testing;Input Generation","startPage":"559","endPage":"570","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886934","citationCount":20,"referenceCount":39,"year":2016,"authors":"N. Mirzaei; J. Garcia; H. Bagheri; A. Sadeghi; S. Malek","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35fa"},"title":"On the Techniques We Create, the Tools We Build, and Their Misalignments: A Study of KLEE","abstract":"Our community constantly pushes the state-of-the-art by introducing “new” techniques. These techniques often build on top of, and are compared against, existing systems that realize previously published techniques. The underlying assumption is that existing systems correctly represent the techniques they implement. This pa- per examines that assumption through a study of KLEE, a popular and well-cited tool in our community. We briefly describe six improvements we made to KLEE, none of which can be considered “new” techniques, that provide order-of-magnitude performance gains. Given these improvements, we then investigate how the results and conclusions of a sample of papers that cite KLEE are affected. Our findings indicate that the strong emphasis on introducing “new” techniques may lead to wasted effort, missed opportunities for progress, an accretion of artifact complexity, and questionable research conclusions (in our study, 27% of the papers that depend on KLEE can be questioned). We conclude by revisiting initiatives that may help to realign the incentives to better support the foundations on which we build.","conference":"IEEE","terms":"Optimization;Computer bugs;Software engineering;Buildings;Software;Complexity theory;Engineering profession,software engineering,KLEE tool;software techniques;performance gain;artifact complexity","keywords":"Research incentives;research tools and infrastructure;replication","startPage":"132","endPage":"143","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886898","citationCount":2,"referenceCount":86,"year":2016,"authors":"E. F. Rizzi; S. Elbaum; M. B. Dwyer","affiliations":"Grammatech Inc., Ithaca, NY, USA; Univ. of Nebraska - Lincoln, Lincoln, NE, USA; Univ. of Nebraska - Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35fb"},"title":"Collective Program Analysis","abstract":"Popularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis (CPA), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69% reduction when compared to a baseline and on average a 36% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets.","conference":"IEEE","terms":"Transfer functions;Cloning;Software engineering;Task analysis;Syntactics;Analytical models;Labeling,data flow analysis;pattern clustering;software engineering,collective program analysis;cluster programs;data-driven software engineering;deeper source code analysis;scale source code analyses;data flow analysis","keywords":"Source code analysis;Clustering;Boa","startPage":"620","endPage":"631","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453131","citationCount":0,"referenceCount":0,"year":2018,"authors":"G. Upadhyaya; H. Rajan","affiliations":"Dept. of Comput. Sci., Iowa State Univ. Ames, Ames, IA, USA; Dept. of Comput. Sci., Iowa State Univ. Ames, Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35fc"},"title":"\"Jumping Through Hoops\": Why do Java Developers Struggle with Cryptography APIs?","abstract":"To protect sensitive data processed by current applications, developers, whether security experts or not, have to rely on cryptography. While cryptography algorithms have become increasingly advanced, many data breaches occur because developers do not correctly use the corresponding APIs. To guide future research into practical solutions to this problem, we perform an empirical investigation into the obstacles developers face while using the Java cryptography APIs, the tasks they use the APIs for, and the kind of (tool) support they desire. We triangulate data from four separate studies that include the analysis of 100 StackOverflow posts, 100 GitHub repositories, and survey input from 48 developers. We find that while developers find it difficult to use certain cryptographic algorithms correctly, they feel surprisingly confident in selecting the right cryptography concepts (e.g., encryption vs. signatures). We also find that the APIs are generally perceived to be too low-level and that developers prefer more task-based solutions.","conference":"IEEE","terms":"Java;Encryption;Face;Public key;Libraries;Complexity theory,application program interfaces;cryptography;Java,Java developers;cryptography API;application program interface;sensitive data protection;cryptography algorithms;StackOverflow;GitHub repositories","keywords":"Cryptography;API misuse;empirical software engineering","startPage":"935","endPage":"946","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886969","citationCount":34,"referenceCount":37,"year":2016,"authors":"S. Nadi; S. Krüger; M. Mezini; E. Bodden","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35fd"},"title":"The Challenges of Staying Together While Moving Fast: An Exploratory Study","abstract":"We report on the results of an empirical study conducted with 35 experienced software developers from 22 high-tech companies, including Google, Facebook, Microsoft, Intel, and others. The goal of the study was to elicit challenges that these developers face, potential solutions that they envision to these challenges, and research initiatives that they think would deliver useful results. Challenges identified by the majority of the study participants relate to the collaborative nature of the work: the availability and discoverability of information, communication, collaborative planning and integration with work of others. Almost all participants also addressed the advantages and disadvantages of the current “fast to the market” trend, and the toll it takes on the quality of the software that they are able to deliver and on their professional and personal satisfaction as software engineers. We describe in depth the identified challenges, supporting our findings with explicit quotes from the study participants. We also put these findings in context of work done by the software engineering community and outline a roadmap for possible future research initiatives.","conference":"IEEE","terms":"Software;Companies;Interviews;Software engineering;Face;Research initiatives,software development management;software quality,software developers;Google;Facebook;Microsoft;Intel;information availability;information discoverability;software quality;software engineering","keywords":"empirical study;industrial software development;challenges","startPage":"982","endPage":"993","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886973","citationCount":9,"referenceCount":56,"year":2016,"authors":"J. Rubin; M. Rinard","affiliations":"Massachusetts Inst. of Technol., Cambridge, MA, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35fe"},"title":"Synthesizing Qualitative Research in Software Engineering: A Critical Review","abstract":"Synthesizing data extracted from primary studies is an integral component of the methodologies in support of Evidence Based Software Engineering (EBSE) such as System Literature Review (SLR). Since a large and increasing number of studies in Software Engineering (SE) incorporate qualitative data, it is important to systematically review and understand different aspects of the Qualitative Research Synthesis (QRS) being used in SE. We have reviewed the use of QRS methods in 328 SLRs published between 2005 and 2015. We also inquired the authors of 274 SLRs to confrm whether or not any QRS methods were used in their respective reviews. 116 of them provided the responses, which were included in our analysis. We found eight QRS methods applied in SE research, two of which, narrative synthesis and thematic synthesis, have been predominantly adopted by SE researchers for synthesizing qualitative data. Our study determines that a signifcant amount of missing knowledge and incomplete understanding of the defned QRS methods in the community. Our effort also identifes an initial set factors that may in?uence the selection and use of appropriate QRS methods in SE.","conference":"IEEE","terms":"Software engineering;Data mining;Bibliographies;Software;Systematics;Tools;Data models,reviews;software engineering,Software Engineering;critical Review;primary studies;integral component;System Literature Review;qualitative data;Qualitative Research Synthesis;SE research;SE researchers;defned QRS methods;appropriate QRS methods;SLR","keywords":"research synthesis;qualitative (synthesis) methods;systematic (literature) review;evidence-based software engineering","startPage":"1207","endPage":"1218","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453202","citationCount":0,"referenceCount":67,"year":2018,"authors":"X. Huang; H. Zhang; X. Zhou; M. Ali Babar; S. Yang","affiliations":"State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China; Sch. of Comput. Sci., Univ. of Adelaide, Adelaide, SA, Australia; State Key Lab. of Novel Software Technol., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d35ff"},"title":"Type-Aware Concolic Testing of JavaScript Programs","abstract":"Conventional concolic testing has been used to provide high coverage of paths in statically typed languages. While it has also been applied in the context of JavaScript (JS) programs, we observe that applying concolic testing to dynamically-typed JS programs involves tackling unique problems to ensure scalability. In particular, a naive type-agnostic extension of concolic testing to JS programs causes generation of large number of inputs. Consequently, many executions operate on undefined values and repeatedly explore same paths resulting in redundant tests, thus diminishing the scalability of testing drastically. In this paper, we address this problem by proposing a simple yet effective approach that incorporates type-awareness intelligently in conventional concolic testing to reduce the number of generated inputs for JS programs. We extend our approach inter-procedurally by generating preconditions for each function that provide a summary of the relation between the variable types and paths. Employing the function preconditions when testing reduces the number of inputs generated even further. We implement our ideas and validate it on a number of open-source JS programs (and libraries). For a significant percentage (on average 50%) of the functions, we observe that type-aware concolic testing generates a minuscule percentage (less than 5%) of the inputs as compared to conventional concolic testing approach implemented on top of Jalangi. On average, this approach achieves over 97% of line coverage and over 94% of branch coverage for all the functions across all benchmarks. Moreover, the use of function preconditions reduces the number of inputs generated by 50%. We also demonstrate the use of function preconditions in automatically avoiding real crashes due to incorrectly typed objects.","conference":"IEEE","terms":"Testing;Computer crashes;Algorithm design and analysis;Scalability;Libraries;Runtime;Redundancy,Java;object-oriented programming;program testing,type-aware concolic testing;JavaScript programs;dynamically-typed JS programs;open-source JS programs","keywords":"Dynamic Analysis;JavaScript;Testing","startPage":"168","endPage":"179","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886901","citationCount":2,"referenceCount":40,"year":2016,"authors":"M. Dhok; M. K. Ramanathan; N. Sinha","affiliations":"NA; NA; IBM Res., Bangalore, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3600"},"title":"Are Code Examples on an Online Q\u0026A Forum Reliable?: A Study of API Misuse on Stack Overflow","abstract":"Programmers often consult an online Q\u0026A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons-missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples.","conference":"IEEE","terms":"Data mining;Java;Software;Software reliability;Syntactics;Libraries,application program interfaces;data mining;Java;learning (artificial intelligence);program diagnostics;public domain software;question answering (information retrieval);Web sites,API misuse;API usage mining framework;potential API usage violations;API calls;alternative API usage details;Stack Overflow posts","keywords":"online Q\u0026A forums;API usage pattern;code example assessment","startPage":"886","endPage":"896","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453166","citationCount":9,"referenceCount":0,"year":2018,"authors":"T. Zhang; G. Upadhyaya; A. Reinhardt; H. Rajan; M. Kim","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3601"},"title":"VDTest: An Automated Framework to Support Testing for Virtual Devices","abstract":"The use of virtual devices in place of physical hardware is increasing in activities such as design, testing and debugging. Yet virtual devices are simply software applications, and like all software they are prone to faults. A full system simulator (FSS), is a class of virtual machine that includes a large set of virtual devices - enough to run the full target software stack. Defects in an FSS virtual device may have cascading effects as the incorrect behavior can be propagated forward to many different platforms as well as to guest programs. In this work we present VDTest, a novel framework for testing virtual devices within an FSS. VDTest begins by generat- ing a test specification obtained through static analysis. It then employs a two-phase testing approach to test virtual components both individually and in combination. It lever- ages a differential oracle strategy, taking advantage of the existence of a physical or golden device to eliminate the need for manually generating test oracles. In an empirical study using both open source and commercial FSSs, we found 64 faults, 83% more than random testing.","conference":"IEEE","terms":"Registers;Testing;Frequency selective surfaces;Software;Hardware;Computers;Debugging,formal specification;program diagnostics;program testing;virtual machines,virtual device testing;VDTest;full system simulator;virtual machine;FSS;test specification;static analysis;two-phase testing;differential oracle strategy","keywords":"Testing;Virtual Devices;Device Drivers;Test Oracles","startPage":"583","endPage":"594","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886936","citationCount":0,"referenceCount":37,"year":2016,"authors":"T. Yu; X. Qu; M. B. Cohen","affiliations":"Dept. of Comp. Sci., Univ. of Kentucky, Lexington, KY, USA; ABB Corp. Res., Raleigh, NC, USA; Dept. of Comp. Sci. \u0026 Eng., Univ. of Nebraska - Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3602"},"title":"A Practical Guide to Select Quality Indicators for Assessing Pareto-Based Search Algorithms in Search-Based Software Engineering","abstract":"Many software engineering problems are multi-objective in nature, which has been largely recognized by the Search-based Software Engineering (SBSE) community. In this regard, Pareto- based search algorithms, e.g., Non-dominated Sorting Genetic Algorithm II, have already shown good performance for solving multi-objective optimization problems. These algorithms produce Pareto fronts, where each Pareto front consists of a set of non- dominated solutions. Eventually, a user selects one or more of the solutions from a Pareto front for their specific problems. A key challenge of applying Pareto-based search algorithms is to select appropriate quality indicators, e.g., hypervolume, to assess the quality of Pareto fronts. Based on the results of an extended literature review, we found that the current literature and practice in SBSE lacks a practical guide for selecting quality indicators despite a large number of published SBSE works. In this direction, the paper presents a practical guide for the SBSE community to select quality indicators for assessing Pareto-based search algorithms in different software engineering contexts. The practical guide is derived from the following complementary theoretical and empirical methods: 1) key theoretical foundations of quality indicators; 2) evidence from an extended literature review; and 3) evidence collected from an extensive experiment that was conducted to evaluate eight quality indicators from four different categories with six Pareto-based search algorithms using three real industrial problems from two diverse domains.","conference":"IEEE","terms":"Classification algorithms;Search problems;Convergence;Software engineering;Software algorithms;Bibliographies;Sociology,Pareto optimisation;search problems;software quality,quality indicators;Pareto-based search algorithms;search-based software engineering;SBSE community;nondominated sorting genetic algorithm II;multiobjective optimization problems;Pareto front","keywords":"Quality Indicators;Multi-objective Software Engineering Problems;Pareto-based Search Algorithms;Practical Guide","startPage":"631","endPage":"642","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886940","citationCount":16,"referenceCount":65,"year":2016,"authors":"S. Wang; S. Ali; T. Yue; Y. Li; M. Liaaen","affiliations":"Simula Res. Lab., Oslo, Norway; Simula Res. Lab., Oslo, Norway; Simula Res. Lab., Oslo, Norway; Beihang Univ., Beijing, China; Cisco Syst., Oslo, Norway","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3603"},"title":"Launch-Mode-Aware Context-Sensitive Activity Transition Analysis","abstract":"Existing static analyses model activity transitions in Android apps context-insensitively, making it impossible to distinguish different activity launch modes, reducing the pointer analysis precision for an activity's callbacks, and potentially resulting in infeasible activity transition paths. In this paper, we introduce Chime, a launch-mode-aware context-sensitive activity transition analysis that models different instances of an activity class according to its launch mode and the transitions between activities context-sensitively, by working together with an object-sensitive pointer analysis. Our evaluation shows that our context-sensitive activity transition analysis is more precise than its context-insensitive counterpart in capturing activity transitions, facilitating GUI testing, and improving the pointer analysis precision.","conference":"IEEE","terms":"Androids;Humanoid robots;Context modeling;Standards;Analytical models;Graphical user interfaces;Navigation,mobile computing;program diagnostics,infeasible activity transition paths;launch-mode-aware context-sensitive activity transition analysis;activity class;launch mode;activities context-sensitively;object-sensitive pointer analysis;pointer analysis precision;static analyses model activity transitions;Android apps context-insensitively","keywords":"Android;Pointer Analysis;Activity Transition Analysis","startPage":"598","endPage":"608","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453129","citationCount":2,"referenceCount":34,"year":2018,"authors":"Y. Zhang; Y. Sui; J. Xue","affiliations":"UNSW Sydney, Sydney, NSW, Australia; Univ. of Technol. Sydney, Sydney, NSW, Australia; UNSW Sydney, Sydney, NSW, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3604"},"title":"Exploring Language Support for Immutability","abstract":"Programming languages can restrict state change by preventing it entirely (immutability) or by restricting which clients may modify state (read-only restrictions). The benefits of immutability and read-only restrictions in software structures have been long-argued by practicing software engineers, researchers, and programming language designers. However, there are many proposals for language mechanisms for restricting state change, with a remarkable diversity of techniques and goals, and there is little empirical data regarding what practicing software engineers want in their tools and what would benefit them. We systematized the large collection of techniques used by programming languages to help programmers prevent undesired changes in state. We interviewed expert software engineers to discover their expectations and requirements, and found that important requirements, such as expressing immutability constraints, were not reflected in features available in the languages participants used. The interview results informed our design of a new language extension for specifying immutability in Java. Through an iterative, participatory design process, we created a tool that reflects requirements from both our interviews and the research literature.","conference":"IEEE","terms":"Java;Software;Data structures;Interviews;C++ languages;Concrete,Java,immutability;language extension;Java;programming language","keywords":"Programming language design;Programming language usability;Immutability;Mutability;Programmer productivity;Empirical studies of programmers","startPage":"736","endPage":"747","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886832","citationCount":7,"referenceCount":45,"year":2016,"authors":"M. Coblenz; J. Sunshine; J. Aldrich; B. Myers; S. Weber; F. Shull","affiliations":"Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Software Eng. Inst., Pittsburgh, PA, USA; Software Eng. Inst., Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3605"},"title":"Quantifying and Mitigating Turnover-Induced Knowledge Loss: Case Studies of Chrome and a Project at Avaya","abstract":"The utility of source code, as of other knowledge artifacts, is predicated on the existence of individuals skilled enough to derive value by using or improving it. Developers leaving a software project deprive the project of the knowledge of the decisions they have made. Previous research shows that the survivors and newcomers maintaining abandoned code have reduced productivity and are more likely to make mistakes. We focus on quantifying the extent of abandoned source files and adapt methods from financial risk analysis to assess the susceptibility of the project to developer turnover. In particular, we measure the historical loss distribution and find (1) that projects are susceptible to losses that are more than three times larger than the expected loss. Using historical simulations we find (2) that projects are susceptible to large losses that are over five times larger than the expected loss. We use Monte Carlo simulations of disaster loss scenarios and find (3) that simplistic estimates of the `truck factor' exaggerate the potential for loss. To mitigate loss from developer turnover, we modify Cataldo et al's coordination requirements matrices. We find (4) that we can recommend the correct successor 34% to 48% of the time. We also find that having successors reduces the expected loss by as much as 15%. Our approach helps large projects assess the risk of turnover thereby making risk more transparent and manageable.","conference":"IEEE","terms":"Loss measurement;Software;Risk management;Software engineering;Productivity;Adaptation models;Monte Carlo methods,knowledge management;Monte Carlo methods;project management;software development management,turnover-induced knowledge loss;Chrome;source code utility;knowledge artifacts;software project;financial risk analysis;historical loss distribution;Monte Carlo simulation;truck factor estimation","keywords":"Quantitative Risk Management;Mining Software Repositories;Knowledge Distribution;Truck Factor;Successors;Turnover","startPage":"1006","endPage":"1016","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886975","citationCount":7,"referenceCount":31,"year":2016,"authors":"P. C. Rigby; Y. C. Zhu; S. M. Donadelli; A. Mockus","affiliations":"Dept. of Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. of Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. of Comput. Sci. \u0026 Software Eng., Concordia Univ., Montreal, QC, Canada; Dept. Electr. Eng. \u0026 Comput. Sci., Univ. of Tennessee, Knoxville, TN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3606"},"title":"Termination-Checking for LLVM Peephole Optimizations","abstract":"Mainstream compilers contain a large number of peephole optimizations, which perform algebraic simplification of the input program with local rewriting of the code. These optimizations are a persistent source of bugs. Our recent research on Alive, a domain-specific language for expressing peephole optimizations in LLVM, addresses a part of the problem by automatically verifying the correctness of these optimizations and generating C++ code for use with LLVM. This paper identifies a class of non-termination bugs that arise when a suite of peephole optimizations is executed until a fixed point. An optimization can undo the effect of another optimization in the suite, which results in non-terminating compilation. This paper (1) proposes a methodology to detect non-termination bugs with a suite of peephole optimizations, (2) identifies the necessary condition to ensure termination while composing peephole optimizations, and (3) provides debugging support by generating concrete input programs that cause non-terminating compilation. We have discovered 184 optimization sequences, involving 38 optimizations, that cause non-terminating compilation in LLVM with Alive-generated C++ code.","conference":"IEEE","terms":"Optimization;Computer bugs;C++ languages;Semantics;Concrete;Toxicology;Pattern matching,C++ language;program compilers;program debugging,termination-checking;LLVM peephole optimizations;nontermination bugs;nonterminating compilation;debugging;input programs;Alive-generated C++ code","keywords":"Compiler Verification;Peephole Optimization;Alive;Termination","startPage":"191","endPage":"202","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886903","citationCount":0,"referenceCount":40,"year":2016,"authors":"D. Menendez; S. Nagarakatte","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3607"},"title":"Propagating Configuration Decisions with Modal Implication Graphs","abstract":"Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.","conference":"IEEE","terms":"Servers;Frequency modulation;Hafnium;Operating systems;Monitoring;Task analysis,backtracking;configuration management;graph theory;large-scale systems;performance evaluation,modal implication graphs;interdependent configuration options;nontrivial configuration process;decision propagation;backtracking-free configuration process;large-scale systems;interactive configuration processes;configuration decisions","keywords":"Software product line;Configuration;Decision Propagation","startPage":"898","endPage":"909","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453168","citationCount":2,"referenceCount":0,"year":2018,"authors":"S. Krieter; T. Thüm; S. Schulze; R. Schröter; G. Saake","affiliations":"Univ. of Magdeburg, Magdeburg, Germany; Tech. Univ. Braunschweig, Braunschweig, Germany; Univ. of Magdeburg, Magdeburg, Germany; Univ. of Magdeburg, Magdeburg, Germany; Univ. of Magdeburg, Magdeburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3608"},"title":"Deep Code Search","abstract":"To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code. In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled. As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.","conference":"IEEE","terms":"Natural languages;XML;Tools;Semantics;Machine learning;Recurrent neural networks,natural language processing;neural nets;query processing;text analysis,code search tool;source code;information retrieval models;relevant code snippets;natural language query;deep neural network;DeepCS;deep code search;large-scale codebase;code-description embedding neural network;CODEnn model;DeepCS;textual documents;matching text similarity","keywords":"code search;deep learning;joint embedding","startPage":"933","endPage":"944","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453172","citationCount":21,"referenceCount":0,"year":2018,"authors":"X. Gu; H. Zhang; S. Kim","affiliations":"Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Univ. of Newcastle, Callaghan, NSW, Australia; Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3609"},"title":"Automated Localization for Unreproducible Builds","abstract":"Reproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries. In this paper we propose an automated framework called RepLoc to localize the problematic files for unreproducible builds. RepLoc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rule-based filtering component that narrows the search scope. By integrating the two components with a weighted file ranking module, RepLoc is able to automatically produce a ranked list of files that are helpful in locating the problematic files for the unreproducible builds. We have implemented a prototype and conducted extensive experiments over 671 real-world unreproducible Debian packages in four different categories. By considering the topmost ranked file only, RepLoc achieves an accuracy rate of 47.09%. If we expand our examination to the top ten ranked files in the list produced by RepLoc, the accuracy rate becomes 79.28%. Considering that there are hundreds of source code, scripts, Makefiles, etc., in a package, RepLoc significantly reduces the scope of localizing problematic files. Moreover, with the help of RepLoc, we successfully identified and fixed six new unreproducible packages from Debian and Guix.","conference":"IEEE","terms":"Software;Task analysis;Feature extraction;Filtering;Hafnium;Software engineering;Computer science,program diagnostics;public domain software;query processing;security of data;software maintenance,detecting attacks;open-source software repositories;unreproducible binaries;problematic files;query augmentation component;build logs;heuristic rule-based filtering component;search scope;weighted file ranking module;ranked list;topmost ranked file;automated localization;unreproducible builds;pre-defined build environments;quality assurance;Bitcoin;Guix;RepLoc features;unreproducible Debian packages","keywords":"Unreproducible Build;Localization;Software Maintenance","startPage":"71","endPage":"81","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453064","citationCount":2,"referenceCount":0,"year":2018,"authors":"Z. Ren; H. Jiang; J. Xuan; Z. Yang","affiliations":"Key Lab. for Ubiquitous Network \u0026 Service Software of Liaoning Province, Dalian Univ. of Technol., Dalian, China; Key Lab. for Ubiquitous Network \u0026 Service Software of Liaoning Province, Dalian Univ. of Technol., Dalian, China; Sch. of Comput. Sci., Wuhan Univ., Wuhan, China; Dept. of Comput. Sci., Western Michigan Univ., Kalamazoo, MI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d360a"},"title":"Missing Data Imputation Based on Low-Rank Recovery and Semi-Supervised Regression for Software Effort Estimation","abstract":"Software effort estimation (SEE) is a crucial step in software development. Effort data missing usually occurs in real-world data collection. Focusing on the missing data problem, existing SEE methods employ the deletion, ignoring, or imputation strategy to address the problem, where the imputation strategy was found to be more helpful for improving the estimation performance. Current imputation methods in SEE use classical imputation techniques for missing data imputation, yet these imputation techniques have their respective disadvantages and might not be appropriate for effort data. In this paper, we aim to provide an effective solution for the effort data missing problem. Incompletion includes the drive factor missing case and effort label missing case. We introduce the low-rank recovery technique for addressing the drive factor missing case. And we employ the semi-supervised regression technique to perform imputation in the case of effort label missing. We then propose a novel effort data imputation approach, named low-rank recovery and semi-supervised regression imputation (LRSRI). Experiments on 7 widely used software effort datasets indicate that: (1) the proposed approach can obtain better effort data imputation effects than other methods; (2) the imputed data using our approach can apply to multiple estimators well.","conference":"IEEE","terms":"Estimation;Software;Data models;Adaptation models;Software engineering;Focusing;Organizations,data handling;regression analysis;software engineering,missing data imputation;low-rank recovery;semisupervised regression;software effort estimation;SEE;software development;data collection;deletion strategy;ignoring strategy;imputation strategy;drive factor missing case;effort label missing case;LRSRI","keywords":"Software effort estimation;Missing data problem;Drive factor missing case;Effort label missing case;Low-rank recovery and semi-supervised regression imputation (LRSRI)","startPage":"607","endPage":"618","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886938","citationCount":3,"referenceCount":62,"year":2016,"authors":"X. Jing; F. Qi; F. Wu; B. Xu","affiliations":"State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; State Key Lab. of Software Eng., Wuhan Univ., Wuhan, China; Dept. of Comput. Sci. \u0026 Technol., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d360b"},"title":"Featured Model-Based Mutation Analysis","abstract":"Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.","conference":"IEEE","terms":"Testing;Unified modeling language;Computational modeling;Analytical models;Software product lines;Software;Scalability,program diagnostics;software product lines,featured model-based mutation analysis;optimization technique;mutant execution process;featured mutant model;software product line paradigm;behavioural variability models;featured transition systems;mutant generation;mutant configuration;mutant execution","keywords":"Mutation Analysis;Variability;Featured Transition Systems","startPage":"655","endPage":"666","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886942","citationCount":9,"referenceCount":63,"year":2016,"authors":"X. Devroey; G. Perrouin; M. Papadakis; A. Legay; P. Schobbens; P. Heymans","affiliations":"PReCISE Res. Center, Univ. of Namur, Namur, Belgium; PReCISE Res. Center, Univ. of Namur, Namur, Belgium; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; INRIA, Rennes, France; PReCISE Res. Center, Univ. of Namur, Namur, Belgium; PReCISE Res. Center, Univ. of Namur, Namur, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d360c"},"title":"Code Review Quality: How Developers See It","abstract":"In a large, long-lived project, an effective code review process is key to ensuring the long-term quality of the code base. In this work, we study code review practices of a large, open source project, and we investigate how the developers themselves perceive code review quality. We present a qualitative study that summarizes the results from a survey of 88 Mozilla core developers. The results provide developer insights into how they define review quality, what factors contribute to how they evaluate submitted code, and what challenges they face when performing review tasks. We found that the review quality is primarily associated with the thoroughness of the feedback, the reviewer's familiarity with the code, and the perceived quality of the code itself. Also, we found that while different factors are perceived to contribute to the review quality, reviewers often find it difficult to keep their technical skills up-to-date, manage personal priorities, and mitigate context switching.","conference":"IEEE","terms":"Software;Data mining;Computer science;Face;Computer bugs;Measurement;Electronic mail,software quality,effective code review process;long-term quality;Mozilla core developers;open source project","keywords":"Code review;review quality;survey;developer perception","startPage":"1028","endPage":"1038","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886977","citationCount":10,"referenceCount":32,"year":2016,"authors":"O. Kononenko; O. Baysal; M. W. Godfrey","affiliations":"Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada; Sch. of Comput. Sci., Carleton Univ., Ottawa, ON, Canada; Sch. of Comput. Sci., Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d360d"},"title":"Floating-Point Precision Tuning Using Blame Analysis","abstract":"While tremendously useful, automated techniques for tuning the precision of floating-point programs face important scalability challenges. We present Blame Analysis, a novel dynamic approach that speeds up precision tuning. Blame Analysis performs floating-point instructions using different levels of accuracy for their operands. The analysis determines the precision of all operands such that a given precision is achieved in the final result of the program. Our evaluation on ten scientific programs shows that Blame Analysis is successful in lowering operand precision. As it executes the program only once, the analysis is particularly useful when targeting reductions in execution time. In such case, the analysis needs to be combined with search-based tools such as Precimonious. Our experiments show that combining Blame Analysis with Precimonious leads to obtaining better results with significant reduction in analysis time: the optimized programs execute faster (in three cases, we observe as high as 39.9% program speedup) and the combined analysis time is 9× faster on average, and up to 38× faster than Precimonious alone.","conference":"IEEE","terms":"Tuning;Scalability;Government;Performance analysis;Search problems;Software;Numerical analysis,program diagnostics,floating-point precision tuning;blame analysis approach;floating-point instructions;operand precision;Precimonious tool","keywords":"floating point;mixed precision;program optimization","startPage":"1074","endPage":"1085","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886981","citationCount":7,"referenceCount":46,"year":2016,"authors":"C. Rubio-González; C. Nguyen; B. Mehne; K. Sen; J. Demmel; W. Kahan; C. Iancu; W. Lavrijsen; D. H. Bailey; D. Hough","affiliations":"Univ. of California, Davis, Davis, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Univ. of California, Berkeley, Berkeley, CA, USA; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA; Lawrence Berkeley Nat. Lab., Berkeley, CA, USA; Oracle Corp., USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d360e"},"title":"iDice: Problem Identification for Emerging Issues","abstract":"One challenge for maintaining a large-scale software system, especially an online service system, is to quickly respond to customer issues. The issue reports typically have many categorical attributes that reflect the characteristics of the issues. For a commercial system, most of the time the volume of reported issues is relatively constant. Sometimes, there are emerging issues that lead to significant volume increase. It is important for support engineers to efficiently and effectively identify and resolve such emerging issues, since they have impacted a large number of customers. Currently, problem identification for an emerging issue is a tedious and error-prone process, because it requires support engineers to manually identify a particular attribute combination that characterizes the emerging issue among a large number of attribute combinations. We call such an attribute combination effective combination, which is important for issue isolation and diagnosis. In this paper, we propose iDice, an approach that can identify the effective combination for an emerging issue with high quality and performance. We evaluate the effectiveness and efficiency of iDice through experiments. We have also successfully applied iDice to several Microsoft online service systems in production. The results confirm that iDice can help identify emerging issues and reduce maintenance effort.","conference":"IEEE","terms":"Manuals;Software systems;Maintenance engineering;Time series analysis;Market research;Software engineering,software maintenance,iDice;large-scale software system;software system maintenance;categorical attributes;problem identification;attribute combination effective combination;Microsoft online service systems","keywords":"Emerging issues;problem identification;effective combination;problem diagnostic;issue reports","startPage":"214","endPage":"224","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886905","citationCount":4,"referenceCount":30,"year":2016,"authors":"Q. Lin; J. Lou; H. Zhang; D. Zhang","affiliations":"Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Microsoft Res., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d360f"},"title":"FaCoY – A Code-to-Code Search Engine","abstract":"Code search is an unavoidable activity in software development. Various approaches and techniques have been explored in the literature to support code search tasks. Most of these approaches focus on serving user queries provided as natural language free-form input. However, there exists a wide range of use-case scenarios where a code-to-code approach would be most beneficial. For example, research directions in code transplantation, code diversity, patch recommendation can leverage a code-to-code search engine to find essential ingredients for their techniques. In this paper, we propose FaCoY, a novel approach for statically finding code fragments which may be semantically similar to user input code. FaCoY implements a query alternation strategy: instead of directly matching code query tokens with code in the search space, FaCoY first attempts to identify other tokens which may also be relevant in implementing the functional behavior of the input code. With various experiments, we show that (1) FaCoY is more effective than online code-to-code search engines; (2) FaCoY can detect more semantic code clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; (3) FaCoY, while static, can detect code fragments which are indeed similar with respect to runtime execution behavior; and (4) FaCoY can be useful in code/patch recommendation.","conference":"IEEE","terms":"Cloning;Search engines;Semantics;Software;Natural languages;Syntactics;Runtime,query processing;search engines;software engineering;software maintenance,code-to-code search engine;code search tasks;code-to-code approach;code-patch recommendation;FaCoY;semantic code clones;directly matching code query tokens;user input code;statically finding code fragments;code diversity;code transplantation","keywords":"code search;semantic clones;code to code search","startPage":"946","endPage":"957","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453174","citationCount":2,"referenceCount":0,"year":2018,"authors":"K. Kim; D. Kim; T. F. Bissyandé; E. Choi; L. Li; J. Klein; Y. Le Traon","affiliations":"SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Fac. of Inf. Technol., Monash Univ., Clayton, VIC, Australia; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3610"},"title":"A Large-Scale Empirical Study on the Effects of Code Obfuscations on Android Apps and Anti-Malware Products","abstract":"The Android platform has been the dominant mobile platform in recent years resulting in millions of apps and security threats against those apps. Anti-malware products aim to protect smartphone users from these threats, especially from malicious apps. However, malware authors use code obfuscation on their apps to evade detection by anti-malware products. To assess the effects of code obfuscation on Android apps and anti-malware products, we have conducted a large-scale empirical study that evaluates the effectiveness of the top anti-malware products against various obfuscation tools and strategies. To that end, we have obfuscated 3,000 benign apps and 3,000 malicious apps and generated 73,362 obfuscated apps using 29 obfuscation strategies from 7 open-source, academic, and commercial obfuscation tools. The findings of our study indicate that (1) code obfuscation significantly impacts Android anti-malware products; (2) the majority of anti-malware products are severely impacted by even trivial obfuscations; (3) in general, combined obfuscation strategies do not successfully evade anti-malware products more than individual strategies; (4) the detection of anti-malware products depend not only on the applied obfuscation strategy but also on the leveraged obfuscation tool; (5) anti-malware products are slow to adopt signatures of malicious apps; and (6) code obfuscation often results in changes to an app's semantic behaviors.","conference":"IEEE","terms":"Androids;Humanoid robots;Tools;Malware;Cryptography;Reflection;Java,Android (operating system);invasive software;smart phones,code obfuscation effects;code obfuscation effects;smartphone users;Android anti-malware products;Android apps","keywords":"Android;empirical study;Security;Code Obfuscation;Anti malware products","startPage":"421","endPage":"431","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453102","citationCount":1,"referenceCount":49,"year":2018,"authors":"M. Hammad; J. Garcia; S. Malek","affiliations":"Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf., Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3611"},"title":"How Does the Degree of Variability Affect Bug Finding?","abstract":"Software projects embrace variability to increase adaptability and to lower cost; however, others blame variability for increasing complexity and making reasoning about programs more difficult. We carry out a controlled experiment to quantify the impact of variability on debugging of preprocessor- based programs. We measure speed and precision for bug finding tasks defined at three different degrees of variability on several subject programs derived from real systems. The results show that the speed of bug finding decreases linearly with the degree of variability, while effectiveness of finding bugs is relatively independent of the degree of variability. Still, identifying the set of configurations in which the bug manifests itself is difficult already for a low degree of variability. Surprisingly, identifying the exact set of affected configurations appears to be harder than finding the bug in the first place. The difficulty in reasoning about several configurations is a likely reason why the variability bugs are actually introduced in configurable programs. We hope that the detailed findings presented here will inspire the creation of programmer support tools addressing the challenges faced by developers when reasoning about configurations, contributing to more effective debugging and, ultimately, fewer bugs in highly-configurable systems.","conference":"IEEE","terms":"Computer bugs;Debugging;Linux;Cognition;Kernel;Color,program debugging,variability degree;bug finding;software projects;preprocessor-based program debugging;configurable program","keywords":"Variability;Preprocessors;Bug Finding","startPage":"679","endPage":"690","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886944","citationCount":5,"referenceCount":36,"year":2016,"authors":"J. Melo; C. Brabrand; A. Wasowski","affiliations":"IT Univ. of Copenhagen, Copenhagen, Denmark; IT Univ. of Copenhagen, Copenhagen, Denmark; IT Univ. of Copenhagen, Copenhagen, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3612"},"title":"IntEQ: Recognizing Benign Integer Overflows via Equivalence Checking across Multiple Precisions","abstract":"Integer overflow (IO) vulnerabilities can be exploited by attackers to compromise computer systems. In the meantime, IOs can be used intentionally by programmers for benign purposes such as hashing and random number generation. Hence, differentiating exploitable and harmful IOs from intentional and benign ones is an important challenge. It allows reducing the number of false positives produced by IO vulnerability detection techniques, helping developers or security analysts to focus on fxing critical IOs without inspecting the numerous false alarms. The difficulty of recognizing benign IOs mainly lies in inferring the intent of programmers from source code. In this paper, we present a novel technique to recognize benign IOs via equivalence checking across multiple precisions. We determine if an IO is benign by comparing the effects of an overflowed integer arithmetic operation in the actual world (with limited precision) and the same operation in the ideal world (with sufficient precision to evade the IO). Specifically, we first extract the data flow path from the overflowed integer arithmetic operation to a security-related program point (i.e., sink) and then create a new version of the path using more precise types with sufficient bits to represent integers so that the IO can be avoided. Using theorem proving we check whether these two versions are equivalent, that is, if they yield the same values at the sink under all possible inputs. If so, the IO is benign. We implement a prototype, named IntEQ, based on the GCC compiler and the Z3 solver, and evaluate it using 26 harmful IO vulnerabilities from 20 real-world programs, and 444 benign IOs from SPECINT 2000, SPECINT 2006, and 7 real-world applications. The experimental results show that IntEQ does not misclassify any harmful IO bugs (no false negatives) and recognizes 355 out of 444 (about 79.95%) benign IOs, whereas the state of the art can only recognize 19 benign IOs.","conference":"IEEE","terms":"Software;Random number generation;Computer bugs;Indexing;Encoding;Computer science,data flow analysis;program debugging;program verification;security of data,IntEQ;benign integer overflow recognition;equivalence checking;random number generation;IO vulnerability detection techniques;security analysts;overflowed integer arithmetic operation;data flow path extraction;GCC compiler;SPECINT;harmful IO bugs","keywords":"Integer Overflow;benign;equivalence checking;precision","startPage":"1051","endPage":"1062","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886979","citationCount":2,"referenceCount":57,"year":2016,"authors":"H. Sun; X. Zhang; Y. Zheng; Q. Zeng","affiliations":"State Key Lab. for Novel Software Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci. \u0026 Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci., Purdue Univ., West Lafayette, IN, USA; IBM T.J. Watson Res. Center, Yorktown Heights, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911deee8435e8e7d3613"},"title":"Scalable Thread Sharing Analysis","abstract":"We present two scalable algorithms for identifying program locations that access thread-shared data in concurrent programs. The static algorithm, though simple, and without performing the expensive whole program information flow analysis, is much more efficient, less memory-demanding, and even more precise than the classical escape analysis algorithm. The dynamic algorithm, powered by a location- based approach, achieves significant runtime speedups over a precise dynamic escape analysis. Our evaluation on a set of large real world complex multithreaded systems such as Apache Derby and Eclipse shows that our algorithms achieve unprecedented scalability. Used by client applications, our algorithms reduce the recording overhead of a record-replay system by 9X on average (as much as 16X) and increase the runtime logging speed of a data race detector by 32% on average (as much as 52%).","conference":"IEEE","terms":"Heuristic algorithms;Algorithm design and analysis;Arrays;Instruction sets;Runtime;Detectors;Java,multi-threading;program diagnostics,thread sharing analysis;scalable algorithms;program locations identification;thread-shared data access;concurrent programs;static algorithm;program information flow analysis;location-based approach;dynamic algorithm;multithreaded systems;Apache Derby;Eclipse;record-replay system;data race detector","keywords":"","startPage":"1097","endPage":"1108","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886983","citationCount":3,"referenceCount":42,"year":2016,"authors":"J. Huang","affiliations":"Parasol Lab., Texas A\u0026M Univ., College Station, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3614"},"title":"An Empirical Study of Practitioners' Perspectives on Green Software Engineering","abstract":"The energy consumption of software is an increasing concern as the use of mobile applications, embedded systems, and data center-based services expands. While research in green software engineering is correspondingly increasing, little is known about the current practices and perspectives of software engineers in the field. This paper describes the first empirical study of how practitioners think about energy when they write requirements, design, construct, test, and maintain their software. We report findings from a quantitative,targeted survey of 464 practitioners from ABB, Google, IBM, and Microsoft, which was motivated by and supported with qualitative data from 18 in-depth interviews with Microsoft employees. The major findings and implications from the collected data contextualize existing green software engineering research and suggest directions for researchers aiming to develop strategies and tools to help practitioners improve the energy usage of their applications.","conference":"IEEE","terms":"Interviews;Software engineering;Software;Google;Encoding;Green products;Conferences,energy consumption;green computing;software engineering,practitioner perspective;green software engineering;energy consumption;mobile applications;embedded systems;data center-based services;ABB;Google;IBM;Microsoft;energy usage","keywords":"Green Software Engineering;Empirical Study;Survey","startPage":"237","endPage":"248","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886907","citationCount":15,"referenceCount":60,"year":2016,"authors":"I. Manotas; C. Bird; R. Zhang; D. Shepherd; C. Jaspan; C. Sadowski; L. Pollock; J. Clause","affiliations":"Univ. of Delaware, Newark, DE, USA; Microsoft Res., Redmond, WA, USA; IBM Res. - Almaden, San Jose, CA, USA; ABB Corp. Res., Raleigh, NC, USA; Google, Inc., Mountain View, CA, USA; Google, Inc., Mountain View, CA, USA; Univ. of Delaware, Newark, DE, USA; Univ. of Delaware, Newark, DE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3615"},"title":"Work Practices and Challenges in Pull-Based Development: The Contributor's Perspective","abstract":"The pull-based development model is an emerging way of contributing to distributed software projects that is gaining enormous popularity within the open source software (OSS) world. Previous work has examined this model by focusing on projects and their owners-we complement it by examining the work practices of project contributors and the challenges they face.We conducted a survey with 645 top contributors to active OSS projects using the pull-based model on GitHub, the prevalent social coding site. We also analyzed traces extracted from corresponding GitHub repositories. Our research shows that: contributors have a strong interest in maintaining awareness of project status to get inspiration and avoid duplicating work, but they do not actively propagate information; communication within pull requests is reportedly limited to low-level concerns and contributors often use communication channels external to pull requests; challenges are mostly social in nature, with most reporting poor responsiveness from integrators; and the increased transparency of this setting is a confirmed motivation to contribute. Based on these findings, we present recommendations for practitioners to streamline the contribution process and discuss potential future research directions.","conference":"IEEE","terms":"Encoding;Software;Face;Collaboration;Electronic mail;Software engineering;Focusing,public domain software;software engineering,pull-based development model;distributed software projects;open source software;OSS;GitHub;project status awareness;contribution process","keywords":"pull-based development;open source contribution;pull request;dis- tributed software development;GitHub","startPage":"285","endPage":"296","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886911","citationCount":29,"referenceCount":54,"year":2016,"authors":"G. Gousios; M. Storey; A. Bacchelli","affiliations":"Radboud Univ. Nijmegen, Nijmegen, Netherlands; Univ. of Victoria, Victoria, BC, Canada; Delft Univ. of Technol., Delft, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3616"},"title":"A Graph Solver for the Automated Generation of Consistent Domain-Specific Models","abstract":"Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.","conference":"IEEE","terms":"Analytical models;Object oriented modeling;Tools;Biological system modeling;IP networks;Testing,formal logic;formal specification;formal verification;graph theory;query processing;specification languages,consistent domain-specific models;automated model generation;software engineering;graph models;domain-specific instance models;mapping-based approach;incremental graph query evaluation;graph solver framework;back-end logic solvers;first-order logic specification;safety standards;tool qualification;systematic generation;systems engineering","keywords":"Graph generation;Test generation;Domain Specific Modeling Languages;Logic Solver;Graph Solver","startPage":"969","endPage":"980","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453176","citationCount":4,"referenceCount":0,"year":2018,"authors":"O. Semeráth; A. S. Nagy; D. Varró","affiliations":"MTA-BME Lendulet Cyber-Phys. Syst. Res. Group, Budapest, Hungary; MTA-BME Lendulet Cyber-Phys. Syst. Res. Group, Budapest, Hungary; McGill Univ., Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3617"},"title":"Testing Vision-Based Control Systems Using Learnable Evolutionary Algorithms","abstract":"Vision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input space that are likely to contain most critical test scenarios). Our evaluation performed on an industrial automotive automotive system shows that: (1) Our algorithm outperforms a baseline evolutionary search algorithm and generates 78% more distinct, critical test scenarios compared to the baseline algorithm. (2) Our algorithm accurately characterizes critical regions of the system under test, thus identifying the conditions that are likely to lead to system failures.","conference":"IEEE","terms":"Testing;Classification algorithms;Roads;Control systems;Evolutionary computation;Decision trees;Automotive engineering,decision trees;evolutionary computation;genetic algorithms;learning (artificial intelligence);mobile robots;robot vision;search problems,vision-based control systems;learnable evolutionary algorithms;autonomous vehicular systems;complex input spaces;multidimensional input spaces;automated testing algorithm;machine learning;multiobjective population-based search algorithms;decision tree classification models;search-based generation;search algorithms refine classification models;test input space;industrial automotive automotive system;baseline evolutionary search algorithm;distinct test scenarios","keywords":"Search-based Software Engineering;Evolutionary algorithms;Software Testing;Automotive Software Systems","startPage":"1016","endPage":"1026","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453180","citationCount":13,"referenceCount":39,"year":2018,"authors":"R. Ben Abdessalem; S. Nejati; L. C. Briand; T. Stifter","affiliations":"SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg City, Luxembourg; IEE S.A., Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3618"},"title":"An Analysis of the Search Spaces for Generate and Validate Patch Generation Systems","abstract":"We present the first systematic analysis of key characteristics of patch search spaces for automatic patch generation systems. We analyze sixteen different configurations of the patch search spaces of SPR and Prophet, two current state-of-the-art patch generation systems. The analysis shows that 1) correct patches are sparse in the search spaces (typically at most one correct patch per search space per defect), 2) incorrect patches that nevertheless pass all of the test cases in the validation test suite are typically orders of magnitude more abundant, and 3) leveraging information other than the test suite is therefore critical for enabling the system to successfully isolate correct patches. We also characterize a key tradeoff in the structure of the search spaces. Larger and richer search spaces that contain correct patches for more defects can actually cause systems to find fewer, not more, correct patches. We identify two reasons for this phenomenon: 1) increased validation times because of the presence of more candidate patches and 2) more incorrect patches that pass the test suite and block the discovery of correct patches. These fundamental properties, which are all characterized for the first time in this paper, help explain why past systems often fail to generate correct patches and help identify challenges, opportunities, and productive future directions for the field.","conference":"IEEE","terms":"Benchmark testing;Space exploration;Systematics;Software;Software engineering;Scalability;Conferences,program diagnostics;software maintenance,automatic patch generation systems;SPR;Prophet;program repair","keywords":"Program repair;Patch generation;Search space","startPage":"702","endPage":"713","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886946","citationCount":20,"referenceCount":41,"year":2016,"authors":"F. Long; M. Rinard","affiliations":"EECS, MIT, Cambridge, MA, USA; EECS, MIT, Cambridge, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3619"},"title":"Coverage-Driven Test Code Generation for Concurrent Classes","abstract":"Previous techniques on concurrency testing have mainly focused on exploring the interleaving space of manually written test code to expose faulty interleavings of shared memory accesses. These techniques assume the availability of failure-inducing tests. In this paper, we present AutoConTest, a coverage-driven approach to generate effective concurrent test code that achieve high interleaving coverage. AutoConTest consists of three components. First, it computes the coverage requirements dynamically and iteratively during sequential test code generation, using a coverage metric that captures the execution context of shared memory accesses. Second, it smartly selects these sequential codes based on the computed result and assembles them for concurrent tests, achieving increased context-sensitive interleaving coverage. Third, it explores the newly covered interleavings. We have implemented AutoConTest as an automated tool and evaluated it using 6 real-world concurrent Java subjects. The results show that AutoConTest is able to generate effective concurrent tests that achieve high interleaving coverage and expose concurrency faults quickly. AutoConTest took less than 65 seconds (including program analysis, test generation and execution) to expose the faults in the program subjects.","conference":"IEEE","terms":"Concurrent computing;Space exploration;Instruction sets;Testing;Synchronization;Programming,concurrency (computers);program testing;sequential codes;shared memory systems,coverage-driven test code generation;AutoConTest;concurrent test code;sequential test code generation;coverage metric;shared memory accesses;context-sensitive interleaving coverage","keywords":"Automated test generation;Interleaving coverage criteria","startPage":"1121","endPage":"1132","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886985","citationCount":4,"referenceCount":68,"year":2016,"authors":"V. Terragni; S. Cheung","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d361a"},"title":"Too Long; Didn't Watch! Extracting Relevant Fragments from Software Development Video Tutorials","abstract":"When knowledgeable colleagues are not available, developers resort to offline and online resources, e.g., tutorials, mailing lists, and Q\u0026A websites. These, however, need to be found, read, and understood, which takes its toll in terms of time and mental energy. A more immediate and accessible resource are video tutorials found on the web, which in recent years have seen a steep increase in popularity. Nonetheless, videos are an intrinsically noisy data source, and finding the right piece of information might be even more cumbersome than using the previously mentioned resources. We present CodeTube, an approach which mines video tutorials found on the web, and enables developers to query their contents. The video tutorials are split into coherent fragments, to return only fragments related to the query. These are complemented with information from additional sources, such as Stack Overflow discussions. The results of two studies to assess CodeTube indicate that video tutorials-if appropriately processed-represent a useful, yet still under-utilized source of information for software development.","conference":"IEEE","terms":"Tutorials;Streaming media;Optical character recognition software;Java;YouTube;Data mining,data mining;software engineering,software development video tutorials;CodeTube;data mining","keywords":"","startPage":"261","endPage":"272","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886909","citationCount":7,"referenceCount":44,"year":2016,"authors":"L. Ponzanelli; G. Bavota; A. Mocci; M. Di Penta; R. Oliveto; M. Hasan; B. Russo; S. Haiduc; M. Lanza","affiliations":"Univ. della Svizzera Italiana, Lugano, Switzerland; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Univ. della Svizzera Italiana, Lugano, Switzerland; Univ. of Sannio, Benevento, Italy; Univ. of Molise, Pesche, Italy; Florida State Univ., Tallahassee, FL, USA; Free Univ. of Bozen-Bolzano, Bolzano, Italy; Florida State Univ., Tallahassee, FL, USA; Univ. della Svizzera Italiana, Lugano, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d361b"},"title":"Cross-Project Defect Prediction Using a Connectivity-Based Unsupervised Classifier","abstract":"Defect prediction on projects with limited historical data has attracted great interest from both researchers and practitioners. Cross-project defect prediction has been the main area of progress by reusing classifiers from other projects. However, existing approaches require some degree of homogeneity (e.g., a similar distribution of metric values) between the training projects and the target project. Satisfying the homogeneity requirement often requires significant effort (currently a very active area of research). An unsupervised classifier does not require any training data, therefore the heterogeneity challenge is no longer an issue. In this paper, we examine two types of unsupervised classifiers: a) distance-based classifiers (e.g., k-means); and b) connectivity-based classifiers. While distance-based unsupervised classifiers have been previously used in the defect prediction literature with disappointing performance, connectivity-based classifiers have never been explored before in our community. We compare the performance of unsupervised classifiers versus supervised classifiers using data from 26 projects from three publicly available datasets (i.e., AEEEM, NASA, and PROMISE). In the cross-project setting, our proposed connectivity-based classifier (via spectral clustering) ranks as one of the top classifiers among five widely-used supervised classifiers (i.e., random forest, naive Bayes, logistic regression, decision tree, and logistic model tree) and five unsupervised classifiers (i.e., k-means, partition around medoids, fuzzy C-means, neural-gas, and spectral clustering). In the within-project setting (i.e., models are built and applied on the same project), our spectral classifier ranks in the second tier, while only random forest ranks in the first tier. Hence, connectivity-based unsupervised classifiers offer a viable solution for cross and within project defect predictions.","conference":"IEEE","terms":"Software;Training;Predictive models;Software metrics;Training data;Clustering algorithms,pattern classification;public domain software;software maintenance;unsupervised learning,cross-project defect prediction;connectivity-based unsupervised classifier;homogeneity degree;distance-based classifiers;connectivity-based classifiers","keywords":"defect prediction;heterogeneity;cross-project;unsupervised;spectral clustering;graph mining","startPage":"309","endPage":"320","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886913","citationCount":27,"referenceCount":66,"year":2016,"authors":"F. Zhang; Q. Zheng; Y. Zou; A. E. Hassan","affiliations":"Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada; Dept. of Electr. \u0026 Comput. Eng., Queen's Univ., Kingston, ON, Canada; Sch. of Comput., Queen's Univ., Kingston, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d361c"},"title":"Nemo: Multi-criteria Test-Suite Minimization with Integer Nonlinear Programming","abstract":"Multi-criteria test-suite minimization aims to remove redundant test cases from a test suite based on some criteria such as code coverage, while trying to optimally maintain the capability of the reduced suite based on other criteria such as fault-detection effectiveness. Existing techniques addressing this problem with integer linear programming claim to produce optimal solutions. However, the multi-criteria test-suite minimization problem is inherently nonlinear, due to the fact that test cases are often dependent on each other in terms of test-case criteria. In this paper, we propose a framework that formulates the multi-criteria test-suite minimization problem as an integer nonlinear programming problem. To solve this problem optimally, we programmatically transform this nonlinear problem into a linear one and then solve the problem using modern linear solvers. We have implemented our framework as a tool, called Nemo, that supports a number of modern linear and nonlinear solvers. We have evaluated Nemo with a publicly available dataset and minimization problems involving multiple criteria including statement coverage, fault-revealing capability, and test execution time. The experimental results show that Nemo can be used to efficiently find an optimal solution for multi-criteria test-suite minimization problems with modern solvers, and the optimal solutions outperform the suboptimal ones by up to 164.29% in terms of the criteria considered in the problem.","conference":"IEEE","terms":"Minimization;Linear programming;Optimization;Mathematical model;Tools;Testing;Programming,integer programming;linear programming;minimisation;nonlinear programming;program testing,redundant test cases;optimal solution;multicriteria test-suite minimization problem;test-case criteria;integer nonlinear programming problem","keywords":"Test-suite minimization;integer programming","startPage":"1039","endPage":"1049","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453182","citationCount":1,"referenceCount":0,"year":2018,"authors":"J. Lin; R. Jabbarvand; J. Garcia; S. Malek","affiliations":"Univ. of California, Irvine, Irvine, CA, USA; Univ. of California, Irvine, Irvine, CA, USA; Univ. of California, Irvine, Irvine, CA, USA; Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d361d"},"title":"Time to Clean Your Test Objectives","abstract":"Testing is the primary approach for detecting software defects. A major challenge faced by testers lies in crafting efficient test suites, able to detect a maximum number of bugs with manageable effort. To do so, they rely on coverage criteria, which define some precise test objectives to be covered. However, many common criteria specify a significant number of objectives that occur to be infeasible or redundant in practice, like covering dead code or semantically equal mutants. Such objectives are well-known to be harmful to the design of test suites, impacting both the efficiency and precision of the tester's effort. This work introduces a sound and scalable technique to prune out a significant part of the infeasible and redundant objectives produced by a panel of white-box criteria. In a nutshell, we reduce this task to proving the validity of logical assertions in the code under test. The technique is implemented in a tool that relies on weakest-precondition calculus and SMT solving for proving the assertions. The tool is built on top of the Frama-C verification platform, which we carefully tune for our specific scalability needs. The experiments reveal that the pruning capabilities of the tool can reduce the number of targeted test objectives in a program by up to 27% and scale to real programs of 200K lines, making it possible to automate a painstaking part of their current testing process.","conference":"IEEE","terms":"Tools;Software safety;Security;Software testing;Scalability,program diagnostics;program testing;program verification,significant number;dead code;semantically equal mutants;scalable technique;infeasible objectives;redundant objectives;white-box criteria;logical assertions;targeted test objectives;current testing process;primary approach;detecting software defects;efficient test suites;coverage criteria;precise test objectives","keywords":"Coverage Criteria;Infeasible Objectives;Redundant Objectives","startPage":"456","endPage":"467","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453106","citationCount":0,"referenceCount":0,"year":2018,"authors":"M. Marcozzi; S. Bardin; N. Kosmatov; M. Papadakis; V. Prevosto; L. Correnson","affiliations":"Dept. of Comput., Imperial Coll. London, London, UK; List Software Safety \u0026 Security Lab., CEA, Gif-sur-Yvette, France; List Software Safety \u0026 Security Lab., CEA, Gif-sur-Yvette, France; SnT, Univ. of Luxembourg, Luxembourg, Luxembourg; List Software Safety \u0026 Security Lab., CEA, Gif-sur-Yvette, France; List Software Safety \u0026 Security Lab., CEA, Gif-sur-Yvette, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d361e"},"title":"Improving Refactoring Speed by 10X","abstract":"Refactoring engines are standard tools in today's Integrated Development Environments (IDEs). They allow programmers to perform one refactoring at a time, but programmers need more. Most design patterns in the Gang-of-Four text can be written as a refactoring script - a programmatic sequence of refactorings. In this paper, we present R3, a new Java refactoring engine that supports refactoring scripts. It builds a main-memory, non-persistent database to encode Java entity declarations (e.g., packages, classes, methods), their containment relationships, and language features such as inheritance and modifiers. Unlike classical refactoring engines that modify Abstract Syntax Trees (ASTs), R3 refactorings modify only the database; refactored code is produced only when pretty-printing ASTs that reference database changes. R3 performs comparable precondition checks to those of the Eclipse Java Development Tools (JDT) but R3's codebase is about half the size of the JDT refactoring engine and runs an order of magnitude faster. Further, a user study shows that R3 improved the success rate of retrofitting design patterns by 25% up to 50%.","conference":"IEEE","terms":"Java;Engines;Databases;Graphics;Computer bugs;Graphical user interfaces;Maintenance engineering,Java;software maintenance,refactoring speed;integrated development environment;IDE;Gang-of-Four text;refactoring script;refactoring sequence;Java refactoring engine;Java entity declarations;abstract syntax trees;AST;Eclipse Java development tools;JDT;retrofitting design patterns","keywords":"","startPage":"1145","endPage":"1156","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886987","citationCount":5,"referenceCount":57,"year":2016,"authors":"J. Kim; D. Batory; D. Dig; M. Azanza","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d361f"},"title":"AntMiner: Mining More Bugs by Reducing Noise Interference","abstract":"Detecting bugs with code mining has proven to be an effective approach. However, the existing methods suffer from reporting serious false positives and false negatives. In this paper, we developed an approach called AntMiner to improve the precision of code mining by carefully preprocessing the source code. Specifically, we employ the program slicing technique to decompose the original source repository into independent sub-repositories, taking critical operations (automatically extracted from source code) as slicing criteria. In this way, the statements irrelevant to a critical operation are excluded from the corresponding sub-repository. Besides, various semantics-equivalent representations are normalized into a canonical form. Eventually, the mining process can be performed on a refined code database, and false positives and false negatives can be significantly pruned. We have implemented AntMiner and applied it to detect bugs in the Linux kernel. It reported 52 violations that have been either confirmed as real bugs by the kernel development community or fixed in new kernel versions. Among them, 41 cannot be detected by a widely used representative analysis tool Coverity. Besides, the result of a comparative analysis shows that our approach can effectively improve the precision of code mining and detect subtle bugs that have previously been missed.","conference":"IEEE","terms":"Computer bugs;Data mining;Kernel;Programming;Linux;Databases,data mining;Linux;operating system kernels;program debugging;program slicing,AntMiner approach;noise interference reduction;bug detection;code mining;source code preprocessing;program slicing technique;semantics-equivalent representation;Linux kernel;Coverity tool","keywords":"Bug detection;Code mining;Program slicing","startPage":"333","endPage":"344","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886915","citationCount":5,"referenceCount":50,"year":2016,"authors":"B. Liang; P. Bian; Y. Zhang; W. Shi; W. You; Y. Cai","affiliations":"Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; Key Lab. of Data Eng. \u0026 Knowledge Eng., Renmin Univ. of China, Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3620"},"title":"Automated Reporting of GUI Design Violations for Mobile Apps","abstract":"The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications. This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called GVT and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that GVT solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers and developers at Huawei to improve the quality of their mobile apps.","conference":"IEEE","terms":"Graphical user interfaces;Tools;Androids;Humanoid robots;Visualization;Layout;Computer vision,graphical user interfaces;mobile computing,GUI design violations;mobile app;design artifacts;mock-ups;intended design specifications;open-source apps;mobile GUI","keywords":"GUI;design;Mobile Apps;Android;Computer Vision","startPage":"165","endPage":"175","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453076","citationCount":4,"referenceCount":0,"year":2018,"authors":"K. Moran; B. Li; C. Bernal-Cárdenas; D. Jelf; D. Poshyvanyk","affiliations":"Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3621"},"title":"Fine-Grained Test Minimization","abstract":"As a software system evolves, its test suite can accumulate redundancies over time. Test minimization aims at removing redundant test cases. However, current techniques remove whole test cases from the test suite using test adequacy criteria, such as code coverage. This has two limitations, namely (1) by removing a whole test case the corresponding test assertions are also lost, which can inhibit test suite effectiveness, (2) the issue of partly redundant test cases, i.e., tests with redundant test statements, is ignored. We propose a novel approach for fine-grained test case minimization. Our analysis is based on the inference of a test suite model that enables automated test reorganization within test cases. It enables removing redundancies at the test statement level, while preserving the coverage and test assertions of the test suite. We evaluated our approach, implemented in a tool called Testler, on the test suites of 15 open source projects. Our analysis shows that over 4,639 (24%) of the tests in these test suites are partly redundant, with over 11,819 redundant test statements in total. Our results show that Testler removes 43% of the redundant test statements, reducing the number of partly redundant tests by 52%. As a result, test suite execution time is reduced by up to 37% (20% on average), while maintaining the original statement coverage, branch coverage, test assertions, and fault detection capability.","conference":"IEEE","terms":"Production;Minimization;Redundancy;Analytical models;Computational modeling;Software engineering;Software systems,program testing,test minimization;test adequacy criteria;corresponding test assertions;test suite effectiveness;partly redundant test cases;fine-grained test case minimization;test suite model;test reorganization;test statement level;partly redundant tests;test suite execution time;redundant test statements","keywords":"test minization;test reduction;test redundancy;test model","startPage":"210","endPage":"221","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453080","citationCount":0,"referenceCount":0,"year":2018,"authors":"A. Vahabzadeh; A. Stocco; A. Mesbah","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3622"},"title":"An Empirical Study on the Impact of C++ Lambdas and Programmer Experience","abstract":"Lambdas have seen increasing use in mainstream programming languages, notably in Java 8 and C++ 11. While the technical aspects of lambdas are known, we conducted the first randomized controlled trial on the human factors impact of C++ 11 lambdas compared to iterators. Because there has been recent debate on having students or professionals in experiments, we recruited undergraduates across the academic pipeline and professional programmers to evaluate these findings in a broader context. Results afford some doubt that lambdas benefit developers and show evidence that students are negatively impacted in regard to how quickly they can write correct programs to a test specification and whether they can complete a task. Analysis from log data shows that participants spent more time with compiler errors, and have more errors, when using lambdas as compared to iterators, suggesting difficulty with the syntax chosen for C++. Finally, experienced users were more likely to complete tasks, with or without lambdas, and could do so more quickly, with experience as a factor explaining 45.7% of the variance in our sample in regard to completion time.","conference":"IEEE","terms":"C++ languages;Java;Syntactics;Context;Algorithm design and analysis;Libraries,C++ language;formal specification,C++ lambda;programmer experience;mainstream programming languages;Java 8 languages;C++ 11 languages;test specification","keywords":"Lambda Expressions;Human Factors;C++11","startPage":"760","endPage":"771","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886954","citationCount":9,"referenceCount":59,"year":2016,"authors":"P. M. Uesbeck; A. Stefik; S. Hanenberg; J. Pedersen; P. Daleiden","affiliations":"Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, Las Vegas, NV, USA; Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, Las Vegas, NV, USA; Inst. for Comput. Sci. \u0026 Bus. Inf. Syst., Univ. of Duisburg-Essen, Essen, Germany; Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, Las Vegas, NV, USA; Dept. of Comput. Sci., Univ. of Nevada, Las Vegas, Las Vegas, NV, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3623"},"title":"ConflictJS: Finding and Understanding Conflicts Between JavaScript Libraries","abstract":"It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects.","conference":"IEEE","terms":"Libraries;Loading;Computer crashes;Cryptography;Content distribution networks;Servers,application program interfaces;Internet;Java;libraries;system monitoring,ConflictJS;JavaScript libraries;client-side web applications;global namespace;search space;dynamic analysis","keywords":"JavaScript libraries;testing","startPage":"741","endPage":"751","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453147","citationCount":2,"referenceCount":0,"year":2018,"authors":"J. Patra; P. N. Dixit; M. Pradel","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3624"},"title":"Understanding Asynchronous Interactions in Full-Stack JavaScript","abstract":"JavaScript has become one of the most popular languages in practice. Developers now use JavaScript not only for the client-side but also for server-side programming, leading to \"full-stack\" applications written entirely in JavaScript. Understanding such applications is challenging for developers, due to the temporal and implicit relations of asynchronous and event-driven entities spread over the client and server side. We propose a technique for capturing a behavioural model of full-stack JavaScript applications' execution. The model is temporal and context-sensitive to accommodate asynchronous events, as well as the scheduling and execution of lifelines of callbacks. We present a visualization of the model to facilitate program understanding for developers. We implement our approach in a tool, called Sahand, and evaluate it through a controlled experiment. The results show that Sahand improves developers' performance in completing program comprehension tasks by increasing their accuracy by a factor of three.","conference":"IEEE","terms":"Servers;Context modeling;Reactive power;Visualization;Writing;Concurrent computing,Java;object-oriented programming,asynchronous interaction;full-stack JavaScript;server-side programming;asynchronous events;program understanding;Sahand;program comprehension task","keywords":"Program comprehension;asynchronicity;full-stack JavaScript","startPage":"1169","endPage":"1180","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886989","citationCount":8,"referenceCount":48,"year":2016,"authors":"S. Alimadadi; A. Mesbah; K. Pattabiraman","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3625"},"title":"SWIM: Synthesizing What I Mean - Code Search and Idiomatic Snippet Synthesis","abstract":"Modern programming frameworks come with large libraries, with diverse applications such as for matching regular expressions, parsing XML files and sending email. Programmers often use search engines such as Google and Bing to learn about existing APIs. In this paper, we describe SWIM, a tool which suggests code snippets given API-related natural language queries such as “generate md5 hash code”. The query does not need to contain framework-specific trivia such as the type names or methods of interest. We translate user queries into the APIs of interest using clickthrough data from the Bing search engine. Then, based on patterns learned from open-source code repositories, we synthesize idiomatic code describing the use of these APIs. We introduce structured call sequences to capture API-usage patterns. Structured call sequences are a generalized form of method call sequences, with if-branches and while-loops to represent conditional and repeated API usage patterns, and are simple to extract and amenable to synthesis. We evaluated SWIM with 30 common C# API-related queries received by Bing. For 70% of the queries, the first suggested snippet was a relevant solution, and a relevant solution was present in the top 10 results for all benchmarked queries. The online portion of the workflow is also very responsive, at an average of 1.5 seconds per snippet.","conference":"IEEE","terms":"Natural languages;Pattern matching;Search engines;C# languages;Data models;Libraries;Web pages,application program interfaces;query processing;search engines,SWIM framework;synthesizing what i mean;code search;idiomatic snippet synthesis;programming framework;search engines;Google;Bing;API-related natural language queries;application program interface;method call sequences;API usage patterns","keywords":"Free form queries;code search;idiomatic snippet synthesis;structured call sequences","startPage":"357","endPage":"367","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886917","citationCount":18,"referenceCount":28,"year":2016,"authors":"M. Raghothaman; Y. Wei; Y. Hamadi","affiliations":"Univ. of Pennsylvania, Philadelphia, PA, USA; Microsoft Res., Cambridge, UK; Lab. d'Inf., Ecole Polytech., Palaiseau, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3626"},"title":"From Word Embeddings to Document Similarities for Improved Information Retrieval in Software Engineering","abstract":"The application of information retrieval techniques to search tasks in software engineering is made difficult by the lexical gap between search queries, usually expressed in natural language (e.g. English), and retrieved documents, usually expressed in code (e.g. programming languages). This is often the case in bug and feature location, community question answering, or more generally the communication between technical personnel and non-technical stake holders in a software project. In this paper, we propose bridging the lexical gap by projecting natural language statements and code snippets as meaning vectors in a shared representation space. In the proposed architecture, word embeddings are rst trained on API documents, tutorials, and reference documents, and then aggregated in order to estimate semantic similarities between documents. Empirical evaluations show that the learned vector space embeddings lead to improvements in a previously explored bug localization task and a newly de ned task of linking API documents to computer programming questions.","conference":"IEEE","terms":"Computer bugs;Natural languages;Context;Semantics;Mathematical model;Software;Vocabulary,application program interfaces;document handling;information retrieval;natural language processing;software engineering;system documentation,word embeddings;document similarities;information retrieval techniques;software engineering;natural language statements;code snippets;API documents;tutorials;reference documents;vector space embeddings;bug localization task","keywords":"word embeddings;skip-gram model;bug localization;bug reports;API documents","startPage":"404","endPage":"415","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886921","citationCount":40,"referenceCount":47,"year":2016,"authors":"X. Ye; H. Shen; X. Ma; R. Bunescu; C. Liu","affiliations":"Sch. of Electr. Eng. \u0026 Comput. Sci., Ohio Univ. Athens, Athens, OH, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Ohio Univ. Athens, Athens, OH, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Ohio Univ. Athens, Athens, OH, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Ohio Univ. Athens, Athens, OH, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Ohio Univ. Athens, Athens, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3627"},"title":"The Road to Live Programming: Insights from the Practice","abstract":"Live Programming environments allow programmers to get feedback instantly while changing software. Liveness is gaining attention among industrial and open-source communities; several IDEs offer high degrees of liveness. While several studies looked at how programmers work during software evolution tasks, none of them consider live environments. We conduct such a study based on an analysis of 17 programming sessions of practitioners using Pharo, a mature Live Programming environment. The study is complemented by a survey and subsequent analysis of 16 programming sessions in additional languages, e.g., JavaScript. We document the approaches taken by developers during their work. We find that some liveness features are extensively used, and have an impact on the way developers navigate source code and objects in their work.","conference":"IEEE","terms":"Programming;Tools;Task analysis;Software;Programming environments;Navigation;Visualization,Java;programming environments;public domain software;software engineering;software maintenance,open-source communities;software evolution tasks;live environments;mature Live Programming environment;subsequent analysis;liveness features;programming sessions;developer navigate source code","keywords":"Live Programming;Software Evolution;Exploratory Study","startPage":"1090","endPage":"1101","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453190","citationCount":2,"referenceCount":0,"year":2018,"authors":"J. Kubelka; R. Robbes; A. Bergel","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3628"},"title":"Repairing Crashes in Android Apps","abstract":"Android apps are omnipresent, and frequently suffer from crashes - leading to poor user experience and economic loss. Past work focused on automated test generation to detect crashes in Android apps. However, automated repair of crashes has not been studied. In this paper, we propose the first approach to automatically repair Android apps, specifically we propose a technique for fixing crashes in Android apps. Unlike most test-based repair approaches, we do not need a test-suite; instead a single failing test is meticulously analyzed for crash locations and reasons behind these crashes. Our approach hinges on a careful empirical study which seeks to establish common root-causes for crashes in Android apps, and then distills the remedy of these root-causes in the form of eight generic transformation operators. These operators are applied using a search-based repair framework embodied in our repair tool Droix. We also prepare a benchmark DroixBench capturing reproducible crashes in Android apps. Our evaluation of Droix on DroixBench reveals that the automatically produced patches are often syntactically identical to the human patch, and on some rare occasion even better than the human patch (in terms of avoiding regressions). These results confirm our intuition that our proposed transformations form a sufficient set of operators to patch crashes in Android.","conference":"IEEE","terms":"Computer crashes;Androids;Humanoid robots;Maintenance engineering;Mobile applications;Transistors;Testing,Android (operating system);program testing,Android apps;test-based repair approaches;benchmark DroixBench capturing reproducible crashes","keywords":"Automated repair;Android apps;Crash;SBSE","startPage":"187","endPage":"198","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453078","citationCount":6,"referenceCount":0,"year":2018,"authors":"S. H. Tan; Z. Dong; X. Gao; A. Roychoudhury","affiliations":"Southern Univ. of Sci. \u0026 Technol., Shenzhen, China; Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3629"},"title":"Towards Refactoring-Aware Regression Test Selection","abstract":"Regression testing checks that recent project changes do not break previously working functionality. Although important, regression testing is costly when changes are frequent. Regression test selection (RTS) optimizes regression testing by running only tests whose results might be affected by a change. Traditionally, RTS collects dependencies (e.g., on files) for each test and skips the tests, at a new project revision, whose dependencies did not change. Existing RTS techniques do not differentiate behavior-preserving transformations (i.e., refactorings) from other code changes. As a result, tests are run more frequently than necessary. We present the first step towards a refactoring-aware RTS technique, dubbed Reks, which skips tests affected only by behavior-preserving changes. Reks defines rules to update the test dependencies without running the tests. To ensure that Reks does not hide any bug introduced by the refactoring engines, we integrate Reks only in the pre-submit testing phase, which happens on the developers' machines. We evaluate Reks by measuring the savings in the testing effort. Specifically, we reproduce 100 refactoring tasks performed by developers of 37 projects on GitHub. Our results show that Reks would not run, on average, 33% of available tests (that would be run by a refactoring-unaware RTS technique). Additionally, we systematically run 27 refactoring types on ten projects. The results, based on 74,160 refactoring tasks, show that Reks would not run, on average, 16% of tests (max: 97% and SD: 24%). Finally, our results show that the Reks update rules are efficient.","conference":"IEEE","terms":"Testing;Computer bugs;Task analysis;Engines;Software;Google;Tools,program testing;regression analysis;software maintenance,Reks update rules;refactoring-aware regression test selection;regression testing checks;recent project changes;RTS techniques;code changes;refactoring-aware RTS technique;dubbed Reks;behavior-preserving changes;test dependencies;refactoring engines;testing effort;refactoring-unaware RTS technique;refactoring tasks","keywords":"Regression test selection;Behavior-preserving changes;Reks","startPage":"233","endPage":"244","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453082","citationCount":5,"referenceCount":80,"year":2018,"authors":"K. Wang; C. Zhu; A. Celik; J. Kim; D. Batory; M. Gligoric","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d362a"},"title":"BigDebug: Debugging Primitives for Interactive Big Data Processing in Spark","abstract":"Developers use cloud computing platforms to process a large quantity of data in parallel when developing big data analytics. Debugging the massive parallel computations that run in today's data-centers is time consuming and error-prone. To address this challenge, we design a set of interactive, real-time debugging primitives for big data processing in Apache Spark, the next generation data-intensive scalable cloud computing platform. This requires re-thinking the notion of step-through debugging in a traditional debugger such as gdb, because pausing the entire computation across distributed worker nodes causes significant delay and naively inspecting millions of records using a watchpoint is too time consuming for an end user.First, BigDebug's simulated breakpoints and on-demand watchpoints allow users to selectively examine distributed, intermediate data on the cloud with little overhead. Second, a user can also pinpoint a crash-inducing record and selectively resume relevant sub-computations after a quick fix. Third, a user can determine the root causes of errors (or delays) at the level of individual records through a fine-grained data provenance capability. Our evaluation shows that BigDebug scales to terabytes and its record-level tracing incurs less than 25% overhead on average. It determines crash culprits orders of magnitude more accurately and provides up to 100% time saving compared to the baseline replay debugger. The results show that BigDebug supports debugging at interactive speeds with minimal performance impact.","conference":"IEEE","terms":"Sparks;Debugging;Big Data;Cloud computing;Distributed databases;Computer crashes;Delays,Big Data;computer centres;interactive systems;program debugging,BigDebug;debugging primitives;interactive big data processing;cloud computing platforms;big data analytics;data-centers;Apache Spark;distributed worker nodes;intermediate data;distributed data;crash-inducing record;fine-grained data provenance capability;record-level tracing","keywords":"Debugging;big data analytics;interactive tools;data-intensive scalable computing (DISC);fault localization and recovery","startPage":"784","endPage":"795","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886956","citationCount":4,"referenceCount":40,"year":2016,"authors":"M. A. Gulzar; M. Interlandi; S. Yoo; S. D. Tetali; T. Condie; T. Millstein; M. Kim","affiliations":"Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA; Univ. of California, Los Angeles, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d362b"},"title":"Are \"Non-functional\" Requirements really Non-functional? An Investigation of Non-functional Requirements in Practice","abstract":"Non-functional requirements (NFRs) are commonly distinguished from functional requirements by differentiating how the system shall do something in contrast to what the system shall do. This distinction is not only prevalent in research, but also influences how requirements are handled in practice. NFRs are usually documented separately from functional requirements, without quantitative measures, and with relatively vague descriptions.As a result, they remain difficult to analyze and test.Several authors argue, however, that many so-called NFRs actually describe behavioral properties and may be treated the same way as functional requirements. In this paper, we empirically investigate this point of view and aim to increase our understanding on the nature of NFRs addressing system properties. We report on the classification of 530 NFRs extracted from 11 industrial requirements specifications and analyze to which extent these NFRs describe system behavior.Our results suggest that most \"non-functional\" requirements are not non-functional as they describe behavior of a system. Consequently, we argue that many so-called NFRs can be handled similarly to functional requirements.","conference":"IEEE","terms":"Software;Unified modeling language;Software engineering;Interviews;Documentation;Security,software engineering,nonfunctional requirements;NFR;quantitative measures","keywords":"Non-functional requirements;classification;model-based development;empirical studies","startPage":"832","endPage":"842","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886960","citationCount":8,"referenceCount":31,"year":2016,"authors":"J. Eckhardt; A. Vogelsang; D. M. Fernández","affiliations":"Tech. Univ. Munchen, Munich, Germany; Tech. Univ. Munchen, Munich, Germany; Tech. Univ. Munchen, Munich, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d362c"},"title":"Automatic Model Generation from Documentation for Java API Functions","abstract":"Modern software systems are becoming increasingly complex, relying on a lot of third-party library support. Library behaviors are hence an integral part of software behaviors. Analyzing them is as important as analyzing the software itself. However, analyzing libraries is highly challenging due to the lack of source code, implementation in different languages, and complex optimizations. We observe that many Java library functions provide excellent documentation, which concisely describes the functionalities of the functions. We develop a novel technique that can construct models for Java API functions by analyzing the documentation. These models are simpler implementations in Java compared to the original ones and hence easier to analyze. More importantly, they provide the same functionalities as the original functions. Our technique successfully models 326 functions from 14 widely used Java classes. We also use these models in static taint analysis on Android apps and dynamic slicing for Java programs, demonstrating the effectiveness and efficiency of our models.","conference":"IEEE","terms":"Libraries;Java;Indexes;Analytical models;Documentation;Software;Generators,application program interfaces;Java;program slicing,automatic model generation;Java API functions;application program interfaces;library behavior;Java library functions;documentation analysis;Java classes;static taint analysis;Android applications;dynamic slicing;Java programs","keywords":"documentation analysis;environment modeling;natural language processing;auto-testing","startPage":"380","endPage":"391","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886919","citationCount":9,"referenceCount":37,"year":2016,"authors":"J. Zhai; J. Huang; S. Ma; X. Zhang; L. Tan; J. Zhao; F. Qin","affiliations":"NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d362d"},"title":"On the \"Naturalness\" of Buggy Code","abstract":"Real software, the kind working programmers produce by the kLOC to solve real-world problems, tends to be “natural”, like speech or natural language; it tends to be highly repetitive and predictable. Researchers have captured this naturalness of software through statistical models and used them to good effect in suggestion engines, porting tools, coding standards checkers, and idiom miners. This suggests that code that appears improbable, or surprising, to a good statistical language model is “unnatural” in some sense, and thus possibly suspicious. In this paper, we investigate this hypothesis. We consider a large corpus of bug fix commits (ca. 7,139), from 10 different Java projects, and focus on its language statistics, evaluating the naturalness of buggy code and the corresponding fixes. We find that code with bugs tends to be more entropic (i.e. unnatural), becoming less so as bugs are fixed. Ordering files for inspection by their average entropy yields cost-effectiveness scores comparable to popular defect prediction methods. At a finer granularity, focusing on highly entropic lines is similar in cost-effectiveness to some well-known static bug finders (PMD, FindBugs) and or- dering warnings from these bug finders using an entropy measure improves the cost-effectiveness of inspecting code implicated in warnings. This suggests that entropy may be a valid, simple way to complement the effectiveness of PMD or FindBugs, and that search-based bug-fixing methods may benefit from using entropy both for fault-localization and searching for fixes.","conference":"IEEE","terms":"Computer bugs;Biological system modeling;Entropy;Predictive models;Inspection;Software;Standards,program debugging,buggy code naturalness;software naturalness;statistical language model;bug fix commits;Java projects;language statistics;cost-effectiveness score;bug finders;entropy measure;PMD;FindBugs","keywords":"","startPage":"428","endPage":"439","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886923","citationCount":29,"referenceCount":57,"year":2016,"authors":"B. Ray; V. Hellendoorn; S. Godhane; Z. Tu; A. Bacchelli; P. Devanbu","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d362e"},"title":"CCAligner: A Token Based Large-Gap Clone Detector","abstract":"Copying code and then pasting with large number of edits is a common activity in software development, and the pasted code is a kind of complicated Type-3 clone. Due to large number of edits, we consider the clone as a large-gap clone. Large-gap clone can reflect the extension of code, such as change and improvement. The existing state-of-the-art clone detectors suffer from several limitations in detecting large-gap clones. In this paper, we propose a tool, CCAligner, using code window that considers e edit distance for matching to detect large-gap clones. In our approach, a novel e-mismatch index is designed and the asymmetric similarity coefficient is used for similarity measure. We thoroughly evaluate CCAligner both for large-gap clone detection, and for general Type-1, Type-2 and Type-3 clone detection. The results show that CCAligner performs better than other competing tools in large-gap clone detection, and has the best execution time for 10MLOC input with good precision and recall in general Type-1 to Type-3 clone detection. Compared with existing state-of-the-art tools, CCAligner is the best performing large-gap clone detection tool, and remains competitive with the best clone detectors in general Type-1, Type-2 and Type-3 clone detection.","conference":"IEEE","terms":"Cloning;Tools;Detectors;Software;Computer science;Software engineering;Indexes,public domain software;software maintenance;software reusability,pasted code;CCAligner;Type-3 clone detection;performing large-gap clone detection tool;large-gap clone detector","keywords":"Clone Detection;Large-gap Clone;Evaluation","startPage":"1066","endPage":"1077","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453188","citationCount":7,"referenceCount":57,"year":2018,"authors":"P. Wang; J. Svajlenko; Y. Wu; Y. Xu; C. K. Roy","affiliations":"Sch. of Comput. Sci., Univ. of Sci. \u0026 Technol. of China, Hefei, China; Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada; Sch. of Comput. Sci., Univ. of Sci. \u0026 Technol. of China, Hefei, China; Sch. of Comput. Sci., Univ. of Sci. \u0026 Technol. of China, Hefei, China; Dept. of Comput. Sci., Univ. of Saskatchewan, Saskatoon, SK, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d362f"},"title":"PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data","abstract":"Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.","conference":"IEEE","terms":"Androids;Humanoid robots;Data models;Games;Testing;Data mining;Mobile communication,collaborative filtering;data mining;smart phones,PRADA;prioritizing android devices for apps;large-scale usage data mining;operational profiling;mobile apps;collaborative filtering technique;Android management app","keywords":"Mobile apps;Android fragmentation;prioritization;usage data","startPage":"3","endPage":"13","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886887","citationCount":10,"referenceCount":41,"year":2016,"authors":"X. Lu; X. Liu; H. Li; T. Xie; Q. Mei; D. Hao; G. Huang; F. Feng","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Univ. of Michigan, Ann Arbor, MI, USA; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Wandoujia Lab., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3630"},"title":"Programming Not Only by Example","abstract":"Recent years have seen great progress in automated synthesis techniques that can automatically generate code based on some intent expressed by the programmer, but communicating this intent remains a major challenge. When the expressed intent is coarse-grained (for example, restriction on the expected type of an expression), the synthesizer often produces a long list of results for the programmer to choose from, shifting the heavy-lifting to the user. An alternative approach, successfully used in end-user synthesis, is programming by example (PBE), where the user leverages examples to interactively and iteratively refine the intent. However, using only examples is not expressive enough for programmers, who can observe the generated program and refine the intent by directly relating to parts of the generated program. We present a novel approach to interacting with a synthesizer using a granular interaction model. Our approach employs a rich interaction model where (i) the synthesizer decorates a candidate program with debug information that assists in understanding the program and identifying good or bad parts, and (ii) the user is allowed to provide feedback not only on the expected output of a program but also on the program itself. After identifying a program as (partially) correct or incorrect, the user can also explicitly indicate the good or bad parts, to allow the synthesizer to accept or discard parts of the program instead of discarding the program as a whole. We show the value of our approach in a controlled user study. Our study shows that participants have a strong preference for granular feedback instead of examples and can provide granular feedback much faster.","conference":"IEEE","terms":"Synthesizers;Task analysis;Programming;Object oriented modeling;Tools;Agriculture;Vocabulary,automatic programming;program compilers;program debugging,granular feedback;automated synthesis techniques;end-user synthesis;granular interaction model;programming by example;debug information","keywords":"program synthesis;programming by example","startPage":"1114","endPage":"1124","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453192","citationCount":0,"referenceCount":41,"year":2018,"authors":"H. Peleg; S. Shoham; E. Yahav","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3631"},"title":"How Modern News Aggregators Help Development Communities Shape and Share Knowledge","abstract":"Many developers rely on modern news aggregator sites such as reddit and hn to stay up to date with the latest technological developments and trends. In order to understand what motivates developers to contribute, what kind of content is shared, and how knowledge is shaped by the community, we interviewed and surveyed developers that participate on the reddit programming subreddit and we analyzed a sample of posts on both reddit and hn. We learned what kind of content is shared in these websites and developer motivations for posting, sharing, discussing, evaluating, and aggregating knowledge on these aggregators, while revealing challenges developers face in terms of how content and participant behavior is moderated. Our insights aim to improve the practices developers follow when using news aggregators, as well as guide tool makers on how to improve their tools. Our findings are also relevant to researchers that study developer communities of practice.","conference":"IEEE","terms":"Computer hacking;Interviews;Software;Programming;Communication channels;Europe;Shape,social networking (online),modern news aggregator sites;reddit programming subreddit;developer motivations;development communities;Websites;participant behavior","keywords":"News aggregators;development communities;knowledge sharing;social computing","startPage":"499","endPage":"510","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453116","citationCount":3,"referenceCount":0,"year":2018,"authors":"M. Aniche; C. Treude; I. Steinmacher; I. Wiese; G. Pinto; M. Storey; M. A. Gerosa","affiliations":"NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3632"},"title":"Generating Performance Distributions via Probabilistic Symbolic Execution","abstract":"Analyzing performance and understanding the potential best-case, worst-case and distribution of program execution times are very important software engineering tasks. There have been model-based and program analysis-based approaches for performance analysis. Model-based approaches rely on analytical or design models derived from mathematical theories or software architecture abstraction, which are typically coarse-grained and could be imprecise. Program analysis-based approaches collect program profiles to identify performance bottlenecks, which often fail to capture the overall program performance. In this paper, we propose a performance analysis framework PerfPlotter. It takes the program source code and usage profile as inputs and generates a performance distribution that captures the input probability distribution over execution times for the program. It heuristically explores high-probability and low-probability paths through probabilistic symbolic execution. Once a path is explored, it generates and runs a set of test inputs to model the performance of the path. Finally, it constructs the performance distribution for the program. We have implemented PerfPlotter based on the Symbolic PathFinder infrastructure, and experimentally demonstrated that PerfPlotter could accurately capture the best-case, worst-case and distribution of program execution times. We also show that performance distributions can be applied to various important tasks such as performance understanding, bug validation, and algorithm selection.","conference":"IEEE","terms":"Analytical models;Mathematical model;Probabilistic logic;Performance analysis;Computational modeling;Software engineering;Algorithm design and analysis,program diagnostics;software architecture,performance distribution generation;probabilistic symbolic execution;software engineering;program analysis-based approach;software architecture abstraction;model-based analysis approach;PerfPlotter framework;probability distribution;Symbolic PathFinder infrastructure;performance understanding;bug validation;algorithm selection","keywords":"Performance Analysis;Symbolic Execution","startPage":"49","endPage":"60","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886891","citationCount":7,"referenceCount":61,"year":2016,"authors":"B. Chen; Y. Liu; W. Le","affiliations":"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Dept. of Comput. Sci., Iowa State Univ., Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3633"},"title":"Almost There: A Study on Quasi-Contributors in Open-Source Software Projects","abstract":"Recent studies suggest that well-known OSS projects struggle to find the needed workforce to continue evolving-in part because external developers fail to overcome their first contribution barriers. In this paper, we investigate how and why quasi-contributors (external developers who did not succeed in getting their contributions accepted to an OSS project) fail. To achieve our goal, we collected data from 21 popular, non-trivial GitHub projects, identified quasi-contributors, and analyzed their pull-requests. In addition, we conducted surveys with quasi-contributors, and projects' integrators, to understand their perceptions about nonacceptance.We found 10,099 quasi-contributors - about 70% of the total actual contributors - that submitted 12,367 non-accepted pull-requests. In five projects, we found more quasi-contributors than actual contributors. About one-third of the developers who took our survey disagreed with the nonacceptance, and around 30% declared the nonacceptance demotivated or prevented them from placing another pull-request. The main reasons for pull-request nonacceptance from the quasi-contributors' perspective were \"superseded/duplicated pull-request\" and \"mismatch between developer's and team's vision/opinion.\" A manual analysis of a representative sample of 263 pull-requests corroborated with this finding. We also found reasons related to the relationship with the community and lack of experience or commitment from the quasi-contributors. This empirical study is particularly relevant to those interested in fostering developers' participation and retention in OSS communities.","conference":"IEEE","terms":"Open source software;Computer languages;Software engineering;Face;Analytical models;Encoding,project management;public domain software;software management,OSS project;open-source software projects;manual analysis;quasicontributors;GitHub projects","keywords":"pull-requests;quasi contributors;newcomers;open source software","startPage":"256","endPage":"266","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453084","citationCount":8,"referenceCount":0,"year":2018,"authors":"I. Steinmacher; G. Pinto; I. S. Wiese; M. A. Gerosa","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3634"},"title":"Revisit of Automatic Debugging via Human Focus-Tracking Analysis","abstract":"In many fields of software engineering, studies on human behavior have attracted a lot of attention; however, few such studies exist in automated debugging. Parnin and Orso conducted a pioneering study comparing the performance of programmers in debugging with and without a ranking-based fault localization technique, namely Spectrum-Based Fault Localization (SBFL). In this paper, we revisit the actual helpfulness of SBFL, by addressing some major problems that were not resolved in Parnin and Orso's study. Our investigation involved 207 participants and 17 debugging tasks. A user-friendly SBFL tool was adopted. It was found that SBFL tended not to be helpful in improving the efficiency of debugging. By tracking and analyzing programmers' focus of attention, we characterized their source code navigation patterns and provided in-depth explanations to the observations. Results indicated that (1) a short “first scan” on the source code tended to result in inefficient debugging; and (2) inspections on the pinpointed statements during the “follow-up browsing” were normally just quick skimming. Moreover, we found that the SBFL assistbrowsing” were normally just quick skimming. Moreover, we found that the SBFL assistanceance may even slightly weaken programmers' abilities in fault detection. Our observations imply interference between the mechanism of automated fault localization and the actual assistance needed by programmers in debugging. To resolve this interference, we provide several insights and suggestions.","conference":"IEEE","terms":"Debugging;Software;Software engineering;Navigation;Computer bugs;Interference;Fault detection,program debugging;program diagnostics,automatic debugging;human focus-tracking analysis;software engineering;human behavior;ranking-based fault localization technique;spectrum-based fault localization;debugging efficiency;follow-up browsing;SBFL assist browsing","keywords":"Automated debugging;spectrum-based fault localization;user studies;attention tracking;navigation pattern;fault comprehension","startPage":"808","endPage":"819","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886958","citationCount":12,"referenceCount":40,"year":2016,"authors":"X. Xie; Z. Liu; S. Song; Z. Chen; J. Xuan; B. Xu","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3635"},"title":"Risk-Driven Revision of Requirements Models","abstract":"Requirements incompleteness is often the result of unanticipated adverse conditions which prevent the software and its environment from behaving as expected. These conditions represent risks that can cause severe software failures. The identification and resolution of such risks is therefore a crucial step towards requirements completeness. Obstacle analysis is a goal-driven form of risk analysis that aims at detecting missing conditions that can obstruct goals from being satisfied in a given domain, and resolving them. This paper proposes an approach for automatically revising goals that may be under-specified or (partially) wrong to resolve obstructions in a given domain. The approach deploys a learning-based revision methodology in which obstructed goals in a goal model are iteratively revised from traces exemplifying obstruction and non-obstruction occurrences. Our revision methodology computes domain-consistent, obstruction-free revisions that are automatically propagated to other goals in the model in order to preserve the correctness of goal models whilst guaranteeing minimal change to the original model. We present the formal foundations of our learning-based approach, and show that it preserves the properties of our formal framework. We validate it against the benchmarking case study of the London Ambulance Service.","conference":"IEEE","terms":"Computational modeling;Software;Analytical models;Automobiles;Requirements engineering;Government;Risk analysis,formal specification;iterative methods;learning (artificial intelligence),risk-driven revision;requirements models;obstacle analysis;learning-based revision methodology;learning-based approach;London Ambulance Service;iterative method","keywords":"","startPage":"855","endPage":"865","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886962","citationCount":2,"referenceCount":37,"year":2016,"authors":"D. Alrajeh; A. van Lamsweerde; J. Kramer; A. Russo; S. Uchitel","affiliations":"Dept. of Comput., Imperial Coll. London, London, UK; ICTEAM, Univ. Catholique de Louvain, Louvain, Belgium; Dept. of Comput., Imperial Coll. London, London, UK; Dept. of Comput., Imperial Coll. London, London, UK; Dept. of Comput., Imperial Coll. London, London, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3636"},"title":"Using (Bio)Metrics to Predict Code Quality Online","abstract":"Finding and fixing code quality concerns, such as defects or poor understandability of code, decreases software development and evolution costs. A common industrial practice to identify code quality concerns early on are code reviews. While code reviews help to identify problems early on, they also impose costs on development and only take place after a code change is already completed. The goal of our research is to automatically identify code quality concerns while a developer is making a change to the code. By using biometrics, such as heart rate variability, we aim to determine the difficulty a developer experiences working on a part of the code as well as identify and help to fix code quality concerns before they are even committed to the repository. In a field study with ten professional developers over a two-week period we investigated the use of biometrics to determine code quality concerns. Our results show that biometrics are indeed able to predict quality concerns of parts of the code while a developer is working on, improving upon a naive classifier by more than 26% and outperforming classifiers based on more traditional metrics. In a second study with five professional developers from a different country and company, we found evidence that some of our findings from our initial study can be replicated. Overall, the results from the presented studies suggest that biometrics have the potential to predict code quality concerns online and thus lower development and evolution costs.","conference":"IEEE","terms":"Biometrics (access control);Software;Heart rate variability;Companies;Manuals;Temperature measurement,software quality,code quality prediction;code understandability;software development costs;software evolution costs;code reviews;biometrics;heart rate variability","keywords":"","startPage":"452","endPage":"463","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886925","citationCount":10,"referenceCount":77,"year":2016,"authors":"S. C. Müller; T. Fritz","affiliations":"Dept. of Inf., Univ. of Zurich, Zurich, Switzerland; Dept. of Inf., Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3637"},"title":"Toward a Framework for Detecting Privacy Policy Violations in Android Application Code","abstract":"Mobile applications frequently access sensitive personal information to meet user or business requirements. Because such information is sensitive in general, regulators increasingly require mobile-app developers to publish privacy policies that describe what information is collected. Furthermore, regulators have fined companies when these policies are inconsistent with the actual data practices of mobile apps. To help mobile-app developers check their privacy policies against their apps' code for consistency, we propose a semi-automated framework that consists of a policy terminology- API method map that links policy phrases to API methods that produce sensitive information, and information flow analysis to detect misalignments. We present an implementation of our framework based on a privacy-policy-phrase ontology and a collection of map- pings from API methods to policy phrases. Our empirical evaluation on 477 top Android apps discovered 341 potential privacy policy violations.","conference":"IEEE","terms":"Privacy;Androids;Humanoid robots;Mobile communication;Ontologies;Natural languages;Google,Android (operating system);application program interfaces;data privacy;mobile computing,privacy policy violation detection;Android application code;mobile applications;semiautomated framework;API method map;information flow analysis;privacy-policy-phrase ontology;map-ping collection","keywords":"Privacy Policies;Android Applications;Violation Detection","startPage":"25","endPage":"36","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886889","citationCount":13,"referenceCount":42,"year":2016,"authors":"R. Slavin; X. Wang; M. B. Hosseini; J. Hester; R. Krishnan; J. Bhatia; T. D. Breaux; J. Niu","affiliations":"Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at Dallas, Dallas, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3638"},"title":"\"Was My Contribution Fairly Reviewed?\" A Framework to Study the Perception of Fairness in Modern Code Reviews","abstract":"Modern code reviews improve the quality of software products. Although modern code reviews rely heavily on human interactions, little is known regarding whether they are performed fairly. Fairness plays a role in any process where decisions that affect others are made. When a system is perceived to be unfair, it affects negatively the productivity and motivation of its participants. In this paper, using fairness theory we create a framework that describes how fairness affects modern code reviews. To demonstrate its applicability, and the importance of fairness in code reviews, we conducted an em-pirical study that asked developers of a large industrial open source ecosystem (OpenStack) what their perceptions are regarding fairness in their code reviewing process. Our study shows that, in general, the code review process in OpenStack is perceived as fair; however, a significant portion of respondents perceive it as unfair. We also show that the variability in the way they prioritize code reviews signals a lack of consistency and the existence of bias (potentially increasing the perception of unfairness). The contributions of this paper are: (1) we propose a framework-based on fairness theory-for studying and managing social behaviour in modern code reviews, (2) we provide support for the framework through the results of a case study on a large industrial-backed open source project, (3) we present evidence that fairness is an issue in the code review process of a large open source ecosystem, and, (4) we present a set of guidelines for practitioners to address unfairness in modern code reviews.","conference":"IEEE","terms":"Process control;Organizations;Software reviews;Standards organizations;Guidelines;Ecosystems,organisational aspects;public domain software;software quality,modern code reviews;code review process;code review signals;software product quality;industrial open source ecosystem;OpenStack","keywords":"Fairness;Software Development;Code Reviews;Open source software;human and social aspects;transparency","startPage":"523","endPage":"534","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453118","citationCount":3,"referenceCount":79,"year":2018,"authors":"D. M. German; G. Robles; G. Poo-Caamaño; X. Yang; H. Iida; K. Inoue","affiliations":"Univ. of Victoria, Victoria, BC, Canada; Univ. Rey Juan Carlos, Mostoles, Spain; Univ. of Victoria, Victoria, BC, Canada; Osaka Univ., Suita, Japan; Nara Inst. of Technol., Nara, Japan; Osaka Univ., Suita, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3639"},"title":"Reliability of Run-Time Quality-of-Service Evaluation Using Parametric Model Checking","abstract":"Run-time Quality-of-Service (QoS) assurance is crucial for business-critical systems. Complex behavioral performance metrics (PMs) are useful but often difficult to monitor or measure. Probabilistic model checking, especially parametric model checking, can support the computation of aggre- gate functions for a broad range of those PMs. In practice, those PMs may be defined with parameters determined by run-time data. In this paper, we address the reliability of QoS evaluation using parametric model checking. Due to the imprecision with the instantiation of parameters, an evaluation outcome may mislead the judgment about requirement violations. Based on a general assumption of run-time data distribution, we present a novel framework that contains light-weight statistical inference methods to analyze the re- liability of a parametric model checking output with respect to an intuitive criterion. We also present case studies in which we test the stability and accuracy of our inference methods and describe an application of our framework to a cloud server management problem.","conference":"IEEE","terms":"Quality of service;Reliability;Parametric statistics;Probabilistic logic;Model checking;Unified modeling language;Random variables,business data processing;cloud computing;formal verification;probability;quality of service,run-time quality-of-service evaluation reliability;parametric model checking;run-time quality-of-service assurance;business-critical systems;complex behavioral performance metrics;probabilistic model checking;QoS evaluation reliability;requirement violations;run-time data distribution;lightweight statistical inference method;cloud server management","keywords":"Data distribution;probabilistic model checking;Quality-of-Service;reliability;run-time evaluation","startPage":"73","endPage":"84","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886893","citationCount":5,"referenceCount":31,"year":2016,"authors":"G. Su; D. S. Rosenblum; G. Tamburrelli","affiliations":"Nat. Univ. of Singapore, Singapore, Singapore; Nat. Univ. of Singapore, Singapore, Singapore; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d363a"},"title":"To Distribute or Not to Distribute? Why Licensing Bugs Matter","abstract":"Software licenses dictate how source code or binaries can be modified, reused, and redistributed. In the case of open source projects, software licenses generally fit into two main categories, permissive and restrictive, depending on the degree to which they allow redistribution or modification under licenses different from the original one(s). Developers and organizations can also modify existing licenses, creating custom licenses with specific permissive/restrictive terms. Having such a variety of software licenses can create confusion among software developers, and can easily result in the introduction of licensing bugs, not necessarily limited to well-known license incompatibilities. In this work, we report a study aimed at characterizing licensing bugs by (i) building a catalog categorizing the types of licensing bugs developers and other stakeholders face, and (ii) understanding the implications licensing bugs have on the software projects they affect. The presented study is the result of the manual analysis of 1,200 discussions related to licensing bugs carried out in issue trackers and in five legal mailing lists of open source communities. Our findings uncover new types of licensing bugs not addressed in prior literature, and a detailed assessment of their implications.","conference":"IEEE","terms":"Licenses;Computer bugs;Software;Law;Guidelines;Stakeholders,program debugging;project management;public domain software;software engineering,software projects;software licenses;custom licenses;software developers;license incompatibilities;licensing bug developers","keywords":"Software Licenses;Empirical Studies;Open Source Practices","startPage":"268","endPage":"279","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453086","citationCount":1,"referenceCount":0,"year":2018,"authors":"C. Vendome; D. German; M. Di Penta; G. Bavota; M. Linares-Vásquez; D. Poshyvanyk","affiliations":"Coll. of William \u0026 Mary Williamsburg, Williamsburg, VA, USA; Univ. of Victoria, Victoria, BC, Canada; Univ. of Sannio, Benevento, Italy; Univ. della Svizzera italiana (USI), Lugano, Switzerland; Univ. de los Andes, Bogota, Colombia; Coll. of William \u0026 Mary Williamsburg, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d363b"},"title":"Behavioral Log Analysis with Statistical Guarantees","abstract":"Scalability is a major challenge for existing behavioral log analysis algorithms, which extract finite-state automaton models or temporal properties from logs generated by running systems. In this paper we present statistical log analysis, which addresses scalability using statistical tools. The key to our approach is to consider behavioral log analysis as a statistical experiment.Rather than analyzing the entire log, we suggest to analyze only a sample of traces from the log and, most importantly, provide means to compute statistical guarantees for the correctness of the analysis result.We present the theoretical foundations of our approach and describe two example applications, to the classic k-Tails algorithm and to the recently presented BEAR algorithm.Finally, based on experiments with logs generated from real-world models and with real-world logs provided to us by our industrial partners, we present extensive evidence for the need for scalable log analysis and for the effectiveness of statistical log analysis.","conference":"IEEE","terms":"Algorithm design and analysis;Scalability;Analytical models;Computational modeling;Context;Software algorithms;Software,program diagnostics;statistical analysis,BEAR algorithm;k-tails algorithm;statistical log analysis;statistical guarantees;behavioral log analysis algorithm","keywords":"Log analysis;specification mining;statistical guarantees","startPage":"877","endPage":"887","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886964","citationCount":6,"referenceCount":36,"year":2016,"authors":"N. Busany; S. Maoz","affiliations":"Sch. of Comput. Sci., Tel Aviv Univ., Tel Aviv, Israel; Sch. of Comput. Sci., Tel Aviv Univ., Tel Aviv, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d363c"},"title":"Traceability in the Wild: Automatically Augmenting Incomplete Trace Links","abstract":"Software and systems traceability is widely accepted as an essential element for supporting many software development tasks. Today's version control systems provide inbuilt features that allow developers to tag each commit with one or more issue ID, thereby providing the building blocks from which project-wide traceability can be established between feature requests, bug fixes, commits, source code, and specific developers. However, our analysis of six open source projects showed that on average only 60% of the commits were linked to specific issues. Without these fundamental links the entire set of project-wide links will be incomplete, and therefore not trustworthy. In this paper we address the fundamental problem of missing links between commits and issues. Our approach leverages a combination of process and text-related features characterizing issues and code changes to train a classifier to identify missing issue tags in commit messages, thereby generating the missing links. We conducted a series of experiments to evaluate our approach against six open source projects and showed that it was able to effectively recommend links for tagging issues at an average of 96% recall and 33% precision. In a related task for augmenting a set of existing trace links, the classifier returned precision at levels greater than 89% in all projects and recall of 50%.","conference":"IEEE","terms":"Computer bugs;Software;Control systems;Task analysis;Software engineering;Jacobian matrices;Tagging,program debugging;public domain software;software maintenance,open source projects;systems traceability;software development tasks;project-wide traceability;source code;project-wide links;text-related features;code changes;missing issue tags identification","keywords":"Traceability;Link Recovery;Machine Learning;Open Source","startPage":"834","endPage":"845","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453157","citationCount":5,"referenceCount":63,"year":2018,"authors":"M. Rath; J. Rendall; J. L. C. Guo; J. Cleland-Huang; P. Mäder","affiliations":"Tech. Univ. Ilmenau, Ilmenau, Germany; Univ. of Notre Dame, Bend, OR, USA; McGill Univ., Montreal, QC, Canada; Univ. of Notre Dame, Bend, OR, USA; Tech. Univ. Ilmenau, Ilmenau, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d363d"},"title":"Symbolic Verification of Regular Properties","abstract":"Verifying the regular properties of programs has been a significant challenge. This paper tackles this challenge by presenting symbolic regular verification (SRV) that offers significant speedups over the state-of-the-art. SRV is based on dynamic symbolic execution (DSE) and enabled by novel techniques for mitigating path explosion: (1) a regular property-oriented path slicing algorithm, and (2) a synergistic combination of property-oriented path slicing and guiding. Slicing prunes redundant paths, while guiding boosts the search for counterexamples. We have implemented SRV for Java and evaluated it on 15 real-world open-source Java programs (totaling 259K lines of code). Our evaluation results demonstrate the effectiveness and efficiency of SRV. Compared with the state-of-the-art - pure DSE, pure guiding, and pure path slicing - SRV achieves average speedups of more than 8.4X, 8.6X, and 7X, respectively, making symbolic regular property verification significantly more practical.","conference":"IEEE","terms":"Java;Software engineering;Software;Task analysis;History;Explosions;Heuristic algorithms,Java;program slicing;program verification;public domain software,symbolic regular verification;SRV;dynamic symbolic execution;regular property-oriented path slicing algorithm;real-world open-source Java programs;symbolic regular property verification;program verification;path explosion mitigation","keywords":"Regular property;Verification;Dynamic Symbolic Execution;Slicing;Guiding","startPage":"871","endPage":"881","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453161","citationCount":0,"referenceCount":49,"year":2018,"authors":"H. Yu; Z. Chen; J. Wang; Z. Su; W. Dong","affiliations":"Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China; Dept. of Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Coll. of Comput., Nat. Univ. of Defense Technol., Changsha, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d363e"},"title":"Disseminating Architectural Knowledge on Open-Source Projects: A Case Study of the Book \"Architecture of Open-Source Applications\"","abstract":"This paper reports on an interview-based study of 18 authors of different chapters of the two-volume book \"Architecture of Open-Source Applications\". The main contributions are a synthesis of the process of authoring essay-style documents (ESDs) on software architecture, a series of observations on important factors that influence the content and presentation of architectural knowledge in this documentation form, and a set of recommendations for readers and writers of ESDs on software architecture. We analyzed the influence of three factors in particular: the evolution of a system, the community involvement in the project, and the personal characteristics of the author. This study provides the first systematic investigation of the creation of ESDs on software architecture. The observations we collected have implications for both readers and writers of ESDs, and for architecture documentation in general.","conference":"IEEE","terms":"Interviews;Computer architecture;Software architecture;Documentation;Software;Electrostatic discharges;Context,public domain software;software architecture,architectural knowledge dissemination;open-source projects;Architecture of Open-Source Applications;ESD;essay-style documents;software architecture;architecture documentation","keywords":"Architecture Description;Open-Source Software","startPage":"476","endPage":"487","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886927","citationCount":1,"referenceCount":58,"year":2016,"authors":"M. P. Robillard; N. Medvidovic","affiliations":"Sch. of Comput. Sci., McGill Univ., Montréal, QC, Canada; Comput. Sci. Dept., Univ. of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d363f"},"title":"Comparing White-Box and Black-Box Test Prioritization","abstract":"Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.","conference":"IEEE","terms":"Testing;Fault detection;Robustness;Flexible printed circuits;Software;Servers;Instruments,program testing;regression analysis,black-box test prioritization;white-box regression test prioritization;black-box prioritization approach;combinatorial interaction testing;diversity-based techniques;input model diversity;input test set diameter","keywords":"Regression Testing;White-box;Black-box","startPage":"523","endPage":"534","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886931","citationCount":25,"referenceCount":69,"year":2016,"authors":"C. Henard; M. Papadakis; M. Harman; Y. Jia; Y. Le Traon","affiliations":"Univ. of Luxembourg, Luxembourg City, Luxembourg; Univ. of Luxembourg, Luxembourg City, Luxembourg; Univ. Coll. London, London, UK; Univ. Coll. London, London, UK; Univ. of Luxembourg, Luxembourg City, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3640"},"title":"Inferring and Asserting Distributed System Invariants","abstract":"Distributed systems are difficult to debug and understand. A key reason for this is distributed state, which is not easily accessible and must be pieced together from the states of the individual nodes in the system. We propose Dinv, an automatic approach to help developers of distributed systems uncover the runtime distributed state properties of their systems. Dinv uses static and dynamic program analyses to infer relations between variables at different nodes. For example, in a leader election algorithm, Dinv can relate the variable leader at different nodes to derive the invariant forall ∀ nodes i, j, leader_i = leader_j. This can increase the developer's confidence in the correctness of their system. The developer can also use Dinv to convert an inferred invariant into a distributed runtime assertion on distributed state. We applied Dinv to several popular distributed systems, such as etcd Raft, Hashicorp Serf, and Taipei-Torrent, which have between 1.7K and 144K LOC and are widely used. Dinv derived useful invariants for these systems, including invariants that capture the correctness of distributed routing strategies, leadership, and key hash distribution. We also used Dinv to assert correctness of the inferred etcd Raft invariants at runtime, using these asserts to detect injected silent bugs.","conference":"IEEE","terms":"Instruments;Clocks;Runtime;Tools;Lattices;Protocols;Computer bugs,dynamic programming;Java;peer-to-peer computing;program debugging;program diagnostics;program verification,Dinv;useful invariants;distributed routing strategies;key hash distribution;individual nodes;runtime distributed state properties;leader election algorithm;variable leader;inferred invariant;distributed runtime assertion","keywords":"distributed systems;specification mining;runtime checking;program analysis","startPage":"1149","endPage":"1159","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453196","citationCount":1,"referenceCount":59,"year":2018,"authors":"S. Grant; H. Cech; I. Beschastnikh","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of Bamberg, Bamberg, Germany; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3641"},"title":"The Emerging Role of Data Scientists on Software Development Teams","abstract":"Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.","conference":"IEEE","terms":"Software;Data science;Interviews;Companies;Software engineering,software development management;team working,data scientists;software development teams;software creation;software development process;customer usage;software engineering skills;software-oriented data analytics;insight providers;modeling specialists;platform builders;polymaths;team leaders","keywords":"Data Science;Software Analytics;Data Scientist","startPage":"96","endPage":"107","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886895","citationCount":14,"referenceCount":49,"year":2016,"authors":"M. Kim; T. Zimmermann; R. DeLine; A. Begel","affiliations":"UCLA, Los Angeles, CA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3642"},"title":"Understanding Developers' Needs on Deprecation as a Language Feature","abstract":"Deprecation is a language feature that allows API producers to mark a feature as obsolete. We aim to gain a deep understanding of the needs of API producers and consumers alike regarding deprecation. To that end, we investigate why API producers deprecate features, whether they remove deprecated features, how they expect consumers to react, and what prompts an API consumer to react to deprecation. To achieve this goal we conduct semi-structured interviews with 17 third-party Java API producers and survey 170 Java developers. We observe that the current deprecation mechanism in Java and the proposal to enhance it does not address all the needs of a developer. This leads us to propose and evaluate three further enhancements to the deprecation mechanism.","conference":"IEEE","terms":"Java;Interviews;Industries;Companies;Proposals;Guidelines;Software engineering,application program interfaces;Java;public domain software,Java developers;current deprecation mechanism;third-party Java API producers;API consumer;language feature","keywords":"API;deprecation;Java","startPage":"561","endPage":"571","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453124","citationCount":2,"referenceCount":0,"year":2018,"authors":"A. A. Sawant; M. Aniche; A. van Deursen; A. Bacchelli","affiliations":"Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3643"},"title":"Towards Optimal Concolic Testing","abstract":"Concolic testing integrates concrete execution (e.g., random testing) and symbolic execution for test case generation. It is shown to be more cost-effective than random testing or symbolic execution sometimes. A concolic testing strategy is a function which decides when to apply random testing or symbolic execution, and if it is the latter case, which program path to symbolically execute. Many heuristics-based strategies have been proposed. It is still an open problem what is the optimal concolic testing strategy. In this work, we make two contributions towards solving this problem. First, we show the optimal strategy can be defined based on the probability of program paths and the cost of constraint solving. The problem of identifying the optimal strategy is then reduced to a model checking problem of Markov Decision Processes with Costs. Secondly, in view of the complexity in identifying the optimal strategy, we design a greedy algorithm for approximating the optimal strategy. We conduct two sets of experiments. One is based on randomly generated models and the other is based on a set of C programs. The results show that existing heuristics have much room to improve and our greedy algorithm often outperforms existing heuristics.","conference":"IEEE","terms":"Greedy algorithms;Cost accounting;Sun;Model checking;Probabilistic logic;Systematics,greedy algorithms;Markov processes;program diagnostics;program testing,concrete execution;random testing;test case generation;symbolic execution;program path;heuristics-based strategies;optimal concolic testing strategy;optimal strategy;optimal concolic testing","keywords":"Concolic Testing;Markov Chain;Test Case Generation","startPage":"291","endPage":"302","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453088","citationCount":1,"referenceCount":0,"year":2018,"authors":"X. Wang; J. Sun; Z. Chen; P. Zhang; J. Wang; Y. Lin","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3644"},"title":"Program Splicing","abstract":"We introduce program splicing, a programming methodology that aims to automate the work ow of copying, pasting, and modifying code available online. Here, the programmer starts by writing a \"draft\" that mixes un nished code, natural language comments, and correctness requirements. A program synthesizer that interacts with a large, searchable database of program snippets is used to automatically complete the draft into a program that meets the re-quirements. The synthesis process happens in two stages. First, the synthesizer identi es a small number of programs in the database that are relevant to the synthesis task. Next it uses an enumerative search to systematically ll the draft with expressions and statements from these relevant programs. The resulting program is returned to the programmer, who can modify it and possibly invoke additional rounds of synthesis. We present an implementation of program splicing, called Splicer, for the Java programming language. Splicer uses a corpus of over 3.5 million procedures from an open-source software repository. Our evaluation uses the system in a suite of everyday programming tasks, and includes a comparison with a state-of-the-art competing approach as well as a user study. The results point to the broad scope and scalability of program splicing and indicate that the approach can signi cantly boost programmer productivity.","conference":"IEEE","terms":"Splicing;Task analysis;Databases;Synthesizers;Programming;Java;Software engineering,Java;public domain software;software engineering,program splicing;programming methodology;program synthesizer;program snippets;relevant programs;resulting program;Java programming language;everyday programming tasks","keywords":"Big Data;Program Synthesis","startPage":"338","endPage":"349","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453092","citationCount":0,"referenceCount":0,"year":2018,"authors":"Y. Lu; S. Chaudhuri; C. Jermaine; D. Melski","affiliations":"Rice Univ., Houston, TX, USA; Rice Univ., Houston, TX, USA; Rice Univ., Houston, TX, USA; Grammatech Inc., Ithaca, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3645"},"title":"Feedback-Directed Instrumentation for Deployed JavaScript Applications","abstract":"Many bugs in JavaScript applications manifest themselves as objects that have incorrect property values when a failure occurs. For this type of error, stack traces and log files are often insufficient for diagnosing problems. In such cases, it is helpful for developers to know the control flow path from the creation of an object to a crashing statement. Such crash paths are useful for understanding where the object originated and whether any properties of the object were corrupted since its creation.We present a feedback-directed instrumentation technique for computing crash paths that allows the instrumentation overhead to be distributed over a crowd of users and to reduce it for users who do not encounter the crash. We implemented our technique in a tool, Crowdie, and evaluated it on 10 real-world issues for which error messages and stack traces are insufficient to isolate the problem. Our results show that feedback-directed instrumentation requires 5% to 25% of the program to be instrumented, that the same crash must be observed 3 to 10 times to discover the crash path, and that feedback-directed instrumentation typically slows down execution by a factor 2x-9x compared to 8x-90x for an approach where applications are fully instrumented.","conference":"IEEE","terms":"Instruments;Computer bugs;Reactive power;Debugging;Software;Servers,Java;system recovery,JavaScript applications;feedback-directed instrumentation technique;crash paths;instrumentation overhead;CROWDIE","keywords":"debugging;dynamic analysis;javascript;crowdsourcing;instrumentation","startPage":"899","endPage":"910","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886966","citationCount":3,"referenceCount":32,"year":2016,"authors":"M. Madsen; F. Tip; E. Andreasen; K. Sen; A. Møller","affiliations":"Univ. of Waterloo, Waterloo, ON, Canada; Samsung Res. America, Mountain View, CA, USA; Aarhus Univ., Aarhus, Denmark; EECS Dept., UC Berkeley, Berkeley, CA, USA; Aarhus Univ., Aarhus, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3646"},"title":"Finding Security Bugs in Web Applications Using a Catalog of Access Control Patterns","abstract":"We propose a specification-free technique for finding missing security checks in web applications using a catalog of access control patterns in which each pattern models a common access control use case. Our implementation, SPACE, checks that every data exposure allowed by an application's code matches an allowed exposure from a security pattern in our catalog. The only user-provided input is a mapping from application types to the types of the catalog; the rest of the process is entirely automatic. In an evaluation on the 50 most watched Ruby on Rails applications on Github, SPACE reported 33 possible bugs---23 previously unknown security bugs, and 10 false positives.","conference":"IEEE","terms":"Computer bugs;Aerospace electronics;Databases;Access control;Rails;Open source software,authorisation;Internet;object-oriented programming;program debugging,security bugs;Web applications;access control patterns catalog;specification-free technique;access control use case;SPACE;data exposure;security pattern;Github","keywords":"web application security;access control;bug finding","startPage":"947","endPage":"958","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886970","citationCount":3,"referenceCount":37,"year":2016,"authors":"J. P. Near; D. Jackson","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3647"},"title":"Context-Aware Patch Generation for Better Automated Program Repair","abstract":"The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (ie correct patches are either generated after incorrect plausible ones or not generated within the time budget). To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood. In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.","conference":"IEEE","terms":"Computer bugs;Maintenance engineering;Search problems;Explosions;Context modeling;Software;Benchmark testing,maximum likelihood estimation;program diagnostics;search problems;ubiquitous computing,search-based automated program repair;context-aware patch generation technique;CapGen;likelihood estimation;AST nodes context information","keywords":"Context-Aware;Automated Program Repair;Patch Prioritization","startPage":"1","endPage":"11","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453055","citationCount":18,"referenceCount":58,"year":2018,"authors":"M. Wen; J. Chen; R. Wu; D. Hao; S. Cheung","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3648"},"title":"Decoupling Level: A New Metric for Architectural Maintenance Complexity","abstract":"Despite decades of research on software metrics, we still cannot reliably measure if one design is more maintainable than another. Software managers and architects need to understand whether their software architecture is \"good enough\", whether it is decaying over time and, if so, by how much. In this paper, we contribute a new architecture maintainability metric---Decoupling Level (DL)---derived from Baldwin andClark's option theory. Instead of measuring how coupled an architecture is, we measure how well the software can be decoupled into small and independently replaceable modules. We measured the DL for 108 open source projects and 21 industrial projects, each of which has multiple releases. Our main result shows that the larger the DL, the better thearchitecture. By \"better\" we mean: the more likely bugs and changes can be localized and separated, and the more likely that developers can make changes independently. The DL metric also opens the possibility of quantifying canonical principles of single responsibility and separation of concerns, aiding cross-project comparison and architecture decay monitoring.","conference":"IEEE","terms":"Computer architecture;Software;Software measurement;Observers;Software architecture;Complexity theory,public domain software;software architecture;software maintenance;software metrics,architectural maintenance complexity metric;software metrics;software architecture;architecture maintainability metric;decoupling level metric;open source projects;canonical principles","keywords":"Software Architecture;Software Quality;Software Metrics","startPage":"499","endPage":"510","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886929","citationCount":14,"referenceCount":39,"year":2016,"authors":"R. Mo; Y. Cai; R. Kazman; L. Xiao; Q. Feng","affiliations":"Drexel Univ., Philadelphia, PA, USA; Drexel Univ., Philadelphia, PA, USA; SEI/CMU, Univ. of Hawaii, Honolulu, HI, USA; Drexel Univ., Philadelphia, PA, USA; Drexel Univ., Philadelphia, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3649"},"title":"The Impact of Test Case Summaries on Bug Fixing Performance: An Empirical Investigation","abstract":"Automated test generation tools have been widely investigated with the goal of reducing the cost of testing activities. However, generated tests have been shownnot to help developers in detecting and finding more bugs even though they reach higher structural coverage compared to manual testing. The main reason is that generated tests are difficult to understand and maintain. Our paper proposes an approach, coined TestDescriber, which automatically generates test case summaries of the portion of code exercised by each individual test, thereby improving understandability. We argue that this approach can complement the current techniques around automated unit test generation or search-based techniques designed to generate a possibly minimal set of test cases. In evaluating our approach we found that (1) developers find twice as many bugs, and (2) test case summaries significantly improve the comprehensibility of test cases, which is considered particularly useful by developers.","conference":"IEEE","terms":"Testing;Computer bugs;Java;Software;Software engineering;Natural languages;Pragmatics,program debugging;program testing,test case summaries;bug fixing performance;automated test generation tools;TestDescriber approach;automated unit test generation;search-based techniques","keywords":"Software testing;Test Case Summarization;Empirical Study","startPage":"547","endPage":"558","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886933","citationCount":16,"referenceCount":57,"year":2016,"authors":"S. Panichella; A. Panichella; M. Beller; A. Zaidman; H. C. Gall","affiliations":"Univ. of Zurich, Zurich, Switzerland; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Delft Univ. of Technol., Delft, Netherlands; Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d364a"},"title":"Grounded Theory in Software Engineering Research: A Critical Review and Guidelines","abstract":"Grounded Theory (GT) has proved an extremely useful research approach in several fields including medical sociology, nursing, education and management theory. However, GT is a complex method based on an inductive paradigm that is fundamentally different from the traditional hypothetico-deductive research model. As there are at least three variants of GT, some ostensibly GT research suffers from method slurring, where researchers adopt an arbitrary subset of GT practices that are not recognizable as GT. In this paper, we describe the variants of GT and identify the core set of GT practices. We then analyze the use of grounded theory in software engineering. We carefully and systematically selected 98 articles that mention GT, of which 52 explicitly claim to use GT, with the other 46 using GT techniques only. Only 16 articles provide detailed accounts of their research procedures. We offer guidelines to improve the quality of both conducting and reporting GT studies. The latter is an important extension since current GT guidelines in software engineering do not cover the reporting process, despite good reporting being necessary for evaluating a study and informing subsequent research.","conference":"IEEE","terms":"Software engineering;Software;Guidelines;Computer science;Interviews;Encoding;Sorting,software engineering,grounded theory;software engineering research;GT practices","keywords":"Grounded theory;software engineering;review;guidelines","startPage":"120","endPage":"131","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886897","citationCount":33,"referenceCount":78,"year":2016,"authors":"K. Stol; P. Ralph; B. Fitzgerald","affiliations":"Lero (Irish Software Res. Centre), Univ. of Limerick, Limerick, Ireland; Dept. of Comput. Sci., Univ. of Auckland, Auckland, New Zealand; Lero (Irish Software Res. Centre), Univ. of Limerick, Limerick, Ireland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d364b"},"title":"UFO: Predictive Concurrency Use-After-Free Detection","abstract":"Use-After-Free (UAF) vulnerabilities are caused by the program operating on a dangling pointer and can be exploited to compromise critical software systems. While there have been many tools to mitigate UAF vulnerabilities, UAF remains one of the most common attack vectors. UAF is particularly di cult to detect in concurrent programs, in which a UAF may only occur with rare thread schedules. In this paper, we present a novel technique, UFO, that can precisely predict UAFs based on a single observed execution trace with a provably higher detection capability than existing techniques with no false positives. The key technical advancement of UFO is an extended maximal thread causality model that captures the largest possible set of feasible traces that can be inferred from a given multithreaded execution trace. By formulating UAF detection as a constraint solving problem atop this model, we can explore a much larger thread scheduling space than classical happens-before based techniques. We have evaluated UFO on several real-world large complex C/C++ programs including Chromium and FireFox. UFO scales to real-world systems with hundreds of millions of events in their execution and has detected a large number of real concurrency UAFs.","conference":"IEEE","terms":"Instruction sets;Concurrent computing;Schedules;Browsers;Tools;Chromium;Encoding,multi-threading;program debugging;program diagnostics;scheduling;security of data,program operating;critical software systems;UAF vulnerabilities;common attack vectors;concurrent programs;rare thread schedules;single observed execution trace;provably higher detection capability;extended maximal thread causality model;UAF detection;larger thread scheduling space;UFO scales;concurrency use-after-free detection;use-after-free vulnerabilities;multithreaded execution trace;concurrency UAF","keywords":"UAF;Concurrency;Vulnerabilities;UFO","startPage":"609","endPage":"619","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453130","citationCount":2,"referenceCount":0,"year":2018,"authors":"J. Huang","affiliations":"Parasol Lab., Texas A\u0026M Univ., College Station, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d364c"},"title":"Perses: Syntax-Guided Program Reduction","abstract":"Given a program P that exhibits a certain property ψ (e.g., a C program that crashes GCC when it is being compiled), the goal of program reduction is to minimize P to a smaller variant P? that still exhibits the same property, i.e., ψ(P'). Program reduction is important and widely demanded for testing and debugging. For example, all compiler/interpreter development projects need effective program reduction to minimize failure-inducing test programs to ease debugging. However, state-of-the-art program reduction techniques - notably Delta Debugging (DD), Hierarchical Delta Debugging (HDD), and C-Reduce - do not perform well in terms of speed (reduction time) and quality (size of reduced programs), or are highly customized for certain languages and thus lack generality. This paper presents Perses, a novel framework for effective, efficient, and general program reduction. The key insight is to exploit, in a general manner, the formal syntax of the programs under reduction and ensure that each reduction step considers only smaller, syntactically valid variants to avoid futile efforts on syntactically invalid variants. Our framework supports not only deletion (as for DD and HDD), but also general, effective program transformations. We have designed and implemented Perses, and evaluated it for two language settings: C and Java. Our evaluation results on 20 C programs triggering bugs in GCC and Clang demonstrate Perses's strong practicality compared to the state-of-the-art: (1) smaller size - Perses's results are respectively 2% and 45% in size of those from DD and HDD; and (2) shorter reduction time - Perses takes 23% and 47% time taken by DD and HDD respectively. Even when compared to the highly customized and optimized C-Reduce for C/C++, Perses takes only 38-60% reduction time.","conference":"IEEE","terms":"Grammar;Debugging;Syntactics;Computer bugs;Program processors;Sun;Java,Java;program compilers;program debugging;program testing,DD;HDD;shorter reduction time - Perses;syntax-guided program reduction;C program;effective program reduction;failure-inducing test programs;reduced programs;general program reduction;reduction step;general program transformations;effective program transformations;compiler-interpreter development projects;program reduction techniques;hierarchical Delta debugging;efficient program reduction","keywords":"program reduction;delta debugging;debugging","startPage":"361","endPage":"371","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453094","citationCount":2,"referenceCount":0,"year":2018,"authors":"C. Sun; Y. Li; Q. Zhang; T. Gu; Z. Su","affiliations":"Univ. of California, Davis, Davis, CA, USA; Univ. of California, Davis, Davis, CA, USA; Univ. of California, Davis, Davis, CA, USA; Univ. of California, Davis, Davis, CA, USA; Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d364d"},"title":"Automated Partitioning of Android Applications for Trusted Execution Environments","abstract":"The co-existence of critical and non-critical applications on computing devices, such as mobile phones, is becoming commonplace. The sensitive segments of a critical application should be executed in isolation on Trusted Execution Environments (TEE) so that the associated code and data can be protected from malicious applications. TEE is supported by different technologies and platforms, such as ARM Trustzone, that allow logical separation of \"secure\" and \"normal\" worlds. We develop an approach for automated partitioning of critical Android applications into \"client\" code to be run in the \"normal\" world and \"TEE commands\" encapsulating the handling of confidential data to be run in the \"secure\" world. We also reduce the overhead due to transitions between the two worlds by choosing appropriate granularity for the TEE commands. The advantage of our proposed solution is evidenced by efficient partitioning of real-world applications.","conference":"IEEE","terms":"Security;Androids;Humanoid robots;Java;Google;Software;Hardware,Android (operating system);mobile computing;trusted computing,Android application partitioning;trusted execution environments;TEE;ARM Trustzone;confidential data handling","keywords":"","startPage":"923","endPage":"934","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886968","citationCount":1,"referenceCount":42,"year":2016,"authors":"K. Rubinov; L. Rosculete; T. Mitra; A. Roychoudhury","affiliations":"DeepSE Group at DEIB, Politec. di Milano, Milan, Italy; Applic. Threat Intell., Ixia, Bucharest, Romania; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore; Sch. of Comput., Nat. Univ. of Singapore, Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d364e"},"title":"Building a Theory of Job Rotation in Software Engineering from an Instrumental Case Study","abstract":"Job Rotation is an organizational practice in which individuals are frequently moved from a job (or project) to another in the same organization. Studies in other areas have found that this practice has both negative and positive effects on individuals' work. However, there are only few studies addressing this issue in software engineering so far. The goal of our study is to investigate the effects of job rotation on work related factors in software engineering by performing a qualitative case study on a large software organization that uses job rotation as an organizational practice. We interviewed senior managers, project managers, and software engineers that had experienced this practice. Altogether, 48 participants were involved in all phases of this research. Collected data was analyzed using qualitative coding techniques and the results were checked and validated with participants through member checking. Our findings suggest that it is necessary to find balance between the positive effects on work variety and learning opportunities, and negative effects on cognitive workload and performance. Further, the lack of feedback resulting from constant movement among projects and teams may have a negative impact on performance feedback. We conclude that job rotation is an important organizational practice with important positive results. However, managers must be aware of potential negative effects and deploy tactics to balance them. We discuss such tactics in this article.","conference":"IEEE","terms":"Multiskilling;Software;Software engineering;Companies;Interviews;Context,organisational aspects;software development management,job rotation theory;software engineering;organizational practice;work related factors;qualitative case study;qualitative coding techniques;cognitive workload;cognitive performance","keywords":"job rotation;software teams;performance;software engineering;case study","startPage":"971","endPage":"981","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886972","citationCount":2,"referenceCount":39,"year":2016,"authors":"R. E. S. Santos; F. Q. B. da Silva; C. V. C. de Magalhães; C. V. F. Monteiro","affiliations":"Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil; Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil; Centro de Inf., Univ. Fed. de Pernambuco, Recife, Brazil; Dept. de Estatistica e Inf., Univ. Fed. Rural de Pernambuco, Recife, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d364f"},"title":"Synthesizing Framework Models for Symbolic Execution","abstract":"Symbolic execution is a powerful program analysis technique, but it is difficult to apply to programs built using frameworks such as Swing and Android, because the framework code itself is hard to symbolically execute. The standard solution is to manually create a framework model that can be symbolically executed, but developing and maintaining a model is difficult and error-prone. In this paper, we present Pasket, a new system that takes a first step toward automatically generating Java framework models to support symbolic execution. Pasket's focus is on creating models by instantiating design patterns. Pasket takes as input class, method, and type information from the framework API, together with tutorial programs that exercise the framework. From these artifacts and Pasket's internal knowledge of design patterns, Pasket synthesizes a framework model whose behavior on the tutorial programs matches that of the original framework. We evaluated Pasket by synthesizing models for subsets of Swing and Android. Our results show that the models derived by Pasket are sufficient to allow us to use off-the-shelf symbolic execution tools to analyze Java programs that rely on frameworks.","conference":"IEEE","terms":"Tutorials;Java;Androids;Humanoid robots;Observers;Analytical models;Synthesizers,Java;object-oriented programming;program diagnostics,symbolic execution;program analysis;PASKET;Java framework models;design pattern instantiation;API;Swing;Android","keywords":"Program Synthesis;Framework Model;Symbolic Execution;Sketch","startPage":"156","endPage":"167","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886900","citationCount":3,"referenceCount":40,"year":2016,"authors":"J. Jeon; X. Qiu; J. Fetter-Degges; J. S. Foster; A. Solar-Lezama","affiliations":"Univ. of Maryland, College Park, MD, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA; Univ. of Maryland, College Park, MD, USA; Univ. of Maryland, College Park, MD, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3650"},"title":"MobiPlay: A Remote Execution Based Record-and-Replay Tool for Mobile Applications","abstract":"The record-and-replay approach for software testing is important and valuable for developers in designing mobile applications. However, the existing solutions for recording and replaying Android applications are far from perfect. When considering the richness of mobile phones' input capabilities including touch screen, sensors, GPS, etc., existing approaches either fall short of covering all these different input types, or require elevated privileges that are not easily attained and can be dangerous. In this paper, we present a novel system, called MobiPlay, which aims to improve record-and-replay testing. By collaborating between a mobile phone and a server, we are the first to capture all possible inputs by doing so at the application layer, instead of at the Android framework layer or the Linux kernel layer, which would be infeasible without a server. MobiPlay runs the to-be-tested application on the server under exactly the same environment as the mobile phone, and displays the GUI of the application in real time on a thin client application installed on the mobile phone. From the perspective of the mobile phone user, the application appears to be local. We have implemented our system and evaluated it with tens of popular mobile applications showing that MobiPlay is efficient, flexible, and comprehensive. It can record all input data, including all sensor data, all touchscreen gestures, and GPS. It is able to record and replay on both the mobile phone and the server. Furthermore, it is suitable for both white-box and black-box testing.","conference":"IEEE","terms":"Servers;Smart phones;Mobile communication;Sensors;Testing;Operating systems,graphical user interfaces;mobile computing;program testing,MobiPlay tool;remote execution;record-and-replay tool;mobile applications;software testing;Android applications;mobile phone;Linux kernel layer;GUI;graphical user interface;sensor data;touchscreen gestures;white-box testing;black-box testing","keywords":"","startPage":"571","endPage":"582","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886935","citationCount":14,"referenceCount":36,"year":2016,"authors":"Z. Qin; Y. Tang; E. Novak; Q. Li","affiliations":"Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA; Dept. of Comput. Sci., Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3651"},"title":"Guiding Dynamic Symbolic Execution toward Unverified Program Executions","abstract":"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.","conference":"IEEE","terms":"Testing;Instruments;Redundancy;Aerospace electronics;Conferences;Software engineering;Performance analysis,program testing,dynamic symbolic execution;unverified program execution;program error detection;program execution;arithmetic overflow;automatic test case generation;code instrumentation;Clousot;Pex tool","keywords":"static analysis;program verification;testing;partial verification;dynamic symbolic execution","startPage":"144","endPage":"155","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899","citationCount":5,"referenceCount":41,"year":2016,"authors":"M. Christakis; P. Müller; V. Wüstholz","affiliations":"Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland; Dept. of Comput. Sci., ETH Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3652"},"title":"Dataflow Tunneling: Mining Inter-Request Data Dependencies for Request-Based Applications","abstract":"Request-based applications, e.g., most server-side applications, expose services to users in a request-based paradigm, in which requests are served by request-handler methods. An important task for request-based applications is inter-request analysis, which analyzes request-handler methods that are related by inter-request data dependencies together. However, in the request-based paradigm, data dependencies between related request-handler methods are implicitly established by the underlying frameworks that execute these methods. As a result, existing analysis tools are usually limited to the scope of each single method without the knowledge of dependencies between different methods. In this paper, we design an approach called dataflow tunneling to capture inter-request data dependencies from concrete application executions and produce data-dependency specifications. Our approach answers two key questions: (1) what request-handler methods have data dependencies and (2) what these data dependencies are. Our evaluation using applications developed with two representative and popular frameworks shows that our approach is general and accurate. We also present a characteristic study and a use case of cache tuning based on the mined specifications. We envision that our approach can provide key information to enable future inter-request analysis techniques.","conference":"IEEE","terms":"Data models;Java;Tunneling;Analytical models;Object recognition;Prototypes;Databases,cache storage;data mining,request-based applications;request-based paradigm;analyzes request-handler methods;single method;dataflow tunneling;concrete application executions;data-dependency specifications;future inter-request analysis techniques;inter-request data dependency mining","keywords":"web application;request based applications;web frameworks;inter request analysis;tracing","startPage":"586","endPage":"597","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453128","citationCount":0,"referenceCount":59,"year":2018,"authors":"X. Yu; G. Jin","affiliations":"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3653"},"title":"Statistical Learning of API Fully Qualified Names in Code Snippets of Online Forums","abstract":"Software developers often make use of the online forums such as StackOverflow to learn how to use software libraries and their APIs. However, the code snippets in such a forum often contain undeclared, ambiguous, or largely unqualified external references. Such declaration ambiguity and external reference ambiguity present challenges for developers in learning to correctly use the APIs. In this paper, we propose StatType, a statistical approach to resolve the fully qualified names (FQNs) for the API elements in such code snippets. Unlike existing approaches that are based on heuristics, StatType has two well-integrated factors. We first learn from a large training code corpus the FQNs that often co-occur. Then, to derive the FQN for an API name in a code snippet, we use that knowledge and leverage the context consisting of neighboring API names. To realize those factors, we treat the problem as statistical machine translation from source code with partially qualified names to source code with FQNs of the APIs. Our empirical evaluation on real-world code and StackOverflow posts shows that StatType achieves very high accuracy with 97.6% precision and 96.7% recall, which is 16.5% relatively higher than the state-of-the-art approach.","conference":"IEEE","terms":"Computational modeling;Software libraries;Training;Software;Tools,application program interfaces;Internet;language translation;learning (artificial intelligence);software libraries;statistical analysis,machine translation;StackOverflow posts;API fully qualified names;statistical learning;partially qualified names;source code;API name;training code corpus;API elements;FQNs;statistical approach;StatType;external reference;declaration ambiguity;code snippet;software libraries;online forums;software developers","keywords":"Type Resolution;Type Inference;Type Annotations;Partial Program Analysis;Statistical Machine Translation;Naturalness;Big Code","startPage":"632","endPage":"642","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453132","citationCount":3,"referenceCount":0,"year":2018,"authors":"H. Phan; H. A. Nguyen; N. M. Tran; L. H. Truong; A. T. Nguyen; T. N. Nguyen","affiliations":"Iowa State Univ., Ames, IA, USA; Iowa State Univ., Ames, IA, USA; Univ. of Texas at Dallas, Dallas, TX, USA; Univ. of Texas at Dallas, Dallas, TX, USA; NA; Univ. of Texas at Dallas, Dallas, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3654"},"title":"StubDroid: Automatic Inference of Precise Data-Flow Summaries for the Android Framework","abstract":"Smartphone users suffer from insucient information on how commercial as well as malicious apps handle sensitive data stored on their phones. Automated taint analyses address this problem by allowing users to detect and investigate how applications access and handle this data. A current problem with virtually all those analysis approaches is, though, that they rely on explicit models of the Android runtime library. In most cases, the existence of those models is taken for granted, despite the fact that the models are hard to come by: Given the size and evolution speed of a modern smartphone operating system it is prohibitively expensive to derive models manually from code or documentation. In this work, we therefore present StubDroid, the first fully automated approach for inferring precise and efficient library models for taint-analysis problems. StubDroid automatically constructs these summaries from a binary distribution of the library. In our experiments, we use StubDroid-inferred models to prevent the static taint analysis FlowDroid from having to re-analyze the Android runtime library over and over again for each analyzed app. As the results show, the models make it possible to analyze apps in seconds whereas most complete re-analyses would time out after 30 minutes. Yet, StubDroid yields comparable precision. In comparison to manually crafted summaries, StubDroid's cause the analysis to be more precise and to use less time and memory.","conference":"IEEE","terms":"Libraries;Androids;Humanoid robots;Analytical models;Software engineering;Smart phones;Operating systems,Android (operating system);program diagnostics;smart phones,StubDroid approach;StubDroid-inferred models;smart phone operating system;Android runtime library;automated taint analysis;sensitive data handling;Android framework;precise data-flow summaries","keywords":"Static analysis;summary;library;framework model;model inference","startPage":"725","endPage":"735","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886831","citationCount":6,"referenceCount":22,"year":2016,"authors":"S. Arzt; E. Bodden","affiliations":"Secure Software Eng. Group, Tech. Univ. Darmstadt, Darmstadt, Germany; Secure Software Eng. Group, Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3655"},"title":"EnMobile: Entity-Based Characterization and Analysis of Mobile Malware","abstract":"Modern mobile malware tend to conduct their malicious exploits through sophisticated patterns of interactions that involve multiple entities, e.g., the mobile platform, human users, and network locations. Such malware often evade the detection by existing approaches due to their limited expressiveness and accuracy in characterizing and detecting these malware. To address these issues, in this paper, we recognize entities in the environment of an app, the app's interactions with such entities, and the provenance of these interactions, i.e., the intent and ownership of each interaction, as the key to comprehensively characterizing modern mobile apps, and mobile malware in particular. With this insight, we propose a novel approach named EnMobile including a new entity-based characterization of mobile-app behaviors, and corresponding static analyses, to accurately characterize an app's interactions with entities. We implement EnMobile and provide a practical application of EnMobile in a signature-based scheme for detecting mobile malware. We evaluate EnMobile on a set of 6614 apps consisting of malware from Genome and Drebin along with benign apps from Google Play. Our results show that EnMobile detects malware with substantially higher precision and recall than four state-of-the-art approaches, namely Apposcopy, Drebin, MUDFLOW, and AppContext.","conference":"IEEE","terms":"Malware;Servers;Security;Databases;Static analysis;Feature extraction,invasive software;mobile computing;program diagnostics,mobile-app behaviors;EnMobile;benign apps;entity-based characterization;modern mobile malware;mobile platform;modern mobile apps","keywords":"Program Analysis;Mobile Security","startPage":"384","endPage":"394","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453096","citationCount":5,"referenceCount":0,"year":2018,"authors":"W. Yang; M. Prasad; T. Xie","affiliations":"NA; Fujitsu Labs. of America, Sunnyvale, CA, USA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3656"},"title":"The Sky Is Not the Limit: Multitasking Across GitHub Projects","abstract":"Software development has always inherently required multitasking: developers switch between coding, reviewing, testing, designing, and meeting with colleagues. The advent of software ecosystems like GitHub has enabled something new: the ability to easily switch between projects. Developers also have social incentives to contribute to many projects; prolific contributors gain social recognition and (eventually) economic rewards. Multitasking, however, comes at a cognitive cost: frequent context-switches can lead to distraction, sub-standard work, and even greater stress. In this paper, we gather ecosystem-level data on a group of programmers working on a large collection of projects. We develop models and methods for measuring the rate and breadth of a developers' context-switching behavior, and we study how context-switching affects their productivity. We also survey developers to understand the reasons for and perceptions of multitasking. We find that the most common reason for multitasking is interrelationships and dependencies between projects. Notably, we find that the rate of switching and breadth (number of projects) of a developer's work matter. Developers who work on many projects have higher productivity if they focus on few projects per day. Developers that switch projects too much during the course of a day have lower productivity as they work on more projects overall. Despite these findings, developers perceptions of the benefits of multitasking are varied.","conference":"IEEE","terms":"Multitasking;Switches;Productivity;Software;Encoding;Context;Interrupters,project management;software development management,GitHub projects;software development;multitasking;ecosystem-level data gathering;context-switching behavior","keywords":"Multitasking;GitHub;productivity","startPage":"994","endPage":"1005","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886974","citationCount":15,"referenceCount":60,"year":2016,"authors":"B. Vasilescu; K. Blincoe; Q. Xuan; C. Casalnuovo; D. Damian; P. Devanbu; V. Filkov","affiliations":"Dept. Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Dept. Electr. \u0026 Comput. Eng., Univ. of Auckland, Auckland, New Zealand; Dept. Autom., Zhejiang Univ. of Technol., Hangzhou, China; Dept. Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Dept. Comput. Sci., Univ. of Victoria, Victoria, BC, Canada; Dept. Comput. Sci., Univ. of California, Davis, Davis, CA, USA; Dept. Comput. Sci., Univ. of California, Davis, Davis, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3657"},"title":"An Empirical Comparison of Compiler Testing Techniques","abstract":"Compilers, as one of the most important infrastructure of today's digital world, are expected to be trustworthy. Different testing techniques are developed for testing compilers automatically. However, it is unknown so far how these testing techniques compared to each other in terms of testing effectiveness: how many bugs a testing technique can find within a time limit. In this paper, we conduct a systematic and comprehensive empirical comparison of three compiler testing techniques, namely, Randomized Differential Testing (RDT), a variant of RDT-Different Optimization Levels (DOL), and Equivalence Modulo Inputs (EMI). Our results show that DOL is more effective at detecting bugs related to optimization, whereas RDT is more effective at detecting other types of bugs, and the three techniques can complement each other to a certain degree. Furthermore, in order to understand why their effectiveness differs, we investigate three factors that influence the effectiveness of compiler testing, namely, efficiency, strength of test oracles, and effectiveness of generated test programs. The results indicate that all the three factors are statistically significant, and efficiency has the most significant impact.","conference":"IEEE","terms":"Computer bugs;Testing;Program processors;Electromagnetic interference;Optimization;History,program compilers;program testing,compiler testing techniques;testing effectiveness;randomized differential testing;RDT;different optimization levels;DOL;equivalence modulo inputs;EMI;bug detection;test oracles;test program generation","keywords":"","startPage":"180","endPage":"190","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886902","citationCount":9,"referenceCount":32,"year":2016,"authors":"J. Chen; W. Hu; D. Hao; Y. Xiong; H. Zhang; L. Zhang; B. Xie","affiliations":"NA; NA; NA; NA; Microsoft Res., Beijing, China; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3658"},"title":"Software Protection on the Go: A Large-Scale Empirical Study on Mobile App Obfuscation","abstract":"The prosperity of smartphone markets has raised new concerns about software security on mobile platforms, leading to a growing demand for effective software obfuscation techniques. Due to various differences between the mobile and desktop ecosystems, obfuscation faces both technical and non-technical challenges when applied to mobile software. Although there have been quite a few software security solution providers launching their mobile app obfuscation services, it is yet unclear how real-world mobile developers perform obfuscation as part of their software engineering practices. Our research takes a first step to systematically studying the deployment of software obfuscation techniques in mobile software development. With the help of an automated but coarse-grained method, we computed the likelihood of an app being obfuscated for over a million app samples crawled from Apple App Store. We then inspected the top 6600 instances and managed to identify 601 obfuscated versions of 539 iOS apps. By analyzing this sample set with extensive manual effort, we made various observations that reveal the status quo of mobile obfuscation in the real world, providing insights into understanding and improving the situation of software protection on mobile platforms.","conference":"IEEE","terms":"Software;Security;Androids;Humanoid robots;Libraries;Software protection;Manuals,computer crime;security of data;smart phones;software engineering,software protection;large-scale empirical study;smartphone markets;mobile platforms;effective software obfuscation techniques;mobile ecosystems;desktop ecosystems;nontechnical challenges;software security solution providers;mobile app obfuscation services;real-world mobile developers;mobile obfuscation;Apple App Store;million app samples;mobile software development;software engineering practices","keywords":"obfuscation;reverse engineering;mobile app;empirical study","startPage":"26","endPage":"36","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453059","citationCount":0,"referenceCount":0,"year":2018,"authors":"P. Wang; Q. Bao; L. Wang; S. Wang; Z. Chen; T. Wei; D. Wu","affiliations":"NA; NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911eeee8435e8e7d3659"},"title":"Neuro-Symbolic Program Corrector for Introductory Programming Assignments","abstract":"Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions.","conference":"IEEE","terms":"Syntactics;Maintenance engineering;Programming;Recurrent neural networks;Prediction algorithms;Semantics,computer aided instruction;educational courses;error handling;program debugging;program diagnostics;programming;recurrent neural nets;trees (mathematics),program repair techniques;neuro-symbolic program corrector;introductory programming assignment;automatic program correction;Abstract Syntax Trees;online courses;error location;syntax errors;constraint-based techniques;syntactically-fixed programs;buggy programs;syntax repairs;Recurrent Neural Network;constraint-based reasoning","keywords":"Neural Program Correction;Automated Feedback Generation;Neural guided search","startPage":"60","endPage":"70","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453063","citationCount":0,"referenceCount":0,"year":2018,"authors":"S. Bhatia; P. Kohli; R. Singh","affiliations":"Netaji Subhas Inst. of Technol., Delhi, India; Google Deepmind, London, UK; Microsoft Res., Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d365a"},"title":"Automated Test Suite Generation for Time-Continuous Simulink Models","abstract":"All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Interdisciplinary domains such as Cyber Physical Systems (CPSs) seek approaches that incorporate different modeling needs and usages. Specifically, the Simulink modeling platform greatly appeals to CPS engineers due to its seamless support for simulation and code generation. In this paper, we propose a test generation approach that is applicable to Simulink models built for both purposes of simulation and code generation. We define test inputs and outputs as signals that capture evolution of values over time. Our test generation approach is implemented as a meta-heuristic search algorithm and is guided to produce test outputs with diverse shapes according to our proposed notion of diversity. Our evaluation, performed on industrial and public domain models, demonstrates that: (1) In contrast to the existing tools for testing Simulink models that are only applicable to a subset of code generation models, our approach is applicable to both code generation and simulation Simulink models. (2) Our new notion of diversity for output signals outperforms random baseline testing and an existing notion of signal diversity in revealing faults in Simulink models. (3) The fault revealing ability of our test generation approach outperforms that of the Simulink Design Verifier, the only testing toolbox for Simulink.","conference":"IEEE","terms":"Software packages;Computational modeling;Fuels;Testing;Mathematical model;Shape,automatic testing;program compilers;program testing;search problems,automated test suite generation;time-continuous Simulink models;code generation;test inputs;test outputs;meta-heuristic search algorithm;industrial domain models;public domain models;simulation Simulink models;signal diversity;fault revealing ability","keywords":"Simulink models;Software testing;Time-continuous behaviors;Search-based software testing;Output diversity;Signal features;Structural coverage;Simulink Design Verifier (SLDV)","startPage":"595","endPage":"606","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886937","citationCount":10,"referenceCount":61,"year":2016,"authors":"R. Matinnejad; S. Nejati; L. C. Briand; T. Bruckmann","affiliations":"SnT Centre, Univ. of Luxembourg, Luxembourg, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg, Luxembourg; SnT Centre, Univ. of Luxembourg, Luxembourg, Luxembourg; Delphi Automotive Syst., Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d365b"},"title":"A Comparison of 10 Sampling Algorithms for Configurable Systems","abstract":"Almost every software system provides configuration options to tailor the system to the target platform and application scenario. Often, this configurability renders the analysis of every individual system configuration infeasible. To address this problem, researchers have proposed a diverse set of sampling algorithms. We present a comparative study of 10 state-of-the-art sampling algorithms regarding their fault-detection capability and size of sample sets. The former is important to improve software quality and the latter to reduce the time of analysis. In a nutshell, we found that sampling algorithms with larger sample sets are able to detect higher numbers of faults, but simple algorithms with small sample sets, such as most-enabled-disabled, are the most efficient in most contexts. Furthermore, we observed that the limiting assumptions made in previous work influence the number of detected faults, the size of sample sets, and the ranking of algorithms. Finally, we have identified a number of technical challenges when trying to avoid the limiting assumptions, which questions the practicality of certain sampling algorithms.","conference":"IEEE","terms":"Algorithm design and analysis;Software algorithms;Software systems;Linux;Kernel;Fault detection;Software quality,program diagnostics;software quality,configurable systems;sampling algorithms;fault-detection capability;software quality;software system configuration","keywords":"Configurable Systems;Sampling Algorithms;Configuration-Related Faults","startPage":"643","endPage":"654","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886941","citationCount":13,"referenceCount":58,"year":2016,"authors":"F. Medeiros; C. Kästner; M. Ribeiro; R. Gheyi; S. Apel","affiliations":"Fed. Univ. of Campina Grande, Campina Grande, Brazil; Carnegie Mellon Univ. Pittsburgh, Pittsburgh, PA, USA; Fed. Univ. of Alagoas, Maceio, Brazil; Fed. Univ. of Campina Grande, Campina Grande, Brazil; Univ. Passau, Passau, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d365c"},"title":"Deuce: A Lightweight User Interface for Structured Editing","abstract":"We present a structure-aware code editor, called Deuce, that is equipped with direct manipulation capabilities for invoking automated program transformations. Compared to traditional refactoring environments, Deuce employs a direct manipulation interface that is tightly integrated within a text-based editing workflow. In particular, Deuce draws (i) clickable widgets atop the source code that allow the user to structurally select the unstructured text for subexpressions and other relevant features, and (ii) a lightweight, interactive menu of potential transformations based on the current selections. We implement and evaluate our design with mostly standard transformations in the context of a small functional programming language. A controlled user study with 21 participants demonstrates that structural selection is preferred to a more traditional text-selection interface and may be faster overall once users gain experience with the tool. These results accord with Deuce's aim to provide human-friendly structural interactions on top of familiar text-based editing.","conference":"IEEE","terms":"Tools;Syntactics;Human computer interaction;Task analysis;Software engineering;Visualization,functional programming;human computer interaction;interactive systems;source code (software);text analysis;text editing;user interfaces,direct manipulation interface;clickable widgets;source code;unstructured text;lightweight menu;interactive menu;functional programming language;structural selection;human-friendly structural interactions;lightweight user interface;structure-aware code editor;automated program transformations;Deuce;Structured Editing;refactoring environments;text-based editing workflow;text-selection interface","keywords":"Structured Editing;Refactoring;Sketch-n-Sketch","startPage":"654","endPage":"664","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453134","citationCount":0,"referenceCount":0,"year":2018,"authors":"B. Hempel; J. Lubin; G. Lu; R. Chugh","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d365d"},"title":"Quality Experience: A Grounded Theory of Successful Agile Projects without Dedicated Testers","abstract":"Context: While successful conventional software development regularly employs separate testing staff, there are successful agile teams with as well as without separate testers. Question: How does successful agile development work without separate testers? What are advantages and disadvantages? Method: A case study, based on Grounded Theory evaluation of interviews and direct observation of three agile teams; one having separate testers, two without. All teams perform long-term development of parts of e-business web portals. Results: Teams without testers use a quality experience work mode centered around a tight field-use feedback loop, driven by a feeling of responsibility, supported by test automation, resulting in frequent deployments. Conclusion: In the given domain, hand-overs to separate testers appear to hamper the feedback loop more than they contribute to quality, so working without testers is preferred. However, Quality Experience is achievable only with modular architectures and in suitable domains.","conference":"IEEE","terms":"Software;Testing;Interviews;Portals;Context;Companies;Data collection,program testing;quality of experience;software architecture;software prototyping,quality experience;agile projects;agile development;grounded theory evaluation;e-business Web portals;feedback loop;modular architectures","keywords":"Agile Development;Software Quality Assurance;Industrial Case Study;Grounded Theory Methodology;Testing","startPage":"1017","endPage":"1027","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886976","citationCount":1,"referenceCount":27,"year":2016,"authors":"L. Prechelt; H. Schmeisky; F. Zieris","affiliations":"Freie Univ. Berlin, Berlin, Germany; Freie Univ. Berlin, Berlin, Germany; Freie Univ. Berlin, Berlin, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d365e"},"title":"Journal First Program Committee of ICSE 2019","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"41","endPage":"41","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812104","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d365f"},"title":"Sponsors and Supporters of ICSE 2019","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"44","endPage":"44","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812036","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3660"},"title":"Message from the Web Chairs of ICSE 2019","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"29","endPage":"29","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812087","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3661"},"title":"Organizing Committee of ICSE 2019","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"31","endPage":"36","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811899","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3662"},"title":"Message from the Social Media Chairs of ICSE 2019","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"30","endPage":"30","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812112","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3663"},"title":"Program Board of ICSE 2019","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"37","endPage":"37","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812078","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3664"},"title":"Message from the Artifact Evaluation Chairs of ICSE 2019","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"28","endPage":"28","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811926","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3665"},"title":"Program Committee of ICSE 2019","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"38","endPage":"40","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812094","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3666"},"title":"Message from the Program Chairs of ICSE 2019","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"24","endPage":"25","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812098","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3667"},"title":"Message from the Workshop Chairs of ICSE 2019","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"27","endPage":"27","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812030","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3668"},"title":"Artifact Evaluation Committee of ICSE 2019","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"43","endPage":"43","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811985","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3669"},"title":"Message from the Journal-First Chair of ICSE 2019","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"26","endPage":"26","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811919","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d366a"},"title":"Workshops Program Committee of ICSE 2019","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"42","endPage":"42","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811958","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d366b"},"title":"Message from the ICSE 2019 General Chair","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"22","endPage":"23","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811895","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d366c"},"title":"Organizing Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xix","endPage":"xxii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985641","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d366d"},"title":"[Publisher's information]","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"786","endPage":"786","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985714","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d366e"},"title":"Foreword","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"xvi","endPage":"xviii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985640","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d366f"},"title":"Program Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxiii","endPage":"xxvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985642","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3670"},"title":"Publicity Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxii","endPage":"xxxii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886882","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3671"},"title":"Workshops Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxiv","endPage":"xxxiv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886884","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3672"},"title":"Sponsors and Supporters","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"45","endPage":"45","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453054","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3673"},"title":"Web Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxi","endPage":"xxxi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886881","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3674"},"title":"Publications Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxiii","endPage":"xxxiii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886883","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3675"},"title":"Message from the Chairs - Volume 1","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"xiv","endPage":"xvii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194551","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3676"},"title":"Committees - Volume 2","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxviii","endPage":"xlvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202940","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3677"},"title":"[Publisher's information - Vol 1]","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"964","endPage":"964","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194642","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3678"},"title":"[Publisher's information - Vol 2]","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"1032","endPage":"1032","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203160","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3679"},"title":"Committees - Volume 1","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xviii","endPage":"xxxvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194552","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d367a"},"title":"Message from the General and Program Chairs - Volume 2","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"xxiv","endPage":"xxvii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202939","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d367b"},"title":"[Publisher's information]","abstract":"The following topics are dealt with: program testing; software engineering; software maintenance; program debugging; program diagnostics; public domain software; Java; software quality; mobile computing; and application program interfaces.","conference":"IEEE","terms":",software engineering,program testing;software engineering;software maintenance;program debugging;program diagnostics;public domain software;Java;software quality;mobile computing;application program interfaces","keywords":"","startPage":"1264","endPage":"1264","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453209","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d367c"},"title":"Program Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxviii","endPage":"xxx","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886880","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d367d"},"title":"Organizing Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxiii","endPage":"xxvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886878","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d367e"},"title":"Message from the Journal First Chair","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"28","endPage":"28","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453050","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d367f"},"title":"Message from the General Chair","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"23","endPage":"25","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453048","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3680"},"title":"Program Board","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"33","endPage":"35","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453052","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3681"},"title":"Welcome Message from the Chairs","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"xvii","endPage":"xxii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886877","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3682"},"title":"Program Board","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxvii","endPage":"xxvii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886879","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3683"},"title":"Organizing Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"29","endPage":"32","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453051","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3684"},"title":"Message from the Program Chairs","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"26","endPage":"27","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453049","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3685"},"title":"Program Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"36","endPage":"44","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453053","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3686"},"title":"[Title page i]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1","endPage":"1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812055","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3687"},"title":"[Title page iii]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812089","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3688"},"title":"[Front cover]","abstract":"Presents the front cover or splash screen of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"c1","endPage":"c1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985635","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3689"},"title":"[Title page iii]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"iii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985637","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d368a"},"title":"Table of contents","abstract":"The following topics are dealt with: traceability; documentation; refactoring; recommendation systems; software process; search-based software engineering; Web applications; concurrency; mobile application security; debugging; program synthesis; program repair; mining software repositories; program analysis; safety; privacy; development tools; testing; defect prediction; formal methods; and software evolution.","conference":"IEEE","terms":",concurrency (computers);Internet;maintenance engineering;mobile computing;program diagnostics;programming;recommender systems;security;software engineering;testing,software evolution;formal methods;defect prediction;testing;development tools;privacy;safety;program analysis;mining software repositories;program repair;program synthesis;debugging;mobile application security;concurrency;Web applications;search-based software engineering;software process;recommendation systems;refactoring;documentation;traceability","keywords":"","startPage":"v","endPage":"xv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985639","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d368b"},"title":"Additional Reviewers","abstract":"The conference offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxvii","endPage":"xxviii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985643","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d368c"},"title":"[Title page i]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"i","endPage":"i","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985636","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d368d"},"title":"[Copyright notice]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"iv","endPage":"iv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985638","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d368e"},"title":"Author index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"781","endPage":"784","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985713","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d368f"},"title":"Sponsors and Benefactors","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"xxix","endPage":"xxxii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985644","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3690"},"title":"Author index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1269","endPage":"1275","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812142","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3691"},"title":"[Copyright notice]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"4","endPage":"4","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811962","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3692"},"title":"Author index - Vol 2","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1023","endPage":"1030","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203159","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3693"},"title":"[Copyright notice - Vol 1]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"iv","endPage":"iv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194549","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3694"},"title":"Additional Reviewers - Volume 1","abstract":"The conference offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxvii","endPage":"xxxix","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194553","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3695"},"title":"[Front cover - Vol 2]","abstract":"Presents the front cover of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"C4","endPage":"C4","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202934","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3696"},"title":"[Title page iii - Vol 2]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"iii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202936","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b911feee8435e8e7d3697"},"title":"Table of contents - Vol 2","abstract":"Presents the table of contents/splash page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"v","endPage":"xxiii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202938","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d3698"},"title":"Sponsors and Supporters - Volume 2","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"l","endPage":"li","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202942","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d3699"},"title":"[Front cover - Vol 1]","abstract":"Presents the front cover of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"C4","endPage":"C4","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194546","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d369a"},"title":"Table of contents - Vol 1","abstract":"Presents the table of contents/splash page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"v","endPage":"xiii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194550","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d369b"},"title":"[Title page iii - Vol 1]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"iii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194548","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d369c"},"title":"Sponsors and Supporters - Volume 1","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"xl","endPage":"xli","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194554","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d369d"},"title":"[Title page i - Vol 2]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"i","endPage":"i","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202935","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d369e"},"title":"[Copyright notice - Vol 2]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"iv","endPage":"iv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202937","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d369f"},"title":"Additional Reviewers - Volume 2","abstract":"The conference offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"xlvii","endPage":"xlvix","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202941","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a0"},"title":"Author index - Vol 1","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"959","endPage":"962","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194641","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a1"},"title":"Table of contents","abstract":"The following topics are dealt with: program testing; program debugging; program diagnostics; Java; software maintenance; public domain software; learning (artificial intelligence); application program interfaces; data mining; mobile computing.","conference":"IEEE","terms":",Java;program debugging;program diagnostics;program testing;software maintenance,program testing;program debugging;program diagnostics;Java;software maintenance;public domain software;learning (artificial intelligence);application program interfaces;data mining;mobile computing","keywords":"","startPage":"5","endPage":"21","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812059","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a2"},"title":"[Title page iii]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"iii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886874","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a3"},"title":"Table of contents","abstract":"Presents the table of contents/splash page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"v","endPage":"xvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886876","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a4"},"title":"Sponsors and supporters","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxvii","endPage":"xxxix","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886886","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a5"},"title":"[Title page iii]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453046","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a6"},"title":"[Copyright notice]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"iv","endPage":"iv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886875","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a7"},"title":"Author index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1193","endPage":"1197","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886991","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a8"},"title":"Additional Reviewers","abstract":"The publication offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxxv","endPage":"xxxv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886885","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36a9"},"title":"[Title page i]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1","endPage":"1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453045","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36aa"},"title":"[Title page i - Vol 1]","abstract":"The following topics are dealt with: software testing; mobile applications; software maintenance; adaptive systems; highly configurable systems; regression testing; data privacy; data security; variability-aware refactoring; data mining; analysis infrastructure; symbolic execution; human factors; organizational factors; prediction models; API; software specification and software verification.","conference":"IEEE","terms":",application program interfaces;data mining;data privacy;formal specification;groupware;human factors;mobile computing;program testing;program verification;security of data;software maintenance;statistical testing;symbol manipulation,software testing;mobile applications;software maintenance;adaptive systems;highly configurable systems;regression testing;data privacy;data security;variability-aware refactoring;data mining;analysis infrastructure;symbolic execution;human factors;organizational factors;prediction models;API;software specification;software verification","keywords":"","startPage":"i","endPage":"i","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194547","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ab"},"title":"[Title page i]","abstract":"The following topics are dealt with: Android; software performance; symbolic execution; compilers and emerging trends; energy profiles; open source software; defect prediction; program synthesis; API; code smells; software architecture; software testing; software effort estimation; search algorithms; repair and model synthesis; and software product lines.","conference":"IEEE","terms":",Android (operating system);application program interfaces;power aware computing;program compilers;program diagnostics;program testing;public domain software;search problems;software architecture;software maintenance;software performance evaluation;software product lines;symbol manipulation,Android;software performance;symbolic execution;compilers;energy profiles;open source software;defect prediction;program synthesis;API;code smells;software architecture;software testing;software effort estimation;search algorithms;software maintenance;software product lines;model synthesis","keywords":"","startPage":"i","endPage":"i","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886873","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ac"},"title":"Author Index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1255","endPage":"1262","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453208","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ad"},"title":"Table of contents","abstract":"Presents the table of contents/splash page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"5","endPage":"22","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453047","citationCount":0,"referenceCount":0,"year":2018,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ae"},"title":"Keynotes","abstract":"Provides an abstract for each of the keynote presentations and may include a brief professional biography of each presenter. The complete presentations were not made available for publication as part of the conference proceedings.","conference":"IEEE","terms":"","keywords":"","startPage":"36","endPage":"38","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952191","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36af"},"title":"Big problems in industry (panel)","abstract":"Software Engineering in practice deals with scale in a variety of dimensions. We build large scale systems operating on vast amount of data. We have millions of customers with billions of queries and transactions. We have distributed teams making thousands of changes, running millions of tests and releasing multiple times per day. These dimensions of scale interact to provide challenges for software development tools and processes. The panelists will describe the challenging aspects of scale in their specific problem domains and discuss which software engineering methods work and which leave room for improvement.","conference":"IEEE","terms":"","keywords":"","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693060","citationCount":1,"referenceCount":0,"year":2013,"authors":"J. Penix","affiliations":"Google, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b0"},"title":"Toward Practical Automatic Program Repair","abstract":"Automated program repair (APR) reduces the burden of debugging by directly suggesting likely fixes for the bugs. We believe scalability, applicability, and accurate patch validation are among the main challenges for building practical APR techniques that the researchers in this area are dealing with. In this paper, we describe the steps that we are taking toward addressing these challenges.","conference":"IEEE","terms":"","keywords":"Program Repair;JVM Bytecode;Mutation Testing","startPage":"1262","endPage":"1264","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952355","citationCount":0,"referenceCount":54,"year":2019,"authors":"A. Ghanbari","affiliations":"University of Texas at Dallas","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b1"},"title":"TsmartGP: A Tool for Finding Memory Defects with Pointer Analysis","abstract":"Precise pointer analysis is desired since it is a core technique to find memory defects. There are several dimensions of pointer analysis precision, flow sensitivity, context sensitivity, field sensitivity and path sensitivity. For static analysis tools utilizing pointer analysis, considering all dimensions is difficult because the trade-off between precision and efficiency should be balanced. This paper presents TsmartGP, a static analysis tool for finding memory defects in C programs with a precise and efficient pointer analysis. The pointer analysis algorithm is flow, context, field, and quasi path sensitive. Control flow automatons are the key structures for our analysis to be flow sensitive. Function summaries are applied to get context information and elements of aggregate structures are handled to improve precision. Path conditions are used to filter unreachable paths. For efficiency, a multi-entry mechanism is proposed. Utilizing the pointer analysis algorithm, we implement a checker in TsmartGP to find uninitialized pointer errors in 13 real-world applications. Cppcheck and Clang Static Analyzer are chosen for comparison. The experimental results show that TsmartGP can find more errors while its accuracy is also higher than Cppcheck and Clang Static Analyzer.","conference":"IEEE","terms":"","keywords":"pointer analysis;uninitialized pointer;sensitivity;multi-entry","startPage":"1170","endPage":"1173","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952426","citationCount":0,"referenceCount":12,"year":2019,"authors":"Y. Wang; G. Chen; M. Zhou; M. Gu; J. Sun","affiliations":"Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b2"},"title":"Retrieve and Refine: Exemplar-Based Neural Comment Generation","abstract":"Code comment generation is a crucial task in the field of automatic software development. Most previous neural comment generation systems used an encoder-decoder neural network and encoded only information from source code as input. Software reuse is common in software development. However, this feature has not been introduced to existing systems. Inspired by the traditional IR-based approaches, we propose to use the existing comments of similar source code as exemplars to guide the comment generation process. Based on an open source search engine, we first retrieve a similar code and treat its comment as an exemplar. Then we applied a seq2seq neural network to conduct an exemplar-based comment generation. We evaluate our approach on a large-scale Java corpus, and experimental results demonstrate that our model significantly outperforms the state-of-the-art methods.","conference":"IEEE","terms":"","keywords":"comment generation;program comprehension;deep learning","startPage":"1250","endPage":"1252","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952536","citationCount":0,"referenceCount":21,"year":2019,"authors":"B. Wei","affiliations":"Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b3"},"title":"Humanoid: A Deep Learning-Based Approach to Automated Black-box Android App Testing","abstract":"Automated input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most black-box input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. We propose Humanoid, an automated black-box Android app testing tool based on deep learning. The key technique behind Humanoid is a deep neural network model that can learn how human users choose actions based on an app's GUI from human interaction traces. The learned model can then be used to guide test input generation to achieve higher coverage. Experiments on both open-source apps and market apps demonstrate that Humanoid is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators. Humanoid is open-sourced at https://github.com/yzygitzh/Humanoid and a demo video can be found at https://youtu.be/PDRxDrkyORs.","conference":"IEEE","terms":"","keywords":"software testing;automated test input generation;graphical user interface;deep learning;mobile application;Android","startPage":"1070","endPage":"1073","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952324","citationCount":0,"referenceCount":14,"year":2019,"authors":"Y. Li; Z. Yang; Y. Guo; X. Chen","affiliations":"Peking University; Peking University; Peking University; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b4"},"title":"A Machine Learning Based Approach to Identify SQL Injection Vulnerabilities","abstract":"This paper presents a machine learning classifier designed to identify SQL injection vulnerabilities in PHP code. Both classical and deep learning based machine learning algorithms were used to train and evaluate classifier models using input validation and sanitization features extracted from source code files. On ten-fold cross validations a model trained using Convolutional Neural Network(CNN) achieved the highest precision (95.4%), while a model based on Multilayer Perceptron(MLP) achieved the highest recall (63.7%) and the highest f-measure (0.746).","conference":"IEEE","terms":"","keywords":"Deep learning;prediction model;SQL injection;vulnerability","startPage":"1286","endPage":"1288","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952467","citationCount":0,"referenceCount":18,"year":2019,"authors":"K. Zhang","affiliations":"Wayne State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b5"},"title":"Test Automation and Its Limitations: A Case Study","abstract":"Modern embedded systems are increasingly complex and contain multiple software layers from BSP (Board Support Packages) to OS to middleware to AI (Artificial Intelligence) algorithms like perception and voice recognition. Integrations of inter-layer and intra-layer in embedded systems provide dedicated services such as taking a picture or movie-streaming. Accordingly, it gets more complicated to find out the root cause of a system failure. This industrial proposal describes a difficulty of testing embedded systems, and presents a case study in terms of integration testing.","conference":"IEEE","terms":"","keywords":"embedded system, software layer, integration test, test automation","startPage":"1208","endPage":"1209","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952292","citationCount":0,"referenceCount":19,"year":2019,"authors":"A. Sung; S. Kim; Y. Kim; Y. Jang; J. Kim","affiliations":"Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics; Samsung Electronics","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b6"},"title":"PHANTA: Diversified Test Code Quality Measurement for Modern Software Development","abstract":"Test code is becoming more essential to the modern software development process. However, practitioners often pay inadequate attention to key aspects of test code quality, such as bug detectability, maintainability and speed. Existing tools also typically report a single test code quality measure, such as code coverage, rather than a diversified set of metrics. To measure and visualize quality of test code in a comprehensive fashion, we developed an integrated test code analysis tool called Phanta. In this show case, we posit that the enhancement of test code quality is key to modernizing software development, and show how Phanta's techniques measure the quality using mutation analysis, test code clone detection, and so on. Further, we present an industrial case study where Phanta was applied to analyze test code in a real Fujitsu project, and share lessons learned from the case study.","conference":"IEEE","terms":"","keywords":"Software Testing;Test Code;Mutation Testing","startPage":"1206","endPage":"1207","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952538","citationCount":0,"referenceCount":6,"year":2019,"authors":"S. Tokumoto; K. Takayama","affiliations":"Fujitsu Laboratories Ltd.; Fujitsu Laboratories Ltd.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b7"},"title":"Enabling Continuous Improvement of a Continuous Integration Process","abstract":"Continuous Integration (CI) is a widely-adopted software engineering practice. Despite its undisputed benefits, like higher software quality and improved developer productivity, mastering CI is not easy. Among the several barriers when transitioning to CI, developers need to face a new type of software failures (i.e., build failures) that requires them to understand complex build logs. Even when a team has successfully introduced a CI culture, living up to its principles and improving the CI practice are also challenging. In my research, I want to provide developers with the right support for establishing CI and the proper recommendations for continuously improving their CI process.","conference":"IEEE","terms":"","keywords":"Continuous Integration, Build Failures, Anti patterns, Best Practices","startPage":"1246","endPage":"1249","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952505","citationCount":0,"referenceCount":26,"year":2019,"authors":"C. Vassallo","affiliations":"University of Zurich","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b8"},"title":"Better Development of Safety Critical Systems: Chinese High Speed Railway System Development Experience Report","abstract":"Ensure the correctness of safety critical systems play a key role in the worldwide software engineering. Over the past years we have been helping CASCO Signal Ltd which is the Chinese biggest high speed railway company to develop high speed railway safety critical software. We have also contributed specific methods for developing better safety critical software, including a search-based model-driven software development approach which uses SysML diagram refinement method to construct SysML model and SAT solver to check the model. This talk aims at sharing the challenge of developing high speed railway safety critical system, what we learn from develop a safety critical software with a Chinese high speed railway company, and we use ZC subsystem as a case study to show the systematic model-driven safety critical software development method.","conference":"IEEE","terms":"","keywords":"SysML;Formal Method;Model-Driven;SAT","startPage":"1216","endPage":"1217","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952294","citationCount":0,"referenceCount":7,"year":2019,"authors":"Z. Wu; J. Liu; X. Chen","affiliations":"East China Normal University; East China Normal University; R\u0026D Institute, CASCO Signal Ltd.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36b9"},"title":"User Preference Aware Multimedia Pricing Model using Game Theory and Prospect Theory for Wireless Communications","abstract":"Providing user satisfaction is a major concern for on-demand multimedia service providers and Internet carriers in Wireless Communications. Traditionally, user satisfaction was measured objectively in terms of throughput and latency. Nowadays the user satisfaction is measured using subjective metrices such as Quality of Experience (QoE). Recently, Smart Media Pricing (SMP) was conceptualized to price the QoE rather than the binary data traffic in multimedia services. In this research, we have leveraged the SMP concept to chalk up a QoE-sensitive multimedia pricing framework to allot price, based on the user preference and multimedia quality achieved by the customer. We begin by defining the utility equations for the provider-carrier and the customer. Then we translate the profit maximizing interplay between the parties into a two-stage Stackelberg game. We model the user personal preference using Prelec weighting function which follows the postulates Prospect Theory (PT). An algorithm has been developed to implement the proposed pricing scheme and determine the Nash Equilibrium. Finally, the proposed smart pricing scheme was tested against the traditional pricing method and simulation results indicate a significant boost in the utility achieved by the mobile customers.","conference":"IEEE","terms":"","keywords":"Network Economics;Game Theory;Prospect Theory;Quality of Experience (QoE);Smart Multimedia Pricing","startPage":"1265","endPage":"1267","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952218","citationCount":0,"referenceCount":11,"year":2019,"authors":"K. M. Kattiyan Ramamoorthy","affiliations":"San Diego State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ba"},"title":"Ares: Inferring Error Specifications through Static Analysis","abstract":"Misuse of APIs happens frequently due to misunderstanding of API semantics and lack of documentation. An important category of API-related defects is the error handling defects, which may result in security and reliability flaws. These defects can be detected with the help of static program analysis, provided that error specifications are known. The error specification of an API function indicates how the function can fail. Writing error specifications manually is time-consuming and tedious. Therefore, automatic inferring the error specification from API usage code is preferred. In this paper, we present Ares, a tool for automatic inferring error specifications for C code through static analysis. We employ multiple heuristics to identify error handling blocks and infer error specifications by analyzing the corresponding condition logic. Ares is evaluated on 19 real world projects, and the results reveal that Ares outperforms the state-of-the-art tool APEx by 37% in precision. Ares can also identify more error specifications than APEx. Moreover, the specifications inferred from Ares help find dozens of API-related bugs in well-known projects such as OpenSSL, among them 10 bugs are confirmed by developers. Video: https://youtu.be/nf1QnFAmu8Q. Repository: https://github.com/lc3412/Ares.","conference":"IEEE","terms":"","keywords":"Error Handling;Error Specification;Static Analysis","startPage":"1174","endPage":"1177","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952222","citationCount":0,"referenceCount":12,"year":2019,"authors":"C. Li; M. Zhou; Z. Gu; M. Gu; H. Zhang","affiliations":"Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36bb"},"title":"Empirical Study of Python Call Graph","abstract":"In recent years, the extensive application of the Python language has made its analysis work more and more valuable. Many static analysis algorithms need to rely on the construction of call graphs. In this paper, we did a comparative empirical analysis of several widely used Python static call graph tools both quantitatively and qualitatively. Experiments show that the existing Python static call graph tools have a large difference in the construction effectiveness, and there is still room for improvement.","conference":"IEEE","terms":"","keywords":"Python, call graph, empirical study, quantitative, qualitative","startPage":"1274","endPage":"1276","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952190","citationCount":0,"referenceCount":13,"year":2019,"authors":"L. Yu","affiliations":"Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36bc"},"title":"Verifying Determinism in Sequential Programs","abstract":"A nondeterministic program is difficult to test and debug. Nondeterminism occurs even in sequential programs: for example, iterating over the elements of a hash table can result in diverging test results. We have created a type system that can express whether a computation is deterministic, nondeterministic, or ordernondeterministic (like a set). While state-of-the-art nondeterminism detection tools unsoundly rely on observing run-time output, our approach soundly verifies determinism at compile time. Our implementation found previously-unknown nondeterminism errors in a 24,000 line program that had been heavily vetted by its developers.","conference":"IEEE","terms":"","keywords":"nondeterminism, type system, verification, specification","startPage":"1271","endPage":"1273","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952328","citationCount":0,"referenceCount":9,"year":2019,"authors":"R. Mudduluru","affiliations":"University of Washington","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36bd"},"title":"Grading-Based Test Suite Augmentation","abstract":"Enrollment in introductory programming (CS1) courses continues to surge and hundreds of CS1 students can produce thousands of submissions for a single problem, all requiring timely and accurate grading. One way that instructors can efficiently grade is to construct a custom instructor test suite that compares a student submission to a reference solution over randomly generated or hand-crafted inputs. However, such test suite is often insufficient, causing incorrect submissions to be marked as correct. To address this issue, we propose the Grasa (GRAding-based test Suite Augmentation) approach consisting of two techniques. Grasa first detects and clusters incorrect submissions by approximating their behavioral equivalence to each other. To augment the existing instructor test suite, Grasa generates a minimal set of additional tests that help detect the incorrect submissions. We evaluate our Grasa approach on a dataset of CS1 student submissions for three programming problems. Our preliminary results show that Grasa can effectively identify incorrect student submissions and minimally augment the instructor test suite.","conference":"IEEE","terms":"","keywords":"programming education;testing;clustering","startPage":"226","endPage":"229","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952332","citationCount":0,"referenceCount":14,"year":2019,"authors":"J. Osei-Owusu; A. Astorga; L. Butler; T. Xie; G. Challen","affiliations":"University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; Peking University; Ministry of Education; University of Illinois at Urbana-Champaign","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36be"},"title":"Crowdsourced Report Generation via Bug Screenshot Understanding","abstract":"Quality control is a challenge of crowdsourcing, especially in software testing. As some unprofessional workers involved, low-quality yieldings may hinder crowdsourced testing from satisfying requesters' requirements. Therefore, it is in demand to assist crowdworkers to raise bug report quality. In this paper, we propose a novel auxiliary method, namely CroReG, to generate crowdsourcing bug reports by analyzing bug screenshots uploaded by crowdworkers with image understanding techniques. The preliminary experiment results show that CroReG can effectively generate bug reports containing accurate screenshot captions and providing positive guidance for crowdworkers.","conference":"IEEE","terms":"","keywords":"Crowdsourced Testing;Mobile App Testing;Bug Report Generation","startPage":"1277","endPage":"1279","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952296","citationCount":0,"referenceCount":11,"year":2019,"authors":"S. Yu","affiliations":"Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36bf"},"title":"How Do API Selections Affect the Runtime Performance of Data Analytics Tasks?","abstract":"As data volume and complexity grow at an unprecedented rate, the performance of data analytics programs is becoming a major concern for developers. We observed that developers sometimes use alternative data analytics APIs to improve program runtime performance while preserving functional equivalence. However, little is known on the characteristics and performance attributes of alternative data analytics APIs. In this paper, we propose a novel approach to extracting alternative implementations that invoke different data analytics APIs to solve the same tasks. A key appeal of our approach is that it exploits the comparative structures in Stack Overflow discussions to discover programming alternatives. We show that our approach is promising, as 86% of the extracted code pairs were validated as true alternative implementations. In over 20% of these pairs, the faster implementation was reported to achieve a 10x or more speedup over its slower alternative. We hope that our study offers a new perspective of API recommendation and motivates future research on optimizing data analytics programs.","conference":"IEEE","terms":"","keywords":"API selection, data analytics, performance optimization, Stack Overflow","startPage":"665","endPage":"668","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952224","citationCount":0,"referenceCount":11,"year":2019,"authors":"Y. Tao; S. Tang; Y. Liu; Z. Xu; S. Qin","affiliations":"Shenzhen University; Shenzhen University; Southern University of Science and Technology; Shenzhen University; Teesside University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c0"},"title":"mCUTE: A Model-Level Concolic Unit Testing Engine for UML State Machines","abstract":"Model Driven Engineering (MDE) techniques raise the level of abstraction at which developers construct software. However, modern cyber-physical systems are becoming more prevalent and complex and hence software models that represent the structure and behavior of such systems still tend to be large and complex. These models may have numerous if not infinite possible behaviors, with complex communications between their components. Appropriate software testing techniques to generate test cases with high coverage rate to put these systems to test at the model-level (without the need to understand the underlying code generator or refer to the generated code) are therefore important. Concolic testing, a hybrid testing technique that benefits from both concrete and symbolic execution, gains a high execution coverage and is used extensively in the industry for program testing but not for software models. In this paper, we present a novel technique and its tool mCUTE1, an open source 2 model-level concolic testing engine. We describe the implementation of our tool in the context of Papyrus-RT, an open source Model Driven Engineering (MDE) tool based on UML-RT, and report the results of validating our tool using a set of benchmark models.","conference":"IEEE","terms":"","keywords":"Concolic Testing, MDE, State machines, Unit Testing, UML","startPage":"1182","endPage":"1185","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952438","citationCount":0,"referenceCount":22,"year":2019,"authors":"R. Ahmadi; K. Jahed; J. Dingel","affiliations":"Queen's University; Queen's University; Queen's University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c1"},"title":"LIRAT: Layout and Image Recognition Driving Automated Mobile Testing of Cross-Platform","abstract":"The fragmentation issue spreads over multiple mobile platforms such as Android, iOS, mobile web, and WeChat, which hinders test scripts from running across platforms. To reduce the cost of adapting scripts for various platforms, some existing tools apply conventional computer vision techniques to replay the same script on multiple platforms. However, because these solutions can hardly identify dynamic or similar widgets. It becomes difficult for engineers to apply them in practice. In this paper, we present an image-driven tool, namely LIRAT, to record and replay test scripts cross platforms, solving the problem of test script cross-platform replay for the first time. LIRAT records screenshots and layouts of the widgets, and leverages image understanding techniques to locate them in the replay process. Based on accurate widget localization, LIRAT supports replaying test scripts across devices and platforms. We employed LIRAT to replay 25 scripts from 5 application across 8 Android devices and 2 iOS devices. The results show that LIRAT can replay 88% scripts on Android platforms and 60% on iOS platforms. The demo can be found at: https: //github.com/YSC9848/LIRAT","conference":"IEEE","terms":"","keywords":"Cross-Platform Testing;Image Recognition;Record and Replay","startPage":"1066","endPage":"1069","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952513","citationCount":0,"referenceCount":11,"year":2019,"authors":"S. Yu; C. Fang; Y. Feng; W. Zhao; Z. Chen","affiliations":"Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c2"},"title":"Automatic Generation of Graphical User Interface Prototypes from Unrestricted Natural Language Requirements","abstract":"High-fidelity GUI prototyping provides a meaningful manner for illustrating the developers' understanding of the requirements formulated by the customer and can be used for productive discussions and clarification of requirements and expectations. However, high-fidelity prototypes are time-consuming and expensive to develop. Furthermore, the interpretation of requirements expressed in informal natural language is often error-prone due to ambiguities and misunderstandings. In this dissertation project, we will develop a methodology based on Natural Language Processing (NLP) for supporting GUI prototyping by automatically translating Natural Language Requirements (NLR) into a formal Domain-Specific Language (DSL) describing the GUI and its navigational schema. The generated DSL can be further translated into corresponding target platform prototypes and directly provided to the user for inspection. Most related systems stop after generating artifacts, however, we introduce an intelligent and automatic interaction mechanism that allows users to provide natural language feedback on generated prototypes in an iterative fashion, which accordingly will be translated into respective prototype changes.","conference":"IEEE","terms":"","keywords":"Graphical;User;Interface;Automatic;GUI;Generation;Processing;Natural;Language;Requirements;Intelligent;Interaction;Prototyping","startPage":"1234","endPage":"1237","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952477","citationCount":0,"referenceCount":29,"year":2019,"authors":"K. Kolthoff","affiliations":"Institute for Enterprise Systems (InES), University of Mannheim","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c3"},"title":"SPrinter: A Static Checker for Finding Smart Pointer Errors in C++ Programs","abstract":"Smart pointers are widely used to prevent memory errors in modern C++ code. However, improper usage of smart pointers may also lead to common memory errors, which makes the code not as safe as expected. To avoid smart pointer errors as early as possible, we present a coding style checker to detect possible bad smart pointer usages during compile time, and notify programmers about bug-prone behaviors. The evaluation indicates that the currently available state-of-the-art static code checkers can only detect 25 out of 116 manually inserted errors, while our tool can detect all these errors. And we also found 521 bugs among 8 open source projects with only 4 false positives.","conference":"IEEE","terms":"","keywords":"C++ Smart Pointer;Memory Error;Coding Styles","startPage":"1122","endPage":"1125","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952230","citationCount":0,"referenceCount":12,"year":2019,"authors":"X. Ma; J. Yan; Y. Li; J. Yan; J. Zhang","affiliations":"Institute of Software, Chinese Academy of Sciences; Institute of Software, Chinese Academy of Sciences; Institute of Software, Chinese Academy of Sciences; Institute of Software, Chinese Academy of Sciences; Institute of Software, Chinese Academy of Sciences","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c4"},"title":"PMExec: An Execution Engine of Partial UML-RT Models","abstract":"This paper presents PMExec, a tool that supports the execution of partial UML-RT models. To this end, the tool implements the following steps: static analysis, automatic refinement, and input-driven execution. The static analysis that respects the execution semantics of UML-RT models is used to detect problematic model elements, i.e., elements that cause problems during execution due to the partiality. Then, the models are refined automatically using model transformation techniques, which mostly add decision points where missing information can be supplied. Third, the refined models are executed, and when the execution reaches the decision points, input required to continue the execution is obtained either interactively or from a script that captures how to deal with partial elements. We have evaluated PMExec using several use-cases that show that the static analysis, refinement, and application of user input can be carried out with reasonable performance, and that the overhead of approach is manageable. https://youtu.be/BRKsselcMnc Note: Interested readers can refer to [1] for a thorough discussion and evaluation of this work.","conference":"IEEE","terms":"","keywords":"MDD;Partial Models;Execution;Debugging;Model level debugging;Model execution","startPage":"1178","endPage":"1181","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952369","citationCount":0,"referenceCount":12,"year":2019,"authors":"M. Bagherzadeh; K. Jahed; N. Kahani; J. Dingel","affiliations":"Queen's University; Queen's University; Queen's University; Queen's University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c5"},"title":"TestCov: Robust Test-Suite Execution and Coverage Measurement","abstract":"We present TestCov, a tool for robust test-suite execution and test-coverage measurement on C programs. TestCov executes program tests in isolated containers to ensure system integrity and reliable resource control. The tool provides coverage statistics per test and for the whole test suite. TestCov uses the simple, XML -based exchange format for test-suite specifications that was established as standard by Test-Comp. TestCov has been successfully used in Test-Comp '19 to execute almost 9 million tests on 1720 different programs. The source code of TestCov is released under the open-source license Apache 2.0 and available at https://gitlab.com/sosy-lab/software/test-suite-validator. A full artifact, including a demonstration video, is available at https://doi.org/10.5281/zenodo.3418726.","conference":"IEEE","terms":"","keywords":"Test Execution, Coverage, Test-Suite Reduction","startPage":"1074","endPage":"1077","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952265","citationCount":0,"referenceCount":13,"year":2019,"authors":"D. Beyer; T. Lemberger","affiliations":"LMU Munich; LMU Munich","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c6"},"title":"Improving Patch Quality by Enhancing Key Components of Automatic Program Repair","abstract":"The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes.","conference":"IEEE","terms":"","keywords":"Automatic Program Repair, Patch Quality","startPage":"1230","endPage":"1233","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952342","citationCount":0,"referenceCount":19,"year":2019,"authors":"M. Soto","affiliations":"Carnegie Mellon University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c7"},"title":"PTracer: A Linux Kernel Patch Trace Bot","abstract":"We present PTracer, a Linux kernel patch trace bot based on an improved PatchNet. PTracer continuously monitors new patches in the git repository of the mainline Linux kernel, filters out unconcerned ones, classifies the rest as bug-fixing or non bug-fixing patches, and reports bug-fixing patches to the kernel experts of commercial operating systems. We use the patches in February 2019 of the mainline Linux kernel to perform the test. As a result, PTracer recommended 151 patches to CGEL kernel experts out of 5,142, and 102 of which were accepted. PTracer has been successfully applied to a commercial operating system and has the advantages of improving software quality and saving labor cost.","conference":"IEEE","terms":"","keywords":"Linux kernel;patch identification;trace bot","startPage":"1210","endPage":"1211","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952485","citationCount":0,"referenceCount":7,"year":2019,"authors":"Y. Wen; J. Cao; S. Cheng","affiliations":"ZTE Corporation; ZTE Corporation; ZTE Corporation","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c8"},"title":"AutoFocus: Interpreting Attention-Based Neural Networks by Code Perturbation","abstract":"Despite being adopted in software engineering tasks, deep neural networks are treated mostly as a black box due to the difficulty in interpreting how the networks infer the outputs from the inputs. To address this problem, we propose AutoFocus, an automated approach for rating and visualizing the importance of input elements based on their effects on the outputs of the networks. The approach is built on our hypotheses that (1) attention mechanisms incorporated into neural networks can generate discriminative scores for various input elements and (2) the discriminative scores reflect the effects of input elements on the outputs of the networks. This paper verifies the hypotheses by applying AutoFocus on the task of algorithm classification (i.e., given a program source code as input, determine the algorithm implemented by the program). AutoFocus identifies and perturbs code elements in a program systematically, and quantifies the effects of the perturbed elements on the network's classification results. Based on evaluation on more than 1000 programs for 10 different sorting algorithms, we observe that the attention scores are highly correlated to the effects of the perturbed code elements. Such a correlation provides a strong basis for the uses of attention scores to interpret the relations between code elements and the algorithm classification results of a neural network, and we believe that visualizing code elements in an input program ranked according to their attention scores can facilitate faster program comprehension with reduced code.","conference":"IEEE","terms":"","keywords":"attention mechanisms, neural networks, algorithm classification, interpretability, explainability, code perturbation, program comprehension","startPage":"38","endPage":"41","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952269","citationCount":0,"referenceCount":17,"year":2019,"authors":"N. D. Q. Bui; Y. Yu; L. Jiang","affiliations":"Singapore Management University; The Open University, UK; Singapore Management University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36c9"},"title":"VeriAbs : Verification by Abstraction and Test Generation","abstract":"Verification of programs continues to be a challenge and no single known technique succeeds on all programs. In this paper we present VeriAbs, a reachability verifier for C programs that incorporates a portfolio of techniques implemented as four strategies, where each strategy is a set of techniques applied in a specific sequence. It selects a strategy based on the kind of loops in the program. We analysed the effectiveness of the implemented strategies on the 3831 verification tasks from the ReachSafety category of the 8th International Competition on Software Verification (SV-COMP) 2019 and found that although classic techniques - explicit state model checking and bounded model checking, succeed on a majority of the programs, a wide range of further techniques are required to analyse the rest. A screencast of the tool is available at https://youtu.be/Hzh3PPiODwk.","conference":"IEEE","terms":"","keywords":"Software Verification;Strategy Selection;Portfolio Verifier;Loop Abstraction;Array Abstraction;Bounded Model Checking;Fuzz Testing;Loop Invariant Generation;Value Analysis","startPage":"1138","endPage":"1141","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952452","citationCount":0,"referenceCount":27,"year":2019,"authors":"M. Afzal; A. Asia; A. Chauhan; B. Chimdyalwar; P. Darke; A. Datar; S. Kumar; R. Venkatesh","affiliations":"Tata Research Development and Design Center, India; Tata Research Development and Design Center, India; Tata Research Development and Design Center, India; Tata Research Development and Design Center, India; Tata Research Development and Design Center, India; Tata Research Development and Design Center, India; Tata Research Development and Design Center, India; Tata Research Development and Design Center, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ca"},"title":"Trusted Software Supply Chain","abstract":"Modern software delivery happens in a geographically distributed environment and resembles like a supply chain – consists of various participants, involves various phases, needs adherence to multiple regulations and needs to maintain artifacts' integrity throughout the delivery phases. This shift in software development brings along with it several challenges ranging from communication of information/knowledge, coordination and control of teams, activities adhering to goals and policies and artifacts adhering to quality, visibility, and management. %Software development processes to be transparent, verifiable, compliant, and accountable thereby increasing software's trustworthiness. With the dispersion of centralized control over software delivery to autonomous delivery organizations, the variety of processes and tools used turns transparency into opacity as autonomous teams use different software processes, tools, and metrics, leading to issues like ineffective compliance monitoring, friction prone coordination, and lack of provenance, and thereby trust. In this paper, we present a delivery governance framework based on distributed ledger technology that uses a notion of 'software telemetry' to record data from disparate delivery partners and enables compliance monitoring and adherence, provenance and traceability, transparency, and thereby trust.","conference":"IEEE","terms":"","keywords":"trust, supply chain, provenance, governance, compliance, integrity, smart advisors","startPage":"1212","endPage":"1213","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952169","citationCount":0,"referenceCount":4,"year":2019,"authors":"K. Singi; J. C. B. R P; S. Podder; A. P. Burden","affiliations":"Accenture Labs, India; Accenture Labs, India; Accenture Labs, India; Accenture, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36cb"},"title":"BuRRiTo: A Framework to Extract, Specify, Verify and Analyze Business Rules","abstract":"An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger \u0026 Acquisition (M\u0026A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching.","conference":"IEEE","terms":"","keywords":"Business Rules Extraction, Rule Document, Rule Components, SBVR, Natural Language Processing, Text Mining, Graphs, Match and Gap, Search and Query, Source Code","startPage":"1190","endPage":"1193","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952315","citationCount":0,"referenceCount":20,"year":2019,"authors":"P. K. Chittimalli; K. Anand; S. Pradhan; S. Mitra; C. Prakash; R. Shere; R. Naik","affiliations":"Tata Consultancy Services Ltd.; TCS Research; Tata Consultancy Services Ltd.; Tata Consultancy Services Ltd.; Tata Consultancy Services Ltd.; Tata Consultancy Services Ltd.; Tata Consultancy Services Ltd.; Tata Consultancy Services Ltd.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36cc"},"title":"Prema: A Tool for Precise Requirements Editing, Modeling and Analysis","abstract":"We present Prema, a tool for Precise Requirement Editing, Modeling and Analysis. It can be used in various fields for describing precise requirements using formal notations and performing rigorous analysis. By parsing the requirements written in formal modeling language, Prema is able to get a model which aptly depicts the requirements. It also provides different rigorous verification and validation techniques to check whether the requirements meet users' expectation and find potential errors. We show that our tool can provide a unified environment for writing and verifying requirements without using tools that are not well inter-related. For experimental demonstration, we use the requirements of the automatic train protection (ATP) system of CASCO signal co. LTD., the largest railway signal control system manufacturer of China. The code of the tool cannot be released here because the project is commercially confidential. However, a demonstration video of the tool is available at https://youtu.be/BX0yv8pRMWs.","conference":"IEEE","terms":"","keywords":"formal methods;requirements modeling;requirements verification;formal engineering methods","startPage":"1166","endPage":"1169","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952250","citationCount":0,"referenceCount":18,"year":2019,"authors":"Y. Huang; J. Feng; H. Zheng; J. Zhu; S. Wang; S. Jiang; W. Miao; G. Pu","affiliations":"East China Normal University; East China Normal University; East China Normal University; East China Normal University; East China Normal University; Eastern Michigan University; East China Normal University; Tongji University, China; East China Normal University; Shanghai Trusted Industrial Control Platform Co., Ltd, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36cd"},"title":"Demystifying Application Performance Management Libraries for Android","abstract":"Since the performance issues of apps can influence users' experience, developers leverage application performance management (APM) tools to locate the potential performance bottleneck of their apps. Unfortunately, most developers do not understand how APMs monitor their apps during the runtime and whether these APMs have any limitations. In this paper, we demystify APMs by inspecting 25 widely-used APMs that target on Android apps. We first report how these APMs implement 8 key functions as well as their limitations. Then, we conduct a large-scale empirical study on 500,000 Android apps from Google Play to explore the usage of APMs. This study has some interesting observations about existing APMs for Android, including 1) some APMs still use deprecated permissions and approaches so that they may not always work properly; 2) some app developers use APMs to collect users' privacy information.","conference":"IEEE","terms":"","keywords":"Android;Application Performance Management Library;Empirical Study","startPage":"682","endPage":"685","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952393","citationCount":0,"referenceCount":19,"year":2019,"authors":"Y. Tang; X. Zhan; H. Zhou; X. Luo; Z. Xu; Y. Zhou; Q. Yan","affiliations":"The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; Wuhan University; Zhejiang University; Michigan State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36ce"},"title":"Generating Tests to Analyse Dynamically-Typed Programs","abstract":"The increasing popularity of dynamically-typed programming languages, such as JavaScript or Python, requires specific support methods for developers to avoid pitfalls arising from the dynamic nature of these languages. Static analyses are frequently used but the dynamic type systems limit their applicability. Dynamic analyses, in contrast, depend on the execution of the code under analysis, and thus depend on the quality of existing tests. This quality of the test suite can be improved by the use of automated test generation but automated test generation for dynamically-typed programming languages itself is hard due to the lack of type information in the programs. The limitations of each of these approaches will be overcome by iteratively combining test generation with static and dynamic analysis techniques for dynamically-typed programs.","conference":"IEEE","terms":"","keywords":"Dynamic Analysis;Static Analysis;Python;Type Inference;Test Generation","startPage":"1226","endPage":"1229","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952500","citationCount":0,"referenceCount":32,"year":2019,"authors":"S. Lukasczyk","affiliations":"University of Passau","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36cf"},"title":"DeepMutation++: A Mutation Testing Framework for Deep Learning Systems","abstract":"Deep neural networks (DNNs) are increasingly expanding their real-world applications across domains, e.g., image processing, speech recognition and natural language processing. However, there is still limited tool support for DNN testing in terms of test data quality and model robustness. In this paper, we introduce a mutation testing-based tool for DNNs, DeepMutation++, which facilitates the DNN quality evaluation, supporting both feed-forward neural networks (FNNs) and stateful recurrent neural networks (RNNs). It not only enables to statically analyze the robustness of a DNN model against the input as a whole, but also allows to identify the vulnerable segments of a sequential input (e.g. audio input) by runtime analysis. It is worth noting that DeepMutation++ specially features the support of RNNs mutation testing. The tool demo video can be found on the project website https://sites.google.com/view/deepmutationpp.","conference":"IEEE","terms":"","keywords":"deep learning;mutation testing","startPage":"1158","endPage":"1161","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952248","citationCount":0,"referenceCount":11,"year":2019,"authors":"Q. Hu; L. Ma; X. Xie; B. Yu; Y. Liu; J. Zhao","affiliations":"Kyushu University, Japan; Kyushu University, Japan; Nanyang Technological University, Singapore; Kyushu University, Japan; Nanyang Technological University, Singapore; Kyushu University, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36d0"},"title":"NeuralVis: Visualizing and Interpreting Deep Learning Models","abstract":"Deep Neural Network(DNN) techniques have been prevalent in software engineering. They are employed to facilitate various software engineering tasks and embedded into many software applications. However, because DNNs are built upon a rich data-driven programming paradigm that employs plenty of labeled data to train a set of neurons to construct the internal system logic, analyzing and understanding their behaviors becomes a difficult task for software engineers. In this paper, we present an instance-based visualization tool for DNN, namely NeuralVis, to support software engineers in visualizing and interpreting deep learning models. NeuralVis is designed for: 1). visualizing the structure of DNN models, i.e., neurons, layers, as well as connections; 2). visualizing the data transformation process; 3). integrating existing adversarial attack algorithms for test input generation; 4). comparing intermediate layers' outputs of different inputs. To demonstrate the effectiveness of NeuralVis, we design a task-based user study involving ten participants on two classic DNN models, i.e., LeNet and VGG-12. The result shows NeuralVis can assist engineers in identifying critical features that determine the prediction results. Video: https://youtu.be/solkJri4Z44","conference":"IEEE","terms":"","keywords":"Visualization;Neural Network Visualization;Program Comprehension","startPage":"1106","endPage":"1109","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952427","citationCount":0,"referenceCount":14,"year":2019,"authors":"X. Zhang; Z. Yin; Y. Feng; Q. Shi; J. Liu; Z. Chen","affiliations":"Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36d1"},"title":"Improving Collaboration Efficiency in Fork-Based Development","abstract":"Fork-based development is a lightweight mechanism that allows developers to collaborate with or without explicit coordination. Although it is easy to use and popular, when developers each create their own fork and develop independently, their contributions are usually not easily visible to others. When the number of forks grows, it becomes very difficult to maintain an overview of what happens in individual forks, which would lead to additional problems and inefficient practices: lost contributions, redundant development, fragmented communities, and so on. Facing the problems mentioned above, we developed two complementary strategies: (1) Identifying existing best practices and suggesting evidence-based interventions for projects that are inefficient; (2) designing new interventions that could improve the awareness of a community using fork-based development, and help developers to detect redundant development to reduce unnecessary effort.","conference":"IEEE","terms":"","keywords":"Fork-based Development, Distributed Collaboration, Awareness of Collaboration, Open-Source Community","startPage":"1218","endPage":"1221","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952395","citationCount":0,"referenceCount":14,"year":2019,"authors":"S. Zhou","affiliations":"Carnegie Mellon University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36d2"},"title":"An Approach for Investigating Emotion Dynamics in Software Development","abstract":"Emotion awareness is critical to interpersonal communication, including that in software development. The SE community has studied emotion in software development using isolated emotion states but it has not considered the dynamic nature of emotion. To investigate the emotion dynamics, SE community needs an effective approach. In this paper, we propose such an approach which can automatically collect project teams' communication records, identify the emotions and their intensities in them, model the emotion dynamics into time series, and provide efficient data management. We demonstrate that this approach can provide end-to-end support for various emotion awareness research and practices through automated data collection, modeling, storage, analysis, and presentation using the IPython's project data on GitHub.","conference":"IEEE","terms":"","keywords":"Emotion dynamics, emotion awareness, emotion intensity, software development, time-series database","startPage":"1268","endPage":"1270","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952358","citationCount":0,"referenceCount":13,"year":2019,"authors":"K. Neupane","affiliations":"Rochester Institute of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9120eee8435e8e7d36d3"},"title":"Inference of Properties from Requirements and Automation of Their Formal Verification","abstract":"Over the past decades, various techniques for the application of formal program analysis of software for embedded systems have been proposed. However, the application of formal methods for software verification is still limited in practise. It is acknowledged that the task of formally stating requirements by specifying the formal properties is a major hindrance. The verification step itself has its shortcoming in its scalability and its limitation to predefined proof tactics in case of automated theorem proving (ATP). These constraints are reduced today by the interaction of the user with the theorem prover (TP) during the execution of the proof. However, this is difficult for non-experts. The objectives of the presented PhD project are the automated inference of declarative property specifications from example data specified by the engineer for a function under development and their automated verification on abstract model level and on code level. We propose the meta-model for Scenario Modeling Language (SML) that allows to specify example data. For the automated property generation we are motivated by Inductive Logic Programming (ILP) techniques for first-order logic in pure mathematics. We propose modifications to its algorithm that allow to process the information that is incorporated in the meta-model of SML. However, this technique is expected to produce too many uninteresting properties. To turn this weakness into strength, our approach proposes to tailor the algorithm towards selection of the right properties that facilitate the automation of the proof. Automated property generation and less user interaction with the prover will leverage formal verification as it will relieve the engineer in the specification as well as in proofing tasks.","conference":"IEEE","terms":"","keywords":"embedded systems, formal verification, specification mining, formal properties, declarative requirement specification","startPage":"1222","endPage":"1225","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952397","citationCount":0,"referenceCount":36,"year":2019,"authors":"M. Reich","affiliations":"Chemnitz University of Technology, Airbus Defence and Space GmbH","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36d4"},"title":"Compile-Time Detection of Machine Image Sniping","abstract":"Machine image sniping is a difficult-to-detect security vulnerability in cloud computing code. When programmatically initializing a machine, a developer specifies a machine image (operating system and file system). The developer should restrict the search to only those machine images which their organization controls: otherwise, an attacker can insert a similarly-named malicious image into the public database, where it might be selected instead of the image the developer intended. We present a lightweight type and effect system that detects requests to a cloud provider that are vulnerable to an image sniping attack, or proves that no vulnerable request exists in a codebase. We prototyped our type system for Java programs that initialize Amazon Web Services machines, and evaluated it on more than 500 codebases, detecting 14 vulnerable requests with only 3 false positives.","conference":"IEEE","terms":"","keywords":"pluggable types, AMI sniping, AWS, EC2, Java, lightweight verification, DescribeImagesRequest","startPage":"1256","endPage":"1258","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952221","citationCount":0,"referenceCount":8,"year":2019,"authors":"M. Kellogg","affiliations":"Paul G. Allen University of Washington","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36d5"},"title":"Boosting Neural Commit Message Generation with Code Semantic Analysis","abstract":"It has been long suggested that commit messages can greatly facilitate code comprehension. However, developers may not write good commit messages in practice. Neural machine translation (NMT) has been suggested to automatically generate commit messages. Despite the efforts in improving NMT algorithms, the quality of the generated commit messages is not yet satisfactory. This paper, instead of improving NMT algorithms, suggests that proper preprocessing of code changes into concise inputs is quite critical to train NMT. We approach it with semantic analysis of code changes. We collect a real-world dataset with 50k+ commits of popular Java projects, and verify our idea with comprehensive experiments. The results show that preprocessing inputs with code semantic analysis can improve NMT significantly. This work sheds light to how to apply existing DNNs designed by the machine learning community, e.g., NMT models, to complete software engineering tasks.","conference":"IEEE","terms":"","keywords":"Neural machine translation (NMT);Commit message generation;Deep learning","startPage":"1280","endPage":"1282","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952539","citationCount":0,"referenceCount":38,"year":2019,"authors":"S. Jiang","affiliations":"Fudan University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36d6"},"title":"FPChecker: Detecting Floating-Point Exceptions in GPU Applications","abstract":"Floating-point arithmetic is widely used in applications from several fields including scientific computing, machine learning, graphics, and finance. Many of these applications are rapidly adopting the use of GPUs to speedup computations. GPUs, however, have limited support to detect floating-point exceptions, which hinders the development of reliable applications in GPU-based systems. We present FPCHECKER, the first tool to automatically detect floating-point exceptions in GPU applications. FPCHECKER uses the clang/LLVM compiler to instrument GPU kernels and to detect exceptions at runtime. Once an exception is detected, it reports to the programmer the code location of the exception as well as other useful information. The programmer can then use this report to avoid the exception, e.g., by modifying the application algorithm or changing the input. We present the design of FPCHECKER, an evaluation of the overhead of the tool, and a real-world case scenario on which the tool is used to identify a hidden exception. The slowdown of FPCHECKER is moderate and the code is publicly available as open source.","conference":"IEEE","terms":"","keywords":"Floating-point arithmetic;exceptions;GPU;scientific computing","startPage":"1126","endPage":"1129","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952258","citationCount":0,"referenceCount":6,"year":2019,"authors":"I. Laguna","affiliations":"Lawrence Livermore National Laboratory","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36d7"},"title":"A Journey Towards Providing Intelligence and Actionable Insights to Development Teams in Software Delivery","abstract":"For delivering high-quality artifacts within the budget and on schedule, software delivery teams ideally should have a holistic and in-process view of the current health and future trajectory of the project. However, such insights need to be at the right level of granularity and need to be derived typically from a heterogeneous project environment, in a way that helps development team members with their tasks at hand. Due to client mandates, software delivery project environments employ many disparate tools and teams tend to be distributed, thus making the relevant information retrieval, insight generation, and developer intelligence augmentation process fairly complex. In this paper, we discuss our journey in this area spanning across facets like software project modelling and new development metrics, studying developer priorities, adoption of new metrics, and different approaches of developer intelligence augmentation. Finally, we present our exploration of new immersive technologies for human-centered software engineering.","conference":"IEEE","terms":"","keywords":"Software Delivery, Software Analytics, Project Management, Actionable Insights","startPage":"1214","endPage":"1215","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952441","citationCount":0,"referenceCount":10,"year":2019,"authors":"V. S. Sharma; R. Mehra; S. Podder; A. P. Burden","affiliations":"Accenture Labs, India; Accenture Labs, India; Accenture Labs, India; Accenture, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36d8"},"title":"Pangolin: An SFL-Based Toolset for Feature Localization","abstract":"Pinpointing the location where a given unit of functionality-or feature-was implemented is a demanding and time-consuming task, yet prevalent in most software maintenance or evolution efforts. To that extent, we present PANGOLIN, an Eclipse plugin that helps developers identifying features among the source code. It borrows Spectrum-based Fault Localization techniques from the software diagnosis research field by framing feature localization as a diagnostic problem. PANGOLIN prompts users to label system executions based on feature involvement, and subsequently presents its spectrum-based feature localization analysis to users with the aid of a color-coded, hierarchic, and navigable visualization which was shown to be effective at conveying diagnostic information to users. Our evaluation shows that PANGOLIN accurately pinpoints feature implementations and is resilient to misclassifications by users. The tool can be downloaded at https://tqrg.github.io/pangolin/.","conference":"IEEE","terms":"","keywords":"Spectrum-based Fault Localization;Program Understanding;Maintenance and Evolution","startPage":"1130","endPage":"1133","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952297","citationCount":0,"referenceCount":10,"year":2019,"authors":"B. Castro; A. Perez; R. Abreu","affiliations":"IST, University of Lisbon; Palo Alto Research Center; IST, University of Lisbon and INESC-ID","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36d9"},"title":"SWAN_ASSIST: Semi-Automated Detection of Code-Specific, Security-Relevant Methods","abstract":"To detect specific types of bugs and vulnerabilities, static analysis tools must be correctly configured with security-relevant methods (SRM), e.g., sources, sinks, sanitizers and authentication methods–usually a very labour-intensive and error-prone process. This work presents the semi-automated tool SWAN_ASSIST, which aids the configuration with an IntelliJ plugin based on active machine learning. It integrates our novel automated machine-learning approach SWAN, which identifies and classifies Java SRM. SWAN_ASSIST further integrates user feedback through iterative learning. SWAN_ASSIST aids developers by asking them to classify at each point in time exactly those methods whose classification best impact the classification result. Our experiments show that SWAN_ASSIST classifies SRM with a high precision, and requires a relatively low effort from the user. A video demo of SWAN_ASSIST can be found at https://youtu.be/fSyD3V6EQOY. The source code is available at https://github.com/secure-software-engineering/swan.","conference":"IEEE","terms":"","keywords":"Program Analysis;Security;Machine-Learning","startPage":"1094","endPage":"1097","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952264","citationCount":0,"referenceCount":9,"year":2019,"authors":"G. Piskachev; L. Nguyen Quang Do; O. Johnson; E. Bodden","affiliations":"Fraunhofer IEM; Paderborn University; Fraunhofer IEM; Paderborn University and Fraunhofer IEM","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36da"},"title":"Kotless: A Serverless Framework for Kotlin","abstract":"Recent trends in Web development demonstrate an increased interest in serverless applications, i.e. applications that utilize computational resources provided by cloud services on demand instead of requiring traditional server management. This approach enables better resource management while being scalable, reliable, and cost-effective. However, it comes with a number of organizational and technical difficulties which stem from the interaction between the application and the cloud infrastructure, for example, having to set up a recurring task of reuploading updated files. In this paper, we present Kotless — a Kotlin Serverless Framework. Kotless is a cloud-agnostic toolkit that solves these problems by interweaving the deployed application into the cloud infrastructure and automatically generating the necessary deployment code. This relieves developers from having to spend their time integrating and managing their applications instead of developing them. Kotless has proven its capabilities and has been used to develop several serverless applications already in production. Its source code is available at https://github.com/JetBrains/kotless, a tool demo can be found at https://www.youtube.com/watch?v=IMSakPNl3TY.","conference":"IEEE","terms":"","keywords":"Kotlin;Serverless;Web;Framework;Kotless;Cloud","startPage":"1110","endPage":"1113","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952482","citationCount":0,"referenceCount":8,"year":2019,"authors":"V. Tankov; Y. Golubev; T. Bryksin","affiliations":"JetBrains; JetBrains Research; JetBrains Research","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36db"},"title":"Towards Comprehensible Representation of Controllers using Machine Learning","abstract":"From the point of view of a software engineer, having safe and optimal controllers for real life systems like cyber physical systems is a crucial requirement before deployment. Given the mathematical model of these systems along with their specifications, model checkers can be used to synthesize controllers for them. The given work proposes novel approaches for making controller analysis easier by using machine learning to represent the controllers synthesized by model checkers in a succinct manner, while also incorporating the domain knowledge of the system. It also proposes the implementation of a visualization tool which will be integrated into existing model checkers. A lucid controller representation along with a tool to visualize it will help the software engineer debug and monitor the system much more efficiently.","conference":"IEEE","terms":"","keywords":"Cyber Physical Systems, Controller Synthesis, Model Checking, Machine Learning, Inductive Logic Programming, Domain Knowledge, Strategy Representation","startPage":"1283","endPage":"1285","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952374","citationCount":0,"referenceCount":17,"year":2019,"authors":"G. Balasubramaniam","affiliations":"Birla Institute of Technology and Science, Pilani, Goa, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36dc"},"title":"FogWorkflowSim: An Automated Simulation Toolkit for Workflow Performance Evaluation in Fog Computing","abstract":"Workflow underlies most process automation software, such as those for product lines, business processes, and scientific computing. However, current Cloud Computing based workflow systems cannot support real-time applications due to network latency, which limits their application in many IoT systems such as smart healthcare and smart traffic. Fog Computing extends the Cloud by providing virtualized computing resources close to the End Devices so that the response time of accessing computing resources can be reduced significantly. However, how to most effectively manage heterogeneous resources and different computing tasks in the Fog is a big challenge. In this paper, we introduce \"FogWorkflowSim\" an efficient and extensible toolkit for automatically evaluating resource and task management strategies in Fog Computing with simulated user-defined workflow applications. Specifically, FogWorkflowSim is able to: 1) automatically set up a simulated Fog Computing environment for workflow applications; 2) automatically execute user submitted workflow applications; 3) automatically evaluate and compare the performance of different computation offloading and task scheduling strategies with three basic performance metrics, including time, energy and cost. FogWorkflowSim can serve as an effective experimental platform for researchers in Fog based workflow systems as well as practitioners interested in adopting Fog Computing and workflow systems for their new software projects. (Demo video: https://youtu.be/AsMovcuSkx8)","conference":"IEEE","terms":"","keywords":"Fog Computing;Workflow;Performance Evaluation;Task Scheduling;Simulation Toolkit","startPage":"1114","endPage":"1117","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952516","citationCount":0,"referenceCount":12,"year":2019,"authors":"X. Liu; L. Fan; J. Xu; X. Li; L. Gong; J. Grundy; Y. Yang","affiliations":"Deakin University; Anhui University; Anhui University; Anhui University; Anhui University; Monash University; Swinburne University of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36dd"},"title":"An Image-Inspired and CNN-Based Android Malware Detection Approach","abstract":"Abstract-Until 2017, Android smartphones occupied approximately 87% of the smartphone market. The vast market also promotes the development of Android malware. Nowadays, the number of malware targeting Android devices found daily is more than 38,000. With the rapid progress of mobile application programming and anti-reverse-engineering techniques, it is harder to detect all kinds of malware. To address challenges in existing detection techniques, such as data obfuscation and limited code coverage, we propose a detection approach that directly learns features of malware from Dalvik bytecode based on deep learning technique (CNN). The average detection time of our model is0.22 seconds, which is much lower than other existing detection approaches. In the meantime, the overall accuracy of our model achieves over 93%.","conference":"IEEE","terms":"","keywords":"Android Malware Detection;Deep learning;CNN","startPage":"1259","endPage":"1261","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952484","citationCount":0,"referenceCount":18,"year":2019,"authors":"X. Xiao","affiliations":"Case Western Reserve University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36de"},"title":"CONVUL: An Effective Tool for Detecting Concurrency Vulnerabilities","abstract":"Concurrency vulnerabilities are extremely harmful and can be frequently exploited to launch severe attacks. Due to the non-determinism of multithreaded executions, it is very difficult to detect them. Recently, data race detectors and techniques based on maximal casual model have been applied to detect concurrency vulnerabilities. However, the former are ineffective and the latter report many false negatives. In this paper, we present CONVUL, an effective tool for concurrency vulnerability detection. CONVUL is based on exchangeable events, and adopts novel algorithms to detect three major kinds of concurrency vulnerabilities. In our experiments, CONVUL detected 9 of 10 known vulnerabilities, while other tools only detected at most 2 out of these 10 vulnerabilities. The 10 vulnerabilities are available at https://github.com/mryancai/ConVul.","conference":"IEEE","terms":"","keywords":"Concurrency;Vulnerabilities","startPage":"1154","endPage":"1157","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952233","citationCount":0,"referenceCount":15,"year":2019,"authors":"R. Meng; B. Zhu; H. Yun; H. Li; Y. Cai; Z. Yang","affiliations":"State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences; University of Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences; University of Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences; University of Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences; University of Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences; GuardStrike Inc.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36df"},"title":"Automatically Repairing Binary Programs Using Adapter Synthesis","abstract":"Bugs in commercial software and third-party components are an undesirable and expensive phenomenon. Such software is usually released to users only in binary form. The lack of source code renders users of such software dependent on their software vendors for repairs of bugs. Such dependence is even more harmful if the bugs introduce new vulnerabilities in the software. Automatically repairing security and functionality bugs in binary code increases software robustness without any developer effort. In this research, we propose development of a binary program repair tool that uses existing bug-free fragments of code to repair buggy code.","conference":"IEEE","terms":"","keywords":"automated program repair;symbolic execution;binary analysis;adapter synthesis","startPage":"1238","endPage":"1241","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952199","citationCount":0,"referenceCount":24,"year":2019,"authors":"V. Sharma","affiliations":"University of Minnesota","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e0"},"title":"Visual Analytics for Concurrent Java Executions","abstract":"Analyzing executions of concurrent software is very difficult. Even if a trace is available, such traces are very hard to read and interpret. A textual trace contains a lot of data, most of which is not relevant to the issue at hand. Past visualization attempts either do not show concurrent behavior, or result in a view that is overwhelming for the user. We provide a visual analytics tool, VA4JVM, for error traces produced by either the Java Virtual Machine, or by Java Pathfinder. Its key features are a layout that spatially associates events with threads, a zoom function, and the ability to filter event data in various ways. We show in examples how filtering and zooming in can highlight a problem without having to read lengthy textual data.","conference":"IEEE","terms":"","keywords":"Execution trace visualization;Visual analytics","startPage":"1102","endPage":"1105","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952488","citationCount":0,"referenceCount":32,"year":2019,"authors":"C. Artho; M. Pande; Q. Tang","affiliations":"KTH Royal Institute of Technology; KTH Royal Institute of Technology; Imperial College London","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e1"},"title":"Tackling Build Failures in Continuous Integration","abstract":"In popular continuous integration(CI) practice, coding is followed by building, integration and system testing, pre-release inspection, and deploying artifacts. This can reduce integration risk and speed up the development process. But large number of CI build failures may interrupt the normal software development process. So, the failures need to be analyzed and fixed quickly. Although various automated program repair techniques have great potential to resolve software failures, the existing techniques mostly focus on repairing source code. So, those techniques cannot directly help resolve software build failures. Apart from that, a special challenge to fix build failures in CI environment is that the failures are often involved with both source code and build scripts. This paper outlines promising preliminary work towards automatic build repair in CI environment that involves both source code and build script. As the first step, we conducted an empirical study on software build failures and build fix patterns. Based on the findings of the empirical study, we developed an approach that can automatically fix build errors involving build scripts. We plan to extend this repair approach considering both source code and build script. Moreover, we plan to quantify our automatic fixes by user study and comparison between fixes generated by our approach and actual fixes.","conference":"IEEE","terms":"","keywords":"build failures, build repair, continuous integration","startPage":"1242","endPage":"1245","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952416","citationCount":0,"referenceCount":21,"year":2019,"authors":"F. Hassan","affiliations":"University of Texas at San Antonio","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e2"},"title":"API Design Implications of Boilerplate Client Code","abstract":"Designing usable APIs is critical to developers' productivity and software quality but is quite difficult. In this paper, I focus on \"boilerplate\" code, sections of code that have to be included in many places with little or no alteration, which many experts in API design have said can be an indicator of API usability problems. I investigate what properties make code count as boilerplate, and present a novel approach to automatically mine boilerplate code from a large set of client code. The technique combines an existing API usage mining algorithm, with novel filters using AST comparison and graph partitioning. With boilerplate candidates identified by the technique, I discuss how this technique could help API designers in reviewing their design decisions and identifying usability issues.","conference":"IEEE","terms":"","keywords":"Boilerplate Code;API Usability;Repository Mining","startPage":"1253","endPage":"1255","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952241","citationCount":0,"referenceCount":20,"year":2019,"authors":"D. Nam","affiliations":"Carnegie Mellon University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e3"},"title":"Manticore: A User-Friendly Symbolic Execution Framework for Binaries and Smart Contracts","abstract":"An effective way to maximize code coverage in software tests is through dynamic symbolic execution—a technique that uses constraint solving to systematically explore a program's state space. We introduce an open-source dynamic symbolic execution framework called Manticore for analyzing binaries and Ethereum smart contracts. Manticore's flexible architecture allows it to support both traditional and exotic execution environments, and its API allows users to customize their analysis. Here, we discuss Manticore's architecture and demonstrate the capabilities we have used to find bugs and verify the correctness of code for our commercial clients.","conference":"IEEE","terms":"","keywords":"manticore;mcore;symbolic execution;ethereum;smart contract","startPage":"1186","endPage":"1189","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952204","citationCount":0,"referenceCount":19,"year":2019,"authors":"M. Mossberg; F. Manzano; E. Hennenfent; A. Groce; G. Grieco; J. Feist; T. Brunson; A. Dinaburg","affiliations":"Trail of Bits; Trail of Bits; Trail of Bits; Trail of Bits; Trail of Bits; Trail of Bits; Trail of Bits; Trail of Bits","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e4"},"title":"Lancer: Your Code Tell Me What You Need","abstract":"Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer.","conference":"IEEE","terms":"","keywords":"Code recommendation;Code reuse;Language model","startPage":"1202","endPage":"1205","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952168","citationCount":0,"referenceCount":18,"year":2019,"authors":"S. Zhou; B. Shen; H. Zhong","affiliations":"Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e5"},"title":"MutAPK: Source-Codeless Mutant Generation for Android Apps","abstract":"The amount of Android application is having a tremendous increasing trend, exerting pressure over practitioners and researchers around application quality, frequent releases, and quick fixing of bugs. This pressure leads practitioners to make usage of automated approaches based on using source-code as input. Nevertheless, third-party services are not able to use these approaches due to privacy factors. In this paper we present MutAPK, an open source mutation testing tool that enables the usage of APK as input for this task. MutAPK generates mutants without the need of having access to source code, because the mutations are done in an intermediate representation of the code (i.e., SMALI) that does not require compilation. MutAPK is publicly available at GitHub: https://bit.ly/2KYvgP9 VIDEO: https://bit.ly/2WOjiyy","conference":"IEEE","terms":"","keywords":"Mutation Testing, Closed Source Apps, Android","startPage":"1090","endPage":"1093","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952526","citationCount":0,"referenceCount":15,"year":2019,"authors":"C. Escobar-Velásquez; M. Osorio-Riaño; M. Linares-Vásquez","affiliations":"Universidad de los Andes; Universidad de los Andes; Universidad de los Andes","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e6"},"title":"Developer Reputation Estimator (DRE)","abstract":"Evidence shows that developer reputation is extremely important when accepting pull requests or resolving reported issues. It is particularly salient in Free/Libre Open Source Software since the developers are distributed around the world, do not work for the same organization and, in most cases, never meet face to face. The existing solutions to expose developer reputation tend to be forge specific (GitHub), focus on activity instead of impact, do not leverage social or technical networks, and do not correct often misspelled developer identities. We aim to remedy this by amalgamating data from all public Git repositories, measuring the impact of developer work, expose developer's collaborators, and correct notoriously problematic developer identity data. We leverage World of Code (WoC), a collection of an almost complete (and continuously updated) set of Git repositories by first allowing developers to select which of the 34 million(M) Git commit author IDs belong to them and then generating their profiles by treating the selected collection of IDs as that single developer. As a side-effect, these selections serve as a training set for a supervised learning algorithm that merges multiple identity strings belonging to a single individual. As we evaluate the tool and the proposed impact measure, we expect to build on these findings to develop reputation badges that could be associated with pull requests and commits so developers could easier trust and prioritize them.","conference":"IEEE","terms":"","keywords":"Developer Reputation;Software Ecosystem;Identity Disambiguation","startPage":"1082","endPage":"1085","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952390","citationCount":0,"referenceCount":7,"year":2019,"authors":"S. Amreen; A. Karnauch; A. Mockus","affiliations":"The University of Tennessee; The University of Tennessee; The University of Tennessee","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e7"},"title":"CocoQa: Question Answering for Coding Conventions Over Knowledge Graphs","abstract":"Coding convention plays an important role in guaranteeing software quality. However, coding conventions are usually informally presented and inconvenient for programmers to use. In this paper, we present CocoQa, a system that answers programmer's questions about coding conventions. CocoQa answers questions by querying a knowledge graph for coding conventions. It employs 1) a subgraph matching algorithm that parses the question into a SPARQL query, and 2) a machine comprehension algorithm that uses an end-to-end neural network to detect answers from searched paragraphs. We have implemented CocoQa, and evaluated it on a coding convention QA dataset. The results show that CocoQa can answer questions about coding conventions precisely. In particular, CocoQa can achieve a precision of 82.92% and a recall of 91.10%. Repository: https://github.com/14dtj/CocoQa/ Video: https://youtu.be/VQaXi1WydAU","conference":"IEEE","terms":"","keywords":"Coding convention;question answering;knowledge graph","startPage":"1086","endPage":"1089","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952314","citationCount":0,"referenceCount":15,"year":2019,"authors":"T. Du; J. Cao; Q. Wu; W. Li; B. Shen; Y. Chen","affiliations":"Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Shanghai Jiao Tong University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e8"},"title":"MuSC: A Tool for Mutation Testing of Ethereum Smart Contract","abstract":"The smart contract cannot be modified when it has been deployed on a blockchain. Therefore, it must be given thorough test before its being deployed. Mutation testing is considered as a practical test methodology to evaluate the adequacy of software testing. In this paper, we introduce MuSC, a mutation testing tool for Ethereum Smart Contract (ESC). It can generate numerous mutants at a fast speed and supports the automatic operations such as creating test nets, deploying and executing tests. Specially, MuSC implements a set of novel mutation operators w.r.t ESC programming language, Solidity. Therefore, it can expose the defects of smart contracts to a certain degree. The demonstration video of MuSC is available at https: //youtu.be/3KBKXJPVjbQ, and the source code can be downloaded at https://github.com/belikout/MuSC-Tool-Demo-repo.","conference":"IEEE","terms":"","keywords":"Blockchain;Ethereum-Smart-Contract;Mutation-Test;Mutation-Operator","startPage":"1198","endPage":"1201","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952206","citationCount":0,"referenceCount":20,"year":2019,"authors":"Z. Li; H. Wu; J. Xu; X. Wang; L. Zhang; Z. Chen","affiliations":"Nanjing University; Nanjing University; Nanjing University; Nanjing University; University of Texas at Dallas; Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36e9"},"title":"SiMPOSE - Configurable N-Way Program Merging Strategies for Superimposition-Based Analysis of Variant-Rich Software","abstract":"Modern software often exists in many different, yet similar versions and/or variants, usually derived from a common code base (e.g., via clone-and-own). In the context of product-line engineering, family-based analysis has shown very promising potential for improving efficiency in applying quality-assurance techniques to variant-rich software, as compared to a variant-by-variant approach. Unfortunately, these strategies rely on a product-line representation superimposing all program variants in a syntactically well-formed, semantically sound and variant-preserving manner, which is manually hard to obtain in practice. We demonstrate the SiMPOSE methodology for automatically generating superimpositions of N given program versions and/or variants facilitating family-based analysis of variant-rich software. SiMPOSE is based on a novel N-way model-merging technique operating at the level of control-flow automata (CFA) representations of C programs. CFAs constitute a unified program abstraction utilized by many recent software-analysis tools. We illustrate different merging strategies supported by SiMPOSE, namely variant-by-variant, N-way merging, incremental 2-way merging, and partition-based N/2-way merging, and demonstrate how SiMPOSE can be used to systematically compare their impact on efficiency and effectiveness of family-based unit-test generation. The SiMPOSE tool, the demonstration of its usage as well as related artifacts and documentation can be found at http://pi.informatik.uni-siegen.de/projects/variance/simpose.","conference":"IEEE","terms":"","keywords":"Program Merging;Model Merging;Software Testing;Family-Based Analyses","startPage":"1134","endPage":"1137","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952245","citationCount":0,"referenceCount":12,"year":2019,"authors":"D. Reuling; U. Kelter; S. Ruland; M. Lochau","affiliations":"University of Siegen; University of Siegen; Real-Time Systems Lab, TU Darmstadt; Real-Time Systems Lab, TU Darmstadt","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36ea"},"title":"Towards search-based modelling and analysis of requirements and architecture decisions","abstract":"Many requirements engineering and software architecture decisions are complicated by uncertainty and multiple conflicting stakeholders objectives. Using quantitative decision models helps clarify these decisions and allows the use of multi-objective simulation optimisation techniques in analysing the impact of decisions on objectives. Existing requirements and architecture decision support methods that use quantitative decision models are limited by the difficulty in elaborating problem-specific decision models and/or lack integrated tool support for automated decision analysis under uncertainty. To address these problems and facilitate requirements and architecture decision analysis, this research proposes a novel modelling language and automated decision analysis technique, implemented in a tool called RADAR. The modelling language is a simplified version of quantitative AND/OR goal models used in requirements engineering and similar to feature models used in software product lines. This research involves developing the RADAR tool and evaluating the tool's applicability, usefulness and scalability on a set of real-world examples.","conference":"IEEE","terms":"Analytical models;Radar;Tools;Decision analysis;Stakeholders;Uncertainty,decision making;decision support systems;production engineering computing;software architecture,requirements engineering;software architecture decisions;multiple conflicting stakeholders objectives;quantitative decision models;multiobjective simulation optimisation techniques;architecture decision support methods;problem-specific decision models;architecture decision analysis;automated decision analysis technique;quantitative goal models;feature models;search-based modelling a;RADAR tool;software product lines","keywords":"","startPage":"1026","endPage":"1029","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115725","citationCount":1,"referenceCount":26,"year":2017,"authors":"S. A. Busari","affiliations":"Software Systems Engineering, Department of Computer Science, University College London, United Kingdom","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36eb"},"title":"Tool Support for Analyzing Mobile App Reviews","abstract":"Mobile app reviews often contain useful user opinions for app developers. However, manual analysis of those reviews is challenging due to their large volume and noisynature. This paper introduces MARK, a supporting tool for review analysis of mobile apps. With MARK, an analyst can describe her interests of one or more apps via a set of keywords. MARK then lists the reviews most relevant to those keywords for further analyses. It can also draw the trends over time of the selected keywords, which might help the analyst to detect sudden changes in the related user reviews. To help the analyst describe her interests more effectively, MARK can automatically extract and rank the keywords by their associations with negative reviews, divide a large set of keywords into more cohesive subgroups, or expand a small set into a broader one.","conference":"IEEE","terms":"Facebook;Market research;Batteries;Energy consumption;Mobile communication;Google;Semantics,data mining;mobile computing;software reviews;text analysis,mobile app reviews;MARK;online reviews;Mining and Analyzing Reviews by Keywords tool","keywords":"App Review;Opinion Mining;Keyword","startPage":"789","endPage":"794","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372068","citationCount":3,"referenceCount":11,"year":2015,"authors":"P. M. Vu; H. V. Pham; T. T. Nguyen; T. T. Nguyen","affiliations":"Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36ec"},"title":"The challenges of verification and validation of automated planning systems (keynote)","abstract":"Mission planning is central to space mission operations, and has benefited from advances in model-based planning software. A model is a description of the objects, actions, constraints and preferences that the planner reasons over to generate plans. Developing, verifying and validating a planning model is, however, a difficult task. Mission planning constraints and preferences arise from many sources, including simulators and engineering specification documents. As mission constraints evolve, planning domain modelers must add and update model constraints efficiently using the available source data, catching errors quickly, and correcting the model. The consequences of erroneous models are very high, especially in the space operations environment. We first describe the space operations environment, particularly the role of the mission planning system. We then describe model-based planning, and briefly review the current state of the practice in designing model-based mission planning tools and the challenges facing model developers. We then describe an Interactive Model Development Environment (IMDE) approach to developing mission planning systems. This approach integrates modeling and simulation environments to reduce model editing time, generate simulations automatically to evaluate plans, and identify modeling errors automatically by evaluating simulation output. The IMDE approach was tested on a small subset of the Lunar Atmosphere and Dust Environment Explorer (LADEE) flight software to demonstrate how to develop the LADEE mission planning system.","conference":"IEEE","terms":"","keywords":"","startPage":"2","endPage":"2","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693059","citationCount":0,"referenceCount":0,"year":2013,"authors":"J. Frank","affiliations":"NASA Ames Research Center, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36ed"},"title":"Keynotes","abstract":"Provides an abstract for each of the keynote presentations and may include a brief professional biography of each","conference":"IEEE","terms":"","keywords":"","startPage":"xxiii","endPage":"xxv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371988","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36ee"},"title":"LED: Tool for Synthesizing Web Element Locators","abstract":"Web applications are growing fast in popularity and complexity. One of the major problems faced by web developers is writing JavaScript code that can retrieve Document Object Model (DOM) tree elements, and is consistent among multiple DOM states. We attempt to solve this problem by automatically synthesizing JavaScript code that interacts with the DOM. We present an automated tool called LED, to analyze the DOM elements, and synthesize code to select the DOM elements based on the DOM hierarchy as well as the nature of task that the user wants to perform. LED provides an interactive drag and drop support inside the browser for selecting positive and negative examples of DOM elements. We find that LED supports at least 86% of the locators used in the JavaScript code of deployed web applications, and that the locators synthesized by LED have a recall of 98% and a precision of 63%. LED is fast, taking only 0.23 seconds on average to synthesize a locator.","conference":"IEEE","terms":"Light emitting diodes;Cascading style sheets;Automation;Writing;Browsers;Mice;Selenium,Internet;Java;object-oriented methods;program diagnostics;trees (mathematics),LED;Web element locator;Web application;Web developer;JavaScript code;document object model tree element;DOM tree element;DOM state;automated tool;DOM element","keywords":"Program synthesis;Programming by example;Element locators;CSS selectors;Web applications","startPage":"848","endPage":"851","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372078","citationCount":0,"referenceCount":14,"year":2015,"authors":"K. Bajaj; K. Pattabiraman; A. Mesbah","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36ef"},"title":"The bounded model checker LLBMC","abstract":"This paper presents LLBMC, a tool for finding bugs and runtime errors in sequential C/C++ programs. LLBMC employs bounded model checking using an SMT-solver for the theory of bitvectors and arrays and thus achieves precision down to the level of single bits. The two main features of LLBMC that distinguish it from other bounded model checking tools for C/C++ are (i) its bit-precise memory model, which makes it possible to support arbitrary type conversions via stores and loads; and (ii) that it operates on a compiler intermediate representation and not directly on the source code.","conference":"IEEE","terms":"Model checking;Computer bugs;Runtime;Program processors;Encoding;Decoding,C++ language;formal verification;program compilers;program debugging,bounded model checker LLBMC;finding bugs;runtime errors;sequential C/C++ programs;bounded model checking;SMT solver;source code;program compilers","keywords":"","startPage":"706","endPage":"709","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693138","citationCount":14,"referenceCount":21,"year":2013,"authors":"S. Falke; F. Merz; C. Sinz","affiliations":"Institute for Theoretical Computer Science, Karlsruhe Institute of Technology (KIT), Germany; Institute for Theoretical Computer Science, Karlsruhe Institute of Technology (KIT), Germany; Institute for Theoretical Computer Science, Karlsruhe Institute of Technology (KIT), Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f0"},"title":"Cobra — An interactive static code analyzer","abstract":"Sadly we know that virtually all software of any significance has residual errors. Some of those errors can be traced back to requirements flaws or faulty design assumptions; others are just plain coding mistakes. Static analyzers have become quite good at spotting these types of errors, but they don't scale very well. If, for instance, you need to check a code base of a few million lines you better be prepared to wait for the result; sometimes hours. Eyeballing a large code base to find flaws is clearly not an option, so what is missing is a static analysis capability that can be used to answer common types of queries interactively, even for large code bases. I will describe the design and use of such a tool in this talk.","conference":"IEEE","terms":"","keywords":"","startPage":"1","endPage":"1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115610","citationCount":0,"referenceCount":0,"year":2017,"authors":"G. Holzmann","affiliations":"Nimble Research, Arcadia, CA 91006, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f1"},"title":"Measuring Object-Oriented Design Principles","abstract":"The idea of automatizing the assessment of object-oriented design is not new. Different approaches define and apply their own quality models, which are composed of single metrics or combinations thereof, to operationalize software design. However, single metrics are too fine-grained to identify core design flaws and they cannot provide hints for making design improvements. In order to deal with these weaknesses of metric-based models, rules-based approaches have proven successful in the realm of source-code quality. Moreover, for developing a well-designed software system, design principles play a key role, as they define fundamental guidelines and help to avoid pitfalls. Therefore, this thesis will enhance and complete a rule-based quality reference model for operationalizing design principles and will provide a measuring tool that implements these rules. The validation of the design quality model and the measurement tool will be based on various industrial projects. Additionally, quantitative and qualitative surveys will be conducted in order to get validated results on the value of object-oriented design principles for software development.","conference":"IEEE","terms":"Object oriented modeling;Context;Software design;Software measurement;Software engineering,knowledge based systems;object-oriented methods;object-oriented programming;software quality;source code (software),object-oriented design;software design;metric-based model;rule-based quality reference model;source-code quality;software development","keywords":"design principles;software-design quality;softwaredesign assessment method;design model","startPage":"882","endPage":"885","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372084","citationCount":1,"referenceCount":14,"year":2015,"authors":"J. Braeuer","affiliations":"Dept. of Bus. Inf. - Software Eng., Johannes Kepler Univ. Linz, Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f2"},"title":"Characterizing and taming non-deterministic bugs in Javascript applications","abstract":"JavaScript has become one of the most popular programming languages for both client-side and server-side applications. In JavaScript applications, events may be generated, triggered and consumed non-deterministically. Thus, JavaScript applications may suffer from non-deterministic bugs, when events are triggered and consumed in an unexpected order. In this proposal, we aim to characterize and combat non-deterministic bugs in JavaScript applications. Specifically, we first perform a comprehensive study about real-world non-deterministic bugs in server-side JavaScript applications. In order to facilitate bug diagnosis, we further propose approaches to isolate the necessary events that are responsible for the occurrence of a failure. We also plan to design new techniques in detecting non-deterministic bugs in JavaScript applications.","conference":"IEEE","terms":"Computer bugs;Debugging;Tools;Proposals;Open source software;Computer languages;Computer architecture,Java;program debugging;program diagnostics;system recovery,nondeterministic bugs;JavaScript applications;server-side applications","keywords":"JavaScript;Node.js;non-deterministic bug;empirical study;record and replay;bug detection","startPage":"1006","endPage":"1009","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115720","citationCount":0,"referenceCount":25,"year":2017,"authors":"J. Wang","affiliations":"State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China, University of Chinese Academy of Sciences, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f3"},"title":"Software engineering without borders","abstract":"DevOps approaches software engineering by advocating the removal of borders between development and operations. DevOps emphasizes operational resilience, continuous feedback from operations back to development, and rapid deployment of features developed. In this talk we will look at selected (automation) aspects related to DevOps, based on our collaborations with various industrial partners. For example, we will explore (automated) methods for analyzing log data to support deployments and monitor REST API integrations, (search-based) test input generation for reproducing crashes and testing complex database queries, and zero downtime database schema evolution and deployment. We will close by looking at borders beyond those between development and operations, in order to see whether there are other borders we need to remove in order to strengthen the impact of software engineering research.","conference":"IEEE","terms":"","keywords":"","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115612","citationCount":0,"referenceCount":0,"year":2017,"authors":"A. van Deursen","affiliations":"Department of Software Technolgy, Delft University of Technology, The Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f4"},"title":"MetaMod: A Modeling Formalism with Modularity at Its Core","abstract":"Because modern engineering products require more and more functionality, the models used in the design of these products get larger and more complex. A way to handle this complexity would be a suitable mechanism to modularize models. However, current approaches in the Model Driven Engineering field have limited support for modularity. This is the gap that our research addresses. We want to tackle the gap by designing and creating a modeling formalism with modularity at its core - MetaMod. We are including the modeling formalism into a prototype such that we can experiment with it. Our evaluation plan includes bootstrapping MetaMod (defining MetaMod in MetaMod) and creating an industrial DSL in MetaMod.","conference":"IEEE","terms":"Object oriented modeling;Documentation;Computational modeling;Complexity theory;Prototypes;Software;Calculus,formal specification;statistical analysis,modeling formalism;model driven engineering field;bootstrapping MetaMod;industrial DSL","keywords":"modeling;modularity;DSLs","startPage":"890","endPage":"893","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372086","citationCount":1,"referenceCount":16,"year":2015,"authors":"A. ?utîi","affiliations":"Dept. of Math. \u0026 Comput. Sci., Eindhoven Univ. of Technol., Eindhoven, Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f5"},"title":"TRAM: A tool for transforming textual requirements into analysis models","abstract":"Tool support for automatically constructing analysis models from the natural language specification of requirements (NLR) is critical to model driven development (MDD), as it can bring forward the use of precise formal languages from the coding to the specification phase in the MDD lifecycle. TRAM provides such a support through a novel approach. By using a set of conceptual patterns to facilitate the transformation of an NLR to its target software model, TRAM has shown its potential as an automated tool to support the earliest phase of MDD. This paper describes TRAM and evaluates the tool against three benchmark approaches.","conference":"IEEE","terms":"Unified modeling language;Analytical models;Object oriented modeling;Software;Natural languages;Semantics,formal languages;formal specification;natural language processing;program diagnostics;software tools,TRAM;textual requirement transformation;analysis models;tool support;natural language specification of requirements;model driven development;formal languages;software model","keywords":"Model transformation;natural language processing;conceptual patterns;semantic object models;analysis models","startPage":"738","endPage":"741","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693146","citationCount":4,"referenceCount":15,"year":2013,"authors":"K. J. Letsholo; L. Zhao; E. Chioasca","affiliations":"School of Computer Science, The University of Manchester, U.K.; School of Computer Science, The University of Manchester, U.K.; School of Computer Science, The University of Manchester, U.K.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f6"},"title":"Understanding, Refactoring, and Fixing Concurrency in C#","abstract":"Industry leaders provide concurrent libraries because asynchronous \u0026 parallel programming are increasingly in demand: responsiveness, scalability, and high-throughput are key elements of all modern applications. However, we know little about how developers use these concurrent libraries in practice and the developer's toolbox for concurrency is very limited. We present the first study that analyzes the usage of concurrent libraries in large codebases, such as 2258 open-source C# apps comprising 54M SLOC and 1378 open-source Windows Phone apps comprising 12M SLOC. Using this data, we find important problems about use and misuse of concurrency. Inspired by our findings, we designed, evaluated, and implemented several static analyses and refactoring tools.","conference":"IEEE","terms":"Libraries;Java;Concurrent computing;Parallel programming;Open source software;Reactive power,concurrency (computers);parallel programming;program diagnostics;public domain software;software maintenance,asynchronous programming;parallel programming;concurrent libraries;open-source C# apps;54M SLOC;open-source Windows Phone apps;12M SLOC;static analyses;refactoring tools;software refactoring","keywords":"","startPage":"898","endPage":"901","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372088","citationCount":0,"referenceCount":25,"year":2015,"authors":"S. Okur","affiliations":"Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f7"},"title":"Privacy-aware data-intensive applications","abstract":"The rise of Big Data is leading to an increasing demand for data-intensive applications (DIAs), which, in many cases, are expected to process massive amounts of sensitive data. In this context, ensuring data privacy becomes paramount. While the way we design and develop DIAs has radically changed over the last few years in order to deal with Big Data, there has been relatively little effort to make such design privacy-aware. As a result, enforcing privacy policies in large-scale data processing is currently an open research problem. This thesis proposal makes one step towards this investigation: after identifying the dataflow model as the reference computational model for large-scale DIAs, (1) we propose a novel language for specifying privacy policies on dataflow applications along with (2) a dataflow rewriting mechanism to enforce such policies during DIA execution. Although a systematic evaluation still needs to be carried out, preliminary results are promising. We plan to implement our approach within a model-driven solution to ultimately simplify the design and development of privacy-aware DIAs, i.e. DIAs that ensure privacy policies at runtime.","conference":"IEEE","terms":"Data privacy;Privacy;Computational modeling;Access control;Data models;Big Data;Context modeling,Big Data;data flow analysis;data privacy;rewriting systems,reference computational model;large-scale DIAs;dataflow applications;dataflow rewriting mechanism;DIA execution;privacy-aware DIAs;privacy-aware data-intensive applications;Big Data;sensitive data;data privacy;design privacy;large-scale data processing;open research problem;dataflow model;privacy policies","keywords":"Data Privacy;Data-Intensive Applications;Big Data;Dataflow computing","startPage":"1030","endPage":"1033","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115726","citationCount":0,"referenceCount":11,"year":2017,"authors":"M. Guerriero","affiliations":"Politecnico di Milano, DEIB, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f8"},"title":"Clone Merge -- An Eclipse Plugin to Abstract Near-Clone C++ Methods","abstract":"Software clones are prevalent. In the work of Laguë et al. [2], they observe that 6.4% and 7.5% of the source code in different versions of a large, mature code base are clones. The work of Baxter et al. [1] reports even higher numbers, sometimes exceeding 25%. We consider the prevalence of such near miss clones to be strong indicators that copy-paste-modify is a wide-spread development methodology. Even though clones are prevalent, they are a significant development headache. Specially, if bugs arise in one of the clones, they need to be fixed in all of the clones. This problem is acknowledged in the work of Juergens et al. [4] who say in their work that \"cloning can be a substantial problem during development and maintenance\", since \"inconsistent clones constitute a major source of faults\". A similar concern is raised in practitioner literature [3] suggesting that clones should be removed in some form or the other. We present a tool that can be installed as a plugin to Eclipse CDT, the development environment for C/C++. The research prototype comes with a refactoring option called \"Copy Paste merge\" refactoring, which is available as a menu option in the modified version of the Eclipse CDT.","conference":"IEEE","terms":"Cloning;Switches;Industries;Software engineering;Maintenance engineering;Prototypes;Syntactics,C++ language;program debugging;software maintenance;source code (software),clone merge;near-clone C++ methods;source code;near miss software clones;bugs;software development;software maintenance;Eclipse CDT plugin;copy paste merge refactoring","keywords":"Refactoring;Eclipse;CDT;Clone;Evolution;Abstraction","startPage":"819","endPage":"823","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372073","citationCount":0,"referenceCount":7,"year":2015,"authors":"K. Narasimhan","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36f9"},"title":"BOOM: Experiences in language and tool design for distributed systems (keynote)","abstract":"With the rapid expansion of cloud infrastructure and mobile devices, distributed systems have quickly emerged as a dominant computing platform. Distributed systems bring significant complexity to programming, due to platform issues including asynchrony, concurrency, and partial failure. Meanwhile, scalable distributed infrastructure—notably “NoSQL” systems—have put additional burdens on programmers by sacrificing traditional infrastructure contracts like linearizable or transactional I/O in favor of high availability. A growing segment of the developer community needs to deal with these issues today, and for the most part developers are still using languages and tools designed for sequential computation on tightly coupled architectures. This has led to software that is increasingly hard to test and hard to trust. Over the past 5 years, the BOOM project at Berkeley has focused on making it easier to write correct and maintainable code for distributed systems. Our work has taken a number of forms, including the development of the Bloom programming language for distributed systems, tools for testing and checking distributed programs, and the CALM Theorem, which connects programmer level concerns of determinism to system-level concerns about the need for distributed coordination. This talk will reflect on this work, and highlight opportunities for improved collaboration between the software engineering and distributed systems research communities.","conference":"IEEE","terms":"","keywords":"","startPage":"1","endPage":"1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693058","citationCount":0,"referenceCount":0,"year":2013,"authors":"J. M. Hellerstein","affiliations":"University of California at Berkeley, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36fa"},"title":"Tool support for automatic model transformation specification using concrete visualisations","abstract":"Complex model transformation is crucial in several domains, including Model-Driven Engineering (MDE), information visualisation and data mapping. Most current approaches use meta-model-driven transformation specification via coding in textual scripting languages. This paper demonstrates a novel approach and tool support that instead provides for specification of correspondences between models using concrete visualisations of source and target models, and generates transformation scripts from these by-example model correspondence specifications.","conference":"IEEE","terms":"Solid modeling;Visualization;Concrete;Computational modeling;Data visualization;Design automation,data visualisation;formal specification,concrete visualisations;automatic model transformation specification;model-driven engineering;information visualisation;data mapping;meta-model-driven transformation specification;textual scripting languages","keywords":"","startPage":"718","endPage":"721","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693141","citationCount":1,"referenceCount":9,"year":2013,"authors":"I. Avazpour; J. Grundy; L. Grunske","affiliations":"Faculty of ICT, Centre for Computing and Engineering Software and Systems (SUCCESS), Swinburne University of Technology, Hawthorn 3122, VIC, Australia; Faculty of ICT, Centre for Computing and Engineering Software and Systems (SUCCESS), Swinburne University of Technology, Hawthorn 3122, VIC, Australia; Institute of Software Technology, Universität Stuttgart, Universitätsstraße 38, D-70569, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36fb"},"title":"Mining structures from massive text data: Will it help software engineering?","abstract":"The real-world big data are largely unstructured, interconnected text data. One of the grand challenges is to turn such massive unstructured text data into structured, actionable knowledge. We propose a text mining approach that requires only distant or minimal supervision but relies on massive text data. We show quality phrases can be mined from such massive text data, types can be extracted from massive text data with distant supervision, and entities/attributes/values can be discovered by meta-path directed pattern discovery. We show text-rich and structure-rich networks can be constructed from massive unstructured data. Finally, we speculate whether such a paradigm could be useful for turning massive software repositories into multi-dimensional structures to help searching and mining software repositories.","conference":"IEEE","terms":"","keywords":"","startPage":"2","endPage":"2","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115611","citationCount":0,"referenceCount":0,"year":2017,"authors":"J. Han","affiliations":"Abel Bliss Professor, Department of Computer Science, University of Illinois at Urbana-Champaign, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36fc"},"title":"Crushinator: A framework towards game-independent testing","abstract":"Testing game applications relies heavily on beta testing methods. The effectiveness of beta testing depends on how well beta testers represent the common game-application users and if users are willing to participate in the beta test. An automated testing tool framework could reduce the dependence upon beta testing by most companies to analyze their game applications. This paper presents the Crushinator as one such framework. This framework provides a game-independent testing tool that implements multiple testing methods that can assist and possibly replace the use of beta testing.","conference":"IEEE","terms":"Testing;Games;Servers;Unified modeling language;Engines;Load modeling;Computer architecture,computer games;program testing,Crushinator;game-independent testing;beta testing method;automated testing tool","keywords":"Crushinator;model-based testing;exploratory testing;event-driven applications","startPage":"726","endPage":"729","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693143","citationCount":5,"referenceCount":14,"year":2013,"authors":"C. Schaefer; Hyunsook Do; B. M. Slator","affiliations":"North Dakota State University, Computer Science, Fargo, USA; North Dakota State University, Computer Science, Fargo, USA; North Dakota State University, Computer Science, Fargo, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36fd"},"title":"Towards API-specific automatic program repair","abstract":"The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges.","conference":"IEEE","terms":"Computer bugs;Maintenance engineering;Software;Data mining;Benchmark testing;Automation;Fasteners,application program interfaces;Java;program debugging;program testing;software maintenance,API-specific automatic program repair;API usage patterns;API-specific information;erroneous code;API-specific fixes;generic bugs","keywords":"Automatic Program Repair;API-specific Bugs;Specification Mining","startPage":"1010","endPage":"1013","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115721","citationCount":0,"referenceCount":38,"year":2017,"authors":"S. Nielebock","affiliations":"Chair of Software Engineering, Faculty of Computer Science, Otto-von-Guericke University Magdeburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36fe"},"title":"Developing self-verifying service-based systems","abstract":"We present a tool-supported framework for the engineering of service-based systems (SBSs) capable of self-verifying their compliance with developer-specified reliability requirements. These self-verifying systems select their services dynamically by using a combination of continual quantitative verification and online updating of the verified models. Our framework enables the practical exploitation of recent theoretical advances in the development of self-adaptive SBSs through (a) automating the generation of the software components responsible for model updating, continual verification and service selection; and (b) employing standard SBS development processes.","conference":"IEEE","terms":"Scattering;Unified modeling language;Reliability;Analytical models;Web services;Quality of service;Adaptation models,formal verification;object-oriented programming;software reliability,self-verifying service-based system development;tool-supported framework;developer-specified reliability requirements;continual quantitative verification;online updating;self-adaptive SBSs development;software component generation;model updating;service selection","keywords":"","startPage":"734","endPage":"737","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693145","citationCount":13,"referenceCount":16,"year":2013,"authors":"R. Calinescu; K. Johnson; Y. Rafiq","affiliations":"Department of Computer Science, University of York, UK; Department of Computer Science, University of York, UK; Department of Computer Science, University of York, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d36ff"},"title":"A Generic Framework for Concept-Based Exploration of Semi-Structured Software Engineering Data","abstract":"Software engineering meta-data (SE data), such as revision control data, Github project data or test reports, is typically semi-structured, it comprises a mixture of formatted and free-text fields and is often self-describing. Semi-structured SE data cannot be queried in a SQL-like manner because of its lack of structure. Consequently, there are a variety of customized tools built to analyze specific datasets but these do not generalize. We propose to develop a generic framework for exploration and querying of semi-structured SE data. Our approach investigates the use of a formal concept lattice as a universal data structure and a tag cloud as an intuitive interface to support data exploration.","conference":"IEEE","terms":"Lattices;Tag clouds;Data visualization;Navigation;Prototypes;Software engineering;Context,data structures;formal concept analysis;meta data;query processing;software engineering;SQL;user interfaces,semistructured software engineering data;revision control data;Github project data;test reports;formatted fields;free-text fields;SQL-like manner;customized tools;semistructured SE data querying;universal data structure;tag cloud;intuitive interface;data exploration","keywords":"formal concept analysis;tag clouds;browsing software repositories","startPage":"894","endPage":"897","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372087","citationCount":5,"referenceCount":21,"year":2015,"authors":"G. J. Greene","affiliations":"Centre for Artificial Intell. Res. Comput. Sci. Div., Matieland Stellenbosch Univ., Stellenbosch, South Africa","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3700"},"title":"Automatic Self-Validation for Code Coverage Profilers","abstract":"Code coverage as the primitive dynamic program behavior information, is widely adopted to facilitate a rich spectrum of software engineering tasks, such as testing, fuzzing, debugging, fault detection, reverse engineering, and program understanding. Thanks to the widespread applications, it is crucial to ensure the reliability of the code coverage profilers. Unfortunately, due to the lack of research attention and the existence of testing oracle problem, coverage profilers are far away from being tested sufficiently. Bugs are still regularly seen in the widely deployed profilers, like gcov and llvm-cov, along with gcc and llvm, respectively. This paper proposes Cod, an automated self-validator for effectively uncovering bugs in the coverage profilers. Starting from a test program (either from a compiler's test suite or generated randomly), Cod detects profiler bugs with zero false positive using a metamorphic relation in which the coverage statistics of that program and a mutated variant are bridged. We evaluated Cod over two of the most well-known code coverage profilers, namely gcov and llvm-cov. Within a four-month testing period, a total of 196 potential bugs (123 for gcov, 73 for llvm-cov) are found, among which 23 are confirmed by the developers.","conference":"IEEE","terms":"","keywords":"Code coverage, Metamorphic testing, Coverage profilers, Bug detection.","startPage":"79","endPage":"90","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952463","citationCount":0,"referenceCount":51,"year":2019,"authors":"Y. Yang; Y. Jiang; Z. Zuo; Y. Wang; H. Sun; H. Lu; Y. Zhou; B. Xu","affiliations":"Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University; Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3701"},"title":"Machine Learning Based Recommendation of Method Names: How Far are We","abstract":"High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.25% and 22.45%, respectively. The comparison suggests that machine learning based recommendation of method names may still have a long way to go.","conference":"IEEE","terms":"","keywords":"Code Recommendation;Machine Learning","startPage":"602","endPage":"614","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952208","citationCount":0,"referenceCount":46,"year":2019,"authors":"L. Jiang; H. Liu; H. Jiang","affiliations":"Beijing Institute of Technology; Beijing Institute of Technology; Dalian University of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3702"},"title":"Learning from Examples to Find Fully Qualified Names of API Elements in Code Snippets","abstract":"Developers often reuse code snippets from online forums, such as Stack Overflow, to learn API usages of software frameworks or libraries. These code snippets often contain ambiguous undeclared external references. Such external references make it difficult to learn and use those APIs correctly. In particular, reusing code snippets containing such ambiguous undeclared external references requires significant manual efforts and expertise to resolve them. Manually resolving fully qualified names (FQN) of API elements is a non-trivial task. In this paper, we propose a novel context-sensitive technique, called COSTER, to resolve FQNs of API elements in such code snippets. The proposed technique collects locally specific source code elements as well as globally related tokens as the context of FQNs, calculates likelihood scores, and builds an occurrence likelihood dictionary (OLD). Given an API element as a query, COSTER captures the context of the query API element, matches that with the FQNs of API elements stored in the OLD, and rank those matched FQNs leveraging three different scores: likelihood, context similarity, and name similarity scores. Evaluation with more than 600K code examples collected from GitHub and two different Stack Overflow datasets shows that our proposed technique improves precision by 4-6% and recall by 3-22% compared to state-of-the-art techniques. The proposed technique significantly reduces the training time compared to the StatType, a state-of-the-art technique, without sacrificing accuracy. Extensive analyses on results demonstrate the robustness of the proposed technique.","conference":"IEEE","terms":"","keywords":"API usages, Context sensitive technique, Recommendation system, Fully Qualified Name","startPage":"243","endPage":"254","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952501","citationCount":0,"referenceCount":41,"year":2019,"authors":"C. M. K. Saifullah; M. Asaduzzaman; C. K. Roy","affiliations":"University of Saskatchewan; Queen's University; University of Saskatchewan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3703"},"title":"Accurate Modeling of Performance Histories for Evolving Software Systems","abstract":"Learning from the history of a software system's performance behavior does not only help discovering and locating performance bugs, but also identifying evolutionary performance patterns and general trends, such as when technical debt accumulates. Exhaustive regression testing is usually impractical, because rigorous performance benchmarking requires executing a realistic workload per revision, which results in large execution times. In this paper, we propose a novel active revision sampling approach, which aims at tracking and understanding a system's performance history by approximating the performance behavior of a software system across all of its revisions. In a nutshell, we iteratively sample and measure the performance of specific revisions that help us building an exact performance-evolution model, and we use Gaussian Process models to assess in which revision ranges our model is most uncertain with the goal to sample further revisions for measurement. We have conducted an empirical analysis of the evolutionary performance behavior modeled as a time series of the histories of six real-world software systems. Our evaluation demonstrates that Gaussian Process models are able to accurately estimate the performance-evolution history of real-world software systems with only few measurements and to reveal interesting behaviors and trends.","conference":"IEEE","terms":"","keywords":"software evolution;software performance;time series analysis","startPage":"640","endPage":"652","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952290","citationCount":0,"referenceCount":39,"year":2019,"authors":"S. Mühlbauer; S. Apel; N. Siegmund","affiliations":"Bauhaus-University Weimar; Saarland University; Bauhaus-University Weimar","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3704"},"title":"Model Checking Embedded Control Software using OS-in-the-Loop CEGAR","abstract":"Verification of multitasking embedded software requires taking into account its underlying operating system w.r.t. its scheduling policy and handling of task priorities in order to achieve a higher degree of accuracy. However, such comprehensive verification of multitasking embedded software together with its underlying operating system is very costly and impractical. To reduce the verification cost while achieving the desired accuracy, we propose a variant of CEGAR, named OiL-CEGAR (OS-in-the-Loop Counterexample-Guided Abstraction Refinement), where a composition of a formal OS model and an abstracted application program is used for comprehensive verification and is successively refined using the counterexamples generated from the composition model. The refinement process utilizes the scheduling information in the counterexample, which acts as a mini-OS to check the executability of the counterexample trace on the concrete program. Our experiments using a prototype implementation of OiL-CEGAR show that OiL-CEGAR greatly improves the accuracy and efficiency of property checking in this domain. It automatically removed all false alarms and accomplished property checking within an average of 476 seconds over a set of multitasking programs, whereas model checking using existing approaches over the same set of programs either showed an accuracy of under 11.1% or was unable to finish the verification due to timeout.","conference":"IEEE","terms":"","keywords":"CEGAR;embedded OS;multitasking","startPage":"565","endPage":"576","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952214","citationCount":0,"referenceCount":47,"year":2019,"authors":"D. Kim; Y. Choi","affiliations":"Kyungpook National University; Kyungpook National University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3705"},"title":"Improving the Decision-Making Process of Self-Adaptive Systems by Accounting for Tactic Volatility","abstract":"When self-adaptive systems encounter changes withintheir surrounding environments, they enacttacticsto performnecessary adaptations. For example, a self-adaptive cloud-basedsystem may have a tactic that initiates additional computingresources when response time thresholds are surpassed, or theremay be a tactic to activate a specific security measure when anintrusion is detected. In real-world environments, these tacticsfrequently experiencetactic volatilitywhich is variable behaviorduring the execution of the tactic.Unfortunately, current self-adaptive approaches do not accountfor tactic volatility in their decision-making processes, and merelyassume that tactics do not experience volatility. This limitationcreates uncertainty in the decision-making process and mayadversely impact the system's ability to effectively and efficientlyadapt. Additionally, many processes do not properly account forvolatility that may effect the system's Service Level Agreement(SLA). This can limit the system's ability to act proactively, especially when utilizing tactics that contain latency.To address the challenge of sufficiently accounting for tacticvolatility, we propose aTactic Volatility Aware(TVA) solution.Using Multiple Regression Analysis (MRA), TVA enables self-adaptive systems to accurately estimate the cost and timerequired to execute tactics. TVA also utilizesAutoregressiveIntegrated Moving Average(ARIMA) for time series forecasting, allowing the system to proactively maintain specifications.","conference":"IEEE","terms":"","keywords":"Artificial Intelligence, Self-Adaptation, Machine Learning","startPage":"949","endPage":"961","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952178","citationCount":0,"referenceCount":45,"year":2019,"authors":"J. Palmerino; Q. Yu; T. Desell; D. Krutz","affiliations":"RIT; RIT; RIT; RIT","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3706"},"title":"Characterizing Android App Signing Issues","abstract":"In the app releasing process, Android requires all apps to be digitally signed with a certificate before distribution. Android uses this certificate to identify the author and ensure the integrity of an app. However, a number of signature issues have been reported recently, threatening the security and privacy of Android apps. In this paper, we present the first large-scale systematic measurement study on issues related to Android app signatures. We first create a taxonomy covering four types of app signing issues (21 anti-patterns in total), including vulnerabilities, potential attacks, release bugs and compatibility issues. Then we developed an automated tool to characterize signature-related issues in over 5 million app items (3 million distinct apks) crawled from Google Play and 24 alternative Android app markets. Our empirical findings suggest that although Google has introduced apk-level signing schemes (V2 and V3) to overcome some of the known security issues, more than 93% of the apps still use only the JAR signing scheme (V1), which poses great security threats. Besides, we also revealed that 7% to 45% of the apps in the 25 studied markets have been found containing at least one signing issue, while a large number of apps have been exposed to security vulnerabilities, attacks and compatibility issues. Among them a considerable number of apps we identified are popular apps with millions of downloads. Finally, our evolution analysis suggested that most of the issues were not mitigated after a considerable amount of time across markets. The results shed light on the emergency for detecting and repairing the app signing issues.","conference":"IEEE","terms":"","keywords":"Signature;Vulnerability;Mobile App;Certificate","startPage":"280","endPage":"292","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952357","citationCount":0,"referenceCount":64,"year":2019,"authors":"H. Wang; H. Liu; X. Xiao; G. Meng; Y. Guo","affiliations":"Beijing University of Posts and Telecommunications; Peking University; Case Western Reserve University; SKLOIS, Chinese Academy of Sciences; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3707"},"title":"SCMiner: Localizing System-Level Concurrency Faults from Large System Call Traces","abstract":"Localizing concurrency faults that occur in production is hard because, (1) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers to reproduce the failure; (2) it is often impractical to assume the availability of multiple failing executions to localize the faults using existing techniques; (3) it is challenging to search for buggy locations in an application given limited runtime data; and, (4) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which can not be handled by existing tools for diagnosing intra-process(thread-level) failures. To address these problems, we present SCMiner, a practical online bug diagnosis tool to help developers understand how a system-level concurrency fault happens based on the logs collected by the default system audit tools. SCMiner achieves online bug diagnosis to obviate the need for offline bug reproduction. SCMiner does not require code instrumentation on the production system or rely on the assumption of the availability of multiple failing executions. Specifically, after the system call traces are collected, SCMiner uses data mining and statistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormal sequence to specific application functions. We have conducted an empirical study on 19 real-world benchmarks. The results show that SCMiner is both effective and efficient at localizing system-level concurrency faults.","conference":"IEEE","terms":"","keywords":"Multi Process Applications, Concurrency Failures, Fault Localization","startPage":"515","endPage":"526","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952396","citationCount":0,"referenceCount":56,"year":2019,"authors":"T. S. Zaman; X. Han; T. Yu","affiliations":"University of Kentucky; University of Kentucky; University of Kentucky","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3708"},"title":"Discovering, Explaining and Summarizing Controversial Discussions in Community Q\u0026A Sites","abstract":"Developers often look for solutions to programming problems in community Q\u0026A sites like Stack Overflow. Due to the crowdsourcing nature of these Q\u0026A sites, many user-provided answers are wrong, less optimal or out-of-date. Relying on community-curated quality indicators (e.g., accepted answer, answer vote) cannot reliably identify these answer problems. Such problematic answers are often criticized by other users. However, these critiques are not readily discoverable when reading the posts. In this paper, we consider the answers being criticized and their critique posts as controversial discussions in community Q\u0026A sites. To help developers notice such controversial discussions and make more informed choices of appropriate solutions, we design an automatic open information extraction approach for systematically discovering and summarizing the controversies in Stack Overflow and exploiting official API documentation to assist the understanding of the discovered controversies. We apply our approach to millions of java/android-tagged Stack overflow questions and answers and discover a large scale of controversial discussions in Stack Overflow. Our manual evaluation confirms that the extracted controversy information is of high accuracy. A user study with 18 developers demonstrates the usefulness of our generated controversy summaries in helping developers avoid the controversial answers and choose more appropriate solutions to programming questions.","conference":"IEEE","terms":"","keywords":"Controversial discussion, Stack Overflow, Open information extraction, Sentence embedding","startPage":"151","endPage":"162","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952216","citationCount":0,"referenceCount":47,"year":2019,"authors":"X. Ren; Z. Xing; X. Xia; G. Li; J. Sun","affiliations":"Zhejiang University, Ningbo Research Institute, PengCheng Laboratory; Australian National University; Monash University; Shanghai Jiao Tong University; Zhejiang University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d3709"},"title":"Cautious Adaptation of Defiant Components","abstract":"Systems-of-systems are formed by the composition of independently created software components. These components are designed to satisfy their individual requirements, rather than the global requirements of the systems-of-systems. We refer to components that cannot be adapted to meet both individual and global requirements as \"defiant\" components. In this paper, we propose a \"cautious\" adaptation approach which supports changing the behaviour of such defiant components under exceptional conditions to satisfy global requirements, while continuing to guarantee the satisfaction of the components' individual requirements. The approach represents both normal and exceptional conditions as scenarios; models the behaviour of exceptional conditions as wrappers implemented using an aspect-oriented technique; and deals with both single and multiple instances of defiant components with different precedence order at runtime. We evaluated an implementation of the approach using drones and boats for an organ delivery application conceived by our industrial partners, in which we assess how the proposed approach help achieve the system-of-systems' global requirements while accommodating increased complexity of hybrid aspects such as multiplicity, precedence ordering, openness, and heterogeneity.","conference":"IEEE","terms":"","keywords":"Defiant Component, Adaptation, Scenarios, Aspects","startPage":"974","endPage":"985","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952359","citationCount":0,"referenceCount":47,"year":2019,"authors":"P. H. Maia; L. Vieira; M. Chagas; Y. Yu; A. Zisman; B. Nuseibeh","affiliations":"State University of Ceará, Fortaleza, CE, Brazil; State University of Ceará, Fortaleza, CE, Brazil; State University of Ceará, Fortaleza, CE, Brazil; The Open University, UK; The Open University, UK; The Open University, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d370a"},"title":"Goal-Driven Exploration for Android Applications","abstract":"This paper proposes a solution for automated goal-driven exploration of Android applications - a scenario in which a user, e.g., a security auditor, needs to dynamically trigger the functionality of interest in an application, e.g., to check whether user-sensitive info is only sent to recognized third-party servers. As the auditor might need to check hundreds or even thousands of apps, manually exploring each app to trigger the desired behavior is too time-consuming to be feasible. Existing automated application exploration and testing techniques are of limited help in this scenario as well, as their goal is mostly to identify faults by systematically exploring different app paths, rather than swiftly navigating to the target functionality. The goal-driven application exploration approach proposed in this paper, called GoalExplorer, automatically generates an executable test script that directly triggers the functionality of interest. The core idea behind GoalExplorer is to first statically model the application's UI screens and transitions between these screens, producing a Screen Transition Graph (STG). Then, GoalExplorer uses the STG to guide the dynamic exploration of the application to the particular target of interest: an Android activity, API call, or a program statement. The results of our empirical evaluation on 93 benchmark applications and the 95 most popular GooglePlay applications show that the STG is substantially more accurate than other Android UI models and that GoalExplorer is able to trigger a target functionality much faster than existing application exploration techniques.","conference":"IEEE","terms":"","keywords":"mobile applications;automated testing;automated GUI exploration","startPage":"115","endPage":"127","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952363","citationCount":0,"referenceCount":60,"year":2019,"authors":"D. Lai; J. Rubin","affiliations":"University of British Columbia; University of British Columbia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d370b"},"title":"InFix: Automatically Repairing Novice Program Inputs","abstract":"This paper presents InFix, a technique for automatically fixing erroneous program inputs for novice programmers. Unlike comparable existing approaches for automatic debugging and maintenance tasks, InFix repairs input data rather than source code, does not require test cases, and does not require special annotations. Instead, we take advantage of patterns commonly used by novice programmers to automatically create helpful, high quality input repairs. InFix iteratively applies error-message based templates and random mutations based on insights about the debugging behavior of novices. This paper presents an implementation of InFix for Python. We evaluate on 29,995 unique scenarios with input-related errors collected from four years of data from Python Tutor, a free online programming tutoring environment. Our results generalize and scale; compared to previous work, we consider an order of magnitude more unique programs. Overall, InFix is able to repair 94.5% of deterministic input errors. We also present the results of a human study with 97 participants. Surprisingly, this simple approach produces high quality repairs; humans judged the output of InFix to be equally helpful and within 4% of the quality of human-generated repairs.","conference":"IEEE","terms":"","keywords":"input repair;novice programs;human study","startPage":"399","endPage":"410","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952434","citationCount":0,"referenceCount":51,"year":2019,"authors":"M. Endres; G. Sakkas; B. Cosman; R. Jhala; W. Weimer","affiliations":"University of Michigan; University of California San Diego; University of California San Diego; University of California San Diego; University of Michigan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d370c"},"title":"Automatic Generation of Pull Request Descriptions","abstract":"Enabled by the pull-based development model, developers can easily contribute to a project through pull requests (PRs). When creating a PR, developers can add a free-form description to describe what changes are made in this PR and/or why. Such a description is helpful for reviewers and other developers to gain a quick understanding of the PR without touching the details and may reduce the possibility of the PR being ignored or rejected. However, developers sometimes neglect to write descriptions for PRs. For example, in our collected dataset with over 333K PRs, more than 34% of the PR descriptions are empty. To alleviate this problem, we propose an approach to automatically generate PR descriptions based on the commit messages and the added source code comments in the PRs. We regard this problem as a text summarization problem and solve it using a novel sequence-to-sequence model. To cope with out-of-vocabulary words in software artifacts and bridge the gap between the training loss function of the sequence-to-sequence model and the evaluation metric ROUGE, which has been shown to correspond to human evaluation, we integrate the pointer generator and directly optimize for ROUGE using reinforcement learning and a special loss function. We build a dataset with over 41K PRs and evaluate our approach on this dataset through ROUGE and a human evaluation. Our evaluation results show that our approach outperforms two baselines by significant margins.","conference":"IEEE","terms":"","keywords":"Pull Request;Document Generation;Sequence to Sequence Learning","startPage":"176","endPage":"188","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952330","citationCount":0,"referenceCount":63,"year":2019,"authors":"Z. Liu; X. Xia; C. Treude; D. Lo; S. Li","affiliations":"Zhejiang University; Monash University; University of Adelaide; Singapore Management University; Zhejiang University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d370d"},"title":"An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms","abstract":"Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively.","conference":"IEEE","terms":"","keywords":"Deep learning frameworks;Deep learning platforms;Deep learning deployment;Empirical study","startPage":"810","endPage":"822","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952401","citationCount":1,"referenceCount":70,"year":2019,"authors":"Q. Guo; S. Chen; X. Xie; L. Ma; Q. Hu; H. Liu; Y. Liu; J. Zhao; X. Li","affiliations":"Tianjin University; Nanyang Technological University; Nanyang Technological University; Kyushu University; Kyushu University; Tianjin University; Nanyang Technological University; Kyushu University; Tianjin University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d370e"},"title":"RENN: Efficient Reverse Execution with Neural-Network-Assisted Alias Analysis","abstract":"Reverse execution and coredump analysis have long been used to diagnose the root cause of software crashes. Each of these techniques, however, face inherent challenges, such as insufficient capability when handling memory aliases. Recent works have used hypothesis testing to address this drawback, albeit with high computational complexity, making them impractical for real world applications. To address this issue, we propose a new deep neural architecture, which could significantly improve memory alias resolution. At the high level, our approach employs a recurrent neural network (RNN) to learn the binary code pattern pertaining to memory accesses. It then infers the memory region accessed by memory references. Since memory references to different regions naturally indicate a non-alias relationship, our neural architecture can greatly reduce the burden of doing hypothesis testing to track down non-alias relation in binary code. Different from previous researches that have utilized deep learning for other binary analysis tasks, the neural network proposed in this work is fundamentally novel. Instead of simply using off-the-shelf neural networks, we designed a new recurrent neural architecture that could capture the data dependency between machine code segments. To demonstrate the utility of our deep neural architecture, we implement it as RENN, a neural network-assisted reverse execution system. We utilize this tool to analyze software crashes corresponding to 40 memory corruption vulnerabilities from the real world. Our experiments show that RENN can significantly improve the efficiency of locating the root cause for the crashes. Compared to a state-of-the-art technique, RENN has 36.25% faster execution time on average, detects an average of 21.35% more non-alias pairs, and successfully identified the root cause of 12.5% more cases.","conference":"IEEE","terms":"","keywords":"Reverse Execution, Deep Learning, Memory Alias","startPage":"924","endPage":"935","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952186","citationCount":0,"referenceCount":50,"year":2019,"authors":"D. Mu; W. Guo; A. Cuevas; Y. Chen; J. Gai; X. Xing; B. Mao; C. Song","affiliations":"Nanjing University; The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; Nanjing University; UC Riverside","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9121eee8435e8e7d370f"},"title":"Experience Paper: Search-Based Testing in Automated Driving Control Applications","abstract":"Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.","conference":"IEEE","terms":"","keywords":"search-based testing;automated driving;automated test generation;experience paper","startPage":"26","endPage":"37","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952365","citationCount":0,"referenceCount":43,"year":2019,"authors":"C. Gladisch; T. Heinz; C. Heinzemann; J. Oehlerking; A. von Vietinghoff; T. Pfitzer","affiliations":"Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch GmbH, Corporate Research; Robert Bosch Automotive Steering GmbH","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3710"},"title":"Assessing the Generalizability of Code2vec Token Embeddings","abstract":"Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.","conference":"IEEE","terms":"","keywords":"Code Embeddings;Distributed Representations;Big Code","startPage":"1","endPage":"12","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952475","citationCount":0,"referenceCount":65,"year":2019,"authors":"H. J. Kang; T. F. Bissyandé; D. Lo","affiliations":"Singapore Management University; University of Luxembourg, Luxembourg; Singapore Management University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3711"},"title":"MAP-Coverage: A Novel Coverage Criterion for Testing Thread-Safe Classes","abstract":"Concurrent programs must be thoroughly tested, as concurrency bugs are notoriously hard to detect. Code coverage criteria can be used to quantify the richness of a test suite (e.g., whether a program has been tested sufficiently) or provide practical guidelines on test case generation (e.g., as objective functions used in program fuzzing engines). Traditional code coverage criteria are, however, designed for sequential programs and thus ineffective for concurrent programs. In this work, we introduce a novel code coverage criterion for testing thread-safe classes called MAP-coverage (short for memory-access patterns). The motivation is that concurrency bugs are often correlated with certain memory-access patterns, and thus it is desirable to comprehensively cover all memory-access patterns. Furthermore, we propose a testing method for maximizing MAP-coverage. Our method has been implemented as a self-contained toolkit, and the experimental results on 20 benchmark programs show that our toolkit outperforms existing testing methods. Lastly, we show empirically that there exists positive correlation between MAP-coverage and the effectiveness of a set of test executions.","conference":"IEEE","terms":"","keywords":"Memory Access Pattern;Coverage Criteria;Thread Safe Classes;Concurrency Bugs","startPage":"722","endPage":"734","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952403","citationCount":0,"referenceCount":48,"year":2019,"authors":"Z. Wang; Y. Zhao; S. Liu; J. Sun; X. Chen; H. Lin","affiliations":"Tianjin University; Tianjin University; Tianjin University; Singapore Management University; Nantong University; Tianjin University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3712"},"title":"CodeKernel: A Graph Kernel Based Approach to the Selection of API Usage Examples","abstract":"Developers often want to find out how to use a certain API (e.g., FileReader.read in JDK library). API usage examples are very helpful in this regard. Over the years, many automated methods have been proposed to generate code examples by clustering and summarizing relevant code snippets extracted from a code corpus. These approaches simplify source code as method invocation sequences or feature vectors. Such simplifications only model partial aspects of the code and tend to yield inaccurate examples. We propose CodeKernel, a graph kernel based approach to the selection of API usage examples. Instead of approximating source code as method invocation sequences or feature vectors, CodeKernel represents source code as object usage graphs. Then, it clusters graphs by embedding them into a continuous space using a graph kernel. Finally, it outputs code examples by selecting a representative graph from each cluster using designed ranking metrics. Our empirical evaluation shows that CodeKernel selects more accurate code examples than the related work (MUSE and eXoaDocs). A user study involving 25 developers in a multinational company also confirms the usefulness of CodeKernel in selecting API usage examples.","conference":"IEEE","terms":"","keywords":"API usage example, graph kernel, code search, software reuse","startPage":"590","endPage":"601","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952546","citationCount":0,"referenceCount":53,"year":2019,"authors":"X. Gu; H. Zhang; S. Kim","affiliations":"The Hong Kong University of Science and Technology; The University of Newcastle; The Hong Kong University of Science and Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3713"},"title":"Empirical Evaluation of the Impact of Class Overlap on Software Defect Prediction","abstract":"Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance.","conference":"IEEE","terms":"","keywords":"Class overlap;Software defect prediction;K Means clustering;Machine learning","startPage":"698","endPage":"709","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952192","citationCount":0,"referenceCount":41,"year":2019,"authors":"L. Gong; S. Jiang; R. Wang; L. Jiang","affiliations":"China University of Mining and Technology; China University of Mining and Technology; China University of Mining and Technology; China University of Mining and Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3714"},"title":"SEGATE: Unveiling Semantic Inconsistencies between Code and Specification of String Inputs","abstract":"Automated testing techniques are often assessed on coverage based metrics. However, despite giving good coverage, the test cases may miss the gap between functional specification and the code implementation. This gap may be subtle in nature, arising due to the absence of logical checks, either in the implementation or in the specification, resulting in inconsistencies in the input definition. The inconsistencies may be prevalent especially for structured inputs, commonly specified using string-based data types. Our study on defects reported over popular libraries reveals that such gaps may not be limited to input validation checks. We propose a test generation technique for structured string inputs where we infer inconsistencies in input definition to expose semantic gaps in the method under test and the method specification. We assess this technique using our tool SEGATE, Semantic Gap Tester. SEGATE uses static analysis and automaton modeling to infer the gap and generate test cases. On our benchmark dataset, comprising of defects reported in 15 popular open-source libraries, written in Java, SEGATE was able to generate tests to expose 80% of the defects.","conference":"IEEE","terms":"","keywords":"testing;static analysis;string input generation;regular expression;automaton modeling;data flow analysis","startPage":"200","endPage":"212","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952371","citationCount":0,"referenceCount":47,"year":2019,"authors":"D. Sondhi; R. Purandare","affiliations":"IIIT Delhi; IIIT Delhi","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3715"},"title":"DaPanda: Detecting Aggressive Push Notifications in Android Apps","abstract":"Mobile push notifications have been widely used in mobile platforms to deliver all sorts of information to app users. Although it offers great convenience for both app developers and mobile users, this feature was frequently reported to serve malicious and aggressive purposes, such as delivering annoying push notification advertisement. However, to the best of our knowledge, this problem has not been studied by our research community so far. To fill the void, this paper presents the first study to detect aggressive push notifications and further characterize them in the global mobile app ecosystem on a large scale. To this end, we first provide a taxonomy of mobile push notifications and identify the aggressive ones using a crowdsourcing-based method. Then we propose sc DaPanda, a novel hybrid approach, aiming at automatically detecting aggressive push notifications in Android apps. sc DaPanda leverages a guided testing approach to systematically trigger and record push notifications. By instrumenting the Android framework, sc DaPanda further collects all notification-relevant runtime information to flag the aggressive ones. Our experimental results show that sc DaPanda is capable of detecting different types of aggressive push notifications effectively in an automated way. By applying sc DaPanda to 20,000 Android apps from different app markets, it yields over 1,000 aggressive notifications, which have been further confirmed as true positives. Our in-depth analysis further reveals that aggressive notifications are prevalent across different markets and could be manifested in all the phases in the lifecycle of push notifications. It is hence urgent for our community to take actions to detect and mitigate apps involving aggressive push notifications.","conference":"IEEE","terms":"","keywords":"Push notification;dynamic analysis;advertisement;Android;mobile app","startPage":"66","endPage":"78","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952509","citationCount":0,"referenceCount":75,"year":2019,"authors":"T. Liu; H. Wang; L. Li; G. Bai; Y. Guo; G. Xu","affiliations":"Beijing University of Posts and Telecommunications; Beijing University of Posts and Telecommunications; Monash University; The University of Queensland; Peking University; Beijing University of Posts and Telecommunications","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3716"},"title":"Efficient Test Generation Guided by Field Coverage Criteria","abstract":"Field-exhaustive testing is a testing criterion suitable for object-oriented code over complex, heap-allocated, data structures. It requires test suites to contain enough test inputs to cover all feasible values for the object's fields within a certain scope (input-size bound). While previous work shows that field-exhaustive suites can be automatically generated, the generation technique required a formal specification of the inputs that can be subject to SAT-based analysis. Moreover, the restriction of producing all feasible values for inputs' fields makes test generation costly. In this paper, we deal with field coverage as testing criteria that measure the quality of a test suite in terms of coverage and mutation score, by examining to what extent the values of inputs' fields are covered. In particular, we consider field coverage in combination with test generation based on symbolic execution to produce underapproximations of field-exhaustive suites, using the Symbolic Pathfinder tool. To underapproximate these suites we use tranScoping, a technique that estimates characteristics of yet to be run analyses for large scopes, based on data obtained from analyses performed in small scopes. This provides us with a suitable condition to prematurely stop the symbolic execution. As we show, tranScoping different metrics regarding field coverage allows us to produce significantly smaller suites using a fraction of the generation time. All this while retaining the effectiveness of field exhaustive suites in terms of test suite quality.","conference":"IEEE","terms":"","keywords":"Field-exhaustive testing, field-based testing, symbolic execution, transcoping","startPage":"91","endPage":"101","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952481","citationCount":0,"referenceCount":32,"year":2019,"authors":"A. Godio; V. Bengolea; P. Ponzio; N. Aguirre; M. F. Frias","affiliations":"ITBA; UNRC; UNRC; UNRC; ITBA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3717"},"title":"Fine-Grain Memory Object Representation in Symbolic Execution","abstract":"Dynamic Symbolic Execution (DSE) has seen rising popularity as it allows to check applications for behaviours such as error patterns automatically. One of its biggest challenges is the state space explosion problem: DSE tries to evaluate all possible execution paths of an application. For every path, it needs to represent the allocated memory and its accesses. Even though different approaches have been proposed to mitigate the state space explosion problem, DSE still needs to represent a multitude of states in parallel to analyse them. If too many states are present, they cannot fit into memory, and DSE needs to terminate them prematurely or store them on disc intermediately. With a more efficient representation of allocated memory, DSE can handle more states simultaneously, improving its performance. In this work, we introduce an enhanced, fine-grain and efficient representation of memory that mimics the allocations of tested applications. We tested Coreutils using three different search strategies with our implementation on top of the symbolic execution engine KLEE. We achieve a significant reduction of the memory consumption of states by up to 99.06% (mean DFS: 2%, BFS: 51%, Cov.: 49%), allowing to represent more states in memory more efficiently. The total execution time is reduced by up to 97.81% (mean DFS: 9%, BFS: 7%, Cov.:4%)—a speedup of 49x in comparison to baseline KLEE.","conference":"IEEE","terms":"","keywords":"symbolic execution;memory representation","startPage":"912","endPage":"923","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952548","citationCount":0,"referenceCount":18,"year":2019,"authors":"M. Nowack","affiliations":"Imperial College London","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3718"},"title":"PeASS: A Tool for Identifying Performance Changes at Code Level","abstract":"We present PeASS (Performance Analysis of Software System versions), a tool for detecting performance changes at source code level that occur between different code versions. By using PeASS, it is possible to identify performance regressions that happened in the past to fix them. PeASS measures the performance of unit tests in different source code versions. To achieve statistic rigor, measurements are repeated and analyzed using an agnostic t-test. To execute a minimal amount of tests, PeASS uses a regression test selection. We evaluate PeASS on a selection of Apache Commons projects and show that 81% of all unit test covered performance changes can be found by PeASS. A video presentation is available at https://www.youtube.com/watch?v=RORFEGSCh6Y and PeASS can be downloaded from https://github.com/DaGeRe/peass.","conference":"IEEE","terms":"","keywords":"software performance engineering;empirical software engineering;performance measurement;performance benchmarking","startPage":"1146","endPage":"1149","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952552","citationCount":0,"referenceCount":22,"year":2019,"authors":"D. G. Reichelt; S. Kühne; W. Hasselbring","affiliations":"Universität Leipzig; Universität Leipzig; Christian-Albrechts-Universität zu Kiel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3719"},"title":"The Impact of Structure on Software Merging: Semistructured Versus Structured Merge","abstract":"Merge conflicts often occur when developers concurrently change the same code artifacts. While state of practice unstructured merge tools (e.g Git merge) try to automatically resolve merge conflicts based on textual similarity, semistructured and structured merge tools try to go further by exploiting the syntactic structure and semantics of the artifacts involved. Although there is evidence that semistructured merge has significant advantages over unstructured merge, and that structured merge reports significantly fewer conflicts than unstructured merge, it is unknown how semistructured merge compares with structured merge. To help developers decide which kind of tool to use, we compare semistructured and structured merge in an empirical study by reproducing more than 40,000 merge scenarios from more than 500 projects. In particular, we assess how often the two merge strategies report different results, we identify conflicts incorrectly reported by one but not by the other (false positives), and conflicts correctly reported by one but missed by the other (false negatives). Our results show that semistructured and structured merge differ in 24% of the scenarios with conflicts. Semistructured merge reports more false positives, whereas structured merge has more false negatives. Finally, we found that adapting a semistructured merge tool to resolve a particular kind of conflict makes semistructured and structured merge even closer.","conference":"IEEE","terms":"","keywords":"software merging, collaborative development, code integration, version control systems","startPage":"1002","endPage":"1013","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952301","citationCount":0,"referenceCount":41,"year":2019,"authors":"G. Cavalcanti; P. Borba; G. Seibt; S. Apel","affiliations":"Federal University of Pernambuco; Federal University of Pernambuco; University of Passau; Saarland University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d371a"},"title":"ReduKtor: How We Stopped Worrying About Bugs in Kotlin Compiler","abstract":"Bug localization is well-known to be a difficult problem in software engineering, and specifically in compiler development, where it is beneficial to reduce the input program to a minimal reproducing example; this technique is more commonly known as delta debugging. What additionally contributes to the problem is that every new programming language has its own unique quirks and foibles, making it near impossible to reuse existing tools and approaches with full efficiency. In this experience paper we tackle the delta debugging problem w.r.t. Kotlin, a relatively new programming language from JetBrains. Our approach is based on a novel combination of program slicing, hierarchical delta debugging and Kotlin-specific transformations, which are synergistic to each other. We implemented it in a prototype called ReduKtor and did extensive evaluation on both synthetic and real Kotlin programs; we also compared its performance with classic delta debugging techniques. The evaluation results support the practical usability of our approach to Kotlin delta debugging and also shows the importance of using both language-agnostic and language-specific techniques to achieve best reduction efficiency and performance.","conference":"IEEE","terms":"","keywords":"program fuzzing;delta debugging;program slicing;input reduction;compiler testing","startPage":"317","endPage":"326","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952444","citationCount":0,"referenceCount":27,"year":2019,"authors":"D. Stepanov; M. Akhin; M. Belyaev","affiliations":"Saint Petersburg Polytechnic University; Saint Petersburg Polytechnic University; Saint Petersburg Polytechnic University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d371b"},"title":"Continuous Incident Triage for Large-Scale Online Service Systems","abstract":"In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach.","conference":"IEEE","terms":"","keywords":"Incident Triage;Online Service Systems;Deep Learning","startPage":"364","endPage":"375","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952483","citationCount":1,"referenceCount":50,"year":2019,"authors":"J. Chen; X. He; Q. Lin; H. Zhang; D. Hao; F. Gao; Z. Xu; Y. Dang; D. Zhang","affiliations":"Tianjin University; Microsoft Research; Microsoft Research; The University of Newcastle; Peking University; Microsoft Azure; Microsoft Azure; Microsoft Azure; Microsoft Research","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d371c"},"title":"Test Transfer Across Mobile Apps Through Semantic Mapping","abstract":"GUI-based testing has been primarily used to examine the functionality and usability of mobile apps. Despite the numerous GUI-based test input generation techniques proposed in the literature, these techniques are still limited by (1) lack of context-aware text inputs; (2) failing to generate expressive tests; and (3) absence of test oracles. To address these limitations, we propose CraftDroid, a framework that leverages information retrieval, along with static and dynamic analysis techniques, to extract the human knowledge from an existing test suite for one app and transfer the test cases and oracles to be used for testing other apps with the similar functionalities. Evaluation of CraftDroid on real-world commercial Android apps corroborates its effectiveness by achieving 73% precision and 90% recall on average for transferring both the GUI events and oracles. In addition, 75% of the attempted transfers successfully generated valid and feature-based tests for popular features among apps in the same category.","conference":"IEEE","terms":"","keywords":"Test transfer, test migration, GUI testing, natural language processing, semantic similarity","startPage":"42","endPage":"53","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952228","citationCount":1,"referenceCount":43,"year":2019,"authors":"J. Lin; R. Jabbarvand; S. Malek","affiliations":"University of California, Irvine; University of California, Irvine; University of California, Irvine","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d371d"},"title":"Root Cause Localization for Unreproducible Builds via Causality Analysis Over System Call Tracing","abstract":"Localization of the root causes for unreproducible builds during software maintenance is an important yet challenging task, primarily due to limited runtime traces from build processes and high diversity of build environments. To address these challenges, in this paper, we propose RepTrace, a framework that leverages the uniform interfaces of system call tracing for monitoring executed build commands in diverse build environments and identifies the root causes for unreproducible builds by analyzing the system call traces of the executed build commands. Specifically, from the collected system call traces, RepTrace performs causality analysis to build a dependency graph starting from an inconsistent build artifact (across two builds) via two types of dependencies: read/write dependencies among processes and parent/child process dependencies, and searches the graph to find the processes that result in the inconsistencies. To address the challenges of massive noisy dependencies and uncertain parent/child dependencies, RepTrace includes two novel techniques: (1) using differential analysis on multiple builds to reduce the search space of read/write dependencies, and (2) computing similarity of the runtime values to filter out noisy parent/child process dependencies. The evaluation results of RepTrace over a set of real-world software packages show that RepTrace effectively finds not only the root cause commands responsible for the unreproducible builds, but also the files to patch for addressing the unreproducible issues. Among its Top-10 identified commands and files, RepTrace achieves high accuracy rate of 90.00% and 90.56% in identifying the root causes, respectively.","conference":"IEEE","terms":"","keywords":"unreproducible builds;localization;system call tracing","startPage":"527","endPage":"538","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952375","citationCount":0,"referenceCount":30,"year":2019,"authors":"Z. Ren; C. Liu; X. Xiao; H. Jiang; T. Xie","affiliations":"Dalian University of Technology; Case Western Reserve University; Case Western Reserve University; Dalian University of Technology; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d371e"},"title":"Targeted Example Generation for Compilation Errors","abstract":"We present TEGCER, an automated feedback tool for novice programmers. TEGCER uses supervised classification to match compilation errors in new code submissions with relevant pre-existing errors, submitted by other students before. The dense neural network used to perform this classification task is trained on 15000+ error-repair code examples. The proposed model yields a test set classification Pred@3 accuracy of 97.7% across 212 error category labels. Using this model as its base, TEGCER presents students with the closest relevant examples of solutions for their specific error on demand. A large scale (N\u003e230) usability study shows that students who use TEGCER are able to resolve errors more than 25% faster on average than students being assisted by human tutors.","conference":"IEEE","terms":"","keywords":"Intelligent Tutoring Systems;Introductory Programming;Compilation Error;Example Generation;Neural Networks","startPage":"327","endPage":"338","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952446","citationCount":0,"referenceCount":39,"year":2019,"authors":"U. Z. Ahmed; R. Sindhgatta; N. Srivastava; A. Karkare","affiliations":"IIT Kanpur; Queensland University of Technology; IIT Kanpur; IIT Kanpur","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d371f"},"title":"Semistructured Merge in JavaScript Systems","abstract":"Industry widely uses unstructured merge tools that rely on textual analysis to detect and resolve conflicts between code contributions. Semistructured merge tools go further by partially exploring the syntactic structure of code artifacts, and, as a consequence, obtaining significant merge accuracy gains for Java-like languages. To understand whether semistructured merge and the observed gains generalize to other kinds of languages, we implement two semistructured merge tools for JavaScript, and compare them to an unstructured tool. We find that current semistructured merge algorithms and frameworks are not directly applicable for scripting languages like JavaScript. By adapting the algorithms, and studying 10,345 merge scenarios from 50 JavaScript projects on GitHub, we find evidence that our JavaScript tools report fewer spurious conflicts than unstructured merge, without compromising the correctness of the merging process. The gains, however, are much smaller than the ones observed for Java-like languages, suggesting that semistructured merge advantages might be limited for languages that allow both commutative and non-commutative declarations at the same syntactic level.","conference":"IEEE","terms":"","keywords":"Collaborative development, Software merging, Semistructured merge, Version control systems, JavaScript","startPage":"1014","endPage":"1025","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952450","citationCount":0,"referenceCount":37,"year":2019,"authors":"A. Trindade Tavares; P. Borba; G. Cavalcanti; S. Soares","affiliations":"Federal University of Pernambuco; Federal University of Pernambuco; Federal University of Pernambuco; Federal University of Pernambuco","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3720"},"title":"Detecting Error-Handling Bugs without Error Specification Input","abstract":"Most software systems frequently encounter errors when interacting with their environments. When errors occur, error-handling code must execute flawlessly to facilitate system recovery. Implementing correct error handling is repetitive but non-trivial, and developers often inadvertently introduce bugs into error-handling code. Existing tools require correct error specifications to detect error-handling bugs. Manually generating error specifications is error-prone and tedious, while automatically mining error specifications is hard to achieve a satisfying accuracy. In this paper, we propose EH-Miner, a novel and practical tool that can automatically detect error-handling bugs without the need for error specifications. Given a function, EH-Miner mines its error-handling rules when the function is frequently checked by an equivalent condition, and handled by the same action. We applied EH-Miner to 117 applications across 15 software domains. EH-Miner mined error-handling rules with the precision of 91.1% and the recall of 46.9%. We reported 142 bugs to developers, and 106 bugs had been confirmed and fixed at the time of writing. We further applied EH-Miner to Linux kernel, and reported 68 bugs for kernel-4.17, of which 42 had been confirmed or fixed.","conference":"IEEE","terms":"","keywords":"Error handling;Library function;Specification","startPage":"213","endPage":"225","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952517","citationCount":0,"referenceCount":47,"year":2019,"authors":"Z. Jia; S. Li; T. Yu; X. Liao; J. Wang; X. Liu; Y. Liu","affiliations":"National University of Defense Technology; National University of Defense Technology; University of Kentucky; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3721"},"title":"CoRA: Decomposing and Describing Tangled Code Changes for Reviewer","abstract":"Code review is an important mechanism for code quality assurance both in open source software and industrial software. Reviewers usually suffer from numerous, tangled and loosely related code changes that are bundled in a single commit, which makes code review very difficult. In this paper, we propose CoRA (Code Review Assistant), an automatic approach to decompose a commit into different parts and generate concise descriptions for reviewers. More specifically, CoRA can decompose a commit into independent parts (e.g., bug fixing, new feature adding, or refactoring) by code dependency analysis and tree-based similar-code detection, then identify the most important code changes in each part based on the PageRank algorithm and heuristic rules. As a result, CoRA can generate a concise description for each part of the commit. We evaluate our approach in seven open source software projects and 50 code commits. The results indicate that CoRA can improve the accuracy of decomposing code changes by 6.3% over the state-ofart practice. At the same time, CoRA can identify the important part from the fine-grained code changes with a mean average precision (MAP) of 87.7%. We also conduct a human study with eight participants to evaluate the performance and usefulness of CoRA, the user feedback indicates that CoRA can effectively help reviewers.","conference":"IEEE","terms":"","keywords":"Code review;Code changes decomposition;Code changes description;Program comprehension","startPage":"1050","endPage":"1061","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952413","citationCount":0,"referenceCount":53,"year":2019,"authors":"M. Wang; Z. Lin; Y. Zou; B. Xie","affiliations":"Peking University; Microsoft Research; Peking University; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3722"},"title":"B2SFinder: Detecting Open-Source Software Reuse in COTS Software","abstract":"COTS software products are developed extensively on top of OSS projects, resulting in OSS reuse vulnerabilities. To detect such vulnerabilities, finding OSS reuses in COTS software has become imperative. While scalable to tens of thousands of OSS projects, existing binary-to-source matching approaches are severely imprecise in analyzing COTS software products, since they support only a limited number of code features, compute matching scores only approximately in measuring OSS reuses, and neglect the code structures in OSS projects. We introduce a novel binary-to-source matching approach, called B2SFINDER, to address these limitations. First of all, B2SFINDER can reason about seven kinds of code featuresthat are traceable in both binary and source code. In order to compute matching scores precisely, B2SFINDER employs a weighted feature matching algorithm that combines three matching methods (for dealing with different code features) with two importance-weighting methods (for computing the weight of an instance of a code feature in a given COTS software application based on its specificity and occurrence frequency). Finally, B2SFINDER identifies different types of code reusesbased on matching scores and code structures of OSS projects. We have implemented B2SFINDER using an optimized datastructure. We have evaluated B2SFINDERusing 21991 binaries from 1000 popular COTS software products and 2189 candidateOSS projects. Our experimental results show that B2SFINDER is not only precise but also scalable. Compared with the state ofthe art, B2SFINDER has successfully found up to 2.15x as many reuse cases in 53.85 seconds per binary file on average. We also discuss how B2SFINDER can be leveraged in detecting OSS reusevulnerabilities in practice.","conference":"IEEE","terms":"","keywords":"COTS Software, OSS, Code Reuse, One Day Vulnerability, Code Feature, Binary-to Source Matching","startPage":"1038","endPage":"1049","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952556","citationCount":0,"referenceCount":43,"year":2019,"authors":"Z. Yuan; M. Feng; F. Li; G. Ban; Y. Xiao; S. Wang; Q. Tang; H. Su; C. Yu; J. Xu; A. Piao; J. Xuey; W. Huo","affiliations":"Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; Chinese Academy of Sciences; University of New South Wales; Chinese Academy of Sciences","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3723"},"title":"Property Inference for Deep Neural Networks","abstract":"We present techniques for automatically inferring formal properties of feed-forward neural networks. We observe that a significant part (if not all) of the logic of feed forward networks is captured in the activation status (on or off) of its neurons. We propose to extract patterns based on neuron decisions as preconditions that imply certain desirable output property e.g., the prediction being a certain class. We present techniques to extract input properties, encoding convex predicates on the input space that imply given output properties and layer properties, representing network properties captured in the hidden layers that imply the desired output behavior. We apply our techniques on networks for the MNIST and ACASXU applications. Our experiments highlight the use of the inferred properties in a variety of tasks, such as explaining predictions, providing robustness guarantees, simplifying proofs, and network distillation.","conference":"IEEE","terms":"","keywords":"Deep Neural Networks;Explainability;Property Inference;Data Mining","startPage":"797","endPage":"809","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952519","citationCount":1,"referenceCount":36,"year":2019,"authors":"D. Gopinath; H. Converse; C. Pasareanu; A. Taly","affiliations":"Carnegie Mellon University; The University of Texas at Austin; Carnegie Mellon University and NASA Ames; Fiddler labs","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3724"},"title":"Combining Spectrum-Based Fault Localization and Statistical Debugging: An Empirical Study","abstract":"Program debugging is a time-consuming task, and researchers have proposed different kinds of automatic fault localization techniques to mitigate the burden of manual debugging. Among these techniques, two popular families are spectrum-based fault localization (SBFL) and statistical debugging (SD), both localizing faults by collecting statistical information at runtime. Though the ideas are similar, the two families have been developed independently and their combinations have not been systematically explored. In this paper we perform a systematical empirical study on the combination of SBFL and SD. We first build a unified model of the two techniques, and systematically explore four types of variations, different predicates, different risk evaluation formulas, different granularities of data collection, and different methods of combining suspicious scores. Our study leads to several findings. First, most of the effectiveness of the combined approach contributed by a simple type of predicates: branch conditions. Second, the risk evaluation formulas of SBFL significantly outperform that of SD. Third, fine-grained data collection significantly outperforms coarse-grained data collection with a little extra execution overhead. Fourth, a linear combination of SBFL and SD predicates outperforms both individual approaches. According to our empirical study, we propose a new fault localization approach, PREDFL (Predicate-based Fault Localization), with the best configuration for each dimension under the unified model. Then, we explore its complementarity to existing techniques by integrating PREDFL with a state-of-the-art fault localization framework. The experimental results show that PREDFL can further improve the effectiveness of state-of-the-art fault localization techniques. More concretely, integrating PREDFL results in an up to 20.8% improvement w.r.t the faults successfully located at Top-1, which reveals that PREDFL complements existing techniques.","conference":"IEEE","terms":"","keywords":"Software engineering, Fault localization, Program debugging","startPage":"502","endPage":"514","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952344","citationCount":0,"referenceCount":79,"year":2019,"authors":"J. Jiang; R. Wang; Y. Xiong; X. Chen; L. Zhang","affiliations":"Peking University; Peking University; Peking University; Sun Yat-sen University; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3725"},"title":"SGUARD: A Feature-Based Clustering Tool for Effective Spreadsheet Defect Detection","abstract":"Spreadsheets are widely used but subject to various defects. In this paper, we present SGUARD to effectively detect spreadsheet defects. SGUARD learns spreadsheet features to cluster cells with similar computational semantics, and then refines these clusters to recognize anomalous cells as defects. SGUARD well balances the trade-off between the precision (87.8%) and recall rate (71.9%) in the defect detection, and achieves an F-measure of 0.79, exceeding existing spreadsheet defect detection techniques. We introduce the SGUARD implementation and its usage by a video presentation (https://youtu.be/gNPmMvQVf5Q), and provide its public download repository (https://github.com/sheetguard/sguard).","conference":"IEEE","terms":"","keywords":"Cell clustering;Defect detection","startPage":"1142","endPage":"1145","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952240","citationCount":0,"referenceCount":23,"year":2019,"authors":"D. Li; H. Wang; C. Xu; R. Zhang; S. Cheung; X. Ma","affiliations":"Nanjing University; Nanjing University; Nanjing University; Microsoft, China; The Hong Kong University of Sci. and Tech.; Nanjing University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3726"},"title":"RANDR: Record and Replay for Android Applications via Targeted Runtime Instrumentation","abstract":"The ability to repeat the execution of a program is a fundamental requirement in many areas of computing from computer system evaluation to software engineering. Reproducing executions of mobile apps, in particular, has proven difficult under real-life scenarios due to multiple sources of external inputs and interactive nature of the apps. Previous works that provide record/replay functionality for mobile apps are restricted to particular input sources (e.g., touchscreen events) and present deployment challenges due to intrusive modifications to the underlying software stack. Moreover, due to their reliance on record and replay of device specific events, the recorded executions cannot be reliably reproduced across different platforms. In this paper, we present a new practical approach, RandR, for record and replay of Android applications. RandR captures and replays multiple sources of input (i.e., UI and network) without requiring source code (OS or app), administrative device privileges, or any special platform support. RandR achieves these qualities by instrumenting a select set of methods at runtime within an application's own sandbox. In addition, to enable portability of recorded executions across different platforms for replay, RandR contextualizes UI events as interactions with particular UI components (e.g., a button) as opposed to relying on platform specific features (e.g., screen coordinates). We demonstrate RandR's accurate cross-platform record and replay capabilities using over 30 real-world Android apps across a variety of platforms including emulators as well as commercial off-the-shelf mobile devices deployed in real life.","conference":"IEEE","terms":"","keywords":"Record;Replay;Android;Testing;Tools;Mobile Systems;Automation","startPage":"128","endPage":"138","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952379","citationCount":0,"referenceCount":40,"year":2019,"authors":"O. Sahin; A. Aliyeva; H. Mathavan; A. Coskun; M. Egele","affiliations":"Boston University; Boston University; Boston University; Boston University; Boston University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3727"},"title":"Performance-Boosting Sparsification of the IFDS Algorithm with Applications to Taint Analysis","abstract":"The IFDS algorithm can be compute-and memoryintensive for some large programs, often running for a long time (more than expected) or terminating prematurely after some time and/or memory budgets have been exhausted. In the latter case, the corresponding IFDS data-flow analyses may suffer from false negatives and/or false positives. To improve this, we introduce a sparse alternative to the traditional IFDS algorithm. Instead of propagating the data-flow facts across all the program points along the program’s (interprocedural) control flow graph, we propagate every data-flow fact directly to its next possible use points along its own sparse control flow graph constructed on the fly, thus reducing significantly both the time and memory requirements incurred by the traditional IFDS algorithm. In our evaluation, we compare FLOWDROID, a taint analysis performed by using the traditional IFDS algorithm, with our sparse incarnation, SPARSEDROID, on a set of 40 Android apps selected. For the time budget (5 hours) and memory budget (220GB) allocated per app, SPARSEDROID can run every app to completion but FLOWDROID terminates prematurely for 9 apps, resulting in an average speedup of 22.0x. This implies that when used as a market-level vetting tool, SPARSEDROID can finish analyzing these 40 apps in 2.13 hours (by issuing 228 leak warnings) while FLOWDROID manages to analyze only 30 apps in the same time period (by issuing only 147 leak warnings).","conference":"IEEE","terms":"","keywords":"IFDS, data-flow analysis, taint analysis","startPage":"267","endPage":"279","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952383","citationCount":0,"referenceCount":51,"year":2019,"authors":"D. He; H. Li; L. Wang; H. Meng; H. Zheng; J. Liu; S. Hu; L. Li; J. Xue","affiliations":"UNSW Sydney; SKL of Computer Architecture, ICT, CAS, China; University of Chinese Academy of Sciences; SKL of Computer Architecture, ICT, CAS, China; University of Chinese Academy of Sciences; SKL of Computer Architecture, ICT, CAS, China; University of Chinese Academy of Sciences; SKL of Computer Architecture, ICT, CAS, China; University of Chinese Academy of Sciences; SKL of Computer Architecture, ICT, CAS, China; University of Chinese Academy of Sciences; UNSW Sydney; Vivo AI Lab, China; SKL of Computer Architecture, ICT, CAS, China; University of Chinese Academy of Sciences; UNSW Sydney","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3728"},"title":"Empirical Study of Programming to an Interface","abstract":"A popular recommendation to programmers in object-oriented software is to \"program to an interface, not an implementation\" (PTI). Expected benefits include increased simplicity from abstraction, decreased dependency on implementations, and higher flexibility. Yet, interfaces must be immutable, excessive class hierarchies can be a form of complexity, and \"speculative generality\" is a known code smell. To advance the empirical knowledge of PTI, we conducted an empirical investigation that involves 126 Java projects on GitHub, aiming to measuring the decreased dependency benefits (in terms of cochange).","conference":"IEEE","terms":"","keywords":"Java interfaces, coupling, empirical study, software repositories, cochange, GitHub","startPage":"847","endPage":"850","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952238","citationCount":0,"referenceCount":18,"year":2019,"authors":"B. Verhaeghe; C. Fuhrman; L. Guerrouj; N. Anquetil; S. Ducasse","affiliations":"Berger-Levrault, France; École de Technologie Supérieure, Montreal, Canada; École de Technologie Supérieure, Montreal, Canada; Université de Lille, France; RMoD - Inria Nord Europe, Lille, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3729"},"title":"Mutation Analysis for Coq","abstract":"Mutation analysis, which introduces artificial defects into software systems, is the basis of mutation testing, a technique widely applied to evaluate and enhance the quality of test suites. However, despite the deep analogy between tests and formal proofs, mutation analysis has seldom been considered in the context of deductive verification. We propose mutation proving, a technique for analyzing verification projects that use proof assistants. We implemented our technique for the Coq proof assistant in a tool dubbed mCoq. mCoq applies a set of mutation operators to Coq definitions of functions and datatypes, inspired by operators previously proposed for functional programming languages. mCoq then checks proofs of lemmas affected by operator application. To make our technique feasible in practice, we implemented several optimizations in mCoq such as parallel proof checking. We applied mCoq to several medium and large scale Coq projects, and recorded whether proofs passed or failed when applying different mutation operators. We then qualitatively analyzed the mutants, finding many instances of incomplete specifications. For our evaluation, we made several improvements to serialization of Coq files and even discovered a notable bug in Coq itself, all acknowledged by developers. We believe mCoq can be useful both to proof engineers for improving the quality of their verification projects and to researchers for evaluating proof engineering techniques.","conference":"IEEE","terms":"","keywords":"mutation proving;Coq;prof assistants;mutation testing","startPage":"539","endPage":"551","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952421","citationCount":0,"referenceCount":61,"year":2019,"authors":"A. Celik; K. Palmskog; M. Parovic; E. Jesús Gallego Arias; M. Gligoric","affiliations":"The University of Texas at Austin; The University of Texas at Austin; The University of Texas at Austin; MINES ParisTech; The University of Texas at Austin","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d372a"},"title":"ACTGAN: Automatic Configuration Tuning for Software Systems with Generative Adversarial Networks","abstract":"Complex software systems often provide a large number of parameters so that users can configure them for their specific application scenarios. However, configuration tuning requires a deep understanding of the software system, far beyond the abilities of typical system users. To address this issue, many existing approaches focus on exploring and learning good performance estimation models. The accuracy of such models often suffers when the number of available samples is small, a thorny challenge under a given tuning-time constraint. By contrast, we hypothesize that good configurations often share certain hidden structures. Therefore, instead of trying to improve the performance estimation of a given configuration, we focus on capturing the hidden structures of good configurations and utilizing such learned structure to generate potentially better configurations. We propose ACTGAN to achieve this goal. We have implemented and evaluated ACTGAN using 17 workloads with eight different software systems. Experimental results show that ACTGAN outperforms default configurations by 76.22% on average, and six state-of-the-art configuration tuning algorithms by 6.58%-64.56%. Furthermore, the ACTGAN-generated configurations are often better than those used in training and show certain features consisting with domain knowledge, both of which supports our hypothesis.","conference":"IEEE","terms":"","keywords":"software system;automatic configuration tuning;generative adversarial networks","startPage":"465","endPage":"476","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952456","citationCount":0,"referenceCount":57,"year":2019,"authors":"L. Bao; X. Liu; F. Wang; B. Fang","affiliations":"XiDian University; University of California, Davis; XiDian University; XiDian University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d372b"},"title":"Statistical Log Differencing","abstract":"Recent works have considered the problem of log differencing: given two or more system's execution logs, output a model of their differences. Log differencing has potential applications in software evolution, testing, and security. In this paper we present statistical log differencing, which accounts for frequencies of behaviors found in the logs. We present two algorithms, s2KDiff for differencing two logs, and snKDiff, for differencing of many logs at once, both presenting their results over a single inferred model. A unique aspect of our algorithms is their use of statistical hypothesis testing: we let the engineer control the sensitivity of the analysis by setting the target distance between probabilities and the statistical significance value, and report only (and all) the statistically significant differences. Our evaluation shows the effectiveness of our work in terms of soundness, completeness, and performance. It also demonstrates its effectiveness compared to previous work via a user-study and its potential applications via a case study using real-world logs.","conference":"IEEE","terms":"","keywords":"Log analysis;Model inference","startPage":"851","endPage":"862","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952205","citationCount":0,"referenceCount":39,"year":2019,"authors":"L. Bao; N. Busany; D. Lo; S. Maoz","affiliations":"Zhejiang University City College, China; Tel Aviv University; Singapore Management University, Singapore; Tel Aviv University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d372c"},"title":"VeriSmart 2.0: Swarm-Based Bug-Finding for Multi-threaded Programs with Lazy-CSeq","abstract":"Swarm-based verification methods split a verification problem into a large number of independent simpler tasks and so exploit the availability of large numbers of cores to speed up verification. Lazy-CSeq is a BMC-based bug-finding tool for C programs using POSIX threads that is based on sequentialization. Here we present the tool VeriSmart 2.0, which extends Lazy-CSeq with a swarm-based bug-finding method. The key idea of this approach is to constrain the interleaving such that context switches can only happen within selected tiles (more specifically, contiguous code segments within the individual threads). This under-approximates the program's behaviours, with the number and size of tiles as additional parameters, which allows us to vary the complexity of the tasks. Overall, this significantly improves peak memory consumption and (wall-clock) analysis time.","conference":"IEEE","terms":"","keywords":"program analysis;verification;concurrency;sequentialization;swarm verification","startPage":"1150","endPage":"1153","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952527","citationCount":0,"referenceCount":19,"year":2019,"authors":"B. Fischer; S. La Torre; G. Parlato","affiliations":"Stellenbosch University; University of Salerno; University of Molise","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d372d"},"title":"Automated Trainability Evaluation for Smart Software Functions","abstract":"More and more software-intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features (e. g. personal driving assistants or recommendation engines). Such systems incorporate a number of smart software functions (SSFs) which gradually learn and adapt to the users' preferences. A key property of SSFs is their ability to learn based on data resulting from the interaction with the user (implicit and explicit feedback)-which we call trainability. Newly developed and enhanced features in a SSF must be evaluated based on their effect on the trainability of the system. Despite recent approaches for continuous deployment of machine learning systems, trainability evaluation is not yet part of continuous integration and deployment (CID) pipelines. In this paper, we describe the different facets of trainability for the development of SSFs. We also present our approach for automated trainability evaluation within an automotive CID framework which proposes to use automated quality gates for the continuous evaluation of machine learning models. The results from our indicative evaluation based on real data from eight BMW cars highlight the importance of continuous and rigorous trainability evaluation in the development of SSFs.","conference":"IEEE","terms":"","keywords":"trainability;smart software functions;continuous deployment","startPage":"998","endPage":"1001","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952173","citationCount":0,"referenceCount":10,"year":2019,"authors":"I. Gerostathopoulos; S. Kugele; C. Segler; T. Bures; A. Knoll","affiliations":"Technical University of Munich; Technical University of Munich; BMW Group Research; Charles University in Prague; Technical University of Munich","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d372e"},"title":"VisFuzz: Understanding and Intervening Fuzzing with Interactive Visualization","abstract":"Fuzzing is widely used for vulnerability detection. One of the challenges for an efficient fuzzing is covering code guarded by constraints such as the magic number and nested conditions. Recently, academia has partially addressed the challenge via whitebox methods. However, high-level constraints such as array sorts, virtual function invocations, and tree set queries are yet to be handled. To meet this end, we present VisFuzz, an interactive tool for better understanding and intervening fuzzing process via real-time visualization. It extracts call graph and control flow graph from source code, maps each function and basic block to the line of source code and tracks real-time execution statistics with detail constraint contexts. With VisFuzz, test engineers first locate blocking constraints and then learn its semantic context, which helps to craft targeted inputs or update test drivers. Preliminary evaluations are conducted on four real-world programs in Google fuzzer-test-suite. Given additional 15 minutes to understand and intervene the state of fuzzing, the intervened fuzzing outperform the original pure AFL fuzzing, and the path coverage improvements range from 10.84% to 150.58%, equally fuzzed by for 12 hours.","conference":"IEEE","terms":"","keywords":"fuzz testing;software testing;visualization","startPage":"1078","endPage":"1081","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952352","citationCount":0,"referenceCount":14,"year":2019,"authors":"C. Zhou; M. Wang; J. Liang; Z. Liu; C. Sun; Y. Jiang","affiliations":"Tsinghua University; Tsinghua University; Tsinghua University; Nanjing University of Aeronautics and Astronautics; Waterloo University; Tsinghua University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d372f"},"title":"Systematically Covering Input Structure","abstract":"Grammar-based testing uses a given grammar to produce syntactically valid inputs. To cover program features, it is necessary to also cover input features—say, all URL variants for a URL parser. Our k-path algorithm for grammar production systematically covers syntactic elements as well as their combinations. In our evaluation, we show that this results in a significantly higher code coverage than state of the art.","conference":"IEEE","terms":"","keywords":"grammar based;fuzz testing","startPage":"189","endPage":"199","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952419","citationCount":0,"referenceCount":52,"year":2019,"authors":"N. Havrikov; A. Zeller","affiliations":"CISPA Helmholtz Institute for Information Security; CISPA Helmholtz Institute for Information Security","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3730"},"title":"Test Migration Between Mobile Apps with Similar Functionality","abstract":"The use of mobile apps is increasingly widespread, and much effort is put into testing these apps to make sure they behave as intended. To reduce this effort, and thus the overall cost of mobile app testing, we propose APPTESTMIGRATOR, a technique for migrating test cases between apps in the same category (e.g., banking apps). The intuition behind APPTESTMIGRATOR is that many apps share similarities in their functionality, and these similarities often result in conceptually similar user interfaces (through which that functionality is accessed). APPTESTMIGRATOR leverages these commonalities between user interfaces to migrate existing tests written for an app to another similar app. Specifically, given (1) a test case for an app (source app) and (2) a second app (target app), APPTESTMIGRATOR attempts to automatically transform the sequence of events and oracles in the test for the source app to events and oracles for the target app. We implemented APPTESTMIGRATOR for Android mobile apps and evaluated it on a set of randomly selected apps from the Google Play Store in four different categories. Our initial results are promising, support our intuition that test migration is possible, and motivate further research in this direction.","conference":"IEEE","terms":"","keywords":"Test migration, GUI testing, mobile apps","startPage":"54","endPage":"65","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952387","citationCount":1,"referenceCount":34,"year":2019,"authors":"F. Behrang; A. Orso","affiliations":"Georgia Institute of Technology; Georgia Institute of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3731"},"title":"Coverage-Guided Fuzzing for Feedforward Neural Networks","abstract":"Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.","conference":"IEEE","terms":"","keywords":"Coverage guided testing;deep learning testing;deep neural network","startPage":"1162","endPage":"1165","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952279","citationCount":0,"referenceCount":15,"year":2019,"authors":"X. Xie; H. Chen; Y. Li; L. Ma; Y. Liu; J. Zhao","affiliations":"Nanyang Technological University; Nanyang Technological University; Nanyang Technological University; Kyushu University; Nanyang Technological University; Kyushu University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3732"},"title":"A Study of Oracle Approximations in Testing Deep Learning Libraries","abstract":"Due to the increasing popularity of deep learning (DL) applications, testing DL libraries is becoming more and more important. Different from testing general software, for which output is often asserted definitely (e.g., an output is compared with an oracle for equality), testing deep learning libraries often requires to perform oracle approximations, i.e., the output is allowed to be within a restricted range of the oracle. However, oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices. The prevalence, common practices, maintenance and evolution challenges of oracle approximations remain unknown in literature. In this work, we study oracle approximation assertions implemented to test four popular DL libraries. Our study shows that there exists a non-negligible portion of assertions that leverage oracle approximation in the test cases of DL libraries. Also, we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study. Moreover, we find that developers frequently modify code related to oracle approximations, i.e., using a different approximation API, modifying the oracle or the output from the code under test, and using a different approximation threshold. Last, we performed an in-depth study to understand the reasons behind the evolution of oracle approximation assertions. Our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in DL libraries.","conference":"IEEE","terms":"","keywords":"Software Quality;Software Testing;Testing Deep Learning Libraries;Test Oracle","startPage":"785","endPage":"796","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952211","citationCount":0,"referenceCount":28,"year":2019,"authors":"M. Nejadgholi; J. Yang","affiliations":"Concordia University; Concordia University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3733"},"title":"Automating CUDA Synchronization via Program Transformation","abstract":"While CUDA has been the most popular parallel computing platform and programming model for general purpose GPU computing, CUDA synchronization undergoes significant challenges for GPU programmers due to its intricate parallel computing mechanism and coding practices. In this paper, we propose AuCS, the first general framework to automate synchronization for CUDA kernel functions. AuCS transforms the original LLVM-level CUDA program control flow graph in a semantic-preserving manner for exploring the possible barrier function locations. Accordingly, AuCS develops mechanisms to correctly place barrier functions for automating synchronization in multiple erroneous (challenging-to-be-detected) synchronization scenarios, including data race, barrier divergence, and redundant barrier functions. To evaluate the effectiveness and efficiency of AuCS, we conduct an extensive set of experiments and the results demonstrate that AuCS can automate 20 out of 24 erroneous synchronization scenarios.","conference":"IEEE","terms":"","keywords":"CUDA;program repair;synchronization automation;program transformation","startPage":"748","endPage":"759","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952529","citationCount":0,"referenceCount":60,"year":2019,"authors":"M. Wu; L. Zhang; C. Liu; S. H. Tan; Y. Zhang","affiliations":"Southern University of Science and Technology; University of Texas at Dallas; University of Texas at Dallas; Southern University of Science and Technology; Southern University of Science and Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3734"},"title":"History-Guided Configuration Diversification for Compiler Test-Program Generation","abstract":"Compilers, like other software systems, contain bugs, and compiler testing is the most widely-used way to assure compiler quality. A critical task of compiler testing is to generate test programs that could effectively and efficiently discover bugs. Though we can configure test generators such as Csmith to control the features of the generated programs, it is not clear what test configuration is effective. In particular, an effective test configuration needs to generate test programs that are bug-revealing, i.e., likely to trigger bugs, and diverse, i.e., able to discover different types of bugs. It is not easy to satisfy both properties. In this paper, we propose a novel test-program generation approach, called HiCOND, which utilizes historical data for configuration diversification to solve this challenge. HiCOND first infers the range for each option in a test configuration where bug-revealing test programs are more likely to be generated based on historical data. Then, it identifies a set of test configurations that can lead to diverse test programs through a search method (particle swarm optimization). Finally, based on the set of test configurations for compiler testing, HiCOND generates test programs, which are likely to be bug-revealing and diverse. We have conducted experiments on two popular compilers GCC and LLVM, and the results confirm the effectiveness of our approach. For example, HiCOND detects 75.00%, 133.33%, and 145.00% more bugs than the three existing approaches, respectively. Moreover, HiCOND has been successfully applied to actual compiler testing in a global IT company and detected 11 bugs during the practical evaluation.","conference":"IEEE","terms":"","keywords":"Compiler Testing;Configuration;History;Search","startPage":"305","endPage":"316","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952321","citationCount":1,"referenceCount":58,"year":2019,"authors":"J. Chen; G. Wang; D. Hao; Y. Xiong; H. Zhang; L. Zhang","affiliations":"Tianjin University; Peking University; Peking University; Peking University; The University of Newcastle; Peking University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3735"},"title":"An Industrial Experience Report on Performance-Aware Refactoring on a Database-Centric Web Application","abstract":"Modern web applications rely heavily on databases to query and update information. To ease the development efforts, Object Relational Mapping (ORM) frameworks provide an abstraction for developers to manage databases by writing in the same Object-Oriented programming languages. Prior studies have shown that there are various types of performance issues caused by inefficient accesses to databases via different ORM frameworks (e.g., Hibernate and ActiveRecord). However, it is not clear whether the reported performance anti-patterns (common performance issues) can be generalizable across various frameworks. In particular, there is no study focusing on detecting performance issues for applications written in PHP, which is the choice of programming languages for the majority (79%) of web applications. In this experience paper, we detail our process on conducting performance-aware refactoring of an industrial web application written in Laravel, the most popular web framework in PHP. We have derived a complete catalog of 17 performance anti-patterns based on prior research and our experimentation. We have found that some of the reported anti-patterns and refactoring techniques are framework or programming language specific, whereas others are general. The performance impact of the anti-pattern instances are highly dependent on the actual usage context (workload and database settings). When communicating the performance differences before and after refactoring, the results of the complex statistical analysis may be sometimes confusing. Instead, developers usually prefer more intuitive measures like percentage improvement. Experiments show that our refactoring techniques can reduce the response time up to 93.0% and 93.4% for the industrial and the open source application under various scenarios.","conference":"IEEE","terms":"","keywords":"performance anti patterns;ORM framework;database centric applications;refactoring;experience report","startPage":"653","endPage":"664","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952177","citationCount":0,"referenceCount":27,"year":2019,"authors":"B. Chen; Z. M. Jiang; P. Matos; M. Lacaria","affiliations":"York University; York University; Copywell Inc.; Copywell Inc.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3736"},"title":"Efficient Transaction-Based Deterministic Replay for Multi-threaded Programs","abstract":"Existing deterministic replay techniques propose strategies which attempt to reduce record log sizes and achieve successful replay. However, these techniques still generate large logs and achieve replay only under certain conditions. We propose a solution based on the division of the sequence of events of each thread into sequential blocks called transactions. Our insight is that there are usually few to no atomicity violations among transactions reported during a program execution. We present TPLAY, a novel deterministic replay technique which records thread access interleavings on shared memory locations at the transactional level. TPLAY also generates an artificial pair of interleavings when an atomicity violation is reported on a transaction. We present an experiment using the Splash2x extension of the PARSEC benchmark suite. Experimental results indicate that TPLAY experiences a 13-fold improvement in record log sizes and achieves a higher replay probability in comparison to existing work.","conference":"IEEE","terms":"","keywords":"Concurrency;Deterministic Replay;Transactions;Multi-threading","startPage":"760","endPage":"771","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952181","citationCount":0,"referenceCount":31,"year":2019,"authors":"E. Pobee; X. Mei; W. K. Chan","affiliations":"City University of Hong Kong; City University of Hong Kong; City University of Hong Kong","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3737"},"title":"Debreach: Mitigating Compression Side Channels via Static Analysis and Transformation","abstract":"Compression is an emerging source of exploitable side-channel leakage that threatens data security, particularly in web applications where compression is indispensable for performance reasons. Current approaches to mitigating compression side channels have drawbacks in that they either degrade compression ratio drastically or require too much effort from developers to be widely adopted. To bridge the gap, we develop Debreach, a static analysis and program transformation based approach to mitigating compression side channels. Debreach consists of two steps. First, it uses taint analysis to soundly identify flows of sensitive data in the program and uses code instrumentation to annotate data before feeding them to the compressor. Second, it enhances the compressor to exploit the freedom to not compress of standard compression protocols, thus removing the dependency between sensitive data and the size of the compressor's output. Since Debreach automatically instruments applications and does not change the compression protocols, it has the advantage of being non-disruptive and compatible with existing systems. We have evaluated Debreach on a set of web server applications written in PHP. Our experiments show that, while ensuring leakage-freedom, Debreach can achieve significantly higher compression performance than state-of-the-art approaches.","conference":"IEEE","terms":"","keywords":"Program Synthesis and Transformations;Automated Defect Repair;Data Privacy;Side Channel","startPage":"899","endPage":"911","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952360","citationCount":0,"referenceCount":69,"year":2019,"authors":"B. Paulsen; C. Sung; P. A.H. Peterson; C. Wang","affiliations":"University of Southern California; University of Southern California; University of Minnesota, Duluth; University of Southern California","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3738"},"title":"Regexes are Hard: Decision-Making, Difficulties, and Risks in Programming Regular Expressions","abstract":"Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation.","conference":"IEEE","terms":"","keywords":"regular expressions, developer process, qualitative research","startPage":"415","endPage":"426","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952499","citationCount":1,"referenceCount":60,"year":2019,"authors":"L. G. Michael; J. Donohue; J. C. Davis; D. Lee; F. Servant","affiliations":"Virginia Tech; University of Bradford; Virginia Tech; Stony Brook University \u0026 Virginia Tech; Virginia Tech","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3739"},"title":"A Qualitative Analysis of Android Taint-Analysis Results","abstract":"In the past, researchers have developed a number of popular taint-analysis approaches, particularly in the context of Android applications. Numerous studies have shown that automated code analyses are adopted by developers only if they yield a good \"signal to noise ratio\", i.e., high precision. Many previous studies have reported analysis precision quantitatively, but this gives little insight into what can and should be done to increase precision further. To guide future research on increasing precision, we present a comprehensive study that evaluates static Android taint-analysis results on a qualitative level. To unravel the exact nature of taint flows, we have designed COVA, an analysis tool to compute partial path constraints that inform about the circumstances under which taint flows may actually occur in practice. We have conducted a qualitative study on the taint flows reported by FlowDroid in 1,022 real-world Android applications. Our results reveal several key findings: Many taint flows occur only under specific conditions, e.g., environment settings, user interaction, I/O. Taint analyses should consider the application context to discern such situations. COVA shows that few taint flows are guarded by multiple different kinds of conditions simultaneously, so tools that seek to confirm true positives dynamically can concentrate on one kind at a time, e.g., only simulating user interactions. Lastly, many false positives arise due to a too liberal source/sink configuration. Taint analyses must be more carefully configured, and their configuration could benefit from better tool assistance.","conference":"IEEE","terms":"","keywords":"taint analysis, path conditions, Android","startPage":"102","endPage":"114","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952502","citationCount":0,"referenceCount":59,"year":2019,"authors":"L. Luo; E. Bodden; J. Späth","affiliations":"Paderborn University; Paderborn University \u0026 Fraunhofer IEM; Fraunhofer IEM","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d373a"},"title":"RefBot: Intelligent Software Refactoring Bot","abstract":"The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any \"open\" or \"merge\" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects","conference":"IEEE","terms":"","keywords":"Software bot;refactoring;Software quality","startPage":"823","endPage":"834","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952287","citationCount":0,"referenceCount":49,"year":2019,"authors":"V. Alizadeh; M. A. Ouali; M. Kessentini; M. Chater","affiliations":"University of Michigan-Dearborn; University of Michigan; University of Michigan; University of Michigan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d373b"},"title":"Understanding Exception-Related Bugs in Large-Scale Cloud Systems","abstract":"Exception mechanism is widely used in cloud systems. This is mainly because it separates the error handling code from main business logic. However, the huge space of potential error conditions and the sophisticated logic of cloud systems present a big hurdle to the correct use of exception mechanism. As a result, mistakes in the exception use may lead to severe consequences, such as system downtime and data loss. To address this issue, the communities direly need a better understanding of the exception-related bugs, i.e., eBugs, which are caused by the incorrect use of exception mechanism, in cloud systems. In this paper, we present a comprehensive study on 210 eBugs from six widely-deployed cloud systems, including Cassandra, HBase, HDFS, Hadoop MapReduce, YARN, and ZooKeeper. For all the studied eBugs, we analyze their triggering conditions, root causes, bug impacts, and their relations. To the best of our knowledge, this is the first study on eBugs in cloud systems, and the first one that focuses on triggering conditions. We find that eBugs are severe in cloud systems: 74% of our studied eBugs affect system availability or integrity. Luckily, exposing eBugs through testing is possible: 54% of the eBugs are triggered by non-semantic conditions, such as network errors; 40% of the eBugs can be triggered by simulating the triggering conditions at simple system states. Furthermore, we find that the triggering conditions are useful for detecting eBugs. Based on such relevant findings, we build a static analysis tool, called DIET, and apply it to the latest versions of the studied systems. Our results show that DIET reports 31 bugs and bad practices, and 23 of them are confirmed by the developers as \"previously-unknown\" ones.","conference":"IEEE","terms":"","keywords":"exception handling;cloud system;empirical study","startPage":"339","endPage":"351","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952179","citationCount":0,"referenceCount":55,"year":2019,"authors":"H. Chen; W. Dou; Y. Jiang; F. Qin","affiliations":"The Ohio State University; Chinese Academy of Sciences; Nanjing University; The Ohio State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d373c"},"title":"Accurate String Constraints Solution Counting with Weighted Automata","abstract":"As an important extension of symbolic execution (SE), probabilistic symbolic execution (PSE) computes execution probabilities of program paths. Using this information, PSE can prioritize path exploration strategies. To calculate the probability of a path PSE relies on solution counting approaches for the path constraint. The correctness of a solution counting approach depends on the methodology used to count solutions and whether a path constraint maintains a one-to-one relation with program input values. This work focuses on the latter aspect of the solution counting correctness for string constraints. In general, maintaining a one-to-one relation is not always possible, especially in the presence of non-linear constraints. To deal with this issue, researchers that work on PSE for numerical domains either analyze programs with linear constraints, or develop novel techniques to handle solution counting of non-linear constraints. For the string domain, however, previous work on PSE mainly focuses on efficient and accurate solution counting for automata-based string models and has not investigated whether a one-to-one relationship between the strings encoded by automata and input string values is preserved. In this work we demonstrate that traditional automata-based string models fail to maintain one-to-one relations and propose to use the weighted automata model, which preserves the one-to-one relation between the path constraint it encodes and the input string values. We use this model to implement a string constraint solver and show its correctness on a set of non-trivial synthetic benchmarks. We also present an empirical evaluation of traditional and proposed automata solvers on real-world string constraints. The evaluations show that while being less efficient than traditional automata models, the weighted automata model maintains correct solution counts.","conference":"IEEE","terms":"","keywords":"probabilistic symbolic execution, string constraints, quantitative program analysis","startPage":"440","endPage":"452","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952608","citationCount":0,"referenceCount":43,"year":2019,"authors":"E. Sherman; A. Harris","affiliations":"Boise State University; Boise State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d373d"},"title":"Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning","abstract":"Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games.","conference":"IEEE","terms":"","keywords":"Game Testing;Artificial Intelligence;Deep Reinforcement Learning;Evolutionary Multi-Objective Optimization","startPage":"772","endPage":"784","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952543","citationCount":0,"referenceCount":47,"year":2019,"authors":"Y. Zheng; X. Xie; T. Su; L. Ma; J. Hao; Z. Meng; Y. Liu; R. Shen; Y. Chen; C. Fan","affiliations":"Tianjin University; Nanyang Technological University; Nanyang Technological University; Kyushu University; Tianjin University; Tianjin University; Nanyang Technological University; Fuxi AI Lab, Neteast, Inc.; Fuxi AI Lab, Netease, Inc.; AI Lab, Netease, Inc.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d373e"},"title":"Verifying Arithmetic in Cryptographic C Programs","abstract":"Cryptographic primitives are ubiquitous for modern security. The correctness of their implementations is crucial to resist malicious attacks. Typical arithmetic computation of these C programs contains large numbers of non-linear operations, hence is challenging existing automatic C verification tools. We present an automated approach to verify cryptographic C programs. Our approach successfully verifies C implementations of various arithmetic operations used in NIST P-224, P-256, P-521 and Curve25519 in OpenSSL. During verification, we expose a bug and a few anomalies that have been existing for a long time. They have been reported to and confirmed by the OpenSSL community. Our results establish the functional correctness of these C implementations for the first time.","conference":"IEEE","terms":"","keywords":"program verification;cryptographic programs;functional correctness;OpenSSL","startPage":"552","endPage":"564","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952256","citationCount":0,"referenceCount":40,"year":2019,"authors":"J. Liu; X. Shi; M. Tsai; B. Wang; B. Yang","affiliations":"Shenzhen University; Shenzhen University; Institute of Information Science, Academia Sinica; Institute of Information Science, Academia Sinica; Institute of Information Science, Academia Sinica","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d373f"},"title":"Understanding Automatically-Generated Patches Through Symbolic Invariant Differences","abstract":"Developer trust is a major barrier to the deployment of automatically-generated patches. Understanding the effect of a patch is a key element of that trust. We find that differences in sets of formal invariants characterize patch differences and that implication-based distances in invariant space characterize patch similarities. When one patch is similar to another it often contains the same changes as well as additional behavior; this pattern is well-captured by logical implication. We can measure differences using a theorem prover to verify implications between invariants implied by separate programs. Although effective, theorem provers are computationally intensive; we find that string distance is an efficient heuristic for implication-based distance measurements. We propose to use distances between patches to construct a hierarchy highlighting patch similarities. We evaluated this approach on over 300 patches and found that it correctly categorizes programs into semantically similar clusters. Clustering programs reduces human effort by reducing the number of semantically distinct patches that must be considered by over 50%, thus reducing the time required to establish trust in automatically generated repairs.","conference":"IEEE","terms":"","keywords":"Automated Program Repair;Program Measurement;Dynamic Invariants","startPage":"411","endPage":"414","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952219","citationCount":0,"referenceCount":20,"year":2019,"authors":"P. Cashin; C. Martinez; W. Weimer; S. Forrest","affiliations":"Arizona State University; University of New Mexico; University of Michigan; Arizona State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3740"},"title":"Get Rid of Inline Assembly through Verification-Oriented Lifting","abstract":"Formal methods for software development have made great strides in the last two decades, to the point that their application in safety-critical embedded software is an undeniable success. Their extension to non-critical software is one of the notable forthcoming challenges. For example, C programmers regularly use inline assembly for low-level optimizations and system primitives. This usually results in rendering state-of-the-art formal analyzers developed for C ineffective. We thus propose TINA, the first automated, generic, verification-friendly and trustworthy lifting technique turning inline assembly into semantically equivalent C code amenable to verification, in order to take advantage of existing C analyzers. Extensive experiments on real-world code (including GMP and ffmpeg) show the feasibility and benefits of TINA.","conference":"IEEE","terms":"","keywords":"inline assembly;software verification;lifting;formal methods","startPage":"577","endPage":"589","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952223","citationCount":0,"referenceCount":59,"year":2019,"authors":"F. Recoules; S. Bardin; R. Bonichon; L. Mounier; M. Potet","affiliations":"CEA LIST; CEA LIST; CEA LIST; Univ. Grenoble Alpes; Univ. Grenoble Alpes","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3741"},"title":"Emotions Extracted from Text vs. True Emotions–An Empirical Evaluation in SE Context","abstract":"Emotion awareness research in SE context has been growing in recent years. Currently, researchers often rely on textual communication records to extract emotion states using natural language processing techniques. However, how well these extracted emotion states reflect people's real emotions has not been thoroughly investigated. In this paper, we report a multi-level, longitudinal empirical study with 82 individual members in 27 project teams. We collected their self-reported retrospective emotion states on a weekly basis during their year-long projects and also extracted corresponding emotions from the textual communication records. We then model and compare the dynamics of these two types of emotions using multiple statistical and time series analysis methods. Our analyses yield a rich set of findings. The most important one is that the dynamics of emotions extracted using text-based algorithms often do not well reflect the dynamics of self-reported retrospective emotions. Besides, the extracted emotions match self-reported retrospective emotions better at the team-level. Our results also suggest that individual personalities and the team's emotion display norms significantly impact the match/mismatch. Our results should warn the research community about the limitations and challenges of applying text-based emotion recognition tools in SE research.","conference":"IEEE","terms":"","keywords":"emotion recognition;emotion dynamics;text based NLP techniques;time series analysis;personality;organizational norms","startPage":"230","endPage":"242","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952437","citationCount":0,"referenceCount":89,"year":2019,"authors":"Y. Wang","affiliations":"Rochester Institute of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3742"},"title":"V2: Fast Detection of Configuration Drift in Python","abstract":"Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions. We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift.","conference":"IEEE","terms":"","keywords":"Configuration Management;Configuration Repair;Configuration Drift;Environment Inference;Dependencies","startPage":"477","endPage":"488","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952262","citationCount":0,"referenceCount":33,"year":2019,"authors":"E. Horton; C. Parnin","affiliations":"North Carolina State University; North Carolina State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3743"},"title":"Automated Refactoring to Reactive Programming","abstract":"Reactive programming languages and libraries, such as ReactiveX, have been shown to significantly improve software design and have seen important industrial adoption over the last years. Asynchronous applications - which are notoriously error-prone to implement and to maintain - greatly benefit from reactive programming because they can be defined in a declarative style, which improves code clarity and extensibility. In this paper, we tackle the problem of refactoring existing software that has been designed with traditional abstractions for asynchronous programming. We propose 2Rx, a refactoring approach to automatically convert asynchronous code to reactive programming. Our evaluation on top-starred GitHub projects shows that 2Rx is effective with the most common asynchronous constructs, covering 12.7% of projects with asynchronous computations, and it can provide a refactoring for 91.7% of their occurrences.","conference":"IEEE","terms":"","keywords":"refactoring;asynchronous programming;reactive programming;Java","startPage":"835","endPage":"846","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952329","citationCount":0,"referenceCount":59,"year":2019,"authors":"M. Köhler; G. Salvaneschi","affiliations":"Technische Universität Darmstadt; Technische Universität Darmstadt","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3744"},"title":"Active Hotspot: An Issue-Oriented Model to Monitor Software Evolution and Degradation","abstract":"Architecture degradation has a strong negative impact on software quality and can result in significant losses. Severe software degradation does not happen overnight. Software evolves continuously, through numerous issues, fixing bugs and adding new features, and architecture flaws emerge quietly and largely unnoticed until they grow in scope and significance when the system becomes difficult to maintain. Developers are largely unaware of these flaws or the accumulating debt as they are focused on their immediate tasks of address individual issues. As a consequence, the cumulative impacts of their activities, as they affect the architecture, go unnoticed. To detect these problems early and prevent them from accumulating into severe ones we propose to monitor software evolution by tracking the interactions among files revised to address issues. In particular, we propose and show how we can automatically detect active hotspots, to reveal architecture problems. We have studied hundreds of hotspots along the evolution timelines of 21 open source projects and showed that there exist just a few dominating active hotspots per project at any given time. Moreover, these dominating active hotspots persist over long time periods, and thus deserve special attention. Compared with state-of-the-art design and code smell detection tools we report that, using active hotspots, it is possible to detect signs of software degradation both earlier and more precisely.","conference":"IEEE","terms":"","keywords":"software evolution;architecture debt","startPage":"986","endPage":"997","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952512","citationCount":0,"referenceCount":55,"year":2019,"authors":"Q. Feng; Y. Cai; R. Kazman; D. Cui; T. Liu; H. Fang","affiliations":"Drexel University; Drexel University; University of Hawaii \u0026 SEI/CMU; Xi'an Jiaotong University; Xi'an Jiaotong University; Drexel University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3745"},"title":"Automating App Review Response Generation","abstract":"Previous studies showed that replying to a user review usually has a positive effect on the rating that is given by the user to the app. For example, Hassan et al. found that responding to a review increases the chances of a user updating their given rating by up to six times compared to not responding. To alleviate the labor burden in replying to the bulk of user reviews, developers usually adopt a template-based strategy where the templates can express appreciation for using the app or mention the company email address for users to follow up. However, reading a large number of user reviews every day is not an easy task for developers. Thus, there is a need for more automation to help developers respond to user reviews. Addressing the aforementioned need, in this work we propose a novel approach RRGen that automatically generates review responses by learning knowledge relations between reviews and their responses. RRGen explicitly incorporates review attributes, such as user rating and review length, and learns the relations between reviews and corresponding responses in a supervised way from the available training data. Experiments on 58 apps and 309,246 review-response pairs highlight that RRGen outperforms the baselines by at least 67.4% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate dialogue response generation systems). Qualitative analysis also confirms the effectiveness of RRGen in generating relevant and accurate responses.","conference":"IEEE","terms":"","keywords":"App reviews;response generation;neural machine translation","startPage":"163","endPage":"175","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952476","citationCount":0,"referenceCount":76,"year":2019,"authors":"C. Gao; J. Zeng; X. Xia; D. Lo; M. R. Lyu; I. King","affiliations":"The Chinese University of Hong Kong; The Chinese University of Hong Kong; Monash University; Singapore Management University; The Chinese University of Hong Kong; The Chinese University of Hong Kong","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3746"},"title":"DIRE: A Neural Approach to Decompiled Identifier Naming","abstract":"The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.","conference":"IEEE","terms":"","keywords":"Decompilation;Deep learning","startPage":"628","endPage":"639","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952404","citationCount":0,"referenceCount":43,"year":2019,"authors":"J. Lacomis; P. Yin; E. Schwartz; M. Allamanis; C. Le Goues; G. Neubig; B. Vasilescu","affiliations":"Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University Software Engineering Institute; Microsoft Research; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3747"},"title":"CLCDSA: Cross Language Code Clone Detection using Syntactical Features and API Documentation","abstract":"Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is two-fold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones.","conference":"IEEE","terms":"","keywords":"Code Clone;API documentation;Word2Vector;Source Code Syntax","startPage":"1026","endPage":"1037","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952189","citationCount":0,"referenceCount":63,"year":2019,"authors":"K. W. Nafi; T. S. Kar; B. Roy; C. K. Roy; K. A. Schneider","affiliations":"University of Saskatchewan; University of Saskatchewan; University of Saskatchewan; University of Saskatchewan; University of Saskatchewan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3748"},"title":"Learning-Guided Network Fuzzing for Testing Cyber-Physical System Defences","abstract":"The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds—a water purification plant and a water distribution system—finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions.","conference":"IEEE","terms":"","keywords":"cyber-physical systems;fuzzing;testing;benchmark generation;machine learning;metaheuristic optimisation","startPage":"962","endPage":"973","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952193","citationCount":0,"referenceCount":67,"year":2019,"authors":"Y. Chen; C. M. Poskitt; J. Sun; S. Adepu; F. Zhang","affiliations":"Singapore University of Technology and Design; Singapore University of Technology and Design; Singapore Management University; Singapore University of Technology and Design; Zhejiang University and Alibaba-Zhejiang University Joint Institute of Frontier Technologies","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d3749"},"title":"Subformula Caching for Model Counting and Quantitative Program Analysis","abstract":"Quantitative program analysis is an emerging area with applications to software reliability, quantitative information flow, side-channel detection and attack synthesis. Most quantitative program analysis techniques rely on model counting constraint solvers, which are typically the bottleneck for scalability. Although the effectiveness of formula caching in expediting expensive model-counting queries has been demonstrated in prior work, our key insight is that many subformulas are shared across non-identical constraints generated during program analyses. This has not been utilized by prior formula caching approaches. In this paper we present a subformula caching framework and integrate it into a model counting constraint solver. We experimentally evaluate its effectiveness under three quantitative program analysis scenarios: 1) model counting constraints generated by symbolic execution, 2) reliability analysis using probabilistic symbolic execution, 3) adaptive attack synthesis for side-channels. Our experimental results demonstrate that our subformula caching approach significantly improves the performance of quantitative program analysis.","conference":"IEEE","terms":"","keywords":"formula caching, model counting, quantitative program analysis","startPage":"453","endPage":"464","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952551","citationCount":0,"referenceCount":36,"year":2019,"authors":"W. Eiers; S. Saha; T. Brennan; T. Bultan","affiliations":"University of California Santa Barbara; University of California Santa Barbara; University of California Santa Barbara; University of California Santa Barbara","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d374a"},"title":"Testing Regex Generalizability And Its Implications: A Large-Scale Many-Language Measurement Study","abstract":"The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.","conference":"IEEE","terms":"","keywords":"Regular expressions;Empirical software engineering;Data driven design;Methods","startPage":"427","endPage":"439","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952443","citationCount":1,"referenceCount":75,"year":2019,"authors":"J. C. Davis; D. Moyer; A. M. Kazerouni; D. Lee","affiliations":"Virginia Tech; Virginia Tech; Virginia Tech; Stony Brook University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9122eee8435e8e7d374b"},"title":"Size and Accuracy in Model Inference","abstract":"Many works infer finite-state models from execution logs. Large models are more accurate but also more difficult to present and understand. Small models are easier to present and understand but are less accurate. In this work we investigate the tradeoff between model size and accuracy in the context of the classic k-Tails model inference algorithm. First, we define mk-Tails, a generalization of k-Tails from one to many parameters, which enables fine-grained control over the tradeoff. Second, we extend mk-Tails with a reduction based on past-equivalence, which effectively reduces the size of the model without decreasing its accuracy. We implemented our work and evaluated its performance and effectiveness on real-world logs as well as on models and generated logs from the literature.","conference":"IEEE","terms":"","keywords":"Log analysis;Model inference","startPage":"887","endPage":"898","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952335","citationCount":0,"referenceCount":31,"year":2019,"authors":"N. Busany; S. Maoz; Y. Yulazari","affiliations":"Tel Aviv University; Tel Aviv University; Tel Aviv University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d374c"},"title":"Logzip: Extracting Hidden Structures via Iterative Clustering for Log Compression","abstract":"System logs record detailed runtime information of software systems and are used as the main data source for many tasks around software engineering. As modern software systems are evolving into large scale and complex structures, logs have become one type of fast-growing big data in industry. In particular, such logs often need to be stored for a long time in practice (e.g., a year), in order to analyze recurrent problems or track security issues. However, archiving logs consumes a large amount of storage space and computing resources, which in turn incurs high operational cost. Data compression is essential to reduce the cost of log storage. Traditional compression tools (e.g., gzip) work well for general texts, but are not tailed for system logs. In this paper, we propose a novel and effective log compression method, namely logzip. Logzip is capable of extracting hidden structures from raw logs via fast iterative clustering and further generating coherent intermediate representations that allow for more effective compression. We evaluate logzip on five large log datasets of different system types, with a total of 63.6 GB in size. The results show that logzip can save about half of the storage space on average over traditional compression tools. Meanwhile, the design of logzip is highly parallel and only incurs negligible overhead. In addition, we share our industrial experience of applying logzip to Huawei's real products.","conference":"IEEE","terms":"","keywords":"logs;structure extraction;log compression;log management;iterative clustering","startPage":"863","endPage":"873","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952406","citationCount":0,"referenceCount":49,"year":2019,"authors":"J. Liu; J. Zhu; S. He; P. He; Z. Zheng; M. R. Lyu","affiliations":"Sun Yat-Sen University \u0026 The Chinese University of Hong Kong; Huawei Noah’s Ark Lab, China; The Chinese University of Hong Kong; ETH Zurich; Sun Yat-Sen University; The Chinese University of Hong Kong","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d374d"},"title":"An Experience Report of Generating Load Tests Using Log-Recovered Workloads at Varying Granularities of User Behaviour","abstract":"Designing field-representative load tests is an essential step for the quality assurance of large-scale systems. Practitioners may capture user behaviour at different levels of granularity. A coarse-grained load test may miss detailed user behaviour, leading to a non-representative load test; while an extremely fine-grained load test would simply replay user actions step by step, leading to load tests that are costly to develop, execute and maintain. Workload recovery is at core of these load tests. Prior research often captures the workload as the frequency of user actions. However, there exists much valuable information in the context and sequences of user actions. Such richer information would ensure that the load tests that leverage such workloads are more field-representative. In this experience paper, we study the use of different granularities of user behaviour, i.e., basic user actions, basic user actions with contextual information and user action sequences with contextual information, when recovering workloads for use in the load testing of large-scale systems. We propose three approaches that are based on the three granularities of user behaviour and evaluate our approaches on four subject systems, namely Apache James, OpenMRS, Google Borg, and an ultra-large-scale industrial system (SA) from Alibaba. Our results show that our approach that is based on user action sequences with contextual information outperforms the other two approaches and can generate more representative load tests with similar throughput and CPU usage to the original field workload (i.e., mostly statistically insignificant or with small/trivial effect sizes). Such representative load tests are generated only based on a small number of clusters of users, leading to a low cost of conducting/maintaining such tests. Finally, we demonstrate that our approaches can detect injected users in the original field workloads with high precision and recall. Our paper demonstrates the importance of user action sequences with contextual information in the workload recovery of large-scale systems.","conference":"IEEE","terms":"","keywords":"Workload recovery, Load tests, Software log analysis, Software performance","startPage":"669","endPage":"681","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952553","citationCount":0,"referenceCount":60,"year":2019,"authors":"J. Chen; W. Shang; A. E. Hassan; Y. Wang; J. Lin","affiliations":"Concordia University; Concordia University; Queen's University; Alibaba Group; Alibaba Group","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d374e"},"title":"Automating Non-Blocking Synchronization In Concurrent Data Abstractions","abstract":"This paper investigates using compiler technology to automatically convert sequential C++ data abstractions, e.g., queues, stacks, maps, and trees, to concurrent lock-free implementations. By automatically tailoring a number of state-of-the-practice synchronization methods to the underlying sequential implementations of different data structures, our automatically synchronized code can attain performance competitive to that of manually-written concurrent data structures by experts and much better performance than heavier-weight support by software transactional memory (STM).","conference":"IEEE","terms":"","keywords":"Concurrency;Parallel Programming;Automation Compilers;Data Structures","startPage":"735","endPage":"747","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952270","citationCount":0,"referenceCount":63,"year":2019,"authors":"J. Zhang; Q. Yi; D. Dechev","affiliations":"University of Colorado at Colorado Springs; University of Colorado at Colorado Springs; University of Central Florida","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d374f"},"title":"Multi-modal Attention Network Learning for Semantic Source Code Retrieval","abstract":"Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query (e.g., a short natural language text describing the functionality for retrieving a particular code snippet). Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results. To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods.","conference":"IEEE","terms":"","keywords":"Code retrieval;multi-modal network;attention mechanism;deep learning","startPage":"13","endPage":"25","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952337","citationCount":0,"referenceCount":57,"year":2019,"authors":"Y. Wan; J. Shu; Y. Sui; G. Xu; Z. Zhao; J. Wu; P. Yu","affiliations":"Zhejiang University; Zhejiang University; University of Technology Sydney; University of Technology Sydney; Zhejiang University; Zhejiang University; University of Illinois at Chicago","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3750"},"title":"iFeedback: Exploiting User Feedback for Real-Time Issue Detection in Large-Scale Online Service Systems","abstract":"Large-scale online systems are complex, fast-evolving, and hardly bug-free despite the testing efforts. Backend system monitoring cannot detect many types of issues, such as UI related bugs, bugs with small impact on backend system indicators, or errors from third-party co-operating systems, etc. However, users are good informers of such issues: They will provide their feedback for any types of issues. This experience paper discusses our design of iFeedback, a tool to perform real-time issue detection based on user feedback texts. Unlike traditional approaches that analyze user feedback with computation-intensive natural language processing algorithms, iFeedback is focusing on fast issue detection, which can serve as a system life-condition monitor. In particular, iFeedback extracts word combination-based indicators from feedback texts. This allows iFeedback to perform fast system anomaly detection with sophisticated machine learning algorithms. iFeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis. We present our representative experiences in successfully applying iFeedback in tens of large-scale production online service systems in ten months.","conference":"IEEE","terms":"","keywords":"Bug and vulnerability detection","startPage":"352","endPage":"363","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952229","citationCount":0,"referenceCount":42,"year":2019,"authors":"W. Zheng; H. Lu; Y. Zhou; J. Liang; H. Zheng; Y. Deng","affiliations":"Tencent Inc.; Fudan University; Fudan University; Tencent Inc.; Tencent Inc.; Tencent Inc.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3751"},"title":"Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models","abstract":"A deep learning (DL) model is inherently imprecise. To address this problem, existing techniques retrain a DL model over a larger training dataset or with the help of fault injected models or using the insight of failing test cases in a DL model. In this paper, we present Apricot, a novel weight-adaptation approach to fixing DL models iteratively. Our key observation is that if the deep learning architecture of a DL model is trained over many different subsets of the original training dataset, the weights in the resultant reduced DL model (rDLM) can provide insights on the adjustment direction and magnitude of the weights in the original DL model to handle the test cases that the original DL model misclassifies. Apricot generates a set of such reduced DL models from the original DL model. In each iteration, for each failing test case experienced by the input DL model (iDLM), Apricot adjusts each weight of this iDLM toward the average weight of these rDLMs correctly classifying the test case and/or away from that of these rDLMs misclassifying the same test case, followed by training the weight-adjusted iDLM over the original training dataset to generate a new iDLM for the next iteration. The experiment using five state-of-the-art DL models shows that Apricot can increase the test accuracy of these models by 0.87%-1.55% with an average of 1.08%. The experiment also reveals the complementary nature of these rDLMs in Apricot.","conference":"IEEE","terms":"","keywords":"Deep Neural Networks;Optimization;Model Evolution;Debugging;Model Fixing;Model Repair","startPage":"376","endPage":"387","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952197","citationCount":0,"referenceCount":60,"year":2019,"authors":"H. Zhang; W. K. Chan","affiliations":"City University of Hong Kong; City University of Hong Kong","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3752"},"title":"OAUTHLINT: An Empirical Study on OAuth Bugs in Android Applications","abstract":"Mobile developers use OAuth APIs to implement Single-Sign-On services. However, the OAuth protocol was originally designed for the authorization for third-party websites not to authenticate users in third-party mobile apps. As a result, it is challenging for developers to correctly implement mobile OAuth securely. These vulnerabilities due to the misunderstanding of OAuth and inexperience of developers could lead to data leakage and account breach. In this paper, we perform an empirical study on the usage of OAuth APIs in Android applications and their security implications. In particular, we develop OAUTHLINT, that incorporates a query-driven static analysis to automatically check programs on the Google Play marketplace. OAUTHLINT takes as input an anti-protocol that encodes a vulnerable pattern extracted from the OAuth specifications and a program P. Our tool then generates a counter-example if the anti-protocol can match a trace of Ps possible executions. To evaluate the effectiveness of our approach, we perform a systematic study on 600+ popular apps which have more than 10 millions of downloads. The evaluation shows that 101 (32%) out of 316 applications that use OAuth APIs make at least one security mistake.","conference":"IEEE","terms":"","keywords":"Security, OAuth, Android, Static Analysis, Bug Finding","startPage":"293","endPage":"304","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952200","citationCount":0,"referenceCount":40,"year":2019,"authors":"T. A. Rahat; Y. Feng; Y. Tian","affiliations":"University of Virginia; University of California at Santa Barbara; University of Virginia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3753"},"title":"Predicting Licenses for Changed Source Code","abstract":"Open source software licenses regulate the circumstances under which software can be redistributed, reused and modified. Ensuring license compatibility and preventing license restriction conflicts among source code during software changes are the key to protect their commercial use. However, selecting the appropriate licenses for software changes requires lots of experience and manual effort that involve examining, assimilating and comparing various licenses as well as understanding their relationships with software changes. Worse still, there is no state-of-the-art methodology to provide this capability. Motivated by this observation, we propose in this paper Automatic License Prediction (ALP), a novel learning-based method and tool for predicting licenses as software changes. An extensive evaluation of ALP on predicting licenses in 700 open source projects demonstrate its effectiveness: ALP can achieve not only a high overall prediction accuracy (92.5% in micro F1 score) but also high accuracies across all license types.","conference":"IEEE","terms":"","keywords":"Mining Software Repository;Software License Prediction","startPage":"686","endPage":"697","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952518","citationCount":0,"referenceCount":28,"year":2019,"authors":"X. Liu; L. Huang; J. Ge; V. Ng","affiliations":"Southern Methodist University; Southern Methodist University; Nanjing University; University of Texas at Dallas","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3754"},"title":"Re-Factoring Based Program Repair Applied to Programming Assignments","abstract":"Automated program repair has been used to provide feedback for incorrect student programming assignments, since program repair captures the code modification needed to make a given buggy program pass a given test-suite. Existing student feedback generation techniques are limited because they either require manual effort in the form of providing an error model, or require a large number of correct student submissions to learn from, or suffer from lack of scalability and accuracy. In this work, we propose a fully automated approach for generating student program repairs in real-time. This is achieved by first re-factoring all available correct solutions to semantically equivalent solutions. Given an incorrect program, we match the program with the closest matching refactored program based on its control flow structure. Subsequently, we infer the input-output specifications of the incorrect program's basic blocks from the executions of the correct program's aligned basic blocks. Finally, these specifications are used to modify the blocks of the incorrect program via search-based synthesis. Our dataset consists of almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. Our experimental results suggest that our method is more effective and efficient than recently proposed feedback generation approaches. About 30% of the patches produced by our tool Refactory are smaller than those produced by the state-of-art tool Clara, and can be produced given fewer correct solutions (often a single correct solution) and in a shorter time. We opine that our method is applicable not only to programming assignments, and could be seen as a general-purpose program repair method that can achieve good results with just a single correct reference solution.","conference":"IEEE","terms":"","keywords":"Program Repair;Programming Education;Software Refactoring","startPage":"388","endPage":"398","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952522","citationCount":0,"referenceCount":20,"year":2019,"authors":"Y. Hu; U. Z. Ahmed; S. Mechtaev; B. Leong; A. Roychoudhury","affiliations":"The University of Texas at Austin; National University of Singapore; University College London; National University of Singapore; National University of Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3755"},"title":"PraPR: Practical Program Repair via Bytecode Mutation","abstract":"Automated program repair (APR) is one of the recent advances in automated software engineering aiming for reducing the burden of debugging by suggesting high-quality patches that either directly fix the bugs, or help the programmers in the course of manual debugging. We believe scalability, applicability, and accurate patch validation are the main design objectives for a practical APR technique. In this paper, we present PraPR, our implementation of a practical APR technique that operates at the level of JVM bytecode. We discuss design decisions made in the development of PraPR, and argue that the technique is a viable baseline toward attaining aforementioned objectives. Our experimental results show that: (1) PraPR can fix more bugs than state-of-the-art APR techniques and can be over 10X faster, (2) state-of-the-art APR techniques suffer from dataset overfitting, while the simplistic template-based PraPR performs more consistently on different datasets, and (3) PraPR can fix bugs for other JVM languages, such as Kotlin. PraPR is publicly available at https://github.com/prapr/prapr.","conference":"IEEE","terms":"","keywords":"Program Repair;JVM Bytecode;Mutation Testing","startPage":"1118","endPage":"1121","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952486","citationCount":0,"referenceCount":44,"year":2019,"authors":"A. Ghanbari; L. Zhang","affiliations":"University of Texas at Dallas; University of Texas at Dallas","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3756"},"title":"Combining Program Analysis and Statistical Language Model for Code Statement Completion","abstract":"Automatic code completion helps improve developers' productivity in their programming tasks. A program contains instructions expressed via code statements, which are considered as the basic units of program execution. In this paper, we introduce AutoSC, which combines program analysis and the principle of software naturalness to fill in a partially completed statement. AutoSC benefits from the strengths of both directions, in which the completed code statement is both frequent and valid. AutoSC is first trained on a large code corpus to derive the templates of candidate statements. Then, it uses program analysis to validate and concretize the templates into syntactically and type-valid candidate statements. Finally, these candidates are ranked by using a language model trained on the lexical form of the source code in the code corpus. Our empirical evaluation on the large datasets of real-world projects shows that AutoSC achieves 38.9-41.3% top-1 accuracy and 48.2-50.1% top-5 accuracy in statement completion. It also outperforms a state-of-the-art approach from 9X-69X in top-1 accuracy.","conference":"IEEE","terms":"","keywords":"Code Completion;Statement Completion;Statistical Language Model;Program Analysis","startPage":"710","endPage":"721","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952235","citationCount":0,"referenceCount":29,"year":2019,"authors":"S. Nguyen; T. Nguyen; Y. Li; S. Wang","affiliations":"The University of Texas at Dallas; The University of Texas at Dallas; New Jersey Institute of Technology; New Jersey Institute of Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3757"},"title":"MalScan: Fast Market-Wide Mobile Malware Scanning by Social-Network Centrality Analysis","abstract":"Malware scanning of an app market is expected to be scalable and effective. However, existing approaches use either syntax-based features which can be evaded by transformation attacks or semantic-based features which are usually extracted by performing expensive program analysis. Therefor, in this paper, we propose a lightweight graph-based approach to perform Android malware detection. Instead of traditional heavyweight static analysis, we treat function call graphs of apps as social networks and perform social-network-based centrality analysis to represent the semantic features of the graphs. Our key insight is that centrality provides a succinct and fault-tolerant representation of graph semantics, especially for graphs with certain amount of inaccurate information (e.g., inaccurate call graphs). We implement a prototype system, MalScan, and evaluate it on datasets of 15,285 benign samples and 15,430 malicious samples. Experimental results show that MalScan is capable of detecting Android malware with up to 98% accuracy under one second which is more than 100 times faster than two state-of-the-art approaches, namely MaMaDroid and Drebin. We also demonstrate the feasibility of MalScan on market-wide malware scanning by performing a statistical study on over 3 million apps. Finally, in a corpus of dataset collected from Google-Play app market, MalScan is able to identify 18 zero-day malware including malware samples that can evade detection of existing tools.","conference":"IEEE","terms":"","keywords":"Lightweight feature, Android Malware, API Centrality, Market-wide","startPage":"139","endPage":"150","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952382","citationCount":0,"referenceCount":43,"year":2019,"authors":"Y. Wu; X. Li; D. Zou; W. Yang; X. Zhang; H. Jin","affiliations":"Huazhong University of Science and Technology; University of Texas at Dallas; Huazhong University of Science and Technology; University of Texas at Dallas; Huazhong University of Science and Technology; Huazhong University of Science and Technology","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3758"},"title":"Sip4J: Statically Inferring Access Permission Contracts for Parallelising Sequential Java Programs","abstract":"This paper presents Sip4J, a fully automated, scalable and effective tool to automatically generate access permission contracts for a sequential Java program. The access permission contracts, which represent the dependency of code blocks, have been frequently used to enable concurrent execution of sequential programs. Those permission contracts, unfortunately, need to be manually created by programmers, which is known to be time-consuming, laborious and error-prone. To mitigate those manual efforts, Sip4J performs inter-procedural static analysis of Java source code to automatically extract the implicit dependencies in the program and subsequently leverages them to automatically generate access permission contracts, following the Design by Contract principle. The inferred specifications are then used to identify the concurrent (immutable) methods in the program. Experimental results further show that Sip4J is useful and effective towards generating access permission contracts for sequential Java programs. The implementation of Sip4J has been published as an open-sourced project at https://github.com/Sip4J/Sip4J and a demo video of Sip4J can be found at https://youtu.be/RjMTIxlhHTg.","conference":"IEEE","terms":"","keywords":"access permissions, static analysis, concurrency, permission inference","startPage":"1098","endPage":"1101","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952453","citationCount":0,"referenceCount":17,"year":2019,"authors":"A. Sadiq; L. Li; Y. Li; I. Ahmed; S. Ling","affiliations":"Monash University, Australia; Monash University, Australia; Monash University, Australia; University of Lahore, Pakistan; Monash University, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3759"},"title":"XRaSE: Towards Virtually Tangible Software using Augmented Reality","abstract":"Software engineering has seen much progress in recent past including introduction of new methodologies, new paradigms for software teams, and from smaller monolithic applications to complex, intricate, and distributed software applications. However, the way we represent, discuss, and collaborate on software applications throughout the software development life cycle is still primarily using the source code, textual representations, or charts on 2D computer screens - the confines of which have long limited how we visualize and comprehend software systems. In this paper, we present XRaSE, a novel prototype implementation that leverages augmented reality to visualize a software application as a virtually tangible entity. This immersive approach is aimed at making activities like application comprehension, architecture analysis, knowledge communication, and analysis of a software's dynamic aspects, more intuitive, richer and collaborative.","conference":"IEEE","terms":"","keywords":"Augmented Reality, Immersive Experience, Software Visualization, Software Comprehension","startPage":"1194","endPage":"1197","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952170","citationCount":0,"referenceCount":16,"year":2019,"authors":"R. Mehra; V. S. Sharma; V. Kaulgud; S. Podder","affiliations":"Accenture Labs, India; Accenture Labs, India; Accenture Labs, India; Accenture Labs, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d375a"},"title":"Code-First Model-Driven Engineering: On the Agile Adoption of MDE Tooling","abstract":"Domain models are the most important asset in widely accepted software development approaches, like Domain-Driven Design (DDD), yet those models are still implicitly represented in programs. Model-Driven Engineering (MDE) regards those models as representable entities that are amenable to automated analysis and processing, facilitating quality assurance while increasing productivity in software development processes. Although this connection is not new, very few approaches facilitate adoption of MDE tooling without compromising existing value, their data. Moreover, switching to MDE tooling usually involves re-engineering core parts of an application, hindering backward compatibility and, thereby, continuous integration, while requiring an up-front investment in training in specialized modeling frameworks. In those approaches that overcome the previous problem, there is no clear indication - from a quantitative point of view - of the extent to which adopting state-of-the-art MDE practices and tooling is feasible or advantageous. In this work, we advocate a code-first approach to modeling through an approach for applying MDE techniques and tools to existing object-oriented software applications that fully preserves the semantics of the original application, which need not be modified. Our approach consists both of a semi-automated method for specifying explicit view models out of existing object-oriented applications and of a conservative extension mechanism that enables the use of such view models at run time, where view model queries are resolved on demand and view model updates are propagated incrementally to the original application. This mechanism enables an iterative, flexible application of MDE tooling to software applications, where metamodels and models do not exist explicitly. An evaluation of this extension mechanism, implemented for Java applications and for view models atop the Eclipse Modeling Framework (EMF), has been conducted with an industry-targeted benchmark for decision support systems, analyzing performance and scalability of the synchronization mechanism. Backward propagation of large updates over very large views is instant.","conference":"IEEE","terms":"","keywords":"domain model, MDE, EMF, roundtrip synchronization, algebraic specification, performance analysis","startPage":"874","endPage":"886","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952237","citationCount":0,"referenceCount":41,"year":2019,"authors":"A. Boronat","affiliations":"University of Leicester","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d375b"},"title":"MARBLE: Mining for Boilerplate Code to Identify API Usability Problems","abstract":"Designing usable APIs is critical to developers' productivity and software quality, but is quite difficult. One of the challenges is that anticipating API usability barriers and real-world usage is difficult, due to a lack of automated approaches to mine usability data at scale. In this paper, we focus on one particular grievance that developers repeatedly express in online discussions about APIs: \"boilerplate code.\" We investigate what properties make code count as boilerplate, the reasons for boilerplate, and how programmers can reduce the need for it. We then present MARBLE, a novel approach to automatically mine boilerplate code candidates from API client code repositories. MARBLE adapts existing techniques, including an API usage mining algorithm, an AST comparison algorithm, and a graph partitioning algorithm. We evaluate MARBLE with 13 Java APIs, and show that our approach successfully identifies both already-known and new API-related boilerplate code instances.","conference":"IEEE","terms":"","keywords":"Boilerplate Code;API Usability;Repository Mining","startPage":"615","endPage":"627","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952239","citationCount":1,"referenceCount":65,"year":2019,"authors":"D. Nam; A. Horvath; A. Macvean; B. Myers; B. Vasilescu","affiliations":"Carnegie Mellon University; Carnegie Mellon University; Google; Carnegie Mellon University; Carnegie Mellon University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d375c"},"title":"Feature-Interaction Aware Configuration Prioritization for Configurable Code","abstract":"Unexpected interactions among features induce most bugs in a configurable software system. Exhaustively analyzing all the exponential number of possible configurations is prohibitively costly. Thus, various sampling techniques have been proposed to systematically narrow down the exponential number of legal configurations to be analyzed. Since analyzing all selected configurations can require a huge amount of effort, fault-based configuration prioritization, that helps detect faults earlier, can yield practical benefits in quality assurance. In this paper, we propose CoPro, a novel formulation of feature-interaction bugs via common program entities enabled/disabled by the features. Leveraging from that, we develop an efficient feature-interaction-aware configuration prioritization technique for a configurable system by ranking the configurations according to their total number of potential bugs. We conducted several experiments to evaluate CoPro on the ability to detect configuration-related bugs in a public benchmark. We found that CoPro outperforms the state-of-the-art configuration prioritization techniques when we add them on advanced sampling algorithms. In 78% of the cases, CoPro ranks the buggy configurations at the top 3 positions in the resulting list. Interestingly, CoPro is able to detect 17 not-yet-discovered feature-interaction bugs.","conference":"IEEE","terms":"","keywords":"Configurable Code;Feature Interaction;Configuration Prioritization;Software Product Lines","startPage":"489","endPage":"501","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952386","citationCount":0,"referenceCount":59,"year":2019,"authors":"S. Nguyen; H. Nguyen; N. Tran; H. Tran; T. Nguyen","affiliations":"The University of Texas at Dallas, USA; Amazon Corporation; The University of Texas at Dallas; The University of Texas at Dallas, USA; The University of Texas at Dallas, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d375d"},"title":"A Quantitative Analysis Framework for Recurrent Neural Network","abstract":"Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo.","conference":"IEEE","terms":"","keywords":"recurrent neural netwrod;model abstraction;quantitative analysis;similarity metrics;coverage criteria","startPage":"1062","endPage":"1065","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952565","citationCount":0,"referenceCount":18,"year":2019,"authors":"X. Du; X. Xie; Y. Li; L. Ma; Y. Liu; J. Zhao","affiliations":"Nanyang Technological University; Nanyang Technological University; Nanyang Technological University; Kyushu University; Nanyang Technological University; Kyushu University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d375e"},"title":"Batch Alias Analysis","abstract":"Many program-analysis based tools require precise points-to/alias information only for some program variables. To meet this requirement efficiently, there have been many works on demand-driven analyses that perform only the work necessary to compute the points-to or alias information on the requested variables (queries). However, these demand-driven analyses can be very expensive when applied on large systems where the number of queries can be significant. Such a blow-up in analysis time is unacceptable in cases where scalability with real-time constraints is crucial; for example, when program analysis tools are plugged into an IDE (Integrated Development Environment). In this paper, we propose schemes to improve the scalability of demand-driven analyses without compromising on precision. Our work is based on novel ideas for eliminating irrelevant and redundant data-flow paths for the given queries. We introduce the idea of batch analysis, which can answer multiple given queries in batch mode. Batch analysis suits the environments with strict time constraints, where the queries come in batch. We present a batch alias analysis framework that can be used to speed up given demand-driven alias analysis. To show the effectiveness of this framework, we use two demand-driven alias analyses (1) the existing best performing demand-driven alias analysis tool for race-detection clients and (2) an optimized version thereof that avoids irrelevant computation. Our evaluations on a simulated data-race client, and on a recent program-understanding tool, show that batch analysis leads to significant performance gains, along with minor gains in precision.","conference":"IEEE","terms":"","keywords":"Batch analysis;Alias analysis;Points to Analysis","startPage":"936","endPage":"948","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952457","citationCount":0,"referenceCount":36,"year":2019,"authors":"J. Vedurada; V. K. Nandivada","affiliations":"Indian Institute of Technology Madras; Indian Institute of Technology Madras","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d375f"},"title":"Inferring Program Transformations From Singular Examples via Big Code","abstract":"Inferring program transformations from concrete program changes has many potential uses, such as applying systematic program edits, refactoring, and automated program repair. Existing work for inferring program transformations usually rely on statistical information over a potentially large set of program-change examples. However, in many practical scenarios we do not have such a large set of program-change examples. In this paper, we address the challenge of inferring a program transformation from one single example. Our core insight is that \"big code\" can provide effective guide for the generalization of a concrete change into a program transformation, i.e., code elements appearing in many files are general and should not be abstracted away. We first propose a framework for transformation inference, where programs are represented as hypergraphs to enable fine-grained generalization of transformations. We then design a transformation inference approach, GENPAT, that infers a program transformation based on code context and statistics from a big code corpus. We have evaluated GENPAT under two distinct application scenarios, systematic editing and program repair. The evaluation on systematic editing shows that GENPAT significantly outperforms a state-of-the-art approach, SYDIT, with up to 5.5x correctly transformed cases. The evaluation on program repair suggests that GENPAT has the potential to be integrated in advanced program repair tools-GENPAT successfully repaired 19 real-world bugs in the Defects4J benchmark by simply applying transformations inferred from existing patches, where 4 bugs have never been repaired by any existing technique. Overall, the evaluation results suggest that GENPAT is effective for transformation inference and can potentially be adopted for many different applications.","conference":"IEEE","terms":"","keywords":"Pattern generation, Program adaptation, Code abstraction","startPage":"255","endPage":"266","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952210","citationCount":1,"referenceCount":63,"year":2019,"authors":"J. Jiang; L. Ren; Y. Xiong; L. Zhang","affiliations":"Peking University; Peking University; Peking University; University of Texas at Dallas","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3760"},"title":"SpyREST: Automated RESTful API Documentation Using an HTTP Proxy Server (N)","abstract":"RESTful API documentation is expensive to produce and maintain due to the lack of reusable tools and automated solutions. Most RESTful APIs are documented manually and the API developers are responsible for keeping the documentation up to date as the API evolves making the process both costly and error-prone. In this paper we introduce a novel technique using an HTTP proxy server that can be used to automatically generate RESTful API documentation and demonstrate SpyREST, an example implementation of the proposed technique. SpyREST uses a proxy to intercept example API calls and intelligently produces API documentation for RESTful Web APIs by processing the request and response data. Using the proposed HTTP proxy server based technique, RESTful API developers can significantly reduce the cost of producing and maintaining API documentation by replacing a large manual process with an automated process.","conference":"IEEE","terms":"Documentation;Servers;Manuals;Databases;Uniform resource locators;Software as a service;Libraries,application program interfaces;hypermedia;Internet;software reusability;software tools;system documentation,automated RESTful API documentation;reusable tools;HTTP proxy server;SpyREST;RESTful Web API","keywords":"RESTful API;Web API;Documentation;Automation;Example based documentation","startPage":"271","endPage":"276","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372015","citationCount":10,"referenceCount":19,"year":2015,"authors":"S. M. Sohan; C. Anslow; F. Maurer","affiliations":"Dept. of Comput. Sci., Univ. of Calgary, Calgary, AB, Canada; Dept. of Comput. Sci., Univ. of Calgary, Calgary, AB, Canada; Dept. of Comput. Sci., Univ. of Calgary, Calgary, AB, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3761"},"title":"Testing intermediate representations for binary analysis","abstract":"Binary lifting, which is to translate a binary executable to a high-level intermediate representation, is a primary step in binary analysis. Despite its importance, there are only few existing approaches to testing the correctness of binary lifters. Furthermore, the existing approaches suffer from low test coverage, because they largely depend on random test case generation. In this paper, we present the design and implementation of the first systematic approach to testing binary lifters. We have evaluated the proposed system on 3 state-of-the-art binary lifters, and found 24 previously unknown semantic bugs. Our result demonstrates that writing a precise binary lifter is extremely difficult even for those heavily tested projects.","conference":"IEEE","terms":"Semantics;Computer bugs;Binary codes;Testing;Tools;Software;C++ languages,program debugging;program testing,binary analysis;binary lifting;binary executable;high-level intermediate representation;random test case generation;systematic approach;precise binary lifter","keywords":"","startPage":"353","endPage":"364","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115648","citationCount":3,"referenceCount":60,"year":2017,"authors":"S. Kim; M. Faerevaag; M. Jung; S. Jung; D. Oh; J. Lee; S. K. Cha","affiliations":"KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea; KAIST, Republic of Korea; Gachon University, Republic of Korea; KAIST, Republic of Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3762"},"title":"Have We Seen Enough Traces? (T)","abstract":"Dynamic specification mining extracts candidate specifications from logs of execution traces. Existing algorithms differ in the kinds of traces they take as input and in the kinds of candidate specification they present as output. One challenge common to all approaches relates to the faithfulness of the mining results: how can we be confident that the extracted specifications faithfully characterize the program we investigate? Since producing and analyzing traces is costly, how would we know we have seen enough traces? And, how would we know we have not wasted resources and seen too many of them?In this paper we address these important questions by presenting a novel, black box, probabilistic framework based on a notion of log completeness, and by applying it to three different well-known specification mining algorithms from the literature: k-Tails, Synoptic, and mining of scenario-based triggers and effects. Extensive evaluation over 24 models taken from 9 different sources shows the soundness, generalizability, and usefulness of the framework and its contribution to the state-of-the-art in dynamic specification mining.","conference":"IEEE","terms":"Heuristic algorithms;Computational modeling;Data mining;Estimation;Probabilistic logic;Adaptation models;Servers,data mining;formal specification;probability;program verification,dynamic specification mining algorithm;program specification;black box;probabilistic framework;k-Tails;Synoptic","keywords":"Specification Mining","startPage":"93","endPage":"103","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371999","citationCount":7,"referenceCount":42,"year":2015,"authors":"H. Cohen; S. Maoz","affiliations":"Sch. of Comput. Sci., Tel Aviv Univ., Tel Aviv, Israel; Sch. of Comput. Sci., Tel Aviv Univ., Tel Aviv, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3763"},"title":"Mining implicit design templates for actionable code reuse","abstract":"In this paper, we propose an approach to detecting project-specific recurring designs in code base and abstracting them into design templates as reuse opportunities. The mined templates allow programmers to make further customization for generating new code. The generated code involves the code skeleton of recurring design as well as the semi-implemented code bodies annotated with comments to remind programmers of necessary modification. We implemented our approach as an Eclipse plugin called MICoDe. We evaluated our approach with a reuse simulation experiment and a user study involving 16 participants. The results of our simulation experiment on 10 open source Java projects show that, to create a new similar feature with a design template, (1) on average 69% of the elements in the template can be reused and (2) on average 60% code of the new feature can be adopted from the template. Our user study further shows that, compared to the participants adopting the copy-paste-modify strategy, the ones using MICoDe are more effective to understand a big design picture and more efficient to accomplish the code reuse task.","conference":"IEEE","terms":"Unified modeling language;Object oriented modeling;Cloning;Skeleton;Java;Feature extraction,data mining;Java;public domain software;software reusability,open source Java projects;implicit design template mining;project-specific recurring design detection;code reuse task;reuse simulation experiment;semiimplemented code bodies;code skeleton;generated code;code base;actionable code reuse","keywords":"","startPage":"394","endPage":"404","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115652","citationCount":3,"referenceCount":41,"year":2017,"authors":"Y. Lin; G. Meng; Y. Xue; Z. Xing; J. Sun; X. Peng; Y. Liu; W. Zhao; J. Dong","affiliations":"National University of Singapore, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Australia National University, Australia; Singapore University of Technology and Design, Singapore; Fudan University, China; Nanyang Technological University, Singapore; Fudan University, China; National University of Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3764"},"title":"The potential of polyhedral optimization: An empirical study","abstract":"Present-day automatic optimization relies on powerful static (i.e., compile-time) analysis and transformation methods. One popular platform for automatic optimization is the polyhedron model. Yet, after several decades of development, there remains a lack of empirical evidence of the model's benefits for real-world software systems. We report on an empirical study in which we analyzed a set of popular software systems, distributed across various application domains. We found that polyhedral analysis at compile time often lacks the information necessary to exploit the potential for optimization of a program's execution. However, when conducted also at run time, polyhedral analysis shows greater relevance for real-world applications. On average, the share of the execution time amenable to polyhedral optimization is increased by a factor of nearly 3. Based on our experimental results, we discuss the merits and potential of polyhedral optimization at compile time and run time.","conference":"IEEE","terms":"Optimization;Benchmark testing;Arrays;Analytical models;Program processors;Multimedia communication;Time measurement,optimisation;optimising compilers;program diagnostics,polyhedral optimization;present-day automatic optimization;static analysis;transformation methods;polyhedron model;real-world software systems;polyhedral analysis;program execution","keywords":"","startPage":"508","endPage":"518","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693108","citationCount":2,"referenceCount":37,"year":2013,"authors":"A. Simbürger; S. Apel; A. Größlinger; C. Lengauer","affiliations":"University of Passau, Germany; University of Passau, Germany; University of Passau, Germany; University of Passau, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3765"},"title":"Automated verification of interactive rule-based configuration systems","abstract":"Rule-based specifications of systems have again become common in the context of product line variability modeling and configuration systems. In this paper, we define a logical foundation for rule-based specifications that has enough expressivity and operational behavior to be practically useful and at the same time enables decidability of important overall properties such as consistency or cycle-freeness. Our logic supports rule-based interactive user transitions as well as the definition of a domain theory via rule transitions. As a running example, we model DOPLER, a rule-based configuration system currently in use at Siemens.","conference":"IEEE","terms":"Semantics;Slabs;Calculus;Silicon;Redundancy;Context;Computer languages,formal logic;formal specification;formal verification;interactive systems;knowledge based systems,automated verification;interactive rule-based configuration system;rule-based specification;product line variability modeling;rule-based interactive user transition;domain theory;rule transition;DOPLER","keywords":"","startPage":"551","endPage":"561","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693112","citationCount":1,"referenceCount":19,"year":2013,"authors":"D. Dhungana; C. H. Tang; C. Weidenbach; P. Wischnewski","affiliations":"Siemens AG Österreich, Vienna, Austria; Max Planck Institute for Informatics, Saarbrücken, Germany; Max Planck Institute for Informatics, Saarbrücken, Germany; Logic4Business GmbH, Saarbrücken, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3766"},"title":"Static Analysis of Implicit Control Flow: Resolving Java Reflection and Android Intents (T)","abstract":"Implicit or indirect control flow is a transfer of control between procedures using some mechanism other than an explicit procedure call. Implicit control flow is a staple design pattern that adds flexibility to system design. However, it is challenging for a static analysis to compute or verify properties about a system that uses implicit control flow. This paper presents static analyses for two types of implicit control flow that frequently appear in Android apps: Java reflection and Android intents. Our analyses help to resolve where control flows and what data is passed. This information improves the precision of downstream analyses, which no longer need to make conservative assumptions about implicit control flow. We have implemented our techniques for Java. We enhanced an existing security analysis with a more precise treatment of reflection and intents. In a case study involving ten real-world Android apps that use both intents and reflection, the precision of the security analysis was increased on average by two orders of magnitude. The precision of two other downstream analyses was also improved.","conference":"IEEE","terms":"Androids;Humanoid robots;Java;Security;Dictionaries;Context,Android (operating system);Java;program diagnostics;security of data,static analysis;implicit control flow;Java reflection;Android intents;indirect control flow;security analysis","keywords":"Android;Static Analysis;Implicit Control Flow;Type systems","startPage":"669","endPage":"679","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372054","citationCount":26,"referenceCount":35,"year":2015,"authors":"P. Barros; R. Just; S. Millstein; P. Vines; W. Dietl; M. dAmorim; M. D. Ernst","affiliations":"Fed. Univ. of Pernambuco, Recife, Brazil; Univ. of Washington, Seattle, WA, USA; Univ. of Washington, Seattle, WA, USA; Univ. of Washington, Seattle, WA, USA; Univ. of Waterloo, Waterloo, ON, Canada; NA; Univ. of Washington, Seattle, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3767"},"title":"Learning effective changes for software projects","abstract":"The primary motivation of much of software analytics is decision making. How to make these decisions? Should one make decisions based on lessons that arise from within a particular project? Or should one generate these decisions from across multiple projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction. Indeed prediction is a useful task, but it is usually followed by \"planning\" about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics by offering clear guidance on what to do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set of actionable plans within and across projects. Each of these plans, if followed will improve the quality of the software project.","conference":"IEEE","terms":"Planning;Software;Tools;Software engineering;Decision trees;Software algorithms,decision making;project management;software engineering,actionable plans;software project;primary motivation;decision making;planning task;actionable analytics;generation software analytics tools focus;XTREE algorithms;BELLTREE algorithms","keywords":"Planning;bellwethers;defect prediction","startPage":"1002","endPage":"1005","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115719","citationCount":0,"referenceCount":29,"year":2017,"authors":"R. Krishna","affiliations":"Comptuer Science, North Carolina State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3768"},"title":"Towards the automatic classification of traceability links","abstract":"A wide range of text-based artifacts contribute to software projects (e.g., source code, test cases, use cases, project requirements, interaction diagrams, etc.). Traceability Link Recovery (TLR) is the software task in which relevant documents in these various sets are linked to one another, uncovering information about the project that is not available when considering only the documents themselves. This information is helpful for enabling other tasks such as improving test coverage, impact analysis, and ensuring that system or regulatory requirements are met. However, while traceability links are useful, performing TLR manually is time consuming and fraught with error. Previous work has applied Information Retrieval (IR) and other techniques to reduce the human effort involved; however, that effort remains significant. In this research we seek to take the next step in reducing it by using machine learning (ML) classification models to predict whether a candidate link is valid or invalid without human oversight. Preliminary results show that this approach has promise for accurately recommending valid links; however, there are several challenges that still must be addressed in order to achieve a technique with high enough performance to consider it a viable, completely automated solution.","conference":"IEEE","terms":"Software;Classification algorithms;Semantics;Predictive models;Measurement;Tuning,information retrieval;learning (artificial intelligence);software maintenance;text analysis,automatic classification;traceability links;software projects;source code;project requirements;interaction diagrams;TLR;software task;relevant documents;test coverage;human effort;machine learning classification models;information retrieval;traceability link recovery","keywords":"software traceability;traceability link recovery;machine learning","startPage":"1018","endPage":"1021","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115723","citationCount":0,"referenceCount":33,"year":2017,"authors":"C. Mills","affiliations":"Department of Computer Science, Florida State University, Tallahassee, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3769"},"title":"Automated verification of pattern-based interaction invariants in Ajax applications","abstract":"When developing asynchronous JavaScript and XML (Ajax) applications, developers implement Ajax design patterns for increasing the usability of the applications. However, unpredictable contexts of running applications might conceal faults that will break the design patterns, which decreases usability. We propose a support tool called JSVerifier that auto-matically verifies interaction invariants; the applications handle their interactions in invariant occurrence and order. We also present a selective set of interaction invariants derived from Ajax design patterns, as input. If the application behavior breaks the design patterns, JSVerifier automatically outputs faulty execution paths for debugging. The results of our case studies show that JSVerifier can verify the interaction invariants in a feasible amount of time, and we conclude that it can help developers increase the usability of Ajax applications.","conference":"IEEE","terms":"Usability;Servers;Context;Testing;Debugging;Web pages;Educational institutions,Java;object-oriented programming;program debugging;program verification;software tools;XML,automated verification;pattern-based interaction invariants;Ajax applications;asynchronous JavaScript applications;XML applications;JSVerifier;Ajax design patterns;faulty execution paths;debugging","keywords":"Ajax;Reverse Engineering;Model Checking;Design Pattern","startPage":"158","endPage":"168","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693076","citationCount":1,"referenceCount":26,"year":2013,"authors":"Y. Maezawa; H. Washizaki; Y. Tanabe; S. Honiden","affiliations":"The University of Tokyo, Japan; Waseda University, Tokyo, Japan; National Institute of Informatics, Tokyo, Japan; The University of Tokyo, National Institute of Informatics, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d376a"},"title":"JFlow: Practical refactorings for flow-based parallelism","abstract":"Emerging applications in the domains of recognition, mining and synthesis (RMS); image and video processing; data warehousing; and automatic financial trading admit a particular style of parallelism termed flow-based parallelism. To help developers exploit flow-based parallelism, popular parallel libraries such as Groovy's GPars, Intel's TBB Flow Graph and Microsoft's TPL Dataflow have begun introducing many new and useful constructs. However, to reap the benefits of such constructs, developers must first use them. This involves refactoring their existing sequential code to incorporate these constructs - a manual process that overwhelms even experts. To alleviate this burden, we introduce a set of novel analyses and transformations targeting flow-based parallelism. We implemented these ideas in JFlow, an interactive refactoring tool integrated into the Eclipse IDE. We used JFlow to parallelize seven applications: four from a previously known benchmark and three from a suite of large open source projects. JFlow, with minimal interaction from the developer, can successfully parallelize applications from the aforementioned domains with good performance (offering up to 3.45x speedup on a 4-core machine) and is fast enough to be used interactively as part of a developer's workflow.","conference":"IEEE","terms":"Parallel processing;Feature extraction;Libraries;Pipelines;Sensitivity;Java;Databases,interactive systems;parallel programming;public domain software;software maintenance,JFlow;flow-based parallelism;RMS domain;recognition, mining and synthesis domain;video processing;image processing;data warehousing;automatic financial trading;parallel libraries;Groovy GPars;Intel TBB flow graph;Microsoft TPL dataflow;sequential code refactoring;interactive refactoring tool;Eclipse IDE;large open source projects","keywords":"","startPage":"202","endPage":"212","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693080","citationCount":2,"referenceCount":56,"year":2013,"authors":"N. Chen; R. E. Johnson","affiliations":"Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d376b"},"title":"Understanding and overcoming parallelism bottlenecks in ForkJoin applications","abstract":"ForkJoin framework is a widely used parallel programming framework upon which both core concurrency libraries and real-world applications are built. Beneath its simple and user-friendly APIs, ForkJoin is a sophisticated managed parallel runtime unfamiliar to many application programmers: the framework core is a work-stealing scheduler, handles fine-grained tasks, and sustains the pressure from automatic memory management. ForkJoin poses a unique gap in the compute stack between high-level software engineering and low-level system optimization. Understanding and bridging this gap is crucial for the future of parallelism support in JVM-supported applications. This paper describes a comprehensive study on parallelism bottlenecks in ForkJoin applications, with a unique focus on how they interact with underlying system-level features, such as work stealing and memory management. We identify 6 bottlenecks, and found that refactoring them can significantly improve performance and energy efficiency. Our field study includes an in-depth analysis of Akka - a real-world actor framework - and 30 additional open-source ForkJoin projects. We sent our patches to the developers of 15 projects, and 7 out of the 9 projects that replied to our patches have accepted them.","conference":"IEEE","terms":"Java;Parallel processing;Programming;Runtime;Optimization;Synchronization,application program interfaces;Java;parallel programming;processor scheduling;software engineering,real-world actor framework;parallelism bottlenecks;core concurrency libraries;real-world applications;application programmers;work-stealing scheduler;automatic memory management;compute stack;high-level software engineering;low-level system optimization;parallelism support;energy efficiency;JVM-supported applications;user-friendly API;parallel programming framework;ForkJoin applications;open-source ForkJoin;system-level features","keywords":"","startPage":"765","endPage":"775","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115687","citationCount":2,"referenceCount":39,"year":2017,"authors":"G. Pinto; A. Canino; F. Castor; G. Xu; Y. D. Liu","affiliations":"UFPA, Brazil; SUNY Binghamton, USA; UFPE, Brazil; UC Irvine, USA; SUNY Binghamton, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d376c"},"title":"Symlnfer: Inferring program invariants using symbolic states","abstract":"We introduce a new technique for inferring program invariants that uses symbolic states generated by symbolic execution. Symbolic states, which consist of path conditions and constraints on local variables, are a compact description of sets of concrete program states and they can be used for both invariant inference and invariant verification. Our technique uses a counterexample-based algorithm that creates concrete states from symbolic states, infers candidate invariants from concrete states, and then verifies or refutes candidate invariants using symbolic states. The refutation case produces concrete counterexamples that prevent spurious results and allow the technique to obtain more precise invariants. This process stops when the algorithm reaches a stable set of invariants. We present Symlnfer, a tool that implements these ideas to automatically generate invariants at arbitrary locations in a Java program. The tool obtains symbolic states from Symbolic PathFinder and uses existing algorithms to infer complex (potentially nonlinear) numerical invariants. Our preliminary results show that Symlnfer is effective in using symbolic states to generate precise and useful invariants for proving program safety and analyzing program runtime complexity. We also show that Symlnfer outperforms existing invariant generation systems.","conference":"IEEE","terms":"Concrete;Inference algorithms;Complexity theory;Encoding;Benchmark testing;Tools;Runtime,formal verification;Java;program diagnostics;program testing;program verification,concrete states;symbolic states;concrete program states;Symlnfer;program invariant inference;symbolic execution;path conditions;local variable constraints;invariant verification;Java program;Symbolic PathFinder;program safety;program runtime complexity analysis","keywords":"","startPage":"804","endPage":"814","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115691","citationCount":1,"referenceCount":37,"year":2017,"authors":"T. Nguyen; M. B. Dwyer; W. Visser","affiliations":"University of Nebraska-Lincoln, USA; University of Nebraska-Lincoln, USA; Stellenbosch University, South Africa","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d376d"},"title":"EHBDroid: Beyond GUI testing for Android applications","abstract":"With the prevalence of Android-based mobile devices, automated testing for Android apps has received increasing attention. However, owing to the large variety of events that Android supports, test input generation is a challenging task. In this paper, we present a novel approach and an open source tool called EHBDroid for testing Android apps. In contrast to conventional GUI testing approaches, a key novelty of EHBDroid is that it does not generate events from the GUI, but directly invokes callbacks of event handlers. By doing so, EHBDroid can efficiently simulate a large number of events that are difficult to generate by traditional UI-based approaches. We have evaluated EHBDroid on a collection of 35 real-world large-scale Android apps and compared its performance with two state-of-the-art UI-based approaches, Monkey and Dynodroid. Our experimental results show that EHBDroid is significantly more effective and efficient than Monkey and Dynodroid: in a much shorter time, EHBDroid achieves as much as 22.3% higher statement coverage (11.1% on average) than the other two approaches, and found 12 bugs in these benchmarks, including 5 new bugs that the other two failed to find.","conference":"IEEE","terms":"Androids;Humanoid robots;Testing;Tools;Instruments;Graphical user interfaces;XML,Android (operating system);graphical user interfaces;mobile computing;program testing,EHBDroid;Android applications;test input generation;conventional GUI testing approaches;event handlers;UI-based approaches;large-scale Android apps;open source tool;Android based mobile devices","keywords":"Android;automated testing;event generation;event handlers","startPage":"27","endPage":"37","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115615","citationCount":5,"referenceCount":39,"year":2017,"authors":"W. Song; X. Qian; J. Huang","affiliations":"School of Computer Sci. \u0026 Eng., Nanjing University of Sci. \u0026 Tech., Nanjing, China; School of Computer Sci. \u0026 Eng., Nanjing University of Sci. \u0026 Tech., Nanjing, China; Parasol Laboratory, Texas A\u0026M University, College Station, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d376e"},"title":"iProbe: A lightweight user-level dynamic instrumentation tool","abstract":"We introduce a new hybrid instrumentation tool for dynamic application instrumentation called iProbe, which is flexible and has low overhead. iProbe takes a novel 2-stage design, and offloads much of the dynamic instrumentation complexity to an offline compilation stage. It leverages standard compiler flags to introduce “place-holders” for hooks in the program executable. Then it utilizes an efficient user-space “HotPatching” mechanism which modifies the functions to be traced and enables execution of instrumented code in a safe and secure manner. In its evaluation on a micro-benchmark and SPEC CPU2006 benchmark applications, the iProbe prototype achieved the instrumentation overhead an order of magnitude lower than existing state-of-the-art dynamic instrumentation tools like SystemTap and DynInst.","conference":"IEEE","terms":"Complexity theory;Scalability;Probes;Kernel;Linux;Benchmark testing,program compilers;program debugging,iProbe;lightweight user-level dynamic instrumentation tool;hybrid instrumentation tool;dynamic application instrumentation;HotPatching mechanism;SystemTap;DynInst","keywords":"Monitoring;Tracing;Hotpatching;Production Systems;Low-Overhead","startPage":"742","endPage":"745","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693147","citationCount":1,"referenceCount":19,"year":2013,"authors":"N. Arora; Hui Zhang; Junghwan Rhee; K. Yoshihira; Guofei Jiang","affiliations":"NEC Laboratories America, USA; NEC Laboratories America, USA; NEC Laboratories America, USA; NEC Laboratories America, USA; NEC Laboratories America, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d376f"},"title":"Context-aware task allocation for distributed agile team","abstract":"The philosophy of Agile software development advocates the spirit of open discussion and coordination among team members to adapt to incremental changes encountered during the process. Based on our observations from 20 agile student development teams over an 8-week study in Beihang University, China, we found that the task allocation strategy as a result of following the Agile process heavily depends on the experience of the users, and cannot be guaranteed to result in efficient utilization of team resources. In this research, we propose a context-aware task allocation decision support system that balances the considerations for quality and timeliness to improve the overall utility derived from an agile software development project.We formulate the agile process as a distributed constraint optimization problem, and propose a technology framework that assesses individual developers' situations based on data collected from a Scrum-based agile process, and helps individual developers make situation-aware decisions on which tasks from the backlog to select in real-time. Preliminary analysis and simulation results show that it can achieve close to optimally efficient utilization of the developers' collective capacity. We plan to build the framework into a computer-supported collaborative development platform and refine the method through more realistic projects.","conference":"IEEE","terms":"Resource management;Software;Variable speed drives;Planning;Educational institutions;Indexes;Delays,groupware;optimisation;software development management;software prototyping;team working,context-aware task allocation;distributed agile team;agile software development;team member discussion;team member coordination;Beihang University;China;task allocation strategy;context-aware task allocation decision support system;agile software development project;distributed constraint optimization problem;technology framework;Scrum-based agile process;developer collective capacity;computer-supported collaborative development platform","keywords":"distributed agile;task allocation;project management","startPage":"758","endPage":"761","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693151","citationCount":4,"referenceCount":11,"year":2013,"authors":"J. Lin","affiliations":"School of Computer Engineering, Nanyang Technological University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3770"},"title":"A Message-Passing Architecture without Public Ids Using Send-to-Behavior","abstract":"We explore a novel model of computation based on nodes that have no public addresses (ids). We define nodes as concurrent, message-passing computational entities in an abstract communication medium, similar to the Actor model, but with all public node ids elided. Instead, drawing inspiration from biological systems, we postulate a send-to-behavior language construct to enable anonymous one-way communication. A behavior, defined as a function of input to actions, is also an intensional definition of the subset of nodes that express it. Sending to a behavior is defined to deliver the message to one or more nodes that implement that behavior.","conference":"IEEE","terms":"Computer architecture;Programming;Computational modeling;Topology;Computers;Hardware;Protocols,application program interfaces;message passing;parallel programming,message-passing architecture;message-passing computational entity;communication medium;actor model;public node;send-to-behavior language","keywords":"behavior composition;message passing;multi node","startPage":"902","endPage":"905","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372089","citationCount":0,"referenceCount":8,"year":2015,"authors":"E. S. Wang; Z. Dang","affiliations":"Sch. of Electr. Eng. \u0026 Comput. Sci., Washington State Univ., Pullman, WA, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Washington State Univ., Pullman, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3771"},"title":"An Automated Framework for Recommending Program Elements to Novices (N)","abstract":"Novice programmers often learn programming by implementing well-known algorithms. There are several challenges in the process. Recommendation systems in software currently focus on programmer productivity and ease of development. Teaching aides for such novice programmers based on recommendation systems still remain an under-explored area. In this paper, we present a general framework for recognizing the desired target for partially-written code and recommending a reliable series of edits to transform the input program into the target solution. Our code analysis is based on graph matching and tree edit algorithms. Our experimental results show that efficient graph comparison techniques can accurately match two portions of source code and produce an accurate set of source code edits. We provide details on implementation of our framework, which is developed as a plugin for Java in Eclipse IDE.","conference":"IEEE","terms":"Programming;Software engineering;Target recognition;Knowledge based systems;Transforms;Algorithm design and analysis;Java,graph theory;Java;pattern matching;programming;recommender systems;source code (software),automated framework;recommending program element;novice programmer;programming;recommendation system;programmer productivity;teaching aide;partially-written code;code analysis;graph matching;tree edit algorithm;graph comparison technique;source code edit;Java;Eclipse IDE","keywords":"Recommendation Framework;pq-Gram Algorithm","startPage":"283","endPage":"288","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372017","citationCount":2,"referenceCount":30,"year":2015,"authors":"K. Zimmerman; C. R. Rupakheti","affiliations":"Dept. of Comput. Sci. \u0026 Software Eng., Rose-Hulman Inst. of Technol., Terre Haute, IN, USA; Dept. of Comput. Sci. \u0026 Software Eng., Rose-Hulman Inst. of Technol., Terre Haute, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3772"},"title":"Search-Based Synthesis of Probabilistic Models for Quality-of-Service Software Engineering (T)","abstract":"The formal verification of finite-state probabilistic models supports the engineering of software with strict quality-of-service (QoS) requirements. However, its use in software design is currently a tedious process of manual multiobjective optimisation. Software designers must build and verify probabilistic models for numerous alternative architectures and instantiations of the system parameters. When successful, they end up with feasible but often suboptimal models. The EvoChecker search-based software engineering approach and tool introduced in our paper employ multiobjective optimisation genetic algorithms to automate this process and considerably improve its outcome. We evaluate EvoChecker for six variants of two software systems from the domains of dynamic power management and foreign exchange trading. These systems are characterised by different types of design parameters and QoS requirements, and their design spaces comprise between 2E+14 and 7.22E+86 relevant alternative designs. Our results provide strong evidence that EvoChecker significantly outperforms the current practice and yields actionable insights for software designers.","conference":"IEEE","terms":"Probabilistic logic;Quality of service;Software systems;Markov processes;Optimization;Software engineering,genetic algorithms;probability;program testing;program verification;quality of service,search-based synthesis;probabilistic models;quality-of-service software engineering;formal verification;finite-state probabilistic models;quality-of-service requirements;software design;suboptimal models;EvoChecker search-based software engineering approach;multiobjective optimisation genetic algorithms;dynamic power management;foreign exchange trading;design parameters;QoS requirements;design space","keywords":"Probabilistic Model Checking;Model Synthesis;Genetic Algorithms;Search-Based Software Engineering;Model Repair","startPage":"319","endPage":"330","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372021","citationCount":15,"referenceCount":68,"year":2015,"authors":"S. Gerasimou; G. Tamburrelli; R. Calinescu","affiliations":"Dept. of Comput. Sci., Univ. of York, York, UK; Dept. of Comput. Sci., Vrije Univ., Amsterdam, Netherlands; Dept. of Comput. Sci., Univ. of York, York, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3773"},"title":"Automatically assessing code understandability: How far are we?","abstract":"Program understanding plays a pivotal role in software maintenance and evolution: a deep understanding of code is the stepping stone for most software-related activities, such as bug fixing or testing. Being able to measure the understandability of a piece of code might help in estimating the effort required for a maintenance activity, in comparing the quality of alternative implementations, or even in predicting bugs. Unfortunately, there are no existing metrics specifically designed to assess the understandability of a given code snippet. In this paper, we perform a first step in this direction, by studying the extent to which several types of metrics computed on code, documentation, and developers correlate with code understandability. To perform such an investigation we ran a study with 46 participants who were asked to understand eight code snippets each. We collected a total of 324 evaluations aiming at assessing the perceived understandability, the actual level of understanding, and the time needed to understand a code snippet. Our results demonstrate that none of the (existing and new) metrics we considered is able to capture code understandability, not even the ones assumed to assess quality attributes strongly related with it, such as code readability and complexity.","conference":"IEEE","terms":"Measurement;Complexity theory;Software;Computer bugs;Correlation;Maintenance engineering;Documentation,program debugging;public domain software;software maintenance;software metrics;software quality,program understanding;software maintenance;software-related activities;perceived understandability;code readability;code snippet;code complexity;automatic code understandability assessibility;quality attributes","keywords":"Software metrics;Code understandability;Empirical study;Negative result","startPage":"417","endPage":"427","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115654","citationCount":7,"referenceCount":37,"year":2017,"authors":"S. Scalabrino; G. Bavota; C. Vendome; M. Linares-Vásquez; D. Poshyvanyk; R. Oliveto","affiliations":"University of Molise, Italy; Università della Svizzera italiana (USI), Switzerland; The College of William and Mary, USA; Universidad de los Andes, Colombia; The College of William and Mary, USA; University of Molise, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3774"},"title":"Detecting system use cases and validations from documents","abstract":"Identifying system use cases and corresponding validations involves analyzing large requirement documents to understand the descriptions of business processes, rules and policies. This consumes a significant amount of effort and time. We discuss an approach to automate the detection of system use cases and corresponding validations from documents. We have devised a representation that allows for capturing the essence of rule statements as a composition of atomic `Rule intents' and key phrases associated with the intents. Rule intents that co-occur frequently constitute `Rule acts' analogous to the Speech acts in Linguistics. Our approach is based on NLP techniques designed around this Rule Model. We employ syntactic and semantic NL analyses around the model to identify and classify rules and annotate them with Rule acts. We map the Rule acts to business process steps and highlight the combinations as potential system use cases and validations for human supervision.","conference":"IEEE","terms":"Business;Switches;Insurance;Access control;Manuals;User interfaces;Databases,business data processing;document handling;knowledge based systems;natural language processing;program verification,human supervision;rule acts;rules annotation;rule identification;rule classification;semantic NL analysis;syntactic NL analysis;rule model;NLP techniques;linguistics;speech acts;atomic rule intents;rule statements;business policies;business rules;business processes;requirement document analysis;validation detection;automatic system use case detection","keywords":"Requirement documents;System use cases;validations;NL analyses;Rules;Rule types;Rule acts;Rule intents","startPage":"568","endPage":"573","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693114","citationCount":5,"referenceCount":26,"year":2013,"authors":"S. Ghaisas; M. Motwani; P. R. Anish","affiliations":"Tata Research, Development and Design Center, 54 Hadapsar Industrial Estates, Pune, 411013, India; Tata Research, Development and Design Center, 54 Hadapsar Industrial Estates, Pune, 411013, India; Tata Research, Development and Design Center, 54 Hadapsar Industrial Estates, Pune, 411013, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3775"},"title":"Semantic Slicing of Software Version Histories (T)","abstract":"Software developers often need to transfer func-tionality, e.g., a set of commits implementing a new feature or a bug fix, from one branch of a configuration management system to another. That can be a challenging task as the existing configuration management tools lack support for matching high-level semantic functionality with low-level version histories. The developer thus has to either manually identify the exact set of semantically-related commits implementing the functionality of interest or sequentially port a specific subset of the change history, \"inheriting\" additional, unwanted functionality. In this paper, we tackle this problem by providing automated support for identifying the set of semantically-related commits implementing a particular functionality, which is defined by a set of tests. We refer to our approach, CSLICER, as semantic slicing of version histories. We formally define the semantic slicing problem, provide an algorithm for identifying a set of commits that constitute a slice, and instantiate it in a specific implementation for Java projects managed in Git. We evaluate the correctness and effectiveness of our approach on a set of open-source software repositories. We show that it allows to identify subsets of change histories that maintain the functionality of interest but are substantially smaller than the original ones.","conference":"IEEE","terms":"History;Semantics;Java;Software;Context;Syntactics;Software algorithms,configuration management;Java;program debugging;program slicing;public domain software,software version history;software developer;bug fix;configuration management system;high-level semantic functionality;low-level version history;change history;unwanted functionality;automated support;CSLICER;semantic slicing problem;Java project;open-source software repository","keywords":"Software changes;version history;dependency","startPage":"686","endPage":"696","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372056","citationCount":9,"referenceCount":38,"year":2015,"authors":"Y. Li; J. Rubin; M. Chechik","affiliations":"Univ. of Toronto, Toronto, ON, Canada; Massachusetts Inst. of Technol., Cambridge, MA, USA; Univ. of Toronto, Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3776"},"title":"Developing a DSL-Based Approach for Event-Based Monitoring of Systems of Systems: Experiences and Lessons Learned (E)","abstract":"Complex software-intensive systems are often described as systems of systems (SoS) comprising heterogeneous architectural elements. As SoS behavior fully emerges during operation only, runtime monitoring is needed to detect deviations from requirements. Today, diverse approaches exist to define and check runtime behavior and performance characteristics. However, existing approaches often focus on specific types of systems and address certain kinds of checks, thus impeding their use in industrial SoS. Furthermore, as many SoS need to run continuously for long periods, the dynamic definition and deployment of constraints needs to be supported. In this paper we describe experiences of developing and applying a DSL-based approach for monitoring an SoS in the domain of industrial automation software. We evaluate both the expressiveness of our DSL as well as the scalability of the constraint checker. We also describe lessons learned.","conference":"IEEE","terms":"Monitoring;Runtime;Steel;Automation;Iron;Casting;Software,computerised monitoring;constraint handling;factory automation;system monitoring;visual programming,DSL-based approach;event-based monitoring;complex software-intensive system;systems of systems;heterogeneous architectural elements;SoS;runtime monitoring;runtime behavior checking;performance characteristics;constraint deployment;industrial automation software;constraint checker scalability;domain-specific languages","keywords":"Systems of systems;requirements monitoring;constraint checking;domain-specific languages","startPage":"715","endPage":"725","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372060","citationCount":10,"referenceCount":45,"year":2015,"authors":"M. Vierhauser; R. Rabiser; P. Grünbacher; A. Egyed","affiliations":"Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Inst. for Software Syst. Eng., Johannes Kepler Univ., Linz, Austria; Inst. for Software Syst. Eng., Johannes Kepler Univ., Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3777"},"title":"A study of repetitiveness of code changes in software evolution","abstract":"In this paper, we present a large-scale study of repetitiveness of code changes in software evolution. We collected a large data set of 2,841 Java projects, with 1.7 billion source lines of code (SLOC) at the latest revisions, 1.8 million code change revisions (0.4 million fixes), 6.2 million changed files, and 2.5 billion changed SLOCs. A change is considered repeated within or cross-project if it matches another change having occurred in the history of the project or another project, respectively. We report the following important findings. First, repetitiveness of changes could be as high as 70-100% at small sizes and decreases exponentially as size increases. Second, repetitiveness is higher and more stable in the cross-project setting than in the within-project one. Third, fixing changes repeat similarly to general changes. Importantly, learning code changes and recommending them in software evolution is beneficial with accuracy for top-1 recommendation of over 30% and top-3 of nearly 35%. Repeated fixing changes could also be useful for automatic program repair.","conference":"IEEE","terms":"Software;Vegetation;Databases;History;Maintenance engineering;Libraries;Programming,automatic programming;Java;software maintenance;source code (software),software evolution;Java projects;source lines of code;SLOC;code change revisions;automatic program repair;code change learning;code change repetitiveness","keywords":"Repetitive Code Changes;Software Evolution","startPage":"180","endPage":"190","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693078","citationCount":34,"referenceCount":39,"year":2013,"authors":"H. A. Nguyen; A. T. Nguyen; T. T. Nguyen; T. N. Nguyen; H. Rajan","affiliations":"Iowa State University, USA; Iowa State University, USA; Iowa State University, USA; Iowa State University, USA; Iowa State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3778"},"title":"Automatically synthesizing SQL queries from input-output examples","abstract":"Many computer end-users, such as research scientists and business analysts, need to frequently query a database, yet lack enough programming knowledge to write a correct SQL query. To alleviate this problem, we present a programming by example technique (and its tool implementation, called SQLSynthesizer) to help end-users automate such query tasks. SQLSynthesizer takes from users an example input and output of how the database should be queried, and then synthesizes a SQL query that reproduces the example output from the example input. If the synthesized SQL query is applied to another, potentially larger, database with a similar schema, the synthesized SQL query produces a corresponding result that is similar to the example output. We evaluated SQLSynthesizer on 23 exercises from a classic database textbook and 5 forum questions about writing SQL queries. SQLSynthesizer synthesized correct answers for 15 textbook exercises and all 5 forum questions, and it did so from relatively small examples.","conference":"IEEE","terms":"Databases;Skeleton;Aggregates;Writing;Standards;Syntactics;Graphical user interfaces,query processing;SQL,automatic SQL query synthesis;input-output examples;computer end-users;database query;programming-by-example technique;SQLSynthesizer tool;forum questions;SQL query writing;database textbook exercises","keywords":"","startPage":"224","endPage":"234","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693082","citationCount":12,"referenceCount":35,"year":2013,"authors":"S. Zhang; Y. Sun","affiliations":"Computer Science \u0026 Engineering, University of Washington, USA; Computer Science \u0026 Engineering, University of Washington, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3779"},"title":"Automatic loop-invariant generation anc refinement through selective sampling","abstract":"Automatic loop-invariant generation is important in program analysis and verification. In this paper, we propose to generate loop-invariants automatically through learning and verification. Given a Hoare triple of a program containing a loop, we start with randomly testing the program, collect program states at run-time and categorize them based on whether they satisfy the invariant to be discovered. Next, classification techniques are employed to generate a candidate loop-invariant automatically. Afterwards, we refine the candidate through selective sampling so as to overcome the lack of sufficient test cases. Only after a candidate invariant cannot be improved further through selective sampling, we verify whether it can be used to prove the Hoare triple. If it cannot, the generated counterexamples are added as new tests and we repeat the above process. Furthermore, we show that by introducing a path-sensitive learning, i.e., partitioning the program states according to program locations they visit and classifying each partition separately, we are able to learn disjunctive loop-invariants. In order to evaluate our idea, a prototype tool has been developed and the experiment results show that our approach complements existing approaches.","conference":"IEEE","terms":"Cost accounting;Tools;Software;Computer science;Testing;Prototypes;Indexes,learning (artificial intelligence);program control structures;program diagnostics;program verification,automatic loop-invariant generation;selective sampling;program analysis;Hoare triple;program states;candidate loop-invariant;program locations;disjunctive loop-invariants;program verification;program state partitioning;path-sensitive learning","keywords":"Loop-invariant;program verification;classification;active learning;selective sampling","startPage":"782","endPage":"792","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115689","citationCount":0,"referenceCount":50,"year":2017,"authors":"J. Li; J. Sun; L. Li; Q. L. Le; S. Lin","affiliations":"Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; School of Computing, Teesside University, United Kingdom; School of Computer Science and Engineering, Nanyang Technological University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d377a"},"title":"Mining constraints for event-based monitoring in systems of systems","abstract":"The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS.","conference":"IEEE","terms":"Monitoring;Data mining;Runtime;Feature extraction;Automation;Heuristic algorithms;System of systems,data mining;formal specification;learning (artificial intelligence);system monitoring,mined constraints;event logs;real-world industrial SoS;deviation detection;constraint language;software-intensive systems of systems;event timing;event occurrence;process mining;specification mining;heterogeneous systems;multiple development teams;SoS environments;deep domain knowledge;declarative specifications;event data;monitored events;temporal logic;expected behavior;runtime monitoring approaches","keywords":"Constraint mining;event-based monitoring;systems of systems","startPage":"826","endPage":"831","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115693","citationCount":0,"referenceCount":29,"year":2017,"authors":"T. Krismayer; R. Rabiser; P. GrUnbacher","affiliations":"Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Johannes Kepler University Linz, Austria; Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Johannes Kepler University Linz, Austria; Christian Doppler Laboratory MEVSS, Institute for Software Systems Engineering, Johannes Kepler University Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d377b"},"title":"Saying ‘Hi!’ is not enough: Mining inputs for effective test generation","abstract":"Automatically generating unit tests is a powerful approach to exercise complex software. Unfortunately, current techniques often fail to provide relevant input values, such as strings that bypass domain-specific sanity checks. As a result, state-of-the-art techniques are effective for generic classes, such as collections, but less successful for domain-specific software. This paper presents TestMiner, the first technique for mining a corpus of existing tests for input values to be used by test generators for effectively testing software not in the corpus. The main idea is to extract literals from thousands of tests and to adapt information retrieval techniques to find values suitable for a particular domain. Evaluating the approach with 40 Java classes from 18 different projects shows that TestMiner improves test coverage by 21% over an existing test generator. The approach can be integrated into various test generators in a straightforward way, increasing their effectiveness on previously difficult-to-test classes.","conference":"IEEE","terms":"Generators;Software;Testing;Indexes;Data mining;Computer science,data mining;information retrieval;Java;program testing,TestMiner;test coverage;test generators;difficult-to-test classes;mining inputs;effective test generation;complex software;relevant input values;state-of-the-art techniques;generic classes;domain-specific software;information retrieval techniques;unit tests automatic generation;domain-specific sanity checks;Java classes","keywords":"","startPage":"44","endPage":"49","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115617","citationCount":0,"referenceCount":57,"year":2017,"authors":"L. D. Toffola; C. Staicu; M. Pradel","affiliations":"Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, TU Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d377c"},"title":"Synthesizing fault-tolerant programs from deontic logic specifications","abstract":"We study the problem of synthesizing fault-tolerant components from specifications, i.e., the problem of automatically constructing a fault-tolerant component implementation from a logical specification of the component, and the system's required level of fault-tolerance. In our approach, the logical specification of the component is given in dCTL, a branching time temporal logic with deontic operators, especially designed for fault-tolerant component specification. The synthesis algorithm takes the component specification, and a user-defined level of fault-tolerance (masking, nonmasking, failsafe), and automatically determines whether a component with the required fault-tolerance is realizable. Moreover, if the answer is positive, then the algorithm produces such a fault-tolerant implementation. Our technique for synthesis is based on the use of (bi)simulation algorithms for capturing different fault-tolerance classes, and the extension of a synthesis algorithm for CTL to cope with dCTL specifications.","conference":"IEEE","terms":"Fault tolerant systems;Fault tolerance;Model checking;Algorithm design and analysis;Cognition;Safety;Writing,formal specification;software fault tolerance;temporal logic,deontic logic specifications;fault-tolerant program synthesis;logical specification;fault-tolerance required level;branching time temporal logic;deontic operators;fault-tolerant component specification;synthesis algorithm;bisimulation algorithm;dCTL specifications","keywords":"Formal specification;Fault-tolerance;Program Synthesis;Temporal Logics;Deontic logics;Correctness by construction","startPage":"750","endPage":"753","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693149","citationCount":0,"referenceCount":16,"year":2013,"authors":"R. Demasi","affiliations":"Department of Computing and Software, McMaster University, Hamilton, Ontario, Canada, L8S 4K1","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d377d"},"title":"Can automated pull requests encourage software developers to upgrade out-of-date dependencies?","abstract":"Developers neglect to update legacy software dependencies, resulting in buggy and insecure software. One explanation for this neglect is the difficulty of constantly checking for the availability of new software updates, verifying their safety, and addressing any migration efforts needed when upgrading a dependency. Emerging tools attempt to address this problem by introducing automated pull requests and project badges to inform the developer of stale dependencies. To understand whether these tools actually help developers, we analyzed 7,470 GitHub projects that used these notification mechanisms to identify any change in upgrade behavior. Our results find that, on average, projects that use pull request notifications upgraded 1.6× as often as projects that did not use any tools. Badge notifications were slightly less effective: users upgraded 1.4× more frequently. Unfortunately, although pull request notifications are useful, developers are often overwhelmed by notifications: only a third of pull requests were actually merged. Through a survey, 62 developers indicated that their most significant concerns are breaking changes, understanding the implications of changes, and migration effort. The implications of our work suggests ways in which notifications can be improved to better align with developers' expectations and the need for new mechanisms to reduce notification fatigue and improve confidence in automated pull requests.","conference":"IEEE","terms":"Tools;Software;Libraries;Security;Google;Safety,project management;software engineering;software maintenance,automated pull requests;software developers;out-of-date dependencies;developers neglect;legacy software dependencies;insecure software;software updates;migration effort;project badges;stale dependencies;notification mechanisms;upgrade behavior;notification fatigue","keywords":"","startPage":"84","endPage":"94","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115621","citationCount":8,"referenceCount":33,"year":2017,"authors":"S. Mirhosseini; C. Parnin","affiliations":"North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d377e"},"title":"Repairing Programs with Semantic Code Search (T)","abstract":"Automated program repair can potentially reduce debugging costs and improve software quality but recent studies have drawn attention to shortcomings in the quality of automatically generated repairs. We propose a new kind of repair that uses the large body of existing open-source code to find potential fixes. The key challenges lie in efficiently finding code semantically similar (but not identical) to defective code and then appropriately integrating that code into a buggy program. We present SearchRepair, a repair technique that addresses these challenges by(1) encoding a large database of human-written code fragments as SMT constraints on input-output behavior, (2) localizing a given defect to likely buggy program fragments and deriving the desired input-output behavior for code to replace those fragments, (3) using state-of-the-art constraint solvers to search the database for fragments that satisfy that desired behavior and replacing the likely buggy code with these potential patches, and (4) validating that the patches repair the bug against program testsuites. We find that SearchRepair repairs 150 (19%) of 778 benchmark C defects written by novice students, 20 of which are not repaired by GenProg, TrpAutoRepair, and AE. We compare the quality of the patches generated by the four techniques by measuring how many independent, not-used-during-repairtests they pass, and find that SearchRepair-repaired programs pass 97.3% ofthe tests, on average, whereas GenProg-, TrpAutoRepair-, and AE-repaired programs pass 68.7%, 72.1%, and 64.2% of the tests, respectively. We concludethat SearchRepair produces higher-quality repairs than GenProg, TrpAutoRepair, and AE, and repairs some defects those tools cannot.","conference":"IEEE","terms":"Maintenance engineering;Semantics;Software;Benchmark testing;Computer bugs;Indexing,program debugging;program diagnostics;software quality,program repair;semantic code search;software quality;SearchRepair technique;human-written code fragments;SMT constraints;buggy code;C defects;GenProg-repaired program;TrpAutoRepair-repaired program;AE-repaired programs","keywords":"automated repair;SearchRepair;semantic code search;repair quality;debugging;fault localization","startPage":"295","endPage":"306","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372019","citationCount":52,"referenceCount":77,"year":2015,"authors":"Y. Ke; K. T. Stolee; C. L. Goues; Y. Brun","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d377f"},"title":"Cost-Efficient Sampling for Performance Prediction of Configurable Systems (T)","abstract":"A key challenge of the development and maintenanceof configurable systems is to predict the performance ofindividual system variants based on the features selected. It isusually infeasible to measure the performance of all possible variants, due to feature combinatorics. Previous approaches predictperformance based on small samples of measured variants, butit is still open how to dynamically determine an ideal samplethat balances prediction accuracy and measurement effort. Inthis paper, we adapt two widely-used sampling strategies forperformance prediction to the domain of configurable systemsand evaluate them in terms of sampling cost, which considersprediction accuracy and measurement effort simultaneously. Togenerate an initial sample, we introduce a new heuristic based onfeature frequencies and compare it to a traditional method basedon t-way feature coverage. We conduct experiments on six realworldsystems and provide guidelines for stakeholders to predictperformance by sampling.","conference":"IEEE","terms":"Predictive models;Training;Testing;Measurement;Buildings;Mathematical model;Electronic mail,sampling methods;software maintenance;software performance evaluation,feature combinatorics;performance prediction accuracy;sampling strategies;configurable system development;configurable system maintenance;sampling cost;measurement effort;feature frequencies","keywords":"performance prediction;sampling;configurable systems","startPage":"342","endPage":"352","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372023","citationCount":35,"referenceCount":21,"year":2015,"authors":"A. Sarkar; J. Guo; N. Siegmund; S. Apel; K. Czarnecki","affiliations":"Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3780"},"title":"Understanding feature requests by leveraging fuzzy method and linguistic analysis","abstract":"In open software development environment, a large number of feature requests with mixed quality are often posted by stakeholders and usually managed in issue tracking systems. Thoroughly understanding and analyzing the real intents that feature requests imply is a labor-intensive and challenging task. In this paper, we introduce an approach to understand feature requests automatically. We generate a set of fuzzy rules based on natural language processing techniques that classify each sentence in feature requests into a set of categories: Intent, Explanation, Benefit, Drawback, Example and Trivia. Consequently, the feature requests can be automatically structured based on the classification results. We conduct experiments on 2,112 sentences taken from 602 feature requests of nine popular open source projects. The results show that our method can reach a high performance on classifying sentences from feature requests. Moreover, when applying fuzzy rules on machine learning methods, the performance can be improved significantly.","conference":"IEEE","terms":"Pragmatics;Software;Terminology;Semantics;Syntactics;Natural language processing;Learning systems,classification;fuzzy set theory;knowledge based systems;learning (artificial intelligence);natural language processing;public domain software;software quality,understanding feature requests;machine learning methods;open source projects;trivia category;example category;drawback category;benefit category;explanation category;intent category;sentence classification;natural language processing techniques;fuzzy rules;open software development environment;linguistic analysis;fuzzy method","keywords":"","startPage":"440","endPage":"450","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115656","citationCount":2,"referenceCount":45,"year":2017,"authors":"L. Shi; C. Chen; Q. Wang; S. Li; B. Boehm","affiliations":"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Center for Systems and Software Engineering, University of Southern California, Los Angeles, USA; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Center for Systems and Software Engineering, University of Southern California, Los Angeles, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3781"},"title":"Software performance self-adaptation through efficient model predictive control","abstract":"A key challenge in software systems that are exposed to runtime variabilities, such as workload fluctuations and service degradation, is to continuously meet performance requirements. In this paper we present an approach that allows performance self-adaptation using a system model based on queuing networks (QNs), a well-assessed formalism for software performance engineering. Software engineers can select the adaptation knobs of a QN (routing probabilities, service rates, and concurrency level) and we automatically derive a Model Predictive Control (MPC) formulation suitable to continuously configure the selected knobs and track the desired performance requirements. Previous MPC approaches have two main limitations: i) high computational cost of the optimization, due to nonlinearity of the models; ii) focus on long-run performance metrics only, due to the lack of tractable representations of the QN's time-course evolution. As a consequence, these limitations allow adaptations with coarse time granularities, neglecting the system's transient behavior. Our MPC adaptation strategy is efficient since it is based on mixed integer programming, which uses a compact representation of a QN with ordinary differential equations. An extensive evaluation on an implementation of a load balancer demonstrates the effectiveness of the adaptation and compares it with traditional methods based on probabilistic model checking.","conference":"IEEE","terms":"Adaptation models;Optimization;Runtime;Quality of service;Concurrent computing;Computational modeling;Throughput,differential equations;integer programming;predictive control;probability;queueing theory;resource allocation;software performance evaluation,model predictive control;mixed integer programming;ordinary differential equations;probabilistic model checking;MPC adaptation strategy;concurrency level;service rates;routing probabilities;adaptation knobs;software performance engineering;queuing networks;system model;service degradation;workload fluctuations;runtime variabilities;software systems;software performance self-adaptation","keywords":"Adaptive software;Control-theory;Model predictive control;Performance requirements","startPage":"485","endPage":"496","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115660","citationCount":1,"referenceCount":66,"year":2017,"authors":"E. Incerto; M. Tribastone; C. Trubiani","affiliations":"Gran Sasso Science Institute, Viale Francesco Crispi, 7, L'Aquila, Italy; IMT School for Advanced Studies, Piazza San Francesco, 19 Lucca, Italy; Gran Sasso Science Institute, Viale Francesco Crispi, 7, L'Aquila, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3782"},"title":"From comparison matrix to Variability Model: The Wikipedia case study","abstract":"Product comparison matrices (PCMs) provide a convenient way to document the discriminant features of a family of related products and now abound on the internet. Despite their apparent simplicity, the information present in existing PCMs can be very heterogeneous, partial, ambiguous, hard to exploit by users who desire to choose an appropriate product. Variability Models (VMs) can be employed to formulate in a more precise way the semantics of PCMs and enable automated reasoning such as assisted configuration. Yet, the gap between PCMs and VMs should be precisely understood and automated techniques should support the transition between the two. In this paper, we propose variability patterns that describe PCMs content and conduct an empirical analysis of 300+ PCMs mined from Wikipedia. Our findings are a first step toward better engineering techniques for maintaining and configuring PCMs.","conference":"IEEE","terms":"Phase change materials;Internet;Encyclopedias;Electronic publishing;Electronic mail;Color,Internet;matrix algebra;Web sites,variability model;Wikipedia;product comparison matrices;discriminant feature documentation;Internet;automated reasoning;assisted configuration;variability patterns;PCM mining","keywords":"","startPage":"580","endPage":"585","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693116","citationCount":6,"referenceCount":21,"year":2013,"authors":"N. Sannier; M. Acher; B. Baudry","affiliations":"University of Rennes 1, Irisa/Inria, Campus Universitaire de Beaulieu, 35042 cedex, France; University of Rennes 1, Irisa/Inria, Campus Universitaire de Beaulieu, 35042 cedex, France; University of Rennes 1, Irisa/Inria, Campus Universitaire de Beaulieu, 35042 cedex, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3783"},"title":"SBFR: A search based approach for reproducing failures of programs with grammar based input","abstract":"Reproducing field failures in-house, a step developers must perform when assigned a bug report, is an arduous task. In most cases, developers must be able to reproduce a reported failure using only a stack trace and/or some informal description of the failure. The problem becomes even harder for the large class of programs whose input is highly structured and strictly specified by a grammar. To address this problem, we present SBFR, a search-based failure-reproduction technique for programs with structured input. SBFR formulates failure reproduction as a search problem. Starting from a reported failure and a limited amount of dynamic information about the failure, SBFR exploits the potential of genetic programming to iteratively find legal inputs that can trigger the failure.","conference":"IEEE","terms":"Production;Grammar;Sociology;Statistics;Trajectory;Genetic algorithms;Search problems,dynamic programming;genetic algorithms;grammars;search problems;system recovery,SBFR;search based approach;program failures;grammar based input;field failures;bug report;informal description;search based failure reproduction technique;search problem;dynamic information;genetic programming","keywords":"","startPage":"604","endPage":"609","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693120","citationCount":4,"referenceCount":32,"year":2013,"authors":"F. M. Kifetew; W. Jin; R. Tiella; A. Orso; P. Tonella","affiliations":"Fondazione Bruno Kessler, Trento, Italy; Georgia Institute of Technology, USA; Fondazione Bruno Kessler, Trento, Italy; Georgia Institute of Technology, USA; Fondazione Bruno Kessler, Trento, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3784"},"title":"Quantification of Software Changes through Probabilistic Symbolic Execution (N)","abstract":"Characterizing software changes is fundamental for software maintenance. However existing techniques are imprecise leading to unnecessary maintenance efforts. We introduce a novel approach that computes a precise numeric characterization of program changes, which quantifies the likelihood of reaching target program events (e.g., assert violations or successful termination) and how that evolves with each program update, together with the percentage of inputs impacted by the change. This precise characterization leads to a natural ranking of different program changes based on their probability of execution and their impact on target events. The approach is based on model counting over the constraints collected with a symbolic execution of the program, and exploits the similarity between program versions to reduce cost and improve the quality of analysis results. We implemented our approach in the Symbolic PathFinder tool and illustrate it on several Java case studies, including the evaluation of different program repairs, mutants used in testing, or incremental analysis after a change.","conference":"IEEE","terms":"Probabilistic logic;Maintenance engineering;Software;Java;IP networks;Probability;Computational modeling,Java;probability;program testing;software maintenance,software change quantification;probabilistic symbolic execution;software maintenance;program update;Symbolic PathFinder tool;Java;program repairs;program testing","keywords":"","startPage":"703","endPage":"708","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372058","citationCount":6,"referenceCount":23,"year":2015,"authors":"A. Filieri; C. S. Pasareanu; G. Yang","affiliations":"Univ. of Stuttgart, Stuttgart, Germany; Carnegie Mellon Silicon Valley, NASA Ames, Moffet Field, CA, USA; Texas State Univ., San Marcos, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3785"},"title":"How Verified is My Code? Falsification-Driven Verification (T)","abstract":"Formal verification has advanced to the point that developers can verify the correctness of small, critical modules. Unfortunately, despite considerable efforts, determining if a \"verification\" verifies what the author intends is still difficult. Previous approaches are difficult to understand and often limited in applicability. Developers need verification coverage in terms of the software they are verifying, not model checking diagnostics. We propose a methodology to allow developers to determine (and correct) what it is that they have verified, and tools to support that methodology. Our basic approach is based on a novel variation of mutation analysis and the idea of verification driven by falsification. We use the CBMC model checker to show that this approach is applicable not only to simple data structures and sorting routines, and verification of a routine in Mozilla's JavaScript engine, but to understanding an ongoing effort to verify the Linux kernel Read-Copy-Update (RCU) mechanism.","conference":"IEEE","terms":"Arrays;Model checking;Software;Sorting;Computer bugs;Software engineering,formal verification;Java;Linux,falsification-driven verification;formal verification;mutation analysis;CBMC model checker;JavaScript engine;Linux kernel read-copy-update mechanism","keywords":"model checking;verification;mutation;oracles;falsification;test harnesses","startPage":"737","endPage":"748","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372062","citationCount":7,"referenceCount":56,"year":2015,"authors":"A. Groce; I. Ahmed; C. Jensen; P. E. McKenney","affiliations":"Sch. of Electr. Eng. \u0026 Comput. Sci., Oregon State Univ., Corvallis, OR, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Oregon State Univ., Corvallis, OR, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3786"},"title":"Characteristic studies of loop problems for structural test generation via symbolic execution","abstract":"Dynamic Symbolic Execution (DSE) is a state-of-the-art test-generation approach that systematically explores program paths to generate high-covering tests. In DSE, the presence of loops (especially unbound loops) can cause an enormous or even infinite number of paths to be explored. There exist techniques (such as bounded iteration, heuristics, and summarization) that assist DSE in addressing loop problems. However, there exists no literature-survey or empirical work that shows the pervasiveness of loop problems or identifies challenges faced by these techniques on real-world open-source applications. To fill this gap, we provide characteristic studies to guide future research on addressing loop problems for DSE. Our proposed study methodology starts with conducting a literature-survey study to investigate how technical problems such as loop problems compromise automated software-engineering tasks such as test generation, and which existing techniques are proposed to deal with such technical problems. Then the study methodology continues with conducting an empirical study of applying the existing techniques on real-world software applications sampled based on the literature-survey results and major open-source project hostings. This empirical study investigates the pervasiveness of the technical problems and how well existing techniques can address such problems among real-world software applications. Based on such study methodology, our two-phase characteristic studies identify that bounded iteration and heuristics are effective in addressing loop problems when used properly. Our studies further identify challenges faced by these techniques and provide guidelines for effectively addressing these challenges.","conference":"IEEE","terms":"Security;Open source software;Software testing;Search problems;Debugging,program testing,characteristic studies;loop problems;structural test generation;dynamic symbolic execution;DSE;unbound loops;infinite number;real world open source applications;software applications;open source project hostings;real-world software applications;software testing","keywords":"","startPage":"246","endPage":"256","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693084","citationCount":21,"referenceCount":54,"year":2013,"authors":"X. Xiao; S. Li; T. Xie; N. Tillmann","affiliations":"North Carolina State University, Raleigh, USA; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; Microsoft Research, Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3787"},"title":"Test suite parallelization in open-source projects: A study on its usage and impact","abstract":"Dealing with high testing costs remains an important problem in Software Engineering. Test suite parallelization is an important approach to address this problem. This paper reports our findings on the usage and impact of test suite parallelization in open-source projects. It provides recommendations to practitioners and tool developers to speed up test execution. Considering a set of 468 popular Java projects we analyzed, we found that 24% of the projects contain costly test suites but parallelization features still seem underutilized in practice - only 19.1% of costly projects use parallelization. The main reported reason for adoption resistance was the concern to deal with concurrency issues. Results suggest that, on average, developers prefer high predictability than high performance in running tests.","conference":"IEEE","terms":"Testing;Parallel processing;Java;Open source software;Resistance;Electrical resistance measurement;Instruction sets,concurrency (computers);Java;parallel processing;program testing;software engineering,test suite parallelization;open-source projects;test execution;parallelization features;Java projects;testing costs;adoption resistance","keywords":"","startPage":"838","endPage":"848","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115695","citationCount":1,"referenceCount":42,"year":2017,"authors":"J. Candido; L. Melo; M. d'Amorim","affiliations":"Federal University of Pernambuco, Pernambuco, Brazil; Federal University of Pernambuco, Pernambuco, Brazil; Federal University of Pernambuco, Pernambuco, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3788"},"title":"The impact of continuous integration on other software development practices: A large-scale empirical study","abstract":"Continuous Integration (CI) has become a disruptive innovation in software development: with proper tool support and adoption, positive effects have been demonstrated for pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential, and \"best practices\" to that end have been proposed. Here we study the adaptation and evolution of code writing and submission, issue and pull request closing, and testing practices as TRAVIS CI is adopted by hundreds of established projects on GITHUB. To help essentialize the quantitative results, we also survey a sample of GITHUB developers about their experiences with adopting TRAVIS CI. Our findings suggest a more nuanced picture of how GITHUB teams are adapting to, and benefiting from, continuous integration technology than suggested by prior work.","conference":"IEEE","terms":"Testing;Tools;Open source software;Best practices;Automation,program testing;project management;public domain software;software engineering,project sizes;GITHUB;pull request throughput;software development;continuous integration technology;TRAVIS CI","keywords":"","startPage":"60","endPage":"71","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115619","citationCount":14,"referenceCount":56,"year":2017,"authors":"Y. Zhao; A. Serebrenik; Y. Zhou; V. Filkov; B. Vasilescu","affiliations":"Nanjing University, China; Eindhoven U of Technology, The Netherlands; Nanjing Universit, China; UC Davis, USA; Carnegie Mellon University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3789"},"title":"SentiCR: A customized sentiment analysis tool for code review interactions","abstract":"Sentiment Analysis tools, developed for analyzing social media text or product reviews, work poorly on a Software Engineering (SE) dataset. Since prior studies have found developers expressing sentiments during various SE activities, there is a need for a customized sentiment analysis tool for the SE domain. On this goal, we manually labeled 2000 review comments to build a training dataset and used our dataset to evaluate seven popular sentiment analysis tools. The poor performances of the existing sentiment analysis tools motivated us to build SentiCR, a sentiment analysis tool especially designed for code review comments. We evaluated SentiCR using one hundred 10-fold cross-validations of eight supervised learning algorithms. We found a model, trained using the Gradient Boosting Tree (GBT) algorithm, providing the highest mean accuracy (83%), the highest mean precision (67.8%), and the highest mean recall (58.4%) in identifying negative review comments.","conference":"IEEE","terms":"Tools;Sentiment analysis;Supervised learning;Training;Algorithm design and analysis;Dictionaries;Social network services,learning (artificial intelligence);natural language processing;pattern classification;social networking (online);text analysis;trees (mathematics),software engineering dataset;gradient boosting tree algorithm;GBT algorithm;SentiCR;negative review comments;code review comments;SE domain;SE activities;social media text;code review interactions;customized sentiment analysis tool","keywords":"","startPage":"106","endPage":"111","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115623","citationCount":8,"referenceCount":48,"year":2017,"authors":"T. Ahmed; A. Bosu; A. Iqbal; S. Rahimi","affiliations":"Department of Computer Science \u0026 Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladeshi; Department of Computer Science, Southern Illinois University Carbondale, IL, USA; Department of Computer Science \u0026 Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladeshi; Department of Computer Science, Southern Illinois University Carbondale, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d378a"},"title":"Performance Prediction of Configurable Software Systems by Fourier Learning (T)","abstract":"Understanding how performance varies across a large number of variants of a configurable software system is important for helping stakeholders to choose a desirable variant. Given a software system with n optional features, measuring all its 2n possible configurations to determine their performances is usually infeasible. Thus, various techniques have been proposed to predict software performances based on a small sample of measured configurations. We propose a novel algorithm based on Fourier transform that is able to make predictions of any configurable software system with theoretical guarantees of accuracy and confidence level specified by the user, while using minimum number of samples up to a constant factor. Empirical results on the case studies constructed from real-world configurable systems demonstrate the effectiveness of our algorithm.","conference":"IEEE","terms":"Software systems;Prediction algorithms;Boolean functions;Software algorithms;Fourier transforms;Estimation;Algorithm design and analysis,configuration management;Fourier transforms;learning (artificial intelligence);software performance evaluation,performance prediction;configurable software system;Fourier learning;optional feature;possible configuration;software performance;Fourier transform;confidence level","keywords":"","startPage":"365","endPage":"373","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372025","citationCount":18,"referenceCount":20,"year":2015,"authors":"Y. Zhang; J. Guo; E. Blais; K. Czarnecki","affiliations":"Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Waterloo, Waterloo, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d378b"},"title":"Gremlin-ATL: A scalable model transformation framework","abstract":"Industrial use of Model Driven Engineering techniques has emphasized the need for efficiently store, access, and transform very large models. While scalable persistence frameworks, typically based on some kind of NoSQL database, have been proposed to solve the model storage issue, the same level of performance improvement has not been achieved for the model transformation problem. Existing model transformation tools (such as the well-known ATL) often require the input models to be loaded in memory prior to the start of the transformation and are not optimized to benefit from lazy-loading mechanisms, mainly due to their dependency on current low-level APIs offered by the most popular modeling frameworks nowadays. In this paper we present Gremlin-ATL, a scalable and efficient model-to-model transformation framework that translates ATL transformations into Gremlin, a query language supported by several NoSQL databases. With Gremlin-ATL, the transformation is computed within the database itself, bypassing the modeling framework limitations and improving its performance both in terms of execution time and memory consumption. Tool support is available online.","conference":"IEEE","terms":"Unified modeling language;Databases;Load modeling;Computational modeling;Transforms;Tools;Database languages,application program interfaces;NoSQL databases;object-oriented programming;query languages;query processing;software engineering;storage management,low-level API;execution time;memory consumption;query language;ATL transformations;model-to-model transformation framework;input models;model transformation tools;model storage issue;NoSQL database;scalable persistence frameworks;Model Driven Engineering techniques;scalable model transformation framework;Gremlin-ATL","keywords":"ATL;Gremlin;OCL Scalability;Persistence Framework;model transformation;NoSQL","startPage":"462","endPage":"472","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115658","citationCount":0,"referenceCount":33,"year":2017,"authors":"G. Daniel; F. Jouault; G. Sunyé; J. Cabot","affiliations":"AtlanMod Team, Inria, IMT Atlantique, LS2N, Nantes, France; TRAME Team, Groupe ESEO, Angers, France; AtlanMod Team, Inria, IMT Atlantique, LS2N, Nantes, France; ICREA, UOC, Barcelona, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d378c"},"title":"A comprehensive study of real-world numerical bug characteristics","abstract":"Numerical software is used in a wide variety of applications including safety-critical systems, which have stringent correctness requirements, and whose failures have catastrophic consequences that endanger human life. Numerical bugs are known to be particularly difficult to diagnose and fix, largely due to the use of approximate representations of numbers such as floating point. Understanding the characteristics of numerical bugs is the first step to combat them more effectively. In this paper, we present the first comprehensive study of real-world numerical bugs. Specifically, we identify and carefully examine 269 numerical bugs from five widely-used numerical software libraries: NumPy, SciPy, LAPACK, GNU Scientific Library, and Elemental. We propose a categorization of numerical bugs, and discuss their frequency, symptoms and fixes. Our study opens new directions in the areas of program analysis, testing, and automated program repair of numerical software, and provides a collection of real-world numerical bugs.","conference":"IEEE","terms":"Computer bugs;Libraries;Software;Tools;Semantics;Roundoff errors;Maintenance engineering,program debugging;program diagnostics;program testing;public domain software;safety-critical software;software libraries;software maintenance,numerical software libraries;safety-critical systems;numerical bug characteristics;NumPy;SciPy;LAPACK;GNU Scientific Library;Elemental;program analysis;program testing;automated program repair","keywords":"","startPage":"509","endPage":"519","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115662","citationCount":3,"referenceCount":45,"year":2017,"authors":"A. Di Franco; H. Guo; C. Rubio-González","affiliations":"Department of Computer Science, University of California, Davis, USA; Department of Computer Science, University of California, Davis, USA; Department of Computer Science, University of California, Davis, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d378d"},"title":"Environment rematching: Toward dependability improvement for self-adaptive applications","abstract":"Self-adaptive applications can easily contain faults. Existing approaches detect faults, but can still leave some undetected and manifesting into failures at runtime. In this paper, we study the correlation between occurrences of application failure and those of consistency failure. We propose fixing consistency failure to reduce application failure at runtime. We name this environment rematching, which can systematically reconnect a self-adaptive application to its environment in a consistent way. We also propose enforcing atomicity for application semantics during the rematching to avoid its side effect. We evaluated our approach using 12 self-adaptive robot-car applications by both simulated and real experiments. The experimental results confirmed our approach's effectiveness in improving dependability for all applications by 12.5-52.5%.","conference":"IEEE","terms":"Noise measurement;Semantics;Correlation;Legged locomotion;Robot sensing systems,automobiles;mobile robots;self-adjusting systems,environment rematching;dependability improvement;consistency failure;application failure;application semantics;self-adaptive robot-car applications","keywords":"Consistency failure;environment rematching","startPage":"592","endPage":"597","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693118","citationCount":1,"referenceCount":25,"year":2013,"authors":"C. Xu; Wenhua Yang; X. Ma; C. Cao; J. Lü","affiliations":"State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d378e"},"title":"Randomizing regression tests using game theory","abstract":"As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to “game” the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.","conference":"IEEE","terms":"Games;Testing;Security;Equations;Vectors;Game theory;Schedules,game theory;processor scheduling;program testing;regression analysis;statistical distributions,randomizing regression tests;game theory;software evolution;regression test suites;randomizing regression test scheduling;Stackelberg games;probabilistic distribution;scheduling test cases;randomized regression test scheduling","keywords":"","startPage":"616","endPage":"621","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693122","citationCount":5,"referenceCount":20,"year":2013,"authors":"N. Kukreja; W. G. J. Halfond; M. Tambe","affiliations":"University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d378f"},"title":"\"What Parts of Your Apps are Loved by Users?\" (T)","abstract":"Recently, Begel et al. found that one of the most important questions software developers ask is \"what parts of software are used/loved by users.\" User reviews provide an effective channel to address this question. However, most existing review summarization tools treat reviews as bags-of-words (i.e., mixed review categories) and are limited to extract software aspects and user preferences. We present a novel review summarization framework, SUR-Miner. Instead of a bags-of-words assumption, it classifies reviews into five categories and extracts aspects for sentences which include aspect evaluation using a pattern-based parser. Then, SUR-Miner visualizes the summaries using two interactive diagrams. Our evaluation on seventeen popular apps shows that SUR-Miner summarizes more accurate and clearer aspects than state-of-the-art techniques, with an F1-score of 0.81, significantly greater than that of ReviewSpotlight (0.56) and Guzmans' method (0.55). Feedback from developers shows that 88% developers agreed with the usefulness of the summaries from SUR-Miner.","conference":"IEEE","terms":"Feature extraction;Software;Semantics;Market research;Data mining;Visualization;Software engineering,data mining;diagrams;feature extraction;grammars;pattern classification;software engineering;software reviews,software development;review summarization tool;bags-of-words;software aspect extraction;user preference;SUR-Miner;review classification;pattern-based parser;interactive diagram","keywords":"Review Summarization;User Feedback;Sentiment Analysis;Data Mining","startPage":"760","endPage":"770","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372064","citationCount":35,"referenceCount":38,"year":2015,"authors":"X. Gu; S. Kim","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3790"},"title":"Detecting bad smells in source code using change history information","abstract":"Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis.","conference":"IEEE","terms":"History;Feature extraction;Surgery;Association rules;Detectors;Measurement,fault tolerant computing;software maintenance;software management;source code (software),bad smells detection;change history information;code smells;fault proneness;code elements;structural information;divergent change;shotgun surgery;parallel inheritance;blob;feature envy;versioning systems;HIST;Historical Information for Smell deTection;software projects;Java;smell detectors;source code analysis","keywords":"Code Smells;Change History Information","startPage":"268","endPage":"278","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693086","citationCount":83,"referenceCount":31,"year":2013,"authors":"F. Palomba; G. Bavota; M. Di Penta; R. Oliveto; A. De Lucia; D. Poshyvanyk","affiliations":"University of Salerno, Fisciano, Italy; University of Sannio, Benevento, Italy; University of Sannio, Benevento, Italy; University of Molise, Pesche (IS), Italy; University of Salerno, Fisciano, Italy; The College of William and Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3791"},"title":"A scalable approach for malware detection through bounded feature space behavior modeling","abstract":"In recent years, malware (malicious software) has greatly evolved and has become very sophisticated. The evolution of malware makes it difficult to detect using traditional signature-based malware detectors. Thus, researchers have proposed various behavior-based malware detection techniques to mitigate this problem. However, there are still serious shortcomings, related to scalability and computational complexity, in existing malware behavior modeling techniques. This raises questions about the practical applicability of these techniques. This paper proposes and evaluates a bounded feature space behavior modeling (BOFM) framework for scalable malware detection. BOFM models the interactions between software (which can be malware or benign) and security-critical OS resources in a scalable manner. Information collected at run-time according to this model is then used by machine learning algorithms to learn how to accurately classify software as malware or benign. One of the key problems with simple malware behavior modeling (e.g., n-gram model) is that the number of malware features (i.e., signatures) grows proportional to the size of execution traces, with a resulting malware feature space that is so large that it makes the detection process very challenging. On the other hand, in BOFM, the malware feature space is bounded by an upper limit N, a constant, and the results of our experiments show that its computation time and memory usage are vastly lower than in currently reported, malware detection techniques, while preserving or even improving their high detection accuracy.","conference":"IEEE","terms":"Malware;Feature extraction;Computational modeling;File systems;Scalability;Instruction sets,computational complexity;invasive software;learning (artificial intelligence);operating systems (computers);pattern classification,scalable malware detection techniques;bounded feature space behavior modeling;malicious software;signature-based malware detectors;behavior-based malware detection techniques;computational complexity;malware behavior modeling techniques;BOFM framework;security-critical OS resources;machine learning algorithms;software classification;execution trace size;malware feature space","keywords":"Malware detection;Malware behavior modeling","startPage":"312","endPage":"322","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693090","citationCount":9,"referenceCount":32,"year":2013,"authors":"M. Chandramohan; H. B. K. Tan; L. C. Briand; L. K. Shar; B. M. Padmanabhuni","affiliations":"School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; SnT Centre, University of Luxembourg, Luxembourg; SnT Centre, University of Luxembourg, Luxembourg; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore; School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9123eee8435e8e7d3792"},"title":"Automatically reducing tree-structured test inputs","abstract":"Reducing the test input given to a program while preserving some property of interest is important, e.g., to localize faults or to reduce test suites. The well-known delta debugging algorithm and its derivatives automate this task by repeatedly reducing a given input. Unfortunately, these approaches are limited to blindly removing parts of the input and cannot reduce the input by restructuring it. This paper presents the Generalized Tree Reduction (GTR) algorithm, an effective and efficient technique to reduce arbitrary test inputs that can be represented as a tree, such as program code, PDF files, and XML documents. The algorithm combines tree transformations with delta debugging and a greedy backtracking algorithm. To reduce the size of the considered search space, the approach automatically specializes the tree transformations applied by the algorithm based on examples of input trees. We evaluate GTR by reducing Python files that cause interpreter crashes, JavaScript files that cause browser inconsistencies, PDF documents with malicious content, and XML files used to tests an XML validator. The GTR algorithm reduces the trees of these files to 45.3%, 3.6%, 44.2%, and 1.3% of the original size, respectively, outperforming both delta debugging and another state-of-the-art algorithm.","conference":"IEEE","terms":"Debugging;Computer bugs;XML;Portable document format;Syntactics,greedy algorithms;Internet;online front-ends;program debugging;program testing;tree data structures;XML,delta debugging algorithm;generalized tree reduction algorithm;XML validator;tree-structured test inputs;GTR algorithm;XML files;JavaScript files;Python files;greedy backtracking algorithm;tree transformations;XML documents;PDF files;program code;arbitrary test inputs","keywords":"","startPage":"861","endPage":"871","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115697","citationCount":4,"referenceCount":46,"year":2017,"authors":"S. Herfert; J. Patra; M. Pradel","affiliations":"Department of Computer Science, TU Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3793"},"title":"Improving software text retrieval using conceptual knowledge in source code","abstract":"A large software project usually has lots of various textual learning resources about its API, such as tutorials, mailing lists, user forums, etc. Text retrieval technology allows developers to search these API learning resources for related documents using free-text queries, but it suffers from the lexical gap between search queries and documents. In this paper, we propose a novel approach for improving the retrieval of API learning resources through leveraging software-specific conceptual knowledge in software source code. The basic idea behind this approach is that the semantic relatedness between queries and documents could be measured according to software-specific concepts involved in them, and software source code contains a large amount of software-specific conceptual knowledge. In detail, firstly we extract an API graph from software source code and use it as software-specific conceptual knowledge. Then we discover API entities involved in queries and documents, and infer semantic document relatedness through analyzing structural relationships between these API entities. We evaluate our approach in three popular open source software projects. Comparing to the state-of-the-art text retrieval approaches, our approach lead to at least 13.77% improvement with respect to mean average precision (MAP).","conference":"IEEE","terms":"Semantics;Knowledge based systems;Software engineering;Internet;Tutorials;Open source software,application program interfaces;information retrieval;learning (artificial intelligence);public domain software;text analysis,MAP;mean average precision;semantic document relatedness;software-specific conceptual knowledge;software text retrieval;popular open source software projects;API entities;API graph;software-specific concepts;software source code;search queries;free-text queries;related documents;API learning resources;text retrieval technology;textual learning resources;software project","keywords":"Software text retrieval;conceptual knowledge;API graph;semantic relatedness","startPage":"123","endPage":"134","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115625","citationCount":3,"referenceCount":53,"year":2017,"authors":"Z. Lin; Y. Zou; J. Zhao; B. Xie","affiliations":"Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China, 100871; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China, 100871; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China, 100871; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China, 100871","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3794"},"title":"Configuration-Aware Change Impact Analysis (T)","abstract":"Understanding variability is essential to allow the configuration of software systems to diverse requirements. Variability-aware program analysis techniques have been proposed for analyzing the space of program variants. Such techniques are highly beneficial, e.g., to determine the potential impact of changes during maintenance. This paper presents an interprocedural and configuration-aware change impact analysis (CIA) approach for determining possibly impacted products when changing source code of a product family. The approach further supports engineers, who are adapting specific product variants after an initial pre-configuration. The approach can be adapted to work with different variability mechanism, it provides more precise results than existing CIA approaches, and it can be implemented using standard control flow and data flow analysis. Using an industrial product line we report evaluation results on the benefit and performance of the approach.","conference":"IEEE","terms":"Software systems;Testing;Mechanical factors;Maintenance engineering;Runtime;Standards,data flow analysis;source code (software),configuration-aware change impact analysis;product family source code;product variants;variability mechanism;CIA;data flow analysis;control flow analysis;industrial product line","keywords":"change impact analysis;program analysis;maintenance;configuration","startPage":"385","endPage":"395","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372027","citationCount":15,"referenceCount":33,"year":2015,"authors":"F. Angerer; A. Grimmer; H. Prähofer; P. Grünbacher","affiliations":"Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Inst. for Syst. Software, Johannes Kepler Univ. Linz, Linz, Austria; Inst. for Syst. Software, Johannes Kepler Univ. Linz, Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3795"},"title":"Automated Test Input Generation for Android: Are We There Yet? (E)","abstract":"Like all software, mobile applications (\"apps\") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.","conference":"IEEE","terms":"Androids;Humanoid robots;Testing;Java;Software;Systematics;Receivers,Android (operating system);graphical user interfaces;mobile computing;program testing,automated test input generation;mobile applications;automatic application testing;open source Android operating systems;ease-of-use metric;work ability metric;code coverage metric;fault detection ability metric","keywords":"Test input generation;automated testing;Android apps","startPage":"429","endPage":"440","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372031","citationCount":119,"referenceCount":35,"year":2015,"authors":"S. R. Choudhary; A. Gorla; A. Orso","affiliations":"Georgia Inst. of Technol., Atlanta, GA, USA; IMDEA Software Inst., Spain; Georgia Inst. of Technol., Atlanta, GA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3796"},"title":"Visualization support for requirements monitoring in systems of systems","abstract":"Industrial software systems are often systems of systems (SoS) whose full behavior only emerges at runtime. The systems and their interactions thus need to be continuously monitored and checked during operation to determine compliance with requirements. Many requirements monitoring approaches have been proposed. However, only few of these come with tools that present and visualize monitoring results and details on requirements violations to end users such as industrial engineers. In this tool demo paper we present visualization capabilities we have been developing motivated by industrial scenarios. Our tool complements ReMinds, an existing requirements monitoring framework, which supports collecting, aggregating, and analyzing events and event data in architecturally heterogeneous SoS. Our visualizations support a `drill-down' scenario for monitoring and diagnosis: starting from a graphical status overview of the monitored systems and their relations, engineers can view trends and statistics about performed analyses and diagnose the root cause of problems by inspecting the events and event data that led to a specific violation. Initial industry feedback we received confirms the usefulness of our tool support. Demo video: https://youtu.be/iv7kWzeNkdk..","conference":"IEEE","terms":"Monitoring;Tools;Data visualization;System of systems;Runtime;Automation;Probes,data visualisation;formal specification;Internet;system monitoring,visualization support;industrial software systems;visualize monitoring results;industrial engineers;tool demo paper;visualization capabilities;industrial scenarios;tool complements ReMinds;event data;architecturally heterogeneous SoS;drill-down scenario;monitored systems;initial industry feedback","keywords":"Requirements monitoring;visualization;systems of systems","startPage":"889","endPage":"894","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115700","citationCount":4,"referenceCount":19,"year":2017,"authors":"L. M. Kritzinger; T. Krismayer; M. Vierhauser; R. Rabiser; P. Grünbacher","affiliations":"Christian Doppler Lab MEVSS, ISSE, Johannes Kepler University Linz, Austria; Christian Doppler Lab MEVSS, ISSE, Johannes Kepler University Linz, Austria; Computer Science and Engineering, University of Notre Dame, IN, USA; Christian Doppler Lab MEVSS, ISSE, Johannes Kepler University Linz, Austria; Christian Doppler Lab MEVSS, ISSE, Johannes Kepler University Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3797"},"title":"Generating simpler AST edit scripts by considering copy-and-paste","abstract":"In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy- and-paste when they are writing source code. Introducing copy- and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developers' actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations.","conference":"IEEE","terms":"Cloning;Visualization;Software;Transforms;Writing;Syntactics;Computer bugs,configuration management;Java;software engineering;source code (software);text editing;tree data structures;XML,software development;editing sequence;AST edit scripts;copy-and-paste action;source code change;tree-based differencing techniques;tree-based edit script;computational time","keywords":"","startPage":"532","endPage":"542","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115664","citationCount":5,"referenceCount":24,"year":2017,"authors":"Y. Higo; A. Ohtani; S. Kusumoto","affiliations":"Graduate School of Information Science and Technology, Osaka University, 1-5, Yamadaoka, Suita, Osaka, 565-0871, Japan; Graduate School of Information Science and Technology, Osaka University, 1-5, Yamadaoka, Suita, Osaka, 565-0871, Japan; Graduate School of Information Science and Technology, Osaka University, 1-5, Yamadaoka, Suita, Osaka, 565-0871, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3798"},"title":"Adding context to fault localization with integration coverage","abstract":"Fault localization is a costly task in the debugging process. Several techniques to automate fault localization have been proposed aiming at reducing effort and time spent. Some techniques use heuristics based on code coverage data. The goal is to indicate program code excerpts more likely to contain faults. The coverage data mostly used in automated debugging is based on white-box unit testing (e.g., statements, basic blocks, predicates). This paper presents a technique which uses integration coverage data to guide the fault localization process. By ranking most suspicious pairs of method invocations, roadmaps-sorted lists of methods to be investigated-are created. At each method, unit coverage (e.g., basic blocks) is used to locate the fault site. Fifty-five bugs of four programs containing 2K to 80K lines of code (LOC) were analyzed. The results indicate that, by using the roadmaps, the effectiveness of the fault localization process is improved: 78% of all the faults are reached within a fixed amount of basic blocks; 40% more than an approach based on the Tarantula technique. Furthermore, fewer blocks have to be investigated until reaching the fault.","conference":"IEEE","terms":"Debugging;Computer bugs;Libraries;Educational institutions;Statistical analysis;Context;Testing,program debugging;program testing,integration coverage;debugging process;fault localization automation;program code excerpts;white-box unit testing;fault localization process;Tarantula technique","keywords":"Coverage-based debugging;Fault localization;Integration coverage","startPage":"628","endPage":"633","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693124","citationCount":1,"referenceCount":20,"year":2013,"authors":"H. A. de Souza; M. L. Chaim","affiliations":"Institute of Mathematics and Statistics, University of Sao Paulo, Brazil; School of Arts, Sciences and Humanities, University of Sao Paulo, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d3799"},"title":"The ReMinds Tool Suite for Runtime Monitoring of Systems of Systems","abstract":"The behavior of systems of systems (SoS) emerges only fully during operation and is hard to predict. SoS thus need to be monitored at runtime to detect deviations from important requirements. However, existing approaches for checking runtime behavior and performance characteristics are limited with respect to the kinds of checks and the types of technologies supported, which impedes their use in industrial SoS. In this tool demonstration paper we describe the ReMinds tool suite for runtime monitoring of SoS developed in response to industrial monitoring scenarios. ReMinds provides comprehensive tool support for instrumenting systems, extracting events and data at runtime, defining constraints to check expected behavior and properties, and visualizing constraint violations to facilitate diagnosis.","conference":"IEEE","terms":"Monitoring;Runtime;Probes;Data visualization;Java;Aggregates,program verification;software tools;system monitoring,ReMinds tool suite;systems of systems runtime monitoring;runtime behavior checking;industrial SoS;industrial monitoring scenarios","keywords":"System of systems;runtime monitoring;tool support","startPage":"777","endPage":"782","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372066","citationCount":1,"referenceCount":33,"year":2015,"authors":"M. Vierhauser; R. Rabiser; P. Grünbacher; J. Thanhofer-Pilisch","affiliations":"Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria; Christian Doppler Lab. MEVSS, Johannes Kepler Univ. Linz, Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d379a"},"title":"FLYAQ: Enabling Non-expert Users to Specify and Generate Missions of Autonomous Multicopters","abstract":"Multicopters are increasingly popular since they promise to simplify a myriad of everyday tasks. Currently, vendors provide low-level APIs and basic primitives to program multicopters, making mission development a task-specific and error-prone activity. As a consequence, current approaches are affordable only for users that have a strong technical expertise. Then, software engineering techniques are needed to support the definition, development, and realization of missions at the right level of abstraction and involving teams of autonomous multicopters that guarantee the safety today's users expect. In this paper we describe a tool that enables end-users with no technical expertise, e.g., firefighters and rescue workers, to specify missions for a team of multicopters. The detailed flight plan that each multicopter must perform to accomplish the specified mission is automatically generated by preventing collisions between multicopters and obstacles, and ensuring the preservation of no-fly zones.","conference":"IEEE","terms":"Drones;Monitoring;Software engineering;Solar panels;Software;Safety;Earthquakes,aerospace computing;application program interfaces;control engineering computing;helicopters;software engineering,FLYAQ;nonexpert users;autonomous multicopters;low-level API;program multicopters;error-prone activity;task-specific activity;software engineering techniques;no-fly zones","keywords":"Multicopter;Model-Driven Engineering;Domain-specific Languages","startPage":"801","endPage":"806","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372070","citationCount":10,"referenceCount":14,"year":2015,"authors":"D. Bozhinoski; D. D. Ruscio; I. Malavolta; P. Pelliccione; M. Tivoli","affiliations":"Gran Sasso Sci. Inst., L'Aquila, Italy; NA; Gran Sasso Sci. Inst., L'Aquila, Italy; Dept. of Inf. Eng., Univ. of L'Aquila, L'Aquila, Italy; Dept. of Inf. Eng., Univ. of L'Aquila, L'Aquila, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d379b"},"title":"Automatic recommendation of API methods from feature requests","abstract":"Developers often receive many feature requests. To implement these features, developers can leverage various methods from third party libraries. In this work, we propose an automated approach that takes as input a textual description of a feature request. It then recommends methods in library APIs that developers can use to implement the feature. Our recommendation approach learns from records of other changes made to software systems, and compares the textual description of the requested feature with the textual descriptions of various API methods. We have evaluated our approach on more than 500 feature requests of Axis2/Java, CXF, Hadoop Common, HBase, and Struts 2. Our experiments show that our approach is able to recommend the right methods from 10 libraries with an average recall-rate@5 of 0.690 and recall-rate@10 of 0.779 respectively. We also show that the state-of-the-art approach by Chan et al., that recommends API methods based on precise text phrases, is unable to handle feature requests.","conference":"IEEE","terms":"Libraries;Vectors;Databases;Control systems;Documentation;Software systems;Java,application program interfaces;Java;software libraries,automatic recommendation;API methods;feature requests;textual description;library APIs;software systems;Axis2/Java;CXF;Hadoop Common;HBase;Struts 2","keywords":"","startPage":"290","endPage":"300","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693088","citationCount":28,"referenceCount":37,"year":2013,"authors":"F. Thung; S. Wang; D. Lo; J. Lawall","affiliations":"Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore; Inria/Lip6 Regal, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d379c"},"title":"Finding architectural flaws using constraints","abstract":"During Architectural Risk Analysis (ARA), security architects use a runtime architecture to look for security vulnerabilities that are architectural flaws rather than coding defects. The current ARA process, however, is mostly informal and manual. In this paper, we propose Scoria, a semi-automated approach for finding architectural flaws. Scoria uses a sound, hierarchical object graph with abstract objects and dataflow edges, where edges can refer to nodes in the graph. The architects can augment the object graph with security properties, which can express security information unavailable in code. Scoria allows architects to write queries on the graph in terms of the hierarchy, reachability, and provenance of a dataflow object. Based on the query results, the architects enhance their knowledge of the system security and write expressive constraints. The expressiveness is richer than previous approaches that check only for the presence or absence of communication or do not track a dataflow as an object. To evaluate Scoria, we apply these constraints to several extended examples adapted from the CERT standard for Java to confirm that Scoria can detect injected architectural flaws. Next, we write constraints to enforce an Android security policy and find one architectural flaw in one Android application.","conference":"IEEE","terms":"Runtime;Abstracts;Connectors;Standards;Encryption;Encoding,Android (operating system);data flow analysis;Java;reachability analysis;security of data;software architecture,architectural flaws;architectural risk analysis;security architects;runtime architecture;security vulnerability;coding defects;ARA process;Scoria;semiautomated approach;object graph;dataflow edges;security property;security information;reachability;dataflow object;system security;CERT standard;Java;Android security policy","keywords":"","startPage":"334","endPage":"344","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693092","citationCount":3,"referenceCount":47,"year":2013,"authors":"R. Vanciu; M. Abi-Antoun","affiliations":"Department of Computer Science, Wayne State University, Detroit, Michigan, USA; Department of Computer Science, Wayne State University, Detroit, Michigan, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d379d"},"title":"SEALANT: A detection and visualization tool for inter-app security vulnerabilities in androic","abstract":"Android's flexible communication model allows interactions among third-party apps, but it also leads to inter-app security vulnerabilities. Specifically, malicious apps can eavesdrop on interactions between other apps or exploit the functionality of those apps, which can expose a user's sensitive information to attackers. While the state-of-the-art tools have focused on detecting inter-app vulnerabilities in Android, they neither accurately analyze realistically large numbers of apps nor effectively deliver the identified issues to users. This paper presents SEALANT, a novel tool that combines static analysis and visualization techniques that, together, enable accurate identification of inter-app vulnerabilities as well as their systematic visualization. SEALANT statically analyzes architectural information of a given set of apps, infers vulnerable communication channels where inter-app attacks can be launched, and visualizes the identified information in a compositional representation. SEALANT has been demonstrated to accurately identify inter-app vulnerabilities from hundreds of real-world Android apps and to effectively deliver the identified information to users. (Demo Video: https://youtu.be/E4lLQonOdUw)","conference":"IEEE","terms":"Sealing materials;Androids;Humanoid robots;Visualization;Tools;Analytical models;Data mining,Android (operating system);mobile computing;program diagnostics;security of data,inter-app security vulnerabilities;third-party apps;detecting inter-app vulnerabilities;static analysis;SEALANT statically analyzes architectural information;infers vulnerable communication channels;real-world Android apps;visualization tool;Android flexible communication model","keywords":"","startPage":"883","endPage":"888","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115699","citationCount":1,"referenceCount":25,"year":2017,"authors":"Y. K. Lee; P. Yoodee; A. Shahbazian; D. Nam; N. Medvidovic","affiliations":"Computer Science Department, University of Southern California, 941 Bloom Walk, Los Angeles, California, USA 90089; Computer Science Department, University of Southern California, 941 Bloom Walk, Los Angeles, California, USA 90089; Computer Science Department, University of Southern California, 941 Bloom Walk, Los Angeles, California, USA 90089; Computer Science Department, University of Southern California, 941 Bloom Walk, Los Angeles, California, USA 90089; Computer Science Department, University of Southern California, 941 Bloom Walk, Los Angeles, California, USA 90089","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d379e"},"title":"Improving missing issue-commit link recovery using positive and unlabeled data","abstract":"Links between issue reports and corresponding fix commits are widely used in software maintenance. The quality of links directly affects maintenance costs. Currently, such links are mainly maintained by error-prone manual efforts, which may result in missing links. To tackle this problem, automatic link recovery approaches have been proposed by building traditional classifiers with positive and negative links. However, these traditional classifiers may not perform well due to the inherent characteristics of missing links. Positive links, which can be used to build link recovery model, are quite limited as the result of missing links. Since the construction of negative links depends on the number of positive links in many existing approaches, the available negative links also become restricted. In this paper, we point out that it is better to consider the missing link problem as a model learning problem by using positive and unlabeled data, rather than the construction of traditional classifier. We propose PULink, an approach that constructs the link recovery model with positive and unlabeled links. Our experiment results show that compared to existing state-of-the-art technologies built on traditional classifier, PULink can achieve competitive performance by utilizing only 70% positive links that are used in those approaches.","conference":"IEEE","terms":"Feature extraction;Indexes;Metadata;Training;Software maintenance,learning (artificial intelligence);pattern classification;software maintenance,improving missing issue-commit link recovery;positive data;missing links;automatic link recovery approaches;traditional classifier;link recovery model;unlabeled link data","keywords":"","startPage":"147","endPage":"152","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115627","citationCount":1,"referenceCount":11,"year":2017,"authors":"Y. Sun; C. Chen; Q. Wang; B. Boehm","affiliations":"University of Chinese Academy of Sciences, Beijing, 100049, P.R. China; Department of Computer Science, Occidental College, Los Angeles, CA; University of Chinese Academy of Sciences, Beijing, 100049, P.R. China; Department of Computer Science, Occidental College, Los Angeles, CA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d379f"},"title":"More effective interpolations in software model checking","abstract":"An approach to CEGAR-based model checking which has proved to be successful on large models employs Craig interpolation to efficiently construct parsimonious abstractions. Following this design, we introduce new applications, universal safety interpolant and existential error interpolant, of Craig interpolation that can systematically reduce the program state space to be explored for safety verification. Whenever the universal safety interpolant is implied by the current path, all paths emanating from that location are guaranteed to be safe. Dually whenever the existential error interpolant is implied by the current path, there is guaranteed to be an unsafe path from the location. We show how these interpolants are computed and applied in safety verification. We have implemented our approach in a tool named InterpChecker by building on an open source software model checker. Experiments on a large number of benchmark programs show that both the interpolations and the auxiliary optimization strategies are effective in improving scalability of software model checking.","conference":"IEEE","terms":"Interpolation;Safety;Model checking;Space exploration;Subspace constraints;Software;Tools,formal verification;interpolation;program diagnostics;program verification;public domain software;theorem proving,universal safety interpolant;existential error interpolant;Craig interpolation;program state space;safety verification;unsafe path;open source software model checking;CEGAR-based model checking;InterpChecker","keywords":"","startPage":"183","endPage":"193","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115631","citationCount":0,"referenceCount":39,"year":2017,"authors":"C. Tian; Z. Duan; Z. Duan; C. -. L. Ong","affiliations":"ICTT and ISN Lab, Xidian University, Xi'an 710071, P.R. China; ICTT and ISN Lab, Xidian University, Xi'an 710071, P.R. China; ICTT and ISN Lab, Xidian University, Xi'an 710071, P.R. China; Department of Computer Science, University of Oxford, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a0"},"title":"Scaling Size and Parameter Spaces in Variability-Aware Software Performance Models (T)","abstract":"In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion -- the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.","conference":"IEEE","terms":"Unified modeling language;Mathematical model;Analytical models;Computational modeling;Software performance;Numerical models;Uncertainty,differential equations;queueing theory;software performance evaluation;software product lines,variability-aware software performance models;software performance engineering;what-if scenarios;architecture optimization;capacity planning;run-time adaptation;uncertainty management;state space explosion;queuing models;product-line engineering methods;ordinary differential equations","keywords":"","startPage":"407","endPage":"417","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372029","citationCount":13,"referenceCount":41,"year":2015,"authors":"M. Kowal; M. Tschaikowski; M. Tribastone; I. Schaefer","affiliations":"Tech. Univ. Braunschweig, Braunschweig, Germany; IMT Inst. for Adv. Studies Lucca, Lucca, Italy; IMT Inst. for Adv. Studies Lucca, Lucca, Italy; Tech. Univ. Braunschweig, Braunschweig, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a1"},"title":"CLAMI: Defect Prediction on Unlabeled Datasets (T)","abstract":"Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distributions among datasets. Approaches for defect prediction on unlabeled datasets have also tried to address the problem by adopting unsupervised learning but it has one major limitation, the necessity for manual effort. In this study, we propose novel approaches, CLA and CLAMI, that show the potential for defect prediction on unlabeled datasets in an automated manner without need for manual effort. The key idea of the CLA and CLAMI approaches is to label an unlabeled dataset by using the magnitude of metric values. In our empirical study on seven open-source projects, the CLAMI approach led to the promising prediction performances, 0.636 and 0.723 in average f-measure and AUC, that are comparable to those of defect prediction based on supervised learning.","conference":"IEEE","terms":"Predictive models;Measurement;Software;Training;Supervised learning;Data models;Manuals,software fault tolerance;software quality;unsupervised learning,CLAMI approach;cross-project defect prediction;software engineering;defect information collection;unsupervised learning;CLA approach;supervised learning","keywords":"","startPage":"452","endPage":"463","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372033","citationCount":39,"referenceCount":63,"year":2015,"authors":"J. Nam; S. Kim","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China; Dept. of Comput. Sci. \u0026 Eng., Hong Kong Univ. of Sci. \u0026 Technol., Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a2"},"title":"TREM: A tool for mining timed regular specifications from system traces","abstract":"Software specifications are useful for software validation, model checking, runtime verification, debugging, monitoring, etc. In context of safety-critical real-time systems, temporal properties play an important role. However, temporal properties are rarely present due to the complexity and evolutionary nature of software systems. We propose Timed Regular Expression Mining (TREM) a hosted tool for specification mining using timed regular expressions (TREs). It is designed for easy and robust mining of dominant temporal properties. TREM uses an abstract structure of the property; the framework constructs a finite state machine to serve as an acceptor. TREM is scalable, easy to access/use, and platform independent specification mining framework. The tool is tested on industrial strength software system traces such as the QNX real-time operating system using traces with more than 1.5 Million entries. The tool demonstration video can be accessed here: youtu.be/cSd_aj3_LH8.","conference":"IEEE","terms":"Data mining;Automata;Tools;Software;Debugging;Real-time systems;Monitoring,data mining;finite state machines;formal specification;operating systems (computers);program verification;safety-critical software,model checking;runtime verification;safety-critical real-time systems;software systems;TREM;regular expressions;robust mining;dominant temporal properties;platform independent specification mining framework;industrial strength software system traces;QNX real-time operating system;tool demonstration video;software specifications;software validation;finite state machine;Timed Regular Expression Mining","keywords":"Specification Mining;Timed Regular Expressions;Real-time systems","startPage":"901","endPage":"906","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115702","citationCount":1,"referenceCount":27,"year":2017,"authors":"L. Schmidt; A. Narayan; S. Fischmeister","affiliations":"Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON N2L 3G1, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON N2L 3G1, Canada; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON N2L 3G1, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a3"},"title":"Semantics-assisted code review: An efficient tool chain and a user study","abstract":"Code changes are often reviewed before they are deployed. Popular source control systems aid code review by presenting textual differences between old and new versions of the code, leaving developers with the difficult task of determining whether the differences actually produced the desired behavior. Fortunately, we can mine such information from code repositories. We propose aiding code review with inter-version semantic differential analysis. During review of a new commit, a developer is presented with summaries of both code differences and behavioral differences, which are expressed as diffs of likely invariants extracted by running the system's test cases. As a result, developers can more easily determine that the code changes produced the desired effect. We created an invariant-mining tool chain, Getty, to support our concept of semantically-assisted code review. To validate our approach, 1) we applied Getty to the commits of 6 popular open source projects, 2) we assessed the performance and cost of running Getty in different configurations, and 3) we performed a comparative user study with 18 developers. Our results demonstrate that semantically-assisted code review is feasible, effective, and that real programmers can leverage it to improve the quality of their reviews.","conference":"IEEE","terms":"Tools;Testing;Computer bugs;Semantics;Software;Navigation,data mining;Internet;project management;software maintenance,semantic-assisted code review;invariant-mining tool chain;code differences;inter-version semantic differential analysis;code repositories;code changes","keywords":"Software behavior;mining software repository;code review;likely invariants;dynamic impact analysis;scalability;software testing","startPage":"554","endPage":"565","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115666","citationCount":0,"referenceCount":38,"year":2017,"authors":"M. Menarini; Y. Yan; W. G. Griswold","affiliations":"Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA, USA; Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA, USA; Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a4"},"title":"Floating-point symbolic execution: A case study in N-version programming","abstract":"Symbolic execution is a well-known program analysis technique for testing software, which makes intensive use of constraint solvers. Recent support for floating-point constraint solving has made it feasible to support floating-point reasoning in symbolic execution tools. In this paper, we present the experience of two research teams that independently added floating-point support to KLEE, a popular symbolic execution engine. Since the two teams independently developed their extensions, this created the rare opportunity to conduct a rigorous comparison between the two implementations, essentially a modern case study on N-version programming. As part of our comparison, we report on the different design and implementation decisions taken by each team, and show their impact on a rigorously assembled and tested set of benchmarks, itself a contribution of the paper.","conference":"IEEE","terms":"Benchmark testing;Tools;Programming;Cognition;Encoding;Concrete,constraint handling;floating point arithmetic;program diagnostics;program testing;public domain software,implementation decisions;N-version programming symbolic execution;testing software;constraint solvers;floating-point constraint solving;floating-point reasoning;symbolic execution tools;program analysis technique;symbolic execution engine;floating-point symbolic execution;floating-point support;research teams","keywords":"","startPage":"601","endPage":"612","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115670","citationCount":2,"referenceCount":82,"year":2017,"authors":"D. Liew; D. Schemmel; C. Cadar; A. F. Donaldson; R. Zahl; K. Wehrle","affiliations":"Imperial College London, United Kingdom; RWTH Aachen University, Germany; Imperial College London, United Kingdom; Imperial College London, United Kingdom; RWTH Aachen University, Germany; RWTH Aachen University, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a5"},"title":"Class level fault prediction using software clustering","abstract":"Defect prediction approaches use software metrics and fault data to learn which software properties associate with faults in classes. Existing techniques predict fault-prone classes in the same release (intra) or in a subsequent releases (inter) of a subject software system. We propose an intra-release fault prediction technique, which learns from clusters of related classes, rather than from the entire system. Classes are clustered using structural information and fault prediction models are built using the properties of the classes in each cluster. We present an empirical investigation on data from 29 releases of eight open source software systems from the PROMISE repository, with predictors built using multivariate linear regression. The results indicate that the prediction models built on clusters outperform those built on all the classes of the system.","conference":"IEEE","terms":"Predictive models;Clustering algorithms;Measurement;Accuracy;Open source software;Linear regression,pattern clustering;program debugging;public domain software;regression analysis;software fault tolerance;software metrics,class level fault prediction;software clustering;defect prediction;software properties;software metrics;fault-prone class prediction;intrarelease fault prediction technique;subject software system;open source software systems;PROMISE repository;multivariate linear regression","keywords":"Empirical Study;Fault Prediction;Software Clustering","startPage":"640","endPage":"645","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693126","citationCount":21,"referenceCount":35,"year":2013,"authors":"G. Scanniello; C. Gravino; A. Marcus; T. Menzies","affiliations":"University of Basilicata, Italy; University of Salerno, Italy; Wayne State University, USA; West Virginia University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a6"},"title":"Semi-automatic generation of metamodels from model sketches","abstract":"Traditionally, metamodeling is an upfront activity performed by experts for defining modeling languages. Modeling tools then typically restrict modelers to using only constructs defined in the metamodel. This is inappropriate when users want to sketch graphical models without any restrictions and only later assign meanings to the sketched elements. Upfront metamodeling also complicates the creation of domain-specific languages, as it requires experts with both domain and metamodeling expertise. In this paper we present a new approach that supports modelers in creating metamodels for diagrams they have sketched or are currently sketching. Metamodels are defined in a semi-automatic, interactive way by annotating diagram elements and automated model analysis. Our approach requires no metamodeling expertise and supports the co-evolution of models and meta-models.","conference":"IEEE","terms":"Unified modeling language;Metamodeling;DSL;Adaptation models;Libraries;Computational modeling;Context,computer graphics;SysML,model sketches;metamodels semiautomatic generation;graphical models;domain-specific languages;automated model analysis;diagram elements annotation","keywords":"Sketch;model;metamodel;inference;semiautomated;end-user","startPage":"664","endPage":"669","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693130","citationCount":8,"referenceCount":31,"year":2013,"authors":"D. Wüest; N. Seyff; M. Glinz","affiliations":"Department of Informatics, University of Zurich, Switzerland; Department of Informatics, University of Zurich, Switzerland; Department of Informatics, University of Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a7"},"title":"SpyREST in Action: An Automated RESTful API Documentation Tool","abstract":"RESTful APIs are often manually documented. As a result, the process of maintaining the documentation of RESTful APIs is both expensive and error-prone. In this demonstration paper, we present SpyREST as an automated software as a service tool that can be used to document RESTful APIs. SpyREST leverages an HTTP Proxy server to intercept real API calls to automatically collect and generate RESTful API documentation by processing HTTP traffic involved in API calls. SpyREST provides an automated yet customizable example based documentation solution for RESTful APIs. RESTful API developers can use SpyREST to automatically generate and maintain updated API documentation.","conference":"IEEE","terms":"Documentation;Servers;Uniform resource locators;Payloads;Collaboration;Software as a service;Context,application program interfaces;cloud computing,SpyREST;automated RESTful API documentation tool;software as a service tool;HTTP proxy server;HTTP traffic","keywords":"RESTful API;Web API;Documentation;Automation;Example based documentation","startPage":"813","endPage":"818","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372072","citationCount":3,"referenceCount":16,"year":2015,"authors":"S. M. Sohan; C. Anslow; F. Maurer","affiliations":"Dept. of Comput. Sci., Univ. of Calgary, Calgary, AB, Canada; Dept. of Comput. Sci., Univ. of Calgary, Calgary, AB, Canada; Dept. of Comput. Sci., Univ. of Calgary, Calgary, AB, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a8"},"title":"Leveraging program equivalence for adaptive program repair: Models and first results","abstract":"Software bugs remain a compelling problem. Automated program repair is a promising approach for reducing cost, and many methods have recently demonstrated positive results. However, success on any particular bug is variable, as is the cost to find a repair. This paper focuses on generate-and-validate repair methods that enumerate candidate repairs and use test cases to define correct behavior. We formalize repair cost in terms of test executions, which dominate most test-based repair algorithms. Insights from this model lead to a novel deterministic repair algorithm that computes a patch quotient space with respect to an approximate semantic equivalence relation. This allows syntactic and dataflow analysis techniques to dramatically reduce the repair search space. Generate-and-validate program repair is shown to be a dual of mutation testing, suggesting several possible cross-fertilizations. Evaluating on 105 real-world bugs in programs totaling 5MLOC and involving 10,000 tests, our new algorithm requires an order-of-magnitude fewer test evaluations than the previous state-of-the-art and is over three times more efficient monetarily.","conference":"IEEE","terms":"Maintenance engineering;Approximation algorithms;Testing;Algorithm design and analysis;Adaptation models;Optimization;Search problems,cost reduction;data flow analysis;deterministic algorithms;program debugging;program testing;software maintenance,program equivalence;adaptive program repair;software bugs;automated program repair;cost reduction;generate-and-validate program repair methods;test cases;repair cost;test-based repair algorithms;deterministic repair algorithm;approximate semantic equivalence relation;patch quotient space;dataflow analysis techniques;syntactic analysis techniques;repair search space reduction;mutation testing;cross-fertilizations","keywords":"Automated program repair;mutation testing;program equivalence;search-based software engineering","startPage":"356","endPage":"366","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693094","citationCount":84,"referenceCount":55,"year":2013,"authors":"W. Weimer; Z. P. Fry; S. Forrest","affiliations":"Computer Science Department, University of Virginia, Charlottesville, USA; Computer Science Department, University of Virginia, Charlottesville, USA; Computer Science Department, University of New Mexico, Albuquerque, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37a9"},"title":"Extracting Visual Contracts from Java Programs (T)","abstract":"Visual contracts model the operations of components or services by pre-and post-conditions formalised as graph transformation rules. They provide a precise intuitive notation to support testing, understanding and analysis of software. However, due to their detailed specification of data states and transformations, modelling real applications is an error-prone process. In this paper we propose a dynamic approach to reverse engineering visual contracts from Java based on tracing the execution of Java operations. The resulting contracts give an accurate description of the observed object transformations, their effects and preconditions in terms of object structures, parameter and attribute values, and their generalised specification by universally quantified (multi) objects. While this paper focusses on the fundamental technique rather than a particular application, we explore potential uses in our evaluation, including in program understanding, review of test reports and debugging.","conference":"IEEE","terms":"Contracts;Unified modeling language;Visualization;Java;Context;Testing;Cities and towns,Java,Java programs;visual contracts model;graph transformation rules;software;error-prone process;reverse engineering visual contracts;Java operations;object transformations;generalised specification;universally quantified objects","keywords":"graph transformation;rule learning;multi objects;extraction of visual contracts","startPage":"104","endPage":"114","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372000","citationCount":6,"referenceCount":31,"year":2015,"authors":"A. Alshanqiti; R. Heckel","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37aa"},"title":"Automatic summarization of API reviews","abstract":"With the proliferation of online developer forums as informal documentation, developers often share their opinions about the APIs they use. However, given the plethora of opinions available for an API in various online developer forums, it can be challenging for a developer to make informed decisions about the APIs. While automatic summarization of opinions have been explored for other domains (e.g., cameras, cars), we found little research that investigates the benefits of summaries of public API reviews. In this paper, we present two algorithms (statistical and aspect-based) to summarize opinions about APIs. To investigate the usefulness of the techniques, we developed, Opiner, an online opinion summarization engine that presents summaries of opinions using both our proposed techniques and existing six off-the-shelf techniques. We investigated the usefulness of Opiner using two case studies, both involving professional software engineers. We found that developers were interested to use our proposed summaries much more frequently than other summaries (daily vs once a year) and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence and in less time.","conference":"IEEE","terms":"Java;Message systems;Software;Heuristic algorithms;Cameras;Engines,application program interfaces;Internet;software engineering;text analysis,public API reviews;Opiner;online opinion summarization engine;automatic summarization;online developer forums;informal documentation;informed decisions;time 11.0 year","keywords":"Opinion mining;API informal documentation;opinion summaries;study;summary quality","startPage":"159","endPage":"170","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115629","citationCount":6,"referenceCount":64,"year":2017,"authors":"G. Uddin; F. Khomh","affiliations":"School of Computer Science, McGill University, Montréal, QC, Canada; SWAT Lab Polytechnique, Montréal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ab"},"title":"Model checker execution reports","abstract":"Software model checking constitutes an undecidable problem and, as such, even an ideal tool will in some cases fail to give a conclusive answer. In practice, software model checkers fail often and usually do not provide any information on what was effectively checked. The purpose of this work is to provide a conceptual framing to extend software model checkers in a way that allows users to access information about incomplete checks. We characterize the information that model checkers themselves can provide, in terms of analyzed traces, i.e. sequences of statements, and safe canes, and present the notion of execution reports (ERs), which we also formalize. We instantiate these concepts for a family of techniques based on Abstract Reachability Trees and implement the approach using the software model checker CPAchecker. We evaluate our approach empirically and provide examples to illustrate the ERs produced and the information that can be extracted.","conference":"IEEE","terms":"Software;Safety;Model checking;Tools;Erbium;Computational modeling;Atmospheric modeling,formal specification;program verification;reachability analysis;trees (mathematics),incomplete checks;model checker execution reports;software model checking;CPAchecker software model checker;abstract reachability trees","keywords":"","startPage":"200","endPage":"205","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115633","citationCount":0,"referenceCount":23,"year":2017,"authors":"R. Castaño; V. Braberman; D. Garbervetsky; S. Uchitel","affiliations":"Departamento de Computación, FCEyN, UBA, CONICET, Buenos Aires, Argentina; Departamento de Computación, FCEyN, UBA, CONICET, Buenos Aires, Argentina; Departamento de Computación, FCEyN, UBA, CONICET, Buenos Aires, Argentina; Departamento de Computación, FCEyN, UBA, CONICET, Buenos Aires, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ac"},"title":"Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N)","abstract":"Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files.","conference":"IEEE","terms":"Feature extraction;History;Metadata;Computer bugs;Software;Bridges;Information retrieval,information retrieval;learning (artificial intelligence);program debugging;support vector machines,deep learning;buggy file localization;bug reports;lexical mismatch;source files;deep neural network;DNN;rVSM;information retrieval technique;IR technique;code tokens;HyLoc;projec bug-fixing history;machine learning techniques","keywords":"Deep Neural Network;Deep Learning;Bug Localization;Information Retrieval;Bug Reports","startPage":"476","endPage":"481","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372035","citationCount":32,"referenceCount":10,"year":2015,"authors":"A. N. Lam; A. T. Nguyen; H. A. Nguyen; T. N. Nguyen","affiliations":"Iowa State Univ., Ames, IA, USA; Iowa State Univ., Ames, IA, USA; Iowa State Univ., Ames, IA, USA; Iowa State Univ., Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ad"},"title":"Incrementally slicing editable submodels","abstract":"Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017.","conference":"IEEE","terms":"Unified modeling language;Adaptation models;Tools;Servers;Load modeling;Computational modeling;Data models,formal specification;object-oriented programming;program slicing;program visualisation;Unified Modeling Language,UML model;autonomous model;editable slices;model editors;analysis tools;model management tools;generic incremental slicer;model differencing framework;model elements;incrementally sliced editable submodels","keywords":"","startPage":"913","endPage":"918","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115704","citationCount":2,"referenceCount":21,"year":2017,"authors":"C. Pietsch; M. Ohrndorf; U. Kelter; T. Kehrer","affiliations":"Software Engineering Group, University of Siegen, Germany; Software Engineering Group, University of Siegen, Germany; Software Engineering Group, University of Siegen, Germany; Department of Computer Science, Humboldt-University of Berlin, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ae"},"title":"Round-up: Runtime checking quasi linearizability of concurrent data structures","abstract":"We propose a new method for runtime checking of a relaxed consistency property called quasi linearizability for concurrent data structures. Quasi linearizability generalizes the standard notion of linearizability by intentionally introducing nondeterminism into the parallel computations and exploiting such nondeterminism to improve the performance. However, ensuring the quantitative aspects of this correctness condition in the low level code is a difficult task. Our method is the first fully automated method for checking quasi linearizability in the unmodified C/C++ code of concurrent data structures. It guarantees that all the reported quasi linearizability violations are real violations. We have implemented our method in a software tool based on LLVM and a concurrency testing tool called Inspect. Our experimental evaluation shows that the new method is effective in detecting quasi linearizability violations in the source code of concurrent data structures.","conference":"IEEE","terms":"History;Data structures;Standards;Law;Instruction sets;Runtime,concurrency control;data structures;formal verification;parallel processing;program testing,runtime checking;concurrent data structures;quasilinearizability property;linearizability notion;nondeterminism notion;parallel computations;correctness condition;C-C++ code;software tool;concurrency testing tool;Inspect tool","keywords":"","startPage":"4","endPage":"14","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693061","citationCount":5,"referenceCount":43,"year":2013,"authors":"Lu Zhang; A. Chattopadhyay; C. Wang","affiliations":"Department of ECE, Virginia Tech, Blacksburg, 24061, USA; Department of ECE, Virginia Tech, Blacksburg, 24061, USA; Department of ECE, Virginia Tech, Blacksburg, 24061, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37af"},"title":"Why and how JavaScript developers use linters","abstract":"Automatic static analysis tools help developers to automatically spot code issues in their software. They can be of extreme value in languages with dynamic characteristics, such as JavaScript, where developers can easily introduce mistakes which can go unnoticed for a long time, e.g. a simple syntactic or spelling mistake. Although research has already shown how developers perceive such tools for strongly-typed languages such as Java, little is known about their perceptions when it comes to dynamic languages. In this paper, we investigate what motivates and how developers make use of such tools in JavaScript projects. To that goal, we apply a qualitative research method to conduct and analyze a series of 15 interviews with developers responsible for the linter configuration in reputable OSS JavaScript projects that apply the most commonly used linter, ESLint. The results describe the benefits that developers obtain when using ESLint, the different ways one can configure the tool and prioritize its rules, and the existing challenges in applying linters in the real world. These results have direct implications for developers, tool makers, and researchers, such as tool improvements, and a research agenda that aims to increase our knowledge about the usefulness of such analyzers.","conference":"IEEE","terms":"Interviews;Tools;Software;Face;Standards;Encoding,Java;program diagnostics;type theory,ESLint;strongly-typed languages;automatic static analysis tools;JavaScript developers;linters;reputable OSS JavaScript projects;dynamic languages","keywords":"","startPage":"578","endPage":"589","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115668","citationCount":5,"referenceCount":59,"year":2017,"authors":"K. F. Tómasdóttir; M. Aniche; A. van Deursen","affiliations":"Delft University of Technology - The Netherlands; Delft University of Technology - The Netherlands; Delft University of Technology - The Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b0"},"title":"Leveraging abstract interpretation for efficient dynamic symbolic execution","abstract":"Dynamic Symbolic Execution (DSE) is a technique to automatically generate test inputs by executing a program with concrete and symbolic values simultaneously. A key challenge in DSE is scalability; executing all feasible program paths is not possible, owing to the potentially exponential or infinite number of paths. Loops are a main source of path explosion, in particular where the number of iterations depends on a program's input. Problems arise because DSE maintains symbolic values that capture only the dependencies on symbolic inputs. This ignores control dependencies, including loop dependencies that depend indirectly on the inputs. We propose a method to increase the coverage achieved by DSE in the presence of input-data dependent loops and loop dependent branches. We combine DSE with abstract interpretation to find indirect control dependencies, including loop and branch indirect dependencies. Preliminary results show that this results in better coverage, within considerably less time compared to standard DSE.","conference":"IEEE","terms":"Tools;Explosions;Concrete;Testing;Scalability,program control structures;program diagnostics;program testing;program verification,symbolic inputs;loop dependencies;input-data dependent loops;dependent branches;indirect control dependencies;branch indirect dependencies;standard DSE;efficient dynamic symbolic execution;test inputs;symbolic values;feasible program paths;path explosion","keywords":"Dynamic symbolic execution;DSE;abstract interpretation;path explosion;test generation","startPage":"619","endPage":"624","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115672","citationCount":2,"referenceCount":21,"year":2017,"authors":"E. Alatawi; H. S⊘ndergaard; T. Miller","affiliations":"School of Computing and Information Systems, The University of Melbourne, Victoria 3010, Australia; School of Computing and Information Systems, The University of Melbourne, Victoria 3010, Australia; School of Computing and Information Systems, The University of Melbourne, Victoria 3010, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b1"},"title":"Flow Permissions for Android","abstract":"This paper proposes Flow Permissions, an extension to the Android permission mechanism. Unlike the existing permission mechanism our permission mechanism contains semantic information based on information flows. Flow Permissions allow users to examine and grant explicit information flows within an application (e.g., a permission for reading the phone number and sending it over the network) as well as implicit information flows across multiple applications (e.g., a permission for reading the phone number and sending it to another application already installed on the user's phone). Our goal with Flow Permissions is to provide visibility into the holistic behavior of the applications installed on a user's phone. Our evaluation compares our approach to dynamic flow tracking techniques; our results with 600 popular applications and 1,200 malicious applications show that our approach is practical and effective in deriving Flow Permissions statically.","conference":"IEEE","terms":"Smart phones;Seals;Androids;Humanoid robots;MySpace;Browsers;Java,Android (operating system),flow permissions;Android permission mechanism;semantic information;information flows","keywords":"","startPage":"652","endPage":"657","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693128","citationCount":17,"referenceCount":20,"year":2013,"authors":"S. Holavanalli; D. Manuel; V. Nanjundaswamy; B. Rosenberg; F. Shen; S. Y. Ko; L. Ziarek","affiliations":"University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA; University at Buffalo, The State University of New York, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b2"},"title":"Natural language requirements quality analysis based on business domain models","abstract":"Quality of requirements written in natural language has always been a critical concern in software engineering. Poorly written requirements lead to ambiguity and false interpretation in different phases of a software delivery project. Further, incomplete requirements lead to partial implementation of the desired system behavior. In this paper, we present a model for harvesting domain (functional or business) knowledge. Subsequently we present natural language processing and ontology based techniques for leveraging the model to analyze requirements quality and for requirements comprehension. The prototype also provides an advisory to business analysts so that the requirements can be aligned to the expected domain standard. The prototype developed is currently being used in practice, and the initial results are very encouraging.","conference":"IEEE","terms":"Business;Ontologies;Natural languages;Analytical models;Portals;OWL;Standards,business data processing;natural language processing;ontologies (artificial intelligence);software engineering;systems analysis,natural language requirements quality analysis;business domain models;software engineering;software delivery project;domain knowledge;natural language processing;ontology based techniques;requirements comprehension;business analysts;domain standard","keywords":"Requirements Engineering;Ontology;Natural Language Processing;Business Domain Modeling","startPage":"676","endPage":"681","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693132","citationCount":3,"referenceCount":21,"year":2013,"authors":"K. M. Annervaz; V. Kaulgud; S. Sengupta; M. Savagaonkar","affiliations":"Accenture Technology Labs, Bangalore, India; Accenture Technology Labs, Bangalore, India; Accenture Technology Labs, Bangalore, India; Accenture Technology Labs, Bangalore, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b3"},"title":"Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code","abstract":"Understanding the behavior of source code written in an unfamiliar programming language is difficult. One way to aid understanding of difficult code is to add corresponding pseudo-code, which describes in detail the workings of the code in a natural language such as English. In spite of its usefulness, most source code does not have corresponding pseudo-code because it is tedious to create. This paper demonstrates a tool Pseudogen that makes it possible to automatically generate pseudo-code from source code using statistical machine translation (SMT). Pseudogen currently supports generation of English or Japanese pseudo-code from Python source code, and the SMT framework makes it easy for users to create new generators for their preferred source code/pseudo-code pairs.","conference":"IEEE","terms":"Computer languages;Natural languages;Generators;Syntactics;Training;Programming;Arrays,language translation;program compilers;source code (software),Pseudogen tool;automatic pseudocode generation;programming language;English language;statistical machine translation;English pseudocode generation;Japanese pseudocode generation;Python source code;SMT framework","keywords":"machine translation;programming language;natural language","startPage":"824","endPage":"829","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372074","citationCount":2,"referenceCount":24,"year":2015,"authors":"H. Fudaba; Y. Oda; K. Akabe; G. Neubig; H. Hata; S. Sakti; T. Toda; S. Nakamura","affiliations":"Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Nara Inst. of Sci. \u0026 Technol., Nara, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b4"},"title":"Lightweight control-flow instrumentation and postmortem analysis in support of debugging","abstract":"Debugging is difficult and costly. As a human programmer looks for a bug, it would be helpful to see a complete trace of events leading to the point of failure. Unfortunately, full tracing is simply too slow to use in deployment, and may even be impractical during testing. We aid post-deployment debugging by giving programmers additional information about program activity shortly before failure. We use latent information in post-failure memory dumps, augmented by low-overhead, tunable run-time tracing. Our results with a realistically-tuned tracing scheme show low enough overhead (0-5%) to be used in production runs. We demonstrate several potential uses of this enhanced information, including a novel postmortem static slice restriction technique and a reduced view of potentially-executed code. Experimental evaluation shows our approach to be very effective, such as shrinking stack-sensitive interprocedural static slices by 49-78% in larger applications.","conference":"IEEE","terms":"Instruments;Core dumps;Debugging;Arrays;Computer crashes;Algorithm design and analysis;Production,program debugging;program slicing,lightweight control-flow instrumentation;postmortem analysis;post-deployment debugging;program activity;latent information;post-failure memory dumps;run-time tracing;realistically-tuned tracing scheme;postmortem static slice restriction technique;potentially-executed code;shrinking stack-sensitive interprocedural static slices","keywords":"","startPage":"378","endPage":"388","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693096","citationCount":8,"referenceCount":45,"year":2013,"authors":"P. Ohmann; B. Liblit","affiliations":"University of Wisconsin-Madison, USA; University of Wisconsin-Madison, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b5"},"title":"Evolutionary Robustness Testing of Data Processing Systems Using Models and Data Mutation (T)","abstract":"System level testing of industrial data processing software poses several challenges. Input data can be very large, even in the order of gigabytes, and with complex constraints that define when an input is valid. Generating the right input data to stress the system for robustness properties (e.g. to test how faulty data is handled) is hence very complex, tedious and error prone when done manually. Unfortunately, this is the current practice in industry. In previous work, we defined a methodology to model the structure and the constraints of input data by using UML class diagrams and OCL constraints. Tests were automatically derived to cover predefined fault types in a fault model. In this paper, to obtain more effective system level test cases, we developed a novel search-based test generation tool. Experiments on a real-world, large industrial data processing system show that our automated approach can not only achieve better code coverage, but also accomplishes this using significantly smaller test suites.","conference":"IEEE","terms":"Unified modeling language;Data models;Robustness;Testing;Data processing;Software;Search problems,data handling;program testing;search problems,evolutionary robustness testing;data mutation;search-based test generation tool;industrial data processing system;automated approach","keywords":"","startPage":"126","endPage":"137","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372002","citationCount":1,"referenceCount":42,"year":2015,"authors":"D. D. Nardo; F. Pastore; A. Arcuri; L. Briand","affiliations":"NA; Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg City, Luxembourg; Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg City, Luxembourg; Interdiscipl. Centre for Security, Univ. of Luxembourg, Luxembourg City, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b6"},"title":"BProVe: A formal verification framework for business process models","abstract":"Business Process Modelling has acquired increasing relevance in software development. Available notations, such as BPMN, permit to describe activities of complex organisations. On the one hand, this shortens the communication gap between domain experts and IT specialists. On the other hand, this permits to clarify the characteristics of software systems introduced to provide automatic support for such activities. Nevertheless, the lack of formal semantics hinders the automatic verification of relevant properties. This paper presents a novel verification framework for BPMN 2.0, called BProVe. It is based on an operational semantics, implemented using MAUDE, devised to make the verification general and effective. A complete tool chain, based on the Eclipse modelling environment, allows for rigorous modelling and analysis of Business Processes. The approach has been validated using more than one thousand models available on a publicly accessible repository. Besides showing the performance of BProVe, this validation demonstrates its practical benefits in identifying correctness issues in real models.","conference":"IEEE","terms":"Semantics;Analytical models;Logic gates;Business;Software systems;Tools;Collaboration,business data processing;formal specification;program verification,automatic verification;relevant properties;novel verification framework;BPMN 2;operational semantics;Eclipse modelling environment;rigorous modelling analysis;Business Processes;formal verification framework;business process models;Business Process Modelling;software development;available notations;domain experts;software systems;automatic support;formal semantics","keywords":"Business Processes;BPMN;Structural Operational Semantics;MAUDE;Software Verification","startPage":"217","endPage":"228","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115635","citationCount":2,"referenceCount":58,"year":2017,"authors":"F. Corradini; F. Fornari; A. Polini; B. Re; F. Tiezzi; A. Vandin","affiliations":"School of Science and Technology, University of Camerino, Camerino, Italy; School of Science and Technology, University of Camerino, Camerino, Italy; School of Science and Technology, University of Camerino, Camerino, Italy; School of Science and Technology, University of Camerino, Camerino, Italy; School of Science and Technology, University of Camerino, Camerino, Italy; DTU Compute, Technical University of Denmark, Lyngby, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b7"},"title":"Learning to Rank for Question-Oriented Software Text Retrieval (T)","abstract":"Question-oriented text retrieval, aka natural language-based text retrieval, has been widely used in software engineering. Earlier work has concluded that questions with the same keywords but different interrogatives (such as how, what) should result in different answers. But what is the difference? How to identify the right answers to a question? In this paper, we propose to investigate the \"answer style\" of software questions with different interrogatives. Towards this end, we build classifiers in a software text repository and propose a re-ranking approach to refine search results. The classifiers are trained by over 16,000 answers from the StackOverflow forum. Each answer is labeled accurately by its question's explicit or implicit interrogatives. We have evaluated the performance of our classifiers and the refinement of our re-ranking approach in software text retrieval. Our approach results in 13.1% and 12.6% respectively improvement with respect to text retrieval criteria nDCG@1 and nDCG@10 compared to the baseline. We also apply our approach to FAQs of 7 open source projects and show 13.2% improvement with respect to nDCG@1. The results of our experiments suggest that our approach could find answers to FAQs more precisely.","conference":"IEEE","terms":"Software;Feature extraction;Indexes;Software engineering;Buildings;Training;Search engines,information retrieval;learning (artificial intelligence);text analysis,learning-to-rank;question-oriented software text retrieval;natural language-based text retrieval;software engineering;software text repository;re-ranking approach;StackOverflow forum","keywords":"text retrieval;interrogative;classifier;rank","startPage":"1","endPage":"11","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371990","citationCount":10,"referenceCount":40,"year":2015,"authors":"Y. Zou; T. Ye; Y. Lu; J. Mylopoulos; L. Zhang","affiliations":"Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China; Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China; Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China; Dept. of Comput. Sci., Univ. of Toronto, Toronto, ON, Canada; Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b8"},"title":"TCA: An Efficient Two-Mode Meta-Heuristic Algorithm for Combinatorial Test Generation (T)","abstract":"Covering arrays (CAs) are often used as test suites for combinatorial interaction testing to discover interaction faults of real-world systems. Most real-world systems involve constraints, so improving algorithms for covering array generation (CAG) with constraints is beneficial. Two popular methods for constrained CAG are greedy construction and meta-heuristic search. Recently, a meta-heuristic framework called two-mode local search has shown great success in solving classic NPhard problems. We are interested whether this method is also powerful in solving the constrained CAG problem. This work proposes a two-mode meta-heuristic framework for constrained CAG efficiently and presents a new meta-heuristic algorithm called TCA. Experiments show that TCA significantly outperforms state-of-the-art solvers on 3-way constrained CAG. Further experiments demonstrate that TCA also performs much better than its competitors on 2-way constrained CAG.","conference":"IEEE","terms":"Software;Heuristic algorithms;Search problems;Testing;Algorithm design and analysis;Software algorithms;Computer science,program testing;search problems,TCA algorithm;two-mode metaheuristic algorithm;combinatorial test generation;covering array generation;CA;combinatorial interaction testing;CAG;greedy construction method;metaheuristic search method;NP-hard problems;two-mode local search","keywords":"","startPage":"494","endPage":"505","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372037","citationCount":13,"referenceCount":57,"year":2015,"authors":"J. Lin; C. Luo; S. Cai; K. Su; D. Hao; L. Zhang","affiliations":"Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China; Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China; State Key Lab. of Comput. Sci., Inst. of Software, Beijing, China; Dept. of Comput. Sci., Jinan Univ., Guangzhou, China; Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China; Sch. of Electron. Eng. \u0026 Comput. Sci., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37b9"},"title":"Model Checking Task Parallel Programs Using Gradual Permissions (N)","abstract":"Habanero is a task parallel programming model that provides correctness guarantees to the programmer. Even so, programs may contain data races that lead to non-determinism, which complicates debugging and verification. This paper presents a sound algorithm based on permission regions to prove data race and deadlock freedom in Habanero programs. Permission regions are user annotations to indicate the use of shared variables over spans of code. The verification algorithm restricts scheduling to permission region boundaries and isolation to reduce verification cost. The effectiveness of the algorithm is shown in benchmarks with an implementation in the Java Pathfinder (JPF) model checker. The implementation uses a verification specific library for Habanero that is tested using JPF for correctness. The results show significant reductions in cost, where cost is controlled with the size of the permission regions, at the risk of rejecting programs that are actually free of any data race or deadlock.","conference":"IEEE","terms":"System recovery;Schedules;Java;Synchronization;Model checking;Data models;Writing,formal verification;parallel programming,model checking;Habanero task parallel programming model;gradual permission;program debugging;program verification;permission region;verification algorithm;Java Pathfinder model checker;JPF model checker","keywords":"Data race;deadlock;model checking;task parallel languages;Habanero;Java Pathfinder","startPage":"535","endPage":"540","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372041","citationCount":2,"referenceCount":14,"year":2015,"authors":"E. G. Mercer; P. Anderson; N. Vrvilo; V. Sarkar","affiliations":"Brigham Young Univ., Provo, UT, USA; Brigham Young Univ., Provo, UT, USA; Rice Univ., Houston, TX, USA; Rice Univ., Houston, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ba"},"title":"A static analysis tool with optimizations for reachability determination","abstract":"To reduce the false positives of static analysis, many tools collect path constraints and integrate SMT solvers to filter unreachable execution paths. However, the accumulated calling and computing of SMT solvers are time and resource consuming. This paper presents TsmartLW, an alternate static analysis tool in which we implement a path constraint solving engine to speed up reachability determination. Within the engine, typical types of constraint-patterns are firstly defined based on an empirical study of a large number of code repositories. For each pattern, a constraint solving algorithm is designed and implemented. For each program, the engine predicts the most suitable strategy and then applies the strategy to solve path constraints. The experimental results on some well-known benchmarks and real-world applications show that TsmartLW is faster than some state-of-the-art static analysis tools. For example, it is 1.32× faster than CPAchecker and our engine is 369× faster than SMT solvers in solving path constraints. The demo video is available at https://www.youtube.com/watch?v=5c3ARhFclHA\u0026t=2s.","conference":"IEEE","terms":"Tools;Algorithm design and analysis;Engines;Benchmark testing;Visualization;Optimization,formal verification;program diagnostics;program testing;program verification,reachability determination;path constraint;SMT solvers;unreachable execution paths;alternate static analysis tool;constraint-patterns;constraint solving algorithm;TsmartLW","keywords":"Reachability determination;constraint pattern;path constraint solving","startPage":"925","endPage":"930","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115706","citationCount":2,"referenceCount":15,"year":2017,"authors":"Y. Wang; M. Zhou; Y. Jiang; X. Song; M. Gu; J. Sun","affiliations":"Key Laboratory for Information System Security, Ministry of Education, China; Key Laboratory for Information System Security, Ministry of Education, China; Key Laboratory for Information System Security, Ministry of Education, China; Electrical and Computer Engineering, Portland State University, USA; Key Laboratory for Information System Security, Ministry of Education, China; Key Laboratory for Information System Security, Ministry of Education, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37bb"},"title":"STARTS: STAtic regression test selection","abstract":"Regression testing is an important part of software development, but it can be very time consuming. Regression test selection (RTS) aims to speed up regression testing by running only impacted tests-the subset of tests that can change behavior due to code changes. We present STARTS, a tool for STAtic Regression Test Selection. Unlike dynamic RTS, STARTS requires no code instrumentation or runtime information to find impacted tests; instead, STARTS uses only compile-time information. Specifically, STARTS builds a dependency graph of program types and finds, as impacted, tests that can reach some changed type in the transitive closure of the dependency graph. STARTS is a Maven plugin that can be easily integrated into any Maven-based Java project. We find that STARTS selects on average 35.2% of tests, leading to an end-to-end runtime that is on average 81.0% of running all the tests. A video demo of STARTS can be found at https://youtu.be/PCNtk8jphrM.","conference":"IEEE","terms":"Tools;Testing;Prototypes;Software;Runtime;Java;Libraries,Java;program compilers;program diagnostics;program testing;regression analysis,program type dependency graph;dynamic RTS;static regression test selection;STARTS selects;code instrumentation","keywords":"","startPage":"949","endPage":"954","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115710","citationCount":5,"referenceCount":22,"year":2017,"authors":"O. Legunsen; A. Shi; D. Marinov","affiliations":"Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37bc"},"title":"Proving MCAPI executions are correct using SMT","abstract":"Asynchronous message passing is an important paradigm in writing applications for embedded heterogeneous multicore systems. The Multicore Association (MCA), an industry consortium promoting multicore technology, is working to standardize message passing into a single API, MCAPI, for bare metal implementation and portability across platforms. Correctness in such an API is difficult to reason about manually, and testing against reference solutions is equally difficult as reference solutions implement an unknown set of allowed behaviors, and programmers have no way to directly control API internals to expose or reproduce errors. This paper provides a way to encode an MCAPI execution as a Satisfiability Modulo Theories (SMT) problem, which if satisfiable, yields a feasible execution schedule on the same trace, such that it resolves non-determinism in the MCAPI runtime in a way that it now fails user provided assertions. The paper proves the problem is NP-complete. The encoding is useful for test, debug, and verification of MCAPI program execution. The novelty in the encoding is the direct use of match pairs (potential send and receive couplings). Match-pair encoding for MCAPI executions, when compared to other encoding strategies, is simpler to reason about, results in significantly fewer terms in the SMT problem, and captures feasible behaviors that are ignored in previously published techniques. Further, to our knowledge, this is the first SMT encoding that is able to run in infinite-buffer semantics, meaning the runtime has unlimited internal buffering as opposed to no internal buffering. Results demonstrate that the SMT encoding, restricted to zero-buffer semantics, uses fewer clauses when compared to another zero-buffer technique, and it runs faster and uses less memory. As a result the encoding scales well for programs with high levels of non-determinism in how sends and receives may potentially match.","conference":"IEEE","terms":"Encoding;Semantics;Schedules;Runtime;Message passing;Multicore processing;Complexity theory,application program interfaces;computability;embedded systems;message passing;multiprocessing systems;program verification,MCAPI executions;asynchronous message passing;embedded heterogeneous multicore systems;Multicore Association;industry consortium;multicore technology;single API;bare metal implementation;portability;reference solutions;API internals;satisfiability modulo theories problem;execution schedule;MCAPI runtime;NP-complete problem;MCAPI program execution testing;MCAPI program execution verification;match-pair encoding;encoding strategies;infinite-buffer semantics;unlimited internal buffering;SMT encoding;zero-buffer semantics;zero-buffer technique;MCAPI program execution debugging","keywords":"Abstraction;Refinement;SMT;Message Passing","startPage":"26","endPage":"36","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693063","citationCount":4,"referenceCount":29,"year":2013,"authors":"Y. Huang; E. Mercer; J. McCarthy","affiliations":"Department of Computer Science, Brigham Young University, Provo, UT, 84602, USA; Department of Computer Science, Brigham Young University, Provo, UT, 84602, USA; Department of Computer Science, Brigham Young University, Provo, UT, 84602, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37bd"},"title":"Contract-based program repair without the contracts","abstract":"Automated program repair (APR) is a promising approach to automatically fixing software bugs. Most APR techniques use tests to drive the repair process; this makes them readily applicable to realistic code bases, but also brings the risk of generating spurious repairs that overfit the available tests. Some techniques addressed the overfitting problem by targeting code using contracts (such as pre- and postconditions), which provide additional information helpful to characterize the states of correct and faulty computations; unfortunately, mainstream programming languages do not normally include contract annotations, which severely limits the applicability of such contract-based techniques. This paper presents JAID, a novel APR technique for Java programs, which is capable of constructing detailed state abstractions-similar to those employed by contract-based techniques-that are derived from regular Java code without any special annotations. Grounding the repair generation and validation processes on rich state abstractions mitigates the overfitting problem, and helps extend APR's applicability: in experiments with the DEFECTS4J benchmark, a prototype implementation of JAID produced genuinely correct repairs, equivalent to those written by programmers, for 25 bugs-improving over the state of the art of comparable Java APR techniques in the number and kinds of correct fixes.","conference":"IEEE","terms":"Maintenance engineering;Contracts;Java;Computer bugs;Monitoring;Tools;Indexes,contracts;formal specification;Java;program debugging;program diagnostics;software maintenance,overfitting problem;mainstream programming languages;contract annotations;Java programs;detailed state abstractions;regular Java code;repair generation;validation processes;comparable Java APR techniques;automated program repair;realistic code bases;spurious repairs;APR technique","keywords":"","startPage":"637","endPage":"647","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115674","citationCount":13,"referenceCount":37,"year":2017,"authors":"L. Chen; Y. Pei; C. A. Furia","affiliations":"Department of Computing, The Hong Kong Polytechnic University, China; Department of Computing, The Hong Kong Polytechnic University, China; Department of Computer Science and Engineering, Chalmers University of Technology, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37be"},"title":"Recovering model transformation traces using multi-objective optimization","abstract":"Model Driven Engineering (MDE) is based on a large set of models that are used and manipulated throughout the development cycle. These models are manually or automatically produced and/or exploited using model transformations. To allow engineers to maintain the models and track their changes, recovering transformation traces is essential. In this paper, we propose an automated approach, based on multi-objective optimization, to recover transformation traces between models. Our approach takes as input a source model in the form of a set of fragments (fragments are defined using the source meta-model cardinalities and OCL constraints), and a target model. The recovered transformation traces take the form of many-to-many mappings between the constructs of the two models.","conference":"IEEE","terms":"Unified modeling language;Sociology;Statistics;Optimization;Vectors;Context;Genetics,optimisation;program diagnostics;software engineering,model transformation trace recovery;multiobjective optimization;model driven engineering;MDE;development cycle;source meta-model cardinalities;OCL constraints;target model;many-to-many mappings","keywords":"","startPage":"688","endPage":"693","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693134","citationCount":7,"referenceCount":19,"year":2013,"authors":"H. Saada; M. Huchard; C. Nebut; H. Sahraoui","affiliations":"LIRMM, Université Montpellier 2 et CNRS, France; LIRMM, Université Montpellier 2 et CNRS, France; LIRMM, Université Montpellier 2 et CNRS, France; DIRO, Université de Montréal, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37bf"},"title":"Refactorings for Android Asynchronous Programming","abstract":"Running compute-intensive or blocking I/O operationsin the UI event thread of smartphone apps can severelydegrade responsiveness. Despite the fact that Android provides several async constructs that developers can use, developers can still miss opportunities to encapsulate long-running operations in async constructs. On the other hand, they can use the inappropriate async constructs, which result in memory leaks, lost results, and wasted energy. Fortunately, refactoring tools can eliminate these problems by retrofitting asynchrony to sequential code and transforming async code to use the appropriate constructs. This demo presents two refactoring tools for Android apps: (i) ASYNCHRONIZER, a refactoring tool that enables developers to extract long-running operations into Android AsyncTask. (ii) ASYNCDROID, a refactoring tool which enables developers to transform existing improperly-used AsyncTask into Android IntentService.","conference":"IEEE","terms":"Androids;Humanoid robots;Graphical user interfaces;Instruction sets;Message systems;Programming;Safety,Android (operating system);smart phones;software maintenance;software tools;user interfaces,Android asynchronous programming;compute-intensive operations;I/O operations;UI event thread;smartphone apps;long-running operation encapsulation;memory leaks;refactoring tools;sequential code;async code;asynchronizer;Android AsyncTask;ASYNCDROID;improperly-used AsyncTask;Android IntentService","keywords":"Refactoring;Android;Asynchronous","startPage":"836","endPage":"841","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372076","citationCount":3,"referenceCount":37,"year":2015,"authors":"Y. Lin; D. Dig","affiliations":"Comput. Sci. Dept., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Oregon State Univ., Corvallis, OR, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c0"},"title":"Model-Based Testing of Stateful APIs with Modbat","abstract":"Modbat makes testing easier by providing a user-friendly modeling language to describe the behavior of systems, from such a model, test cases are generated and executed. Modbat's domain-specific language is based on Scala, its features include probabilistic and non-deterministic transitions, component models with inheritance, and exceptions. We demonstrate the versatility of Modbat by finding a confirmed defect in the currently latest version of Java, and by testing SAT solvers.","conference":"IEEE","terms":"Java;Data models;Arrays;DSL;Testing;Libraries,application program interfaces;computability;Java;program testing;simulation languages,model-based testing;stateful API;Modbat;user-friendly modeling language;domain-specific language;Scala;nondeterministic transitions;probabilistic transitions;Java;SAT solvers","keywords":"model-based testing;software test tools;domain-specific language;extended finite-state machines;component-based systems;exception testing","startPage":"858","endPage":"863","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372080","citationCount":2,"referenceCount":31,"year":2015,"authors":"C. Artho; M. Seidl; Q. Gros; E. Choi; T. Kitamura; A. Mori; R. Ramler; Y. Yamagata","affiliations":"Nat. Inst. of Adv. Ind. Sci. \u0026 Technol., Amagasaki, Japan; Johannes Kepler Univ., Linz, Austria; Univ. of Nantes, Nantes, France; Nat. Inst. of Adv. Ind. Sci. \u0026 Technol., Amagasaki, Japan; Nat. Inst. of Adv. Ind. Sci. \u0026 Technol., Amagasaki, Japan; Nat. Inst. of Adv. Ind. Sci. \u0026 Technol., Amagasaki, Japan; Software Competence Center Hagenberg, Hagenberg, Austria; Software Competence Center Hagenberg, Hagenberg, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c1"},"title":"Dangling references in multi-configuration and dynamic PHP-based Web applications","abstract":"PHP is a dynamic language popularly used in Web development for writing server-side code to dynamically create multiple versions of client-side pages at run time for different configurations. A PHP program contains code to be executed or produced for multiple configurations/versions. That dynamism and multi-configuration nature leads to dangling references. Specifically, in the execution for a configuration, a reference to a variable or a call to a function is dangling if its corresponding declaration cannot be found. We conducted an exploratory study to confirm the existence of such dangling reference errors including dangling cross-language and embedded references in the client-side HTML/JavaScript code and in data-accessing SQL code that are embedded in scattered PHP code. Dangling references have caused run-time fatal failures and security vulnerabilities. We developed DRC, a static analysis method to detect such dangling references. DRC uses symbolic execution to collect PHP declarations/references and to approximate all versions of the generated output, and then extracts embedded declarations/references. It associates each detected declaration/reference with a conditional constraint that represents the execution paths (i.e. configurations/versions) containing that declaration/reference. It then validates references against declarations via a novel dangling reference detection algorithm. Our empirical evaluation shows that DRC detects dangling references with high accuracy. It revealed 83 yet undiscovered defects caused by dangling references.","conference":"IEEE","terms":"HTML;Thumb;Databases;Reactive power;Security;Servers;Detection algorithms,Internet;program diagnostics;SQL,dynamic PHP-based Web applications;dynamic language;Web development;writing server-side code;client-side pages;PHP program;multiple configurations;multiple versions;multiconfiguration nature;dangling cross-language;embedded references;HTML code;JavaScript code;data-accessing SQL code;scattered PHP code;run-time fatal failures;static analysis method;dangling references","keywords":"Dangling References;Web Code Analysis","startPage":"399","endPage":"409","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693098","citationCount":5,"referenceCount":34,"year":2013,"authors":"H. V. Nguyen; H. A. Nguyen; T. T. Nguyen; A. T. Nguyen; T. N. Nguyen","affiliations":"Electrical and Computer Engineering Department, Iowa State University, USA; Electrical and Computer Engineering Department, Iowa State University, USA; Electrical and Computer Engineering Department, Iowa State University, USA; Electrical and Computer Engineering Department, Iowa State University, USA; Electrical and Computer Engineering Department, Iowa State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c2"},"title":"Test Analysis: Searching for Faults in Tests (N)","abstract":"Tests are increasingly specified as programs. Expressing tests as code is advantageous in that developers are comfortable writing and running code, and tests can be automated and reused as the software evolves. Tests expressed as code, however, can also contain faults. Some test faults are similar to those found in application code, while others are more subtle, caused by incorrect implementation of testing concepts and processes. These faults may cause a test to fail when it should not, or allow program faults to go undetected. In this work we explore whether lightweight static analyses can be cost-effective in pinpointing patterns associated with faults tests. Our exploration includes a categorization and explanation of test patterns, and their application to 12 open source projects that include over 40K tests. We found that several patterns, detectable through simple and efficient static analyses of just the test code, can detect faults with a low false positive rate, while other patterns would require a more sophisticated and extensive code analysis to be useful.","conference":"IEEE","terms":"Software;Testing;Encoding;Syntactics;Dynamic scheduling;Fault diagnosis;Detectors,program diagnostics;program testing;software fault tolerance;source code (software),test analysis;test faults;application code;program faults;static analyses","keywords":"coding patterns;lightweight static analysis;test faults","startPage":"149","endPage":"154","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372004","citationCount":1,"referenceCount":21,"year":2015,"authors":"M. Waterloo; S. Person; S. Elbaum","affiliations":"Comput. Sci. \u0026 Eng. Dept., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Nebraska-Lincoln, Lincoln, NE, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Nebraska-Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c3"},"title":"PAD: Programming third-party web advertisement censorship","abstract":"In the current online advertisement delivery, an ad slot on a publisher's website may go through multiple layers of bidding and reselling until the final ad content is delivered. The publishers have little control on the ads being displayed on their web pages. As a result, website visitors may suffer from unwanted ads such as malvertising, intrusive ads, and information disclosure ads. Unfortunately, the visitors often blame the publisher for their unpleasant experience and switch to competitor websites. In this paper, we propose a novel programming support system for ad delivery, called PAD, for publisher programmers, who specify their policies on regulating third-party ads shown on their websites. PAD features an expressive specification language and a novel persistent policy enforcement runtime that can self-install and self-protect throughout the entire ad delegation chain. It also provides an ad-specific memory protection scheme that prevents malvertising by corrupting malicious payloads. Our experiments show that PAD has negligible runtime overhead. It effectively suppresses a set of malvertising cases and unwanted ad behaviors reported in the real world, without affecting normal functionalities and regular ads.","conference":"IEEE","terms":"Internet;Runtime;Browsers;Advertising;Trojan horses,advertising data processing;Internet;security of data;Web sites,current online advertisement delivery;bidding;unwanted ads;intrusive ads;information disclosure ads;unpleasant experience;programming support system;publisher programmers;third-party ads;expressive specification language;ad-specific memory protection scheme;negligible runtime overhead;malvertising cases;unwanted ad behaviors;Web pages;programming third-party Web advertisement censorship;Website visitors;competitor Websites;PAD;persistent policy enforcement runtime","keywords":"","startPage":"240","endPage":"251","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115637","citationCount":0,"referenceCount":64,"year":2017,"authors":"W. Wang; Y. Kwon; Y. Zheng; Y. Aafer; I. Kim; W. Lee; Y. Liu; W. Meng; X. Zhang; P. Eugster","affiliations":"Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; IBM T.J. Watson Research Center, Yorktown Height, New York, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c4"},"title":"Learning to share: Engineering adaptive decision-support for online social networks","abstract":"Some online social networks (OSNs) allow users to define friendship-groups as reusable shortcuts for sharing information with multiple contacts. Posting exclusively to a friendship-group gives some privacy control, while supporting communication with (and within) this group. However, recipients of such posts may want to reuse content for their own social advantage, and can bypass existing controls by copy-pasting into a new post; this cross-posting poses privacy risks. This paper presents a learning to share approach that enables the incorporation of more nuanced privacy controls into OSNs. Specifically, we propose a reusable, adaptive software architecture that uses rigorous runtime analysis to help OSN users to make informed decisions about suitable audiences for their posts. This is achieved by supporting dynamic formation of recipient-groups that benefit social interactions while reducing privacy risks. We exemplify the use of our approach in the context of Facebook.","conference":"IEEE","terms":"Privacy;Facebook;Monitoring;Sensitivity;Computational modeling;Adaptation models,data privacy;social networking (online),engineering adaptive decision-support;online social networks;friendship-group;reusable shortcuts;privacy control;social advantage;nuanced privacy controls;reusable software architecture;adaptive software architecture;OSN users;informed decisions;recipient-groups;social interactions;cross-posting pose privacy risks;Facebook","keywords":"","startPage":"280","endPage":"285","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115641","citationCount":0,"referenceCount":42,"year":2017,"authors":"Y. Rafiq; L. Dickens; A. Russo; A. K. Bandara; M. Yang; A. Stuart; M. Levine; G. Calikli; B. A. Price; B. Nuseibeh","affiliations":"Imperial College London UK; University College London, UK; Imperial College London UK; The Open University, UK; University of Southampton, UK; University of Exeter, UK; University of Exeter, UK; Chalmers \u0026 University of Gothenburg, Sweden; The Open University, UK; The Open University, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c5"},"title":"CodeExchange: Supporting Reformulation of Internet-Scale Code Queries in Context (T)","abstract":"Programming today regularly involves searching for source code online, whether through a general search engine such as Google or a specialized code search engine such as SearchCode, Ohloh, or GitHub. Searching typically is an iterative process, with develop-ers adjusting the keywords they use based on the results of the previous query. However, searching in this manner is not ideal, because just using keywords places limits on what developers can express as well as the overall interaction that is required. Based on the observation that the results from one query create a con-text in which a next is formulated, we present CodeExchange, a new code search engine that we developed to explicitly leverage this context to support fluid, expressive reformulation of queries. We motivate the need for CodeExchange, highlight its key design decisions and overall architecture, and evaluate its use in both a field deployment and a laboratory study.","conference":"IEEE","terms":"Search engines;Context;Google;Manuals;Java;Programming;Face,Internet;iterative methods;query processing;search engines;source code (software),CodeExchange;Internet-scale code queries;source code;Google;specialized code search engine;SearchCode;Ohloh;GitHub;iterative process;code search engine;field deployment","keywords":"Code search;query reformulation;context;interface;internet-scale","startPage":"24","endPage":"35","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371992","citationCount":10,"referenceCount":60,"year":2015,"authors":"L. Martie; T. D. LaToza; A. v. d. Hoek","affiliations":"Dept. of Inf. Irvine, Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf. Irvine, Univ. of California, Irvine, Irvine, CA, USA; Dept. of Inf. Irvine, Univ. of California, Irvine, Irvine, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c6"},"title":"TzuYu: Learning stateful typestates","abstract":"Behavioral models are useful for various software engineering tasks. They are, however, often missing in practice. Thus, specification mining was proposed to tackle this problem. Existing work either focuses on learning simple behavioral models such as finite-state automata, or relies on techniques (e.g., symbolic execution) to infer finite-state machines equipped with data states, referred to as stateful typestates. The former is often inadequate as finite-state automata lack expressiveness in capturing behaviors of data-rich programs, whereas the latter is often not scalable. In this work, we propose a fully automated approach to learn stateful typestates by extending the classic active learning process to generate transition guards (i.e., propositions on data states). The proposed approach has been implemented in a tool called TzuYu and evaluated against a number of Java classes. The evaluation results show that TzuYu is capable of learning correct stateful typestates more efficiently.","conference":"IEEE","terms":"Testing;Support vector machines;Concrete;Java;Educational institutions;Learning automata;Automata,data mining;formal specification;Java;learning (artificial intelligence),TzuYu;software engineering tasks;specification mining;behavioral model learning;data-rich programs;fully automated approach;stateful typestate learning;classic active learning process;transition guards;data state propositions;Java classes","keywords":"","startPage":"432","endPage":"442","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693101","citationCount":14,"referenceCount":37,"year":2013,"authors":"H. Xiao; J. Sun; Y. Liu; S. Lin; C. Sun","affiliations":"School of Computer Engineering, Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; Temasek Laboratories, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c7"},"title":"Region and Effect Inference for Safe Parallelism (T)","abstract":"In this paper, we present the first full regions-and-effects inference algorithm for explicitly parallel fork-join programs. We infer annotations inspired by Deterministic Parallel Java (DPJ) for a type-safe subset of C++. We chose the DPJ annotations because they give the strongest safety guarantees of any existing concurrency-checking approach we know of, static or dynamic, and it is also the most expressive static checking system we know of that gives strong safety guarantees. This expressiveness, however, makes manual annotation difficult and tedious, which motivates the need for automatic inference, but it also makes the inference problem very challenging: the code may use region polymorphism, imperative updates with complex aliasing, arbitrary recursion, hierarchical region specifications, and wildcard elements to describe potentially infinite sets of regions. We express the inference as a constraint satisfaction problem and develop, implement, and evaluate an algorithm for solving it. The region and effect annotations inferred by the algorithm constitute a checkable proof of safe parallelism, and it can be recorded both for documentation and for fast and modular safety checking.","conference":"IEEE","terms":"Inference algorithms;Parallel processing;Heuristic algorithms;Manuals;Yttrium;Safety;Java,C++ language;Java;parallel programming;program diagnostics,safe parallelism;region-and-effect inference algorithm;parallel fork-join program;deterministic parallel Java;type-safe subset;C++;concurrency-checking approach;static checking system;region polymorphism;complex aliasing;arbitrary recursion;hierarchical region specification;constraint satisfaction problem;modular safety checking","keywords":"Annotation Inference;Safe Parallelism;Static Analysis;Checkable Proof","startPage":"512","endPage":"523","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372039","citationCount":1,"referenceCount":49,"year":2015,"authors":"A. Tzannes; S. T. Heumann; L. Eloussi; M. Vakilian; V. S. Adve; M. Han","affiliations":"NA; NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c8"},"title":"Static Analysis of JavaScript Web Applications in the Wild via Practical DOM Modeling (T)","abstract":"We present SAFEWapp, an open-source static analysis framework for JavaScript web applications. It provides a faithful (partial) model of web application execution environments of various browsers, based on empirical data from the main web pages of the 9,465 most popular websites. A main feature of SAFEWapp is the configurability of DOM tree abstraction levels to allow users to adjust a trade-off between analysis performance and precision depending on their applications. We evaluate SAFEWapp on the 5 most popular JavaScript libraries and the main web pages of the 10 most popular websites in terms of analysis performance, precision, and modeling coverage. Additionally, as an application of SAFEWapp, we build a bug detector for JavaScript web applications that uses static analysis results from SAFEWapp. Our bug detector found previously undiscovered bugs including ones from wikipedia.org and amazon.com.","conference":"IEEE","terms":"Browsers;HTML;Analytical models;Web pages;Internet;Encyclopedias,Java;program debugging;program diagnostics;Web sites,javascript Web application;SAFEWapp;open-source static analysis;Web sites;DOM tree abstraction level;bug detector","keywords":"","startPage":"552","endPage":"562","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372043","citationCount":14,"referenceCount":29,"year":2015,"authors":"C. Park; S. Won; J. Jin; S. Ryu","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37c9"},"title":"BProVe: Tool support for business process verification","abstract":"This demo introduces BProVe, a tool supporting automated verification of Business Process models. BProVe analysis is based on a formal operational semantics defined for the BPMN 2.0 modelling language, and is provided as a freely accessible service that uses open standard formats as input data. Furthermore a plug-in for the Eclipse platform has been developed making available a tool chain supporting users in modelling and visualising, in a friendly manner, the results of the verification. Finally we have conducted a validation through more than one thousand models, showing the effectiveness of our verification tool in practice. (Demo video: https://youtu.be/iF5OM7vKtDA).","conference":"IEEE","terms":"Tools;Business;Atmospheric modeling;Load modeling;Semantics;Analytical models;Collaboration,business data processing;formal specification;program verification,business process verification;automated verification;formal operational semantics;BPMN 2;freely accessible service;open standard formats;Eclipse platform;BProVe tool support;business process models","keywords":"Business Processes;BPMN;Structural Operational Semantics;MAUDE;Software Verification","startPage":"937","endPage":"942","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115708","citationCount":1,"referenceCount":26,"year":2017,"authors":"F. Corradini; F. Fornari; A. Polini; B. Re; F. Tiezzi; A. Vandin","affiliations":"University of Camerino, Italy; University of Camerino, Italy; University of Camerino, Italy; University of Camerino, Italy; University of Camerino, Italy; DTU Compute, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ca"},"title":"ANDROFLEET: Testing WiFi peer-to-peer mobile apps in the large","abstract":"WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04.","conference":"IEEE","terms":"Wireless fidelity;Peer-to-peer computing;Testing;Mobile communication;Androids;Humanoid robots;Mobile handsets,mobile computing;peer-to-peer computing;program testing;wireless LAN,P2P testing;acceptance testing framework;An-drofleet;point-to-point interaction testing;WiFi peer-to-peer mobile application testing;black-box acceptance tests;Androfleet;alpha testing phase;mobile devices","keywords":"","startPage":"961","endPage":"966","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115712","citationCount":2,"referenceCount":38,"year":2017,"authors":"L. Meftah; M. Gomez; R. Rouvoy; I. Chrisment","affiliations":"Inria / University of Lille, France; Saarland University, Germany; University of Lille / Inria / Institut Universitaire de France, France; Telecom Nancy / Inria, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37cb"},"title":"PIEtrace: Platform independent executable trace","abstract":"To improve software dependability, a large number of software engineering tools have been developed over years. Many of them are difficult to apply in practice because their system and library requirements are incompatible with those of the subject software. We propose a technique called platform independent executable trace. Our technique traces and virtualizes a regular program execution that is platform dependent, and generates a stand-alone program called the trace program. Running the trace program re-generates the original execution. More importantly, trace program execution is completely independent of the underlying operating system and libraries such that it can be compiled and executed on arbitrary platforms. As such, it can be analyzed by a third party tool on a platform preferred by the tool. We have implemented the technique on x86 and sensor platforms. We show that buggy executions of 10 real-world Windows and sensor applications can be traced and virtualized, and later analyzed by existing Linux tools. We also demonstrate how the technique can be used in cross-platform malware analysis.","conference":"IEEE","terms":"Registers;Libraries;Malware;Linux;Virtualization;Operating systems,invasive software;Linux;software engineering,PIEtrace;platform independent executable trace;software dependability;software engineering tools;library requirements;subject software;program execution;sensor platforms;Linux tools;malware analysis","keywords":"","startPage":"48","endPage":"58","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693065","citationCount":2,"referenceCount":47,"year":2013,"authors":"Y. Kwon; X. Zhang; D. Xu","affiliations":"Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; Department of Computer Science, Purdue University, West Lafayette, Indiana, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37cc"},"title":"Leveraging syntax-related code for automated program repair","abstract":"We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java.","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;Syntactics;Semantics;Databases;Java;Fault diagnosis,computational linguistics;Java;program debugging;program diagnostics;program testing;software maintenance,automated program repair technique ssFix;code database;code chunk;syntactic code search;candidate code chunks;local faulty program;chunk components;translated candidate chunk;syntax-related code;target chunk;external code repository;unmatched components;matched components;Defects4J bug dataset","keywords":"Automated program repair;code search;code transfer","startPage":"660","endPage":"670","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115676","citationCount":14,"referenceCount":54,"year":2017,"authors":"Q. Xin; S. P. Reiss","affiliations":"Department of Computer Science, Brown University, Providence, RI, USA; Department of Computer Science, Brown University, Providence, RI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37cd"},"title":"Predicting relevance of change recommendations","abstract":"Software change recommendation seeks to suggest artifacts (e.g., files or methods) that are related to changes made by a developer, and thus identifies possible omissions or next steps. While one obvious challenge for recommender systems is to produce accurate recommendations, a complimentary challenge is to rank recommendations based on their relevance. In this paper, we address this challenge for recommendation systems that are based on evolutionary coupling. Such systems use targeted association-rule mining to identify relevant patterns in a software system's change history. Traditionally, this process involves ranking artifacts using interestingness measures such as confidence and support. However, these measures often fall short when used to assess recommendation relevance. We propose the use of random forest classification models to assess recommendation relevance. This approach improves on past use of various interestingness measures by learning from previous change recommendations. We empirically evaluate our approach on fourteen open source systems and two systems from our industry partners. Furthermore, we consider complimenting two mining algorithms: Co-Change and Tarmaq. The results find that random forest classification significantly outperforms previous approaches, receives lower Brier scores, and has superior trade-off between precision and recall. The results are consistent across software system and mining algorithm.","conference":"IEEE","terms":"Data mining;History;Couplings;Software algorithms;Software measurement;Software systems,data mining;learning (artificial intelligence);pattern classification;recommender systems;relevance feedback;software engineering,software system;association-rule mining;relevance prediction;open source systems;Co-Change mining algorithm;Tarmaq mining algorithm;Brier scores;precision value;recall value;random forest classification models;recommendation relevance;interestingness measures;recommender systems;artifacts;software change recommendation","keywords":"recommendation confidence;evolutionary coupling;targeted association rule mining;random forests","startPage":"694","endPage":"705","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115680","citationCount":2,"referenceCount":44,"year":2017,"authors":"T. Rolfsnes; L. Moonen; D. Binkley","affiliations":"Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Loyola University Maryland, Baltimore, Maryland, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37ce"},"title":"Smart Cloud Broker: Finding your home in the clouds","abstract":"As the rate of cloud computing adoption grows, so does the need for consumption assistance. Enterprises looking to migrate their IT systems to the cloud require assistance in identifying providers that offer resources with the most appropriate pricing and performance levels to match their specific business needs. In this paper, we present Smart Cloud Broker - a suite of software tools that allows cloud infrastructure consumers to evaluate and compare the performance of different Infrastructure as a Service (IaaS) offerings from competing cloud service providers, and consequently supports selection of the cloud configuration and provider with the specifications that best meet the user's requirements. Using Smart Cloud Broker, prospective cloud users can estimate the performance of the different cloud platforms by running live tests against representative benchmark applications under representative load conditions.","conference":"IEEE","terms":"Benchmark testing;Cloud computing;Servers;Pricing;Databases;Catalogs,cloud computing;program testing;software performance evaluation,Smart Cloud Broker;cloud computing;software tools;cloud infrastructure consumers;Infrastructure as a Service;IaaS;cloud service providers;cloud configuration;cloud platform performance","keywords":"","startPage":"698","endPage":"701","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693136","citationCount":5,"referenceCount":14,"year":2013,"authors":"M. Baruwal Chhetri; S. Chichin; Q. Bao Vo; R. Kowalczyk","affiliations":"Faculty of Information and Communication Technologies, Swinburrne University of Technology, Melbourne, Victoria 3122, Australia; Faculty of Information and Communication Technologies, Swinburrne University of Technology, Melbourne, Victoria 3122, Australia; Faculty of Information and Communication Technologies, Swinburrne University of Technology, Melbourne, Victoria 3122, Australia; Faculty of Information and Communication Technologies, Swinburrne University of Technology, Melbourne, Victoria 3122, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37cf"},"title":"Automated testing of cloud-based elastic systems with AUToCLES","abstract":"Cloud-based elastic computing systems dynamically change their resources allocation to provide consistent quality of service and minimal usage of resources in the face of workload fluctuations. As elastic systems are increasingly adopted to implement business critical functions in a cost-efficient way, their reliability is becoming a key concern for developers. Without proper testing, cloud-based systems might fail to provide the required functionalities with the expected service level and costs. Using system testing techniques, developers can expose problems that escaped the previous quality assurance activities and have a last chance to fix bugs before releasing the system in production. System testing of cloud-based systems accounts for a series of complex and time demanding activities, from the deployment and configuration of the elastic system, to the execution of synthetic clients, and the collection and persistence of execution data. Furthermore, clouds enable parallel executions of the same elastic system that can reduce the overall test execution time. However, manually managing the concurrent testing of multiple system instances might quickly overwhelm developers' capabilities, and automatic support for test generation, system test execution, and management of execution data is needed. In this demo we showcase AUToCLES, our tool for automatic testing of cloud-based elastic systems. Given specifications of the test suite and the system under test, AUToCLES implements testing as a service (TaaS): It automatically instantiates the SUT, configures the testing scaffoldings, and automatically executes test suites. If required, AUToCLES can generate new test inputs. Designers can inspect executions both during and after the tests.","conference":"IEEE","terms":"Elasticity;Monitoring;Standards;System testing;Cloud computing,cloud computing;program testing,automated testing;AUToCLES;cloud-based elastic computing system;resources allocation;quality of service;business critical function;system testing technique;quality assurance;concurrent testing;test generation;system test execution;testing as a service;TaaS;SUT","keywords":"","startPage":"714","endPage":"717","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693140","citationCount":10,"referenceCount":18,"year":2013,"authors":"A. Gambi; W. Hummer; S. Dustdar","affiliations":"University of Lugano, Switzerland; Vienna University of Technology, Austria; Vienna University of Technology, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37d0"},"title":"Investigating Program Behavior Using the Texada LTL Specifications Miner","abstract":"Temporal specifications, relating program events through time, are useful for tasks ranging from bug detection to program comprehension. Unfortunately, such specifications are often lacking from system descriptions, leading researchers to investigate methods for inferring these specifications from code, execution traces, code comments, and other artifacts. This paper describes Texada, a tool to dynamically mine temporal specifications in LTL from traces of program activity. We review Texada's key features and demonstrate how it can be used to investigate program behavior through two scenarios: validating an implementation that solves the dining philosophers problem and supporting comprehension of a stack implementation. We also detail Texada's other, more advanced, usage options. Texada is an open source tool: https://bitbucket.org/bestchai/texada.","conference":"IEEE","terms":"Runtime;Concurrent computing;Data structures;Java;Distance measurement,data mining;formal specification;program debugging;public domain software,program behavior;Texada LTL specification miner;program events;bug detection;program comprehension;execution traces;code comments;dynamic temporal specification mining;program activity;open source tool","keywords":"texada;specification mining;linear temporal logic;program comprehension","startPage":"870","endPage":"875","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372082","citationCount":1,"referenceCount":19,"year":2015,"authors":"C. Lemieux; I. Beschastnikh","affiliations":"Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada; Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37d1"},"title":"Fast and Precise Symbolic Analysis of Concurrency Bugs in Device Drivers (T)","abstract":"Concurrency errors, such as data races, make device drivers notoriously hard to develop and debug without automated tool support. We present Whoop, a new automated approach that statically analyzes drivers for data races. Whoop is empowered by symbolic pairwise lockset analysis, a novel analysis that can soundly detect all potential races in a driver. Our analysis avoids reasoning about thread interleavings and thus scales well. Exploiting the race-freedom guarantees provided by Whoop, we achieve a sound partial-order reduction that significantly accelerates Corral, an industrial-strength bug-finder for concurrent programs. Using the combination of Whoop and Corral, we analyzed 16 drivers from the Linux 4.0 kernel, achieving 1.5 -- 20× speedups over standalone Corral.","conference":"IEEE","terms":"Concurrent computing;Programming;Linux;Computer bugs;Kernel;Instruction sets;Context,concurrency (computers);device drivers;Linux;program debugging;symbol manipulation,data races;device drivers;WHOOP;automated approach;symbolic pairwise lockset analysis;statical analysis;data races all;thread interleavings;race-freedom guarantees;partial-order reduction;CORRAL;industrial-strength bug-finder;concurrent programs;concurrent errors;debugging;Linux 4.0 kernel;concurrency bugs","keywords":"","startPage":"166","endPage":"177","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372006","citationCount":9,"referenceCount":57,"year":2015,"authors":"P. Deligiannis; A. F. Donaldson; Z. Rakamaric","affiliations":"Dept. of Comput., Imperial Coll. London, London, UK; Dept. of Comput., Imperial Coll. London, London, UK; Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9124eee8435e8e7d37d2"},"title":"GRT: Program-Analysis-Guided Random Testing (T)","abstract":"We propose Guided Random Testing (GRT), which uses static and dynamic analysis to include information on program types, data, and dependencies in various stages of automated test generation. Static analysis extracts knowledge from the system under test. Test coverage is further improved through state fuzzing and continuous coverage analysis. We evaluated GRT on 32 real-world projects and found that GRT outperforms major peer techniques in terms of code coverage (by 13 %) and mutation score (by 9 %). On the four studied benchmarks of Defects4J, which contain 224 real faults, GRT also shows better fault detection capability than peer techniques, finding 147 faults (66 %). Furthermore, in an in-depth evaluation on the latest versions of ten popular real-world projects, GRT successfully detects over 20 unknown defects that were confirmed by developers.","conference":"IEEE","terms":"Testing;Software;Tin;Data mining;Computer bugs;Impurities;Frequency measurement,object-oriented programming;program testing,GRT;program analysis guided random testing;program types;automated test generation;static analysis;continuous coverage analysis;fault detection capability;peer techniques","keywords":"Automatic test generation;random testing;static analysis;dynamic analysis","startPage":"212","endPage":"223","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372010","citationCount":19,"referenceCount":66,"year":2015,"authors":"L. Ma; C. Artho; C. Zhang; H. Sato; J. Gmeiner; R. Ramler","affiliations":"Univ. of Tokyo, Tokyo, Japan; AIST / ITRI, Japan; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Tokyo, Tokyo, Japan; Software Competence Center Hagenberg, Hagenberg, Austria; Software Competence Center Hagenberg, Hagenberg, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d3"},"title":"Detecting information flow by mutating input data","abstract":"Analyzing information flow is central in assessing the security of applications. However, static and dynamic analyses of information flow are easily challenged by non-available or obscure code. We present a lightweight mutation-based analysis that systematically mutates dynamic values returned by sensitive sources to assess whether the mutation changes the values passed to sensitive sinks. If so, we found a flow between source and sink. In contrast to existing techniques, mutation-based flow analysis does not attempt to identify the specific path of the flow and is thus resilient to obfuscation. In its evaluation, our MUTAFLOW prototype for Android programs showed that mutation-based flow analysis is a lightweight yet effective complement to existing tools. Compared to the popular FlowDroid static analysis tool, MutaFlow requires less than 10% of source code lines but has similar accuracy; on 20 tested real-world apps, it is able to detect 75 flows that FlowDroid misses.","conference":"IEEE","terms":"Tools;Java;Instruments;Concrete;Prototypes;Smart phones,Android (operating system);data flow analysis;program diagnostics;security of data;source coding,static analyses;dynamic analyses;obscure code;lightweight mutation;sensitive sources;sensitive sinks;flow analysis;lightweight yet effective complement;source code lines;information flow analysis;FlowDroid static analysis tool;MUTAFLOW prototype;systematically mutate dynamic values","keywords":"","startPage":"263","endPage":"273","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115639","citationCount":2,"referenceCount":24,"year":2017,"authors":"B. Mathis; V. Avdiienko; E. O. Soremekun; M. Böhme; A. Zeller","affiliations":"Saarland University, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; Saarland University, Saarbrücken, Germany; National University of Singapore, Singapore; Saarland University, Saarbrücken, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d4"},"title":"SimplyDroid: Efficient event sequence simplification for android application","abstract":"To ensure the quality of Android applications, many automatic test case generation techniques have been proposed. Among them, the Monkey fuzz testing tool and its variants are simple, effective and widely applicable. However, one major drawback of those Monkey tools is that they often generate many events in a failure-inducing input trace, which makes the follow-up debugging activities hard to apply. It is desirable to simplify or reduce the input event sequence while triggering the same failure. In this paper, we propose an efficient event trace representation and the SimplyDroid tool with three hierarchical delta-debugging algorithms each operating on this trace representation to simplify crash traces. We have evaluated SimplyDroid on a suite of real-life Android applications with 92 crash traces. The empirical result shows that our new algorithms in SimplyDroid are both efficient and effective in reducing these event traces.","conference":"IEEE","terms":"Graphical user interfaces;Computer crashes;Tools;Debugging;Smart phones;Algorithm design and analysis;Androids,Android (operating system);program debugging;program testing;software tools,android application;automatic test case generation techniques;Monkey fuzz testing tool;Monkey tools;failure-inducing input trace;debugging activities;input event sequence;SimplyDroid tool;hierarchical delta-debugging algorithms;real-life Android applications;event sequence simplification;event trace representation;crash traces","keywords":"Test case reduction;delta debugging;event sequence reduction;Android","startPage":"297","endPage":"307","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115643","citationCount":2,"referenceCount":41,"year":2017,"authors":"B. Jiang; Y. Wu; T. Li; W. K. Chan","affiliations":"School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d5"},"title":"Efficient Data Model Verification with Many-Sorted Logic (T)","abstract":"Misuse or loss of web application data can have catastrophic consequences in today's Internet oriented world. Hence, verification of web application data models is of paramount importance. We have developed a framework for verification of web application data models via translation to First Order Logic (FOL), followed by automated theorem proving. Due to the undecidability of FOL, this automated approach does not always produce a conclusive answer. In this paper, we investigate the use of many-sorted logic in data model verification in order to improve the effectiveness of this approach. Many-sorted logic allows us to specify type information explicitly, thus lightening the burden of reasoning about type information during theorem proving. Our experiments demonstrate that using many-sorted logic improves the verification performance significantly, and completely eliminates inconclusive results in all cases over 7 real world web applications, down from an 17% inconclusive rate.","conference":"IEEE","terms":"Data models;Semantics;Complexity theory;Rails;Java;Libraries;Encoding,formal verification;Internet;theorem proving,many-sorted logic;Internet;Web application data model verification;first order logic;FOL;automated theorem proving","keywords":"Data Models;Verification;Logic;Many-Sorted Logic","startPage":"42","endPage":"52","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371994","citationCount":4,"referenceCount":41,"year":2015,"authors":"I. Bocic; T. Bultan","affiliations":"Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Dept. of Comput. Sci., Univ. of California, Santa Barbara, Santa Barbara, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d6"},"title":"Measuring the structural complexity of feature models","abstract":"The automated analysis of feature models (FM) is based on SAT, BDD, and CSP - known NP-complete problems. Therefore, the analysis could have an exponential worst-case execution time. However, for many practical relevant analysis cases, state-of-the-art (SOTA) analysis tools quite successfully master the problem of exponential worst-case execution time based on heuristics. So far, however, very little is known about the structure of FMs that cause the cases in which the execution time (hardness) for analyzing a given FM increases unpredictably for SOTA analysis tools. In this paper, we propose to use width measures from graph theory to characterize the structural complexity of FMs as a basis for an estimation of the hardness of analysis operations on FMs with SOTA analysis tools. We present an experiment that we use to analyze the reasonability of graph width measures as metric for the structural complexity of FMs and the hardness of FM analysis. Such a complexity metric can be used as a basis for a unified method to systematically improve SOTA analysis tools.","conference":"IEEE","terms":"Complexity theory;Frequency modulation;Encoding;Analytical models;Measurement;Data structures;Boolean functions,computational complexity;optimisation;software product lines,structural complexity metric;feature models;automated analysis;NP complete problems;exponential worst case execution time;practical relevant analysis;state-of-the-art analysis tools;SOTA analysis tools;heuristics;graph theory","keywords":"software product line;feature model;automated analysis;performance measurement","startPage":"454","endPage":"464","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693103","citationCount":10,"referenceCount":37,"year":2013,"authors":"R. Pohl; V. Stricker; K. Pohl","affiliations":"paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Gerlingstr. 16, 45127, Germany; paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Gerlingstr. 16, 45127, Germany; paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Gerlingstr. 16, 45127, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d7"},"title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)","abstract":"Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.","conference":"IEEE","terms":"Natural languages;Computer languages;Software engineering;Programming profession;Generators;Software,language translation;natural language processing;statistical analysis,source code;statistical machine translation;natural language;programming language;SMT framework;English pseudocode;Japanese pseudocode;Python statement","keywords":"Algorithms;Education;Statistical Approach","startPage":"574","endPage":"584","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372045","citationCount":31,"referenceCount":36,"year":2015,"authors":"Y. Oda; H. Fudaba; G. Neubig; H. Hata; S. Sakti; T. Toda; S. Nakamura","affiliations":"Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. \u0026 Technol., Nara, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d8"},"title":"TiQi: A natural language interface for querying software project data","abstract":"Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo.","conference":"IEEE","terms":"Structured Query Language;Natural languages;Hazards;Software;Tools;Distributed databases;Unified modeling language,distributed databases;natural language interfaces;program debugging;public domain software;query processing;software engineering;SQL;Unified Modeling Language,TiQi;natural language interface;software projects;design artifacts;source code;safety cases;release plans;bug reports;project intelligence;release planning;software analytics;project stakeholders;complex queries;software project data querying","keywords":"Natural Language Interface;Project Data;Query","startPage":"973","endPage":"977","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115714","citationCount":1,"referenceCount":20,"year":2017,"authors":"J. Lin; Y. Liu; J. Guo; J. Cleland-Huang; W. Goss; W. Liu; S. Lohar; N. Monaikul; A. Rasin","affiliations":"University of Notre Dame, South Bend, IN, USA; University of Notre Dame, South Bend, IN, USA; University of Notre Dame, South Bend, IN, USA; University of Notre Dame, South Bend, IN, USA; School of Computing, DePaul University, Chicago, USA; School of Computing, DePaul University, Chicago, USA; School of Computing, DePaul University, Chicago, USA; School of Computing, DePaul University, Chicago, USA; School of Computing, DePaul University, Chicago, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37d9"},"title":"Efficient parametric runtime verification with deterministic string rewriting","abstract":"Early efforts in runtime verification show that parametric regular and temporal logic specifications can be monitored efficiently. These approaches, however, have limited expressiveness: their specifications always reduce to monitors with finite state. More recent developments showed that parametric context-free properties can be efficiently monitored with overheads generally lower than 12-15%. While context-free grammars are more expressive than finite-state languages, they still do not allow every computable safety property. This paper presents a monitor synthesis algorithm for string rewriting systems (SRS). SRSs are well known to be Turing complete, allowing for the formal specification of any computable safety property. Earlier attempts at Turing complete monitoring have been relatively inefficient. This paper demonstrates that monitoring parametric SRSs is practical. The presented algorithm uses a modified version of Aho-Corasick string searching for quick pattern matching with an incremental rewriting approach that avoids reexamining parts of the string known to contain no redexes.","conference":"IEEE","terms":"Monitoring;Runtime;Pattern matching;Automata;Safety;Algorithm design and analysis;Java,context-free grammars;finite state machines;formal specification;formal verification;rewriting systems;temporal logic;Turing machines,parametric runtime verification;deterministic string rewriting;temporal logic specifications;parametric regular specifications;context-free grammars;finite-state languages;string rewriting systems;SRS;formal specification;Turing complete monitoring;Aho-Corasick string searching;quick pattern matching;incremental rewriting approach","keywords":"Runtime Verification;Monitoring;String Rewriting","startPage":"70","endPage":"80","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693067","citationCount":6,"referenceCount":30,"year":2013,"authors":"P. Meredith; G. Roşu","affiliations":"University of Illinois at Urbana-Champaign, United States of America; University of Illinois at Urbana-Champaign, United States of America","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37da"},"title":"Testing properties of dataflow program operators","abstract":"Dataflow programming languages, which represent programs as graphs of data streams and operators, are becoming increasingly popular and being used to create a wide array of commercial software applications. The dependability of programs written in these languages, as well as the systems used to compile and run these programs, hinges on the correctness of the semantic properties associated with operators. Unfortunately, these properties are often poorly defined, and frequently are not checked, and this can lead to a wide range of problems in the programs that use the operators. In this paper we present an approach for improving the dependability of dataflow programs by checking operators for necessary properties. Our approach is dynamic, and involves generating tests whose results are checked to determine whether specific properties hold or not. We present empirical data that shows that our approach is both effective and efficient at assessing the status of properties.","conference":"IEEE","terms":"Testing;Aggregates;Optimization;Semantics;Ports (Computers);System recovery;Program processors,data flow analysis;program testing,testing properties;dataflow program operators;dataflow programming languages;data streams;commercial software applications;semantic properties","keywords":"","startPage":"103","endPage":"113","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693071","citationCount":7,"referenceCount":26,"year":2013,"authors":"Z. Xu; M. Hirzel; G. Rothermel; K. Wu","affiliations":"University of Nebraska, Lincoln, USA; IBM Watson Research, Yorktown Heights, NY, USA; University of Nebraska, Lincoln, USA; IBM Watson Research, Yorktown Heights, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37db"},"title":"A language model for statements of software code","abstract":"Building language models for source code enables a large set of improvements on traditional software engineering tasks. One promising application is automatic code completion. State-of-the-art techniques capture code regularities at token level with lexical information. Such language models are more suitable for predicting short token sequences, but become less effective with respect to long statement level predictions. In this paper, we have proposed PCC to optimize the token-level based language modeling. Specifically, PCC introduced an intermediate representation (IR) for source code, which puts tokens into groups using lexeme and variable relative order. In this way, PCC is able to handle long token sequences, i.e., group sequences, to suggest a complete statement with the precise synthesizer. Further more, PCC employed a fuzzy matching technique which combined genetic and longest common subsequence algorithms to make the prediction more accurate. We have implemented a code completion plugin for Eclipse and evaluated it on open-source Java projects. The results have demonstrated the potential of PCC in generating precise long statement level predictions. In 30%-60% of the cases, it can correctly suggest the complete statement with only six candidates, and 40%-90% of the cases with ten candidates.","conference":"IEEE","terms":"Predictive models;Synthesizers;Java;Training data;Training;Software;Context modeling,fuzzy set theory;genetic algorithms;Java;pattern matching;public domain software;software engineering;source code (software),PCC;precise long statement level predictions;complete statement;software code;source code;automatic code completion;lexical information;short token sequences;token-level based language modeling;variable relative order;long token sequences;group sequences;fuzzy matching technique;genetic subsequence algorithms;longest common subsequence algorithms;code completion plugin;open-source Java projects","keywords":"Code Completion;Language Model;IR","startPage":"682","endPage":"687","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115678","citationCount":2,"referenceCount":15,"year":2017,"authors":"Y. Yang; Y. Jiang; M. Gu; J. Sun; J. Gao; H. Liu","affiliations":"School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37dc"},"title":"Recommending crowdsourced software developers in consideration of skill improvement","abstract":"Finding suitable developers for a given task is critical and challenging for successful crowdsourcing software development. In practice, the development skills will be improved as developers accomplish more development tasks. Prior studies on crowdsourcing developer recommendation do not consider the changing of skills, which can underestimate developers' skills to fulfill a task. In this work, we first conducted an empirical study of the performance of 74 developers on Topcoder. With a difficulty-weighted algorithm, we re-compute the scores of each developer by eliminating the effect of task difficulty from the performance. We find out that the skill improvement of Topcoder developers can be fitted well with the negative exponential learning curve model. Second, we design a skill prediction method based on the learning curve. Then we propose a skill improvement aware framework for recommending developers for software development with crowdsourcing.","conference":"IEEE","terms":"Software;Correlation;Reliability;Crowdsourcing;Algorithm design and analysis;Prediction algorithms,crowdsourcing;professional aspects;software engineering,development skills;development tasks;crowdsourcing developer recommendation;Topcoder developers;skill prediction method;skill improvement aware framework;crowdsourcing software development;negative exponential learning curve model","keywords":"Crowdsourcing;recommender systems;software development;Topcoder","startPage":"717","endPage":"722","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115682","citationCount":3,"referenceCount":13,"year":2017,"authors":"Z. Wang; H. Sun; Y. Fu; L. Ye","affiliations":"State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37dd"},"title":"CCmutator: A mutation generator for concurrency constructs in multithreaded C/C++ applications","abstract":"We introduce CCmutator, a mutation generation tool for multithreaded C/C++ programs written using POSIX threads and the recently standardized C++11 concurrency constructs. CCmutator is capable of performing partial mutations and generating higher order mutants, which allow for more focused and complex combinations of elementary mutation operators leading to higher quality mutants. We have implemented CCmutator based on the popular Clang/LLVM compiler framework, which allows CCmutator to be extremely scalable and robust in handling real-world C/C++ applications. CCmutator is also designed in such a way that all mutants of the same order can be generated in parallel, which allows the tool to be easily parallelized on commodity multicore hardware to improve performance.","conference":"IEEE","terms":"Concurrent computing;Computer bugs;Instruction sets;Synchronization;Java;Software testing,C++ language;multiprocessing programs;multi-threading;program compilers;program testing;software performance evaluation,multithreaded C++ applications;multithreaded C applications;CCmutator;mutation generation tool;POSIX threads;standardized C++11 concurrency constructs;partial mutations;higher order mutant generation;elementary mutation operators;Clang-LLVM compiler framework;commodity multicore hardware;performance improvement;software testing;software development process","keywords":"","startPage":"722","endPage":"725","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693142","citationCount":13,"referenceCount":21,"year":2013,"authors":"M. Kusano; Chao Wang","affiliations":"Virginia Tech, Blacksburg, 24061, USA; Virginia Tech, Blacksburg, 24061, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37de"},"title":"Generating Fixtures for JavaScript Unit Testing (T)","abstract":"In today's web applications, JavaScript code interacts with the Document Object Model (DOM) at runtime. This runtime interaction between JavaScript and the DOM is error-prone and challenging to test. In order to unit test a JavaScript function that has read/write DOM operations, a DOM instance has to be provided as a test fixture. This DOM fixture needs to be in the exact structure expected by the function under test. Otherwise, the test case can terminate prematurely due to a null exception. Generating these fixtures is challenging due to the dynamic nature of JavaScript and the hierarchical structure of the DOM. We present an automated technique, based on dynamic symbolic execution, which generates test fixtures for unit testing JavaScript functions. Our approach is implemented in a tool called ConFix. Our empirical evaluation shows that ConFix can effectively generate tests that cover DOM-dependent paths. We also find that ConFix yields considerably higher coverage compared to an existing JavaScript input generation technique.","conference":"IEEE","terms":"Testing;Runtime;HTML;Fixtures;Computer bugs;Concrete;Generators,data flow analysis;Internet;object-oriented programming,fixtures generation;JavaScript Unit Testing;Web applications;JavaScript code;document object model;runtime interaction;DOM fixture;null exception;dynamic symbolic execution;JavaScript functions;ConFix","keywords":"Test fixture;test generation;dynamic symbolic execution;concolic execution;DOM;JavaScript;web applications","startPage":"190","endPage":"200","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372008","citationCount":7,"referenceCount":41,"year":2015,"authors":"A. M. Fard; A. Mesbah; E. Wohlstadter","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37df"},"title":"Tracking the Software Quality of Android Applications Along Their Evolution (T)","abstract":"Mobile apps are becoming complex software systems that must be developed quickly and evolve continuously to fit new user requirements and execution contexts. However, addressing these requirements may result in poor design choices, also known as antipatterns, which may incidentally degrade software quality and performance. Thus, the automatic detection and tracking of antipatterns in this apps are important activities in order to ease both maintenance and evolution. Moreover, they guide developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in its infancy. In this paper, we analyze the evolution of mobile apps quality on 3, 568 versions of 106 popular Android applications downloaded from the Google Play Store. For this purpose, we use a tooled approach, called PAPRIKA, to identify 3 object-oriented and 4 Android-specific antipatterns from binaries of mobile apps, and to analyze their quality along evolutions.","conference":"IEEE","terms":"Androids;Humanoid robots;Mobile communication;Software quality;Java;Measurement,mobile computing;object-oriented methods;software maintenance;software quality,software quality;Android applications;mobile apps quality;Google Play Store;PAPRIKA;Android-specific antipatterns;object-oriented antipatterns;software evolution","keywords":"Android;antipattern;mobile app;software quality","startPage":"236","endPage":"247","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372012","citationCount":29,"referenceCount":56,"year":2015,"authors":"G. Hecht; O. Benomar; R. Rouvoy; N. Moha; L. Duchien","affiliations":"Univ. of Lille / Inria, Lille, France; Univ. du Quebec a Montreal, Montreal, QC, Canada; Univ. of Lille / Inria, Lille, France; Univ. du Quebec a Montreal, Montreal, QC, Canada; Univ. of Lille / Inria, Lille, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e0"},"title":"In-memory fuzzing for binary code similarity analysis","abstract":"Detecting similar functions in binary executables serves as a foundation for many binary code analysis and reuse tasks. By far, recognizing similar components in binary code remains a challenge. Existing research employs either static or dynamic approaches to capture program syntax or semantics-level features for comparison. However, there exist multiple design limitations in previous work, which result in relatively high cost, low accuracy and scalability, and thus severely impede their practical use. In this paper, we present a novel method that leverages in-memory fuzzing for binary code similarity analysis. Our prototype tool IMF-SIM applies in-memory fuzzing to launch analysis towards every function and collect traces of different kinds of program behaviors. The similarity score of two behavior traces is computed according to their longest common subsequence. To compare two functions, a feature vector is generated, whose elements are the similarity scores of the behavior trace-level comparisons. We train a machine learning model through labeled feature vectors; later, for a given feature vector by comparing two functions, the trained model gives a final score, representing the similarity score of the two functions. We evaluate IMF-SIM against binaries compiled by different compilers, optimizations, and commonly-used obfuscation methods, in total over one thousand binary executables. Our evaluation shows that IMF-SIM notably outperforms existing tools with higher accuracy and broader application scopes.","conference":"IEEE","terms":"Binary codes;Tools;Runtime;Indexes;Syntactics,feature extraction;fuzzy set theory;learning (artificial intelligence);program compilers,similar components;program syntax;binary code similarity analysis;in-memory fuzzing;similarity score;behavior trace-level comparisons;IMF-SIM;binary code analysis;similar functions","keywords":"In-memory fuzzing;code similarity;reverse engineering;taint analysis","startPage":"319","endPage":"330","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115645","citationCount":4,"referenceCount":65,"year":2017,"authors":"S. Wang; D. Wu","affiliations":"The Pennsylvania State University, University Park, PA 16802, USA; The Pennsylvania State University, University Park, PA 16802, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e1"},"title":"Interpolation Guided Compositional Verification (T)","abstract":"Model checking suffers from the state space explosion problem. Compositional verification techniques such as assume-guarantee reasoning (AGR) have been proposed to alleviate the problem. However, there are at least three challenges in applying AGR. Firstly, given a system M1 ? M2, how do we automatically construct and refine (in the presence of spurious counterexamples) an assumption A2, which must be an abstraction of M2? Previous approaches suggest to incrementally learn and modify the assumption through multiple invocations of a model checker, which could be often time consuming. Secondly, how do we keep the state space small when checking M1 ? A2 = f if multiple refinements of A2 are necessary? Lastly, in the presence of multiple parallel components, how do we partition the components? In this work, we propose interpolation-guided compositional verification. The idea is to tackle three challenges by using interpolations to generate and refine the abstraction of M2, to abstract M1 at the same time (so that the state space is reduced even if A2 is refined all the way to M2), and to find good partitions. Experimental results show that the proposed approach outperforms existing approaches consistently.","conference":"IEEE","terms":"Model checking;Interpolation;Reactive power;Radiation detectors;Cognition;Explosions;Computer security,formal verification;interpolation;parallel processing,interpolation guided compositional verification;model checking;state space explosion problem;compositional verification techniques;assume-guarantee reasoning;AGR;parallel components","keywords":"model checking;automatic compositional verification;satisfiability;interpolation","startPage":"65","endPage":"74","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371996","citationCount":3,"referenceCount":39,"year":2015,"authors":"S. Lin; J. Sun; T. K. Nguyen; Y. Liu; J. S. Dong","affiliations":"NA; NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e2"},"title":"Software analytics for incident management of online services: An experience report","abstract":"As online services become more and more popular, incident management has become a critical task that aims to minimize the service downtime and to ensure high quality of the provided services. In practice, incident management is conducted through analyzing a huge amount of monitoring data collected at runtime of a service. Such data-driven incident management faces several significant challenges such as the large data scale, complex problem space, and incomplete knowledge. To address these challenges, we carried out two-year software-analytics research where we designed a set of novel data-driven techniques and developed an industrial system called the Service Analysis Studio (SAS) targeting real scenarios in a large-scale online service of Microsoft. SAS has been deployed to worldwide product datacenters and widely used by on-call engineers for incident management. This paper shares our experience about using software analytics to solve engineers' pain points in incident management, the developed data-analysis techniques, and the lessons learned from the process of research development and technology transfer.","conference":"IEEE","terms":"Monitoring;Measurement;Software;Synthetic aperture sonar;Servers;Runtime;Radiation detectors,computer centres;data handling;Internet;program diagnostics;technology transfer,software analytics;online services;monitoring data;data-driven incident management;large data scale;software-analytics research;data-driven techniques;Service Analysis Studio;large-scale online service;Microsoft;worldwide product datacenters;data-analysis techniques;research development;technology transfer","keywords":"Online service;service incident diagnosis;incident management","startPage":"475","endPage":"485","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693105","citationCount":18,"referenceCount":27,"year":2013,"authors":"J. Lou; Q. Lin; R. Ding; Q. Fu; D. Zhang; T. Xie","affiliations":"Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; Microsoft Research Asia, Beijing, China; University of Illinois at Urbana-Champaign, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e3"},"title":"Experiences from Designing and Validating a Software Modernization Transformation (E)","abstract":"Software modernization often involves complex code transformations that convert legacy code to new architectures or platforms, while preserving the semantics of the original programs. We present the lessons learnt from an industrial software modernization project of considerable size. This includes collecting requirements for a code-to-model transformation, designing and implementing the transformation algorithm, and then validating correctness of this transformation for the code-base at hand. Our transformation is implemented in the TXL rewriting language and assumes specifically structured C++ code as input, which it translates to a declarative configuration model. The correctness criterion for the transformation is that the produced model admits the same configurations as the input code. The transformation converts C++ functions specifying around a thousand configuration parameters. We verify the correctness for each run individually, using translation validation and symbolic execution. The technique is formally specified and is applicable automatically for most of the code-base.","conference":"IEEE","terms":"Software;Semantics;Switches;Object oriented modeling;Complexity theory;Industries;Power electronics,C++ language;formal specification;rewriting systems;software maintenance,software modernization transformation;complex code transformation;legacy code;industrial software modernization project;code-to-model transformation;transformation algorithm;code-base;TXL rewriting language;C++ code;declarative configuration model;correctness criterion;input code;C++ function;configuration parameter;translation validation;symbolic execution;formal specification","keywords":"Experience Report;Functional Equivalence;Program Transformation;Symbolic Execution","startPage":"597","endPage":"607","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372047","citationCount":7,"referenceCount":30,"year":2015,"authors":"A. F. Iosif-Lazar; A. S. Al-Sibahi; A. S. Dimovski; J. E. Savolainen; K. Sierszecki; A. Wasowski","affiliations":"IT Univ. of Copenhagen, Copenhagen, Denmark; IT Univ. of Copenhagen, Copenhagen, Denmark; IT Univ. of Copenhagen, Copenhagen, Denmark; NA; NA; IT Univ. of Copenhagen, Copenhagen, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e4"},"title":"Detecting Broken Pointcuts Using Structural Commonality and Degree of Interest (N)","abstract":"Pointcut fragility is a well-documented problem in Aspect-Oriented Programming, changes to the base-code can lead to join points incorrectly falling in or out of the scope of pointcuts. Deciding which pointcuts have broken due to base-code changes is a daunting venture, especially in large and complex systems. We present an automated approach that recommends pointcuts that are likely to require modification due to a particular base-code change, as well as ones that do not. Our hypothesis is that join points selected by a pointcut exhibit common structural characteristics. Patterns describing such commonality are used to recommend pointcuts that have potentially broken to the developer. The approach is implemented as an extension to the popular Mylyn Eclipse IDE plug-in, which maintains focused contexts of entities relevant to the task at hand using a Degree of Interest (DOI) model.","conference":"IEEE","terms":"Context;Software;Programming;Complex systems;Java;Software engineering;Cities and towns,aspect-oriented programming;program debugging,broken pointcut detection;structural commonality;pointcut fragility;aspect-oriented programming;complex systems;Mylyn Eclipse IDE plug-in;degree of interest model;DOI model;structural characteristics","keywords":"Aspect-Oriented programming;software evolution","startPage":"641","endPage":"646","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372051","citationCount":1,"referenceCount":25,"year":2015,"authors":"R. Khatchadourian; A. Rashid; H. Masuhara; T. Watanabe","affiliations":"City Univ. of New York, New York, NY, USA; Lancaster Univ., Lancaster, UK; Tokyo Inst. of Technol., Tokyo, Japan; Edirium K.K., Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e5"},"title":"Defaultification refactoring: A tool for automatically converting Java methods to default","abstract":"Enabling interfaces to declare (instance) method implementations, Java 8 default methods can be used as a substitute for the ubiquitous skeletal implementation software design pattern. Performing this transformation on legacy software manually, though, may be non-trivial. The refactoring requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods. All of this is necessary to preserve type-correctness and confirm semantics preservation. We demonstrate an automated refactoring tool called MIGRATE Skeletal Implementation to Interface for transforming legacy Java code to use the new default construct. The tool, implemented as an Eclipse plug-in, is driven by an efficient, fully-automated, type constraint-based refactoring approach. It features an extensive rule set covering various corner-cases where default methods cannot be used. The resulting code is semantically equivalent to the original, more succinct, easier to comprehend, less complex, and exhibits increased modularity. A demonstration can be found at http://youtu.be/YZHIy0yePh8.","conference":"IEEE","terms":"Tools;Java;Semantics;Software;Computer architecture;Software engineering,Java;object-oriented programming;software maintenance,legacy Java code;type constraint;refactoring approach;defaultification refactoring;Java methods;method implementations;Java 8 default methods;ubiquitous skeletal implementation software design pattern;legacy software;complex type hierarchies;multiple implementation inheritance issues;dispatch precedence;semantics preservation;automated refactoring tool;MIGRATE Skeletal Implementation","keywords":"refactoring;java;interfaces;default methods;type constraints;eclipse","startPage":"984","endPage":"989","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115716","citationCount":3,"referenceCount":19,"year":2017,"authors":"R. Khatchadourian; H. Masuhara","affiliations":"City University of New York, USA; Tokyo Institute of Technology, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e6"},"title":"Identifying execution points for dynamic analyses","abstract":"Dynamic analyses rely on the ability to identify points within or across executions. In spite of this being a core task for dynamic analyses, new solutions are frequently developed without an awareness of existing solutions, their strengths, their weaknesses, or their caveats. This paper surveys the existing approaches for identifying execution points and examines their analytical and empirical properties that researchers and developers should be aware of when using them within an analysis. In addition, based on limitations in precision, correctness, and efficiency for techniques that identify corresponding execution points across multiple executions, we designed and implemented a new technique, Precise Execution Point IDs. This technique avoids correctness and precision issues in prior solutions, enabling analyses that use our approach to also produce more correct results. Empirical comparison with the surveyed techniques shows that our approach has 25% overhead on average, several times less than existing solutions.","conference":"IEEE","terms":"Context;Radiation detectors;Instruments;Runtime;Silicon carbide;Core dumps;Indexing,system monitoring,execution points identification;dynamic analysis;precision;precise execution point ID","keywords":"","startPage":"81","endPage":"91","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693069","citationCount":3,"referenceCount":39,"year":2013,"authors":"W. N. Sumner; X. Zhang","affiliations":"School of Computing Science, Simon Fraser University, Canada; Department of Computer Science, Purdue University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e7"},"title":"SABRINE: State-based robustness testing of operating systems","abstract":"The assessment of operating systems robustness with respect to unexpected or anomalous events is a fundamental requirement for mission-critical systems. Robustness can be tested by deliberately exposing the system to erroneous events during its execution, and then analyzing the OS behavior to evaluate its ability to gracefully handle these events. Since OSs are complex and stateful systems, robustness testing needs to account for the timing of erroneous events, in order to evaluate the robust behavior of the OS under different states. This paper presents SABRINE (StAte-Based Robustness testIng of operatiNg systEms), an approach for state-aware robustness testing of OSs. SABRINE automatically extracts state models from execution traces, and generates a set of test cases that cover different OS states. We evaluate the approach on a Linux-based Real-Time Operating System adopted in the avionic domain. Experimental results show that SABRINE can automatically identify relevant OS states, and find robustness vulnerabilities while keeping low the number of test cases.","conference":"IEEE","terms":"Robustness;Testing;Kernel;Probes;Hardware;Monitoring,Linux;operating system kernels;program diagnostics;program testing,SABRINE;operating systems robustness assessment;mission-critical systems;erroneous event timing;OS;state-based robustness testing of operating systems;execution traces;Linux-based real-time operating system;avionic domain;Linux kernel","keywords":"Robustness Testing;Fault Injection;Operating Systems;Linux kernel;Fault Tolerance;Dependability Benchmarking","startPage":"125","endPage":"135","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693073","citationCount":11,"referenceCount":47,"year":2013,"authors":"D. Cotroneo; D. Di Leo; F. Fucci; R. Natella","affiliations":"DIETI department, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Italy; DIETI department, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Italy; DIETI department, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Italy; DIETI department, Università degli Studi di Napoli Federico II, Via Claudio 21, 80125, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e8"},"title":"UNDEAD: Detecting and preventing deadlocks in production software","abstract":"Deadlocks are critical problems afflicting parallel applications, causing software to hang with no further progress. Existing detection tools suffer not only from significant recording performance overhead, but also from excessive memory and/or storage overhead. In addition, they may generate numerous false alarms. Subsequently, after problems have been reported, tremendous manual effort is required to confirm and fix these deadlocks. This paper designs a novel system, UnDead, that helps defeat deadlocks in production software. Different from existing detection tools, UnDead imposes negligible runtime performance overhead (less than 3 % on average) and small memory overhead (around 6%), without any storage consumption. After detection, UnDead automatically strengthens erroneous programs to prevent future occurrences of both existing and potential deadlocks, which is similar to the existing work-Dimmunix. However, UnDead exceeds Dimmunix with several orders of magnitude lower performance overhead, while eliminating numerous false positives. Extremely low runtime and memory overhead, convenience, and automatic prevention make UnDead an always-on detection tool, and a \"band-aid\" prevention system for production software.","conference":"IEEE","terms":"System recovery;Software;Production;Tools;Runtime;Concurrent computing;Computer bugs,concurrency control;production engineering computing;program debugging;program diagnostics,Dimmunix;performance overhead;false positives;runtime performance overhead;band-aid prevention system;automatic prevention;memory overhead;storage consumption;UnDead;storage overhead;production software;UNDEAD","keywords":"","startPage":"729","endPage":"740","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115684","citationCount":4,"referenceCount":42,"year":2017,"authors":"J. Zhou; S. Silvestro; H. Liu; Y. Cai; T. Liu","affiliations":"Department of Computer Science, University of Texas at San Antonio, USA; Department of Computer Science, University of Texas at San Antonio, USA; Department of Computer Science, University of Texas at San Antonio, USA; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, University of Texas at San Antonio, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37e9"},"title":"Pex4Fun: A web-based environment for educational gaming via automated test generation","abstract":"Pex4Fun (http://www.pex4fun.com/) is a web-based educational gaming environment for teaching and learning programming and software engineering. Pex4Fun can be used to teach and learn programming and software engineering at many levels, from high school all the way through graduate courses. With Pex4Fun, a student edits code in any browser - with Intellisense - and Pex4Fun executes it and analyzes it in the cloud. Pex4Fun connects teachers, curriculum authors, and students in a unique social experience, tracking and streaming progress updates in real time. In particular, Pex4Fun finds interesting and unexpected input values (with Pex, an advanced test-generation tool) that help students understand what their code is actually doing. The real fun starts with coding duels where a student writes code to implement a teacher's secret specification (in the form of sample-solution code not visible to the student). Pex4Fun finds any discrepancies in behavior between the student's code and the secret specification. Such discrepancies are given as feedback to the student to guide how to fix the student's code to match the behavior of the secret specification. This tool demonstration shows how Pex4Fun can be used in teaching and learning, such as solving coding duels, exploring course materials in feature courses, creating and teaching a course, creating and publishing coding duels, and learning advanced topics behind Pex4Fun.","conference":"IEEE","terms":"Encoding;Software engineering;Games;Programming profession;Education;Testing,cloud computing;computer aided instruction;computer games;computer science education;educational courses;programming;software engineering;teaching,Pex4Fun;automated test generation;Web-based educational gaming environment;programming teaching;software engineering teaching;high school;graduate courses;Intellisense;cloud;advanced test-generation tool;secret specification;coding duels;course materials","keywords":"","startPage":"730","endPage":"733","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693144","citationCount":6,"referenceCount":25,"year":2013,"authors":"N. Tillmann; J. de Halleux; T. Xie; J. Bishop","affiliations":"Microsoft Research, One Microsoft Way, Redmond, WA, USA; Microsoft Research, One Microsoft Way, Redmond, WA, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Microsoft Research, One Microsoft Way, Redmond, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37ea"},"title":"CodeHow: Effective Code Search Based on API Understanding and Extended Boolean Model (E)","abstract":"Over the years of software development, a vast amount of source code has been accumulated. Many code search tools were proposed to help programmers reuse previously-written code by performing free-text queries over a large-scale codebase. Our experience shows that the accuracy of these code search tools are often unsatisfactory. One major reason is that existing tools lack of query understanding ability. In this paper, we propose CodeHow, a code search technique that can recognize potential APIs a user query refers to. Having understood the potentially relevant APIs, CodeHow expands the query with the APIs and performs code retrieval by applying the Extended Boolean model, which considers the impact of both text similarity and potential APIs on code search. We deploy the backend of CodeHow as a Microsoft Azure service and implement the front-end as a Visual Studio extension. We evaluate CodeHow on a large-scale codebase consisting of 26K C# projects downloaded from GitHub. The experimental results show that when the top 1 results are inspected, CodeHow achieves a precision score of 0.794 (i.e., 79.4% of the first returned results are relevant code snippets). The results also show that CodeHow outperforms conventional code search tools. Furthermore, we perform a controlled experiment and a survey of Microsoft developers. The results confirm the usefulness and effectiveness of CodeHow in programming practices.","conference":"IEEE","terms":"Programming;Standards;Documentation;Software;Visualization;Indexes;Libraries,application program interfaces;Boolean algebra;query processing;text analysis,software development;free-text queries;large-scale codebase;CodeHow;code search technique;user query;code retrieval;extended Boolean model;text similarity;Microsoft Azure service;Visual Studio extension;C# projects;GitHub;precision score;Microsoft developers;programming practices;API","keywords":"code search;API understanding;Extended Boolean model;software reuse","startPage":"260","endPage":"270","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372014","citationCount":29,"referenceCount":43,"year":2015,"authors":"F. Lv; H. Zhang; J. Lou; S. Wang; D. Zhang; J. Zhao","affiliations":"Sch. of Software, Shanghai Jiao Tong Univ., Shanghai, China; Microsoft Res., Beijing, China; Microsoft Res., Beijing, China; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Microsoft Res., Beijing, China; Dept. of Comput. Sci. \u0026 Eng., Shanghai Jiao Tong Univ., Shanghai, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37eb"},"title":"Towards robust instruction-level trace alignment of binary code","abstract":"Program trace alignment is the process of establishing a correspondence between dynamic instruction instances in executions of two semantically similar but syntactically different programs. In this paper we present what is, to the best of our knowledge, the first method capable of aligning realistically long execution traces of real programs. To maximize generality, our method works entirely on the machine code level, i.e. it does not require access to source code. Moreover, the method is based entirely on dynamic analysis, which avoids the many challenges associated with static analysis of binary code, and which additionally makes our approach inherently resilient to e.g. static code obfuscation. Therefore, we believe that our trace alignment method could prove to be a useful aid in many program analysis tasks, such as debugging, reverse-engineering, investigating plagiarism, and malware analysis. We empirically evaluate our method on 11 popular Linux programs, and show that it is capable of producing meaningful alignments in the presence of various code transformations such as optimization or obfuscation, and that it easily scales to traces with tens of millions of instructions.","conference":"IEEE","terms":"Optimization;Time series analysis;Syntactics;Concrete;Malware;Semantics;Computer architecture,invasive software;Linux;program compilers;program debugging;program diagnostics,static code obfuscation;trace alignment method;program analysis tasks;malware analysis;code transformations;robust instruction-level trace alignment;binary code;program trace alignment;dynamic instruction instances;machine code level;source code;dynamic analysis;static analysis;Linux programs","keywords":"","startPage":"342","endPage":"352","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115647","citationCount":0,"referenceCount":25,"year":2017,"authors":"U. Kargén; N. Shahmehri","affiliations":"Department of Computer and Information Science, Linköping University, Linköping, Sweden; Department of Computer and Information Science, Linköping University, Linköping, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37ec"},"title":"General LTL Specification Mining (T)","abstract":"Temporal properties are useful for describing and reasoning about software behavior, but developers rarely write down temporal specifications of their systems. Prior work on inferring specifications developed tools to extract likely program specifications that fit particular kinds of tool-specific templates. This paper introduces Texada, a new temporal specification mining tool for extracting specifications in linear temporal logic (LTL) of arbitrary length and complexity. Texada takes a user-defined LTL property type template and a log of traces as input and outputs a set of instantiations of the property type (i.e., LTL formulas) that are true on the traces in the log. Texada also supports mining of almost invariants: properties with imperfect confidence. We formally describe Texada's algorithms and evaluate the tool's performance and utility.","conference":"IEEE","terms":"Semantics;Context;Data mining;Cognition;Software;Complexity theory;Software engineering,data mining;formal specification;temporal logic,LTL specification mining;temporal property;software behavior;program specification;tool-specific template;Texada;temporal specification mining tool;linear temporal logic;arbitrary length;user-defined LTL property type template;LTL formula","keywords":"specification mining;linear temporal logic;dynamic analysis","startPage":"81","endPage":"92","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371998","citationCount":32,"referenceCount":47,"year":2015,"authors":"C. Lemieux; D. Park; I. Beschastnikh","affiliations":"Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada; Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada; Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37ed"},"title":"RuntimeSearch: Ctrl+F for a running program","abstract":"Developers often try to find occurrences of a certain term in a software system. Traditionally, a text search is limited to static source code files. In this paper, we introduce a simple approach, RuntimeSearch, where the given term is searched in the values of all string expressions in a running program. When a match is found, the program is paused and its runtime properties can be explored with a traditional debugger. The feasibility and usefulness of RuntimeSearch is demonstrated on a medium-sized Java project.","conference":"IEEE","terms":"Runtime;Debugging;Java;Tools;Graphical user interfaces;Instruments,Java;program compilers;program debugging;program diagnostics,RuntimeSearch;ctrl+f;running program;software system;text search;static source code files;string expressions;traditional debugger;medium-sized Java project","keywords":"program comprehension;dynamic analysis;debugger;text search;concept location","startPage":"388","endPage":"393","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115651","citationCount":1,"referenceCount":27,"year":2017,"authors":"M. Sulír; J. Porubän","affiliations":"Technical University of Košice, Slovakia; Technical University of Košice, Slovakia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37ee"},"title":"Towards contextual and on-demand code clone management by continuous monitoring","abstract":"Effective clone management is essential for developers to recognize the introduction and evolution of code clones, to judge their impact on software quality, and to take appropriate measures if required. Our previous study shows that cloning practice is not simply a technical issue. It must be interpreted and considered in a larger context from technical, personal, and organizational perspectives. In this paper, we propose a contextual and on-demand code clone management approach called CCEvents (Code Cloning Events). Our approach provides timely notification about relevant code cloning events for different stakeholders through continuous monitoring of code repositories. It supports on-demand customization of clone monitoring strategies in specific technical, personal, and organizational contexts using a domain-specific language. We implemented the proposed approach and conducted an empirical study with an industrial project. The results confirm the requirements for contextual and on-demand code clone management and show the effectiveness of CCEvents in providing timely code cloning notifications and in helping to achieve effective clone management.","conference":"IEEE","terms":"Cloning;Monitoring;Context;Organizations;Outsourcing;Navigation;Detectors,software quality;source code (software),continuous monitoring;software quality;CCEvents;code cloning events;contextual and on-demand code clone management approach;code repositories continuous monitoring;clone monitoring strategies;domain-specific language;organizational contexts;code cloning notifications","keywords":"","startPage":"497","endPage":"507","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693107","citationCount":5,"referenceCount":30,"year":2013,"authors":"G. Zhang; X. Peng; Z. Xing; Shihai Jiang; Hai Wang; W. Zhao","affiliations":"School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37ef"},"title":"Model based test validation and oracles for data acquisition systems","abstract":"This paper presents an automated, model based test validation and oracle approach for systems with complex input and output structures, such as Data Acquisition (DAQ) systems, which are common in many sectors including the satellite communications industry. We present a customised modelling methodology for such systems and a tool that automatically validates test inputs and, after test execution, applies an oracle that is based on mappings between the input and output. We also apply our proposed approach and tool to a complex industrial DAQ system and investigate the scalability and effectiveness of the approach in validating test cases, the DAQ system, or its specifications (captured as models). The results of the case study show that the approach is indeed scalable with respect to two dimensions: (1) model size and (2) test validation and oracle execution time. The size of the model for the DAQ system under study remains within practical bounds, and far below that of typical system models, as it includes a class diagram with 68 classes and 49 constraints. The developed test validation and oracles tool can handle satellite transmission files up to two GB within practical time constraints, taking, on a standard PC, less than three minutes for test validation and less than 50 minutes for applying the oracle. The approach was also effective in automatically applying the oracle successfully for the actual test suite of the DAQ system, accurately identifying all issues and violations that were expected, thus showing that an approach based on models can be sufficiently accurate.","conference":"IEEE","terms":"Unified modeling language;Data acquisition;Data models;Testing;Complexity theory;Context;Context modeling,automatic testing;data acquisition;file organisation;program testing;program verification;Unified Modeling Language,automated model based test validation;complex input structures;complex output structures;data acquisition systems;satellite communication industry;customised modelling methodology;test execution;complex industrial DAQ system;oracle execution time;class diagram;satellite transmission file handling;time constraints;standard PC","keywords":"","startPage":"540","endPage":"550","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693111","citationCount":6,"referenceCount":32,"year":2013,"authors":"D. Di Nardo; N. Alshahwan; L. C. Briand; E. Fourneret; T. Nakić-Alfirević; V. Masquelier","affiliations":"Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; SES S.A., Betzdorf, Luxembourg; SES S.A., Betzdorf, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f0"},"title":"Access-Path Abstraction: Scaling Field-Sensitive Data-Flow Analysis with Unbounded Access Paths (T)","abstract":"Precise data-flow analyses frequently model field accesses through access paths with varying length. While using longer access paths increases precision, their size must be bounded to assure termination, and should anyway be small to enable a scalable analysis. We present Access-Path Abstraction, which for the first time combines efficiency with maximal precision. At control-flow merge points Access-Path Abstraction represents all those access paths that are rooted at the same base variable through this base variable only. The full access paths are reconstructed on demand where required. This makes it unnecessary to bound access paths to a fixed maximal length. Experiments with Stanford SecuriBench and the Java Class Library compare our open-source implementation against a field-based approach and against a field-sensitive approach that uses bounded access paths. The results show that the proposed approach scales as well as a field-based approach, whereas the approach using bounded access paths runs out of memory.","conference":"IEEE","terms":"Analytical models;Scalability;Explosions;Open source software;Context;Computational modeling;Target tracking,data flow analysis;Java;public domain software,access-path abstraction;field-sensitive data-flow analysis;unbounded access paths;field accesses;scalable analysis;control-flow merge points;Stanford SecuriBench;Java class library;open-source implementation;field-based approach","keywords":"static analysis;access path;field sensitive","startPage":"619","endPage":"629","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372049","citationCount":8,"referenceCount":20,"year":2015,"authors":"J. Lerch; J. Späth; E. Bodden; M. Mezini","affiliations":"Tech. Univ. Darmstadt, Darmstadt, Germany; Fraunhofer SIT, Lancaster, UK; Tech. Univ. Darmstadt, Darmstadt, Germany; Tech. Univ. Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f1"},"title":"Static Window Transition Graphs for Android (T)","abstract":"This work develops a static analysis to create a model of the behavior of an Android application's GUI. We propose the window transition graph (WTG), a model representing the possible GUI window sequences and their associated events and callbacks. A key component and contribution of our work is the careful modeling of the stack of currently-active windows, the changes to this stack, and the effects of callbacks related to these changes. To the best of our knowledge, this is the first detailed study of this important static analysis problem for Android. We develop novel analysis algorithms for WTG construction and traversal, based on this modeling of the window stack. We also describe an application of the WTG for GUI test generation, using path traversals. The evaluation of the proposed algorithms indicates their effectiveness and practicality.","conference":"IEEE","terms":"Androids;Humanoid robots;Graphical user interfaces;Analytical models;Smart phones;Pressing;Hardware,graph theory;graphical user interfaces;mobile computing;program diagnostics;smart phones,window transition graph;WTG;Android;static analysis;graphical user interface;GUI window sequence;window stack modelling","keywords":"Android;GUI;static analysis;control-flow analysis;testing","startPage":"658","endPage":"668","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372053","citationCount":24,"referenceCount":52,"year":2015,"authors":"S. Yang; H. Zhang; H. Wu; Y. Wang; D. Yan; A. Rountev","affiliations":"Ohio State Univ., Columbus, OH, USA; Ohio State Univ., Columbus, OH, USA; Ohio State Univ., Columbus, OH, USA; Ohio State Univ., Columbus, OH, USA; Google Inc., Mountain View, CA, USA; Ohio State Univ., Columbus, OH, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f2"},"title":"IntPTI: Automatic integer error repair with proper-type inference","abstract":"Integer errors in C/C++ are caused by arithmetic operations yielding results which are unrepresentable in certain type. They can lead to serious safety and security issues. Due to the complicated semantics of C/C++ integers, integer errors are widely harbored in real-world programs and it is error-prone to repair them even for experts. An automatic tool is desired to 1) automatically generate fixes which assist developers to correct the buggy code, and 2) provide sufficient hints to help developers review the generated fixes and better understand integer types in C/C++. In this paper, we present a tool IntPTI that implements the desired functionalities for C programs. IntPTI infers appropriate types for variables and expressions to eliminate representation issues, and then utilizes the derived types with fix patterns codified from the successful human-written patches. IntPTI provides a user-friendly web interface which allows users to review and manage the fixes. We evaluate IntPTI on 7 real-world projects and the results show its competitive repair accuracy and its scalability on large code bases. The demo video for IntPTI is available at: https://youtu.be/9Tgd4A_FgZM.","conference":"IEEE","terms":"Maintenance engineering;Tools;Semantics;Security;Scalability;Runtime;Computer bugs,human computer interaction;inference mechanisms;Internet;probability;program debugging;program diagnostics;security of data;user interfaces,complicated semantics;C/C++ integers;integer errors;automatic tool;generated fixes;tool IntPTI;desired functionalities;appropriate types;derived types;fix patterns;competitive repair accuracy;automatic integer error repair;proper-type inference;arithmetic operations;security issues;integer types","keywords":"integer error;type inference;fix pattern","startPage":"996","endPage":"1001","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115718","citationCount":1,"referenceCount":31,"year":2017,"authors":"X. Cheng; M. Zhou; X. Song; M. Gu; J. Sun","affiliations":"School of Software, TNLIST, KLISS, Tsinghua University, China; School of Software, TNLIST, KLISS, Tsinghua University, China; Electrical and Computer Engineering, Portland State University, USA; School of Software, TNLIST, KLISS, Tsinghua University, China; School of Software, TNLIST, KLISS, Tsinghua University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f3"},"title":"Managing software evolution through semantic history slicing","abstract":"Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history instances obtained from real-world open source software projects on GitHub.","conference":"IEEE","terms":"History;Semantics;Software;Computer bugs;Heuristic algorithms;Algorithm design and analysis,program slicing;program testing;public domain software;software maintenance;software reusability,semantic history slicing;incremental updates;software development process;sequential organization;semantic structure;program functionality;real-world open source software projects;software evolution management;software functionalities;software reuse;GitHub;software maintenance;software understanding","keywords":"software changes;version histories;program analysis;software reuse","startPage":"1014","endPage":"1017","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115722","citationCount":0,"referenceCount":24,"year":2017,"authors":"Y. Li","affiliations":"Department of Computer Science, University of Toronto, Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f4"},"title":"Ranger: Parallel analysis of alloy models by range partitioning","abstract":"We present a novel approach for parallel analysis of models written in Alloy, a declarative extension of first-order logic based on relations. The Alloy language is supported by the fully automatic Alloy Analyzer, which translates models into propositional formulas and uses off-the-shelf SAT technology to solve them. Our key insight is that the underlying constraint satisfaction problem can be split into subproblems of lesser complexity by using ranges of candidate solutions, which partition the space of all candidate solutions. Conceptually, we define a total ordering among the candidate solutions, split this space of candidates into ranges, and let independent SAT searches take place within these ranges' endpoints. Our tool, Ranger, embodies our insight. Experimental evaluation shows that Ranger provides substantial speedups (in several cases, superlinear ones) for a variety of hard-to-solve Alloy models, and that adding more hardware reduces analysis costs almost linearly.","conference":"IEEE","terms":"Metals;Analytical models;Partitioning algorithms;Computational modeling;Hardware;Vectors;Scalability,computability;constraint satisfaction problems;parallel processing;specification languages,parallel analysis;range partitioning;first-order logic;Alloy language models;propositional formulas;off-the-shelf SAT technology;constraint satisfaction problem;candidate solution space partitioning;range endpoints;Ranger tool;analysis cost reduction;Alloy Analyzer","keywords":"Static analysis;Alloy;Parallel analysis;SAT","startPage":"147","endPage":"157","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693075","citationCount":6,"referenceCount":40,"year":2013,"authors":"N. Rosner; J. H. Siddiqui; N. Aguirre; S. Khurshid; M. F. Frias","affiliations":"Department of Computer Science, FCEyN, UBA, Argentina; Department of Computer Science, LUMS School of Science and Engineering, Pakistan; Department of Computer Science, FCEFQyN, UNRC, Argentina; Department of Electrical and Computer Engineering, The University of Texas at Austin, USA; Department of Software Engineering, Instituto Tecnológico de Buenos Aires, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f5"},"title":"Parallel bug-finding in concurrent programs via reduced interleaving instances","abstract":"Concurrency poses a major challenge for program verification, but it can also offer an opportunity to scale when subproblems can be analysed in parallel. We exploit this opportunity here and use a parametrizable code-to-code translation to generate a set of simpler program instances, each capturing a reduced set of the original program's interleavings. These instances can then be checked independently in parallel. Our approach does not depend on the tool that is chosen for the final analysis, is compatible with weak memory models, and amplifies the effectiveness of existing tools, making them find bugs faster and with fewer resources. We use Lazy-CSeq as an off-the-shelf final verifier to demonstrate that our approach is able, already with a small number of cores, to find bugs in the hardest known concurrency benchmarks in a matter of minutes, whereas other dynamic and static tools fail to do so in hours.","conference":"IEEE","terms":"Computer bugs;Tools;Concurrent computing;Instruction sets;Programming;Model checking,concurrency control;multi-threading;program debugging;program verification,weak memory models;off-the-shelf final verifier;dynamic tools;static tools;parallel bug-finding;concurrent programs;reduced interleaving instances;program verification;parametrizable code-to-code translation;simpler program instances;capturing a reduced set;original program;concurrency benchmarks","keywords":"Verification;concurrency;sequentialization;swarm verification","startPage":"753","endPage":"764","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115686","citationCount":3,"referenceCount":50,"year":2017,"authors":"T. L. Nguyen; P. Schrammel; B. Fischer; S. L. Torre; G. Parlato","affiliations":"University of Southampton, UK; University of Sussex, UK; Stellenbosch University, South Africa; Universita degli Studi di Salerno, Italy; University of Southampton, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f6"},"title":"FiB: Squeezing loop invariants by interpolation between forward/backward predicate transformers","abstract":"Loop invariant generation is a fundamental problem in program analysis and verification. In this work, we propose a new approach to automatically constructing inductive loop invariants. The key idea is to aggressively squeeze an inductive invariant based on Craig interpolants between forward and backward reachability analysis. We have evaluated our approach by a set of loop benchmarks, and experimental results show that our approach is promising.","conference":"IEEE","terms":"Reachability analysis;Interpolation;Tools;Syntactics;Sun;Computer science;Benchmark testing,formal verification;interpolation;program control structures;program diagnostics;program verification;reachability analysis,loop benchmarks;FiB;invariant generation;fundamental problem;program analysis;inductive loop invariants;inductive invariant;Craig interpolants;reachability analysis","keywords":"","startPage":"793","endPage":"803","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115690","citationCount":1,"referenceCount":53,"year":2017,"authors":"S. Lin; J. Sun; H. Xiao; Y. Liu; D. Sanán; H. Hansen","affiliations":"School of Computer Science and Engineering, Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Tampere University of Technology, Finland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f7"},"title":"Crowd intelligence enhances automated mobile testing","abstract":"We show that information extracted from crowd-based testing can enhance automated mobile testing. We introduce Polariz, which generates replicable test scripts from crowd-based testing, extracting cross-app `motif' events: automatically-inferred reusable higher-level event sequences composed of lower-level observed event actions. Our empirical study used 434 crowd workers from Mechanical Turk to perform 1,350 testing tasks on 9 popular Google Play apps, each with at least 1 million user installs. The findings reveal that the crowd was able to achieve 60.5% unique activity coverage and proved to be complementary to automated search-based testing in 5 out of the 9 subjects studied. Our leave-one-out evaluation demonstrates that coverage attainment can be improved (6 out of 9 cases, with no disimprovement on the remaining 3) by combining crowd-based and search-based testing.","conference":"IEEE","terms":"Testing;Mobile communication;Tools;Data mining;Androids;Humanoid robots;Mobile handsets,automatic testing;mobile computing;program testing,replicable test scripts;cross-app motif events;reusable higher-level event sequences;lower-level observed event actions;crowd workers;automated search;testing tasks;crowd intelligence;automated mobile testing;Google Play apps;crowd-based testing;search-based testing","keywords":"Crowdsourced Software Engineering;Mobile App Testing;Test Generation","startPage":"16","endPage":"26","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115614","citationCount":9,"referenceCount":42,"year":2017,"authors":"K. Mao; M. Harman; Y. Jia","affiliations":"Facebook London, Facebook, 10 Brock Street, London, NW1 3FG, UK CREST, University College London, Malet Place, London, WC1E 6BT, UK; Facebook London, Facebook, 10 Brock Street, London, NW1 3FG, UK CREST, University College London, Malet Place, London, WC1E 6BT, UK; Facebook London, Facebook, 10 Brock Street, London, NW1 3FG, UK CREST, University College London, Malet Place, London, WC1E 6BT, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f8"},"title":"Supporting bug investigation using history analysis","abstract":"In my research, I propose an automated technique to support bug investigation by using a novel analysis of the history of the source code. During the bug-fixing process, developers spend a high amount of manual effort investigating the bug in order to answer a series of questions about it. My research will support developers in answering the following questions about a bug: Who is the most suitable developer to fix the bug?, Where is the bug located?, When was the bug inserted? and Why was the bug inserted?","conference":"IEEE","terms":"History;Visualization;Software;Software engineering;Computer bugs;Debugging;Conferences,program debugging;program diagnostics,bug investigation support;history analysis;automated technique;source code;bug-fixing process","keywords":"","startPage":"754","endPage":"757","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693150","citationCount":1,"referenceCount":25,"year":2013,"authors":"F. Servant","affiliations":"University of California, Irvine, U.S.A.","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37f9"},"title":"Tracking and Analyzing Cross-Cutting Activities in Developers' Daily Work (N)","abstract":"Developers use many software applications to process large amounts of diverse information in their daily work. The information is usually meaningful beyond the context of an application that manages it. However, as different applications function independently, developers have to manually track, correlate and re-find cross-cutting information across separate applications. We refer to this difficulty as information fragmentation problem. In this paper, we present ActivitySpace, an interapplication activity tracking and analysis framework for tackling information fragmentation problem in software development. ActivitySpace can monitor the developer's activity in many applications at a low enough level to obviate application-specific support while accounting for the ways by which low-level activity information can be effectively aggregated to reflect the developer's activity at higher-level of abstraction. A system prototype has been implemented on Microsoft Windows. Our preliminary user study showed that the ActivitySpace system is promising in supporting interapplication information needs in developers' daily work.","conference":"IEEE","terms":"Software;Monitoring;History;Java;Mice;Databases;Context,software development management,software application;information fragmentation problem;ActivitySpace;interapplication activity tracking;software development;cross-cutting activities","keywords":"","startPage":"277","endPage":"282","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372016","citationCount":8,"referenceCount":22,"year":2015,"authors":"L. Bao; Z. Xing; X. Wang; B. Zhou","affiliations":"Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37fa"},"title":"Fixing Recurring Crash Bugs via Analyzing Q\u0026A Sites (T)","abstract":"Recurring bugs are common in software systems, especially in client programs that depend on the same framework. Existing research uses human-written templates, and is limited to certain types of bugs. In this paper, we propose a fully automatic approach to fixing recurring crash bugs via analyzing Q\u0026A sites. By extracting queries from crash traces and retrieving a list of Q\u0026A pages, we analyze the pages and generate edit scripts. Then we apply these scripts to target source code and filter out the incorrect patches. The empirical results show that our approach is accurate in fixing real-world crash bugs, and can complement existing bug-fixing approaches.","conference":"IEEE","terms":"Computer bugs;Web pages;Search engines;Registers;Context;Software,program debugging;query processing;source code (software),recurring crash bugs;Q\u0026A sites;software systems;human-written templates;query extraction;crash traces;edit scripts;source code;real-world crash bugs;bug-fixing approaches","keywords":"","startPage":"307","endPage":"318","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372020","citationCount":27,"referenceCount":58,"year":2015,"authors":"Q. Gao; H. Zhang; J. Wang; Y. Xiong; L. Zhang; H. Mei","affiliations":"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37fb"},"title":"Comprehensive failure characterization","abstract":"There is often more than one way to trigger a fault. Standard static and dynamic approaches focus on exhibiting a single witness for a failing execution. In this paper, we study the problem of computing a comprehensive characterization which safely bounds all failing program behavior while exhibiting a diversity of witnesses for those failures. This information can be used to facilitate software engineering tasks ranging from fault localization and repair to quantitative program analysis for reliability. Our approach combines the results of overapproximating and underapproximating static analyses in an alternating iterative framework to produce upper and lower bounds on the failing input space of a program, which we call a comprehensive failure characterization (CFC). We evaluated a prototype implementation of this alternating framework on a set of 168 C programs from the SV-COMP benchmarks, and the data indicate that it is possible to efficiently, accurately, and safely characterize failure spaces.","conference":"IEEE","terms":"Upper bound;Tools;Maintenance engineering;Software;Manuals;Standards,iterative methods;program diagnostics;program verification;software engineering,comprehensive failure characterization;dynamic approaches focus;comprehensive characterization;program behavior;software engineering tasks;fault localization;quantitative program analysis;overapproximating analyses;underapproximating static analyses;alternating iterative framework;upper bounds;lower bounds","keywords":"","startPage":"365","endPage":"376","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115649","citationCount":1,"referenceCount":52,"year":2017,"authors":"M. J. Gerrard; M. B. Dwyer","affiliations":"University of Nebraska-Lincoln, Lincoln, NE, USA; University of Nebraska-Lincoln, Lincoln, NE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37fc"},"title":"Exploring regular expression comprehension","abstract":"The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluate the understandability of various regex language features. We further analyze regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [\\d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics.","conference":"IEEE","terms":"Tools;Standards;Measurement;Pattern matching;Automata;Syntactics;Concrete,formal languages;software maintenance;software metrics,regular expression comprehension;regex language features;regex expression representations;nonsmelly regex representations;software engineering tasks;regexes;smelly regex representations;community standards;understandability metrics","keywords":"Regular expression comprehension;equivalence class;regex representations","startPage":"405","endPage":"416","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115653","citationCount":8,"referenceCount":43,"year":2017,"authors":"C. Chapman; P. Wang; K. T. Stolee","affiliations":"Sandia National Laboratories, Albuquerque, NM, USA; Department of Computer Science, North Carolina State University, USA; Department of Computer Science, North Carolina State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37fd"},"title":"Automated unit testing of large industrial embedded software using concolic testing","abstract":"Current testing practice in industry is often ineffective and slow to detect bugs, since most projects utilize manually generated test cases. Concolic testing alleviates this problem by automatically generating test cases that achieve high coverage. However, specialized execution platforms and resource constraints of embedded software hinder application of concolic testing to embedded software. To overcome these limitations, we have developed CONcrete and symBOLic (CONBOL) testing framework to unit test large size industrial embedded software automatically. To address the aforementioned limitations, CONBOL tests target units on a host PC platform by generating symbolic unit testing drivers/stubs automatically and applying heuristics to reduce false alarms caused by the imprecise drivers/stubs. We have applied CONBOL to four million lines long industrial embedded software and detected 24 new crash bugs. Furthermore, the development team of the target software adopted CONBOL to their development process to apply CONBOL to the revised target software regularly.","conference":"IEEE","terms":"Testing;Computer bugs;Arrays;Embedded software;Hardware,embedded systems;program testing,automated unit testing;large size industrial embedded software;concolic testing;crash bugs detection;specialized execution platforms;embedded software resource constraints;concrete and symbolic testing framework;CONBOL testing framework;host PC platform;symbolic unit testing drivers;false alarm reduction;imprecise driver-stubs;heuristics","keywords":"","startPage":"519","endPage":"528","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693109","citationCount":18,"referenceCount":36,"year":2013,"authors":"Y. Kim; Y. Kim; Taeksu Kim; Gunwoo Lee; Y. Jang; M. Kim","affiliations":"CS Dept. KAIST, South Korea; Samsung Electronics, South Korea; Samsung Electronics, South Korea; Samsung Electronics, South Korea; Samsung Electronics, South Korea; CS Dept. KAIST, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37fe"},"title":"AutoComment: Mining question and answer sites for automatic comment generation","abstract":"Code comments improve software maintainability. To address the comment scarcity issue, we propose a new automatic comment generation approach, which mines comments from a large programming Question and Answer (Q\u0026A) site. Q\u0026A sites allow programmers to post questions and receive solutions, which contain code segments together with their descriptions, referred to as code-description mappings.We develop AutoComment to extract such mappings, and leverage them to generate description comments automatically for similar code segments matched in open-source projects. We apply AutoComment to analyze Java and Android tagged Q\u0026A posts to extract 132,767 code-description mappings, which help AutoComment to generate 102 comments automatically for 23 Java and Android projects. The user study results show that the majority of the participants consider the generated comments accurate, adequate, concise, and useful in helping them understand the code.","conference":"IEEE","terms":"Java;Software;Androids;Humanoid robots;Databases;Cloning;Natural language processing,Android (operating system);data mining;Java;public domain software;question answering (information retrieval);software maintenance,AutoComment;programming question-and-answer site mining;automatic comment generation;code comments;software maintainability improvement;comment scarcity issue;code segments;open-source projects;Java tagged Q\u0026A post analysis;Android tagged Q\u0026A post analysis;code-description mapping extraction;program comprehension;natural language processing;software engineering","keywords":"automated comment generation;documentation;program comprehension;natural language processing for software engineering","startPage":"562","endPage":"567","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693113","citationCount":61,"referenceCount":23,"year":2013,"authors":"E. Wong; Jinqiu Yang; Lin Tan","affiliations":"University of Waterloo, Ontario, Canada; University of Waterloo, Ontario, Canada; University of Waterloo, Ontario, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d37ff"},"title":"String Analysis of Android Applications (N)","abstract":"The desire to understand mobile applications has resulted in researchers adapting classical static analysis techniques to the mobile domain. Examination of data and control flows in Android apps is now a common practice to classify them. Important to these analyses is a fine-grained examination and understanding of strings, since in Android they are heavily used in intents, URLs, reflection, and content providers. Rigorous analysis of string creation, usage, and value characteristics offers additional information to increase precision of app classification. This paper shows that inter-procedural static analysis that specifically targets string construction and usage can be used to reveal valuable insights for classifying Android apps. To this end, we first present case studies to illustrate typical uses of strings in Android apps. We then present the results of our analysis on real-world malicious and benign apps. Our analysis examines how strings are created and used for URL objects, Java reflection, and Android intents, and infers the actual string values used as much as possible. Our results demonstrate that string disambiguation based on creation, usage, and value indeed provides additional information that may be used to improve precision of classifying application behaviors.","conference":"IEEE","terms":"Androids;Humanoid robots;Smart phones;Uniform resource locators;Java;Malware;Servers,Android (operating system);Java,string analysis;Android applications;fine-grained examination;URL objects;Java reflection;Android intents;string disambiguation","keywords":"","startPage":"680","endPage":"685","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372055","citationCount":1,"referenceCount":23,"year":2015,"authors":"J. D. Vecchio; F. Shen; K. M. Yee; B. Wang; S. Y. Ko; L. Ziarek","affiliations":"NA; SUNY - Univ. at Buffalo, Buffalo, NY, USA; SUNY - Univ. at Buffalo, Buffalo, NY, USA; SUNY - Univ. at Buffalo, Buffalo, NY, USA; SUNY - Univ. at Buffalo, Buffalo, NY, USA; SUNY - Univ. at Buffalo, Buffalo, NY, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3800"},"title":"Towards a software vulnerability prediction model using traceable code patterns and software metrics","abstract":"Software security is an important aspect of ensuring software quality. The goal of this study is to help developers evaluate software security using traceable patterns and software metrics during development. The concept of traceable patterns is similar to design patterns but they can be automatically recognized and extracted from source code. If these patterns can better predict vulnerable code compared to traditional software metrics, they can be used in developing a vulnerability prediction model to classify code as vulnerable or not. By analyzing and comparing the performance of traceable patterns with metrics, we propose a vulnerability prediction model. This study explores the performance of some code patterns in vulnerability prediction and compares them with traditional software metrics. We use the findings to build an effective vulnerability prediction model. We evaluate security vulnerabilities reported for Apache Tomcat, Apache CXF and three stand-alone Java web applications. We use machine learning and statistical techniques for predicting vulnerabilities using traceable patterns and metrics as features. We found that patterns have a lower false negative rate and higher recall in detecting vulnerable code than the traditional software metrics.","conference":"IEEE","terms":"Predictive models;Software metrics;Security;Tools;Software;Testing,Internet;Java;learning (artificial intelligence);security of data;software metrics;software quality,software vulnerability prediction model;traceable code patterns;software security;software quality;traceable patterns;source code;vulnerable code;traditional software metrics;effective vulnerability prediction model;security vulnerabilities","keywords":"","startPage":"1022","endPage":"1025","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115724","citationCount":2,"referenceCount":14,"year":2017,"authors":"K. Z. Sultana","affiliations":"Department of Computer Science and Engineering, Mississippi State University, MS, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3801"},"title":"Software model checking for distributed systems with selector-based, non-blocking communication","abstract":"Many modern software systems are implemented as client/server architectures, where a server handles multiple clients concurrently. Testing does not cover the outcomes of all possible thread and communication schedules reliably. Software model checking, on the other hand, covers all possible outcomes but is often limited to subsets of commonly used protocols and libraries. Earlier work in cache-based software model checking handles implementations using socket-based TCP/IP networking, with one thread per client connection using blocking input/output. Recently, servers using non-blocking, selector-based input/output have become prevalent. This paper describes our work extending the Java PathFinder extension net-iocache to such software, and the application of our tool to modern server software.","conference":"IEEE","terms":"Servers;Java;Software;Message systems;Libraries;Model checking;Computer architecture,client-server systems;formal verification,distributed systems;selector-based nonblocking communication;client-server architectures;thread schedule;communication schedule;cache-based software model checking;socket-based TCP-IP networking;transport control protocol;Internet protocol;blocking input-output;Java PathFinder extension;input-output cache;server software","keywords":"software model checking;caching;software verification;distributed systems;non-blocking input/output;selector-based input/output","startPage":"169","endPage":"179","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693077","citationCount":10,"referenceCount":23,"year":2013,"authors":"C. Artho; M. Hagiya; R. Potter; Y. Tanabe; F. Weitl; M. Yamamoto","affiliations":"AIST/RISEC, Amagasaki, Japan; The University of Tokyo, Japan; The University of Tokyo, Japan; National Institute of Informatics, Tokyo, Japan; Chiba University, Japan; Chiba University, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3802"},"title":"Automated planning for software architecture evolution","abstract":"In previous research, we have developed a theoretical framework to help software architects make better decisions when planning software evolution. Our approach is based on representation and analysis of candidate evolution paths-sequences of transitional architectures leading from the current system to a desired target architecture. One problem with this kind of approach is that it imposes a heavy burden on the software architect, who must explicitly define and model these candidate paths. In this paper, we show how automated planning techniques can be used to support automatic generation of evolution paths, relieving this burden on the architect. We illustrate our approach by applying it to a data migration scenario, showing how this architecture evolution problem can be translated into a planning problem and solved using existing automated planning tools.","conference":"IEEE","terms":"Computer architecture;Planning;Software architecture;Software;Connectors;Measurement;Availability,planning;software architecture,software architecture evolution;candidate evolution paths;transitional architectures;target architecture;automated planning techniques;automatic generation;data migration scenario;architecture evolution problem;automated planning tools","keywords":"","startPage":"213","endPage":"223","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693081","citationCount":12,"referenceCount":28,"year":2013,"authors":"J. M. Barnes; A. Pandey; D. Garlan","affiliations":"Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA; Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA; Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3803"},"title":"Quick verification of concurrent programs by iteratively relaxed scheduling","abstract":"The most prominent advantage of software verification over testing is a rigorous check of every possible software behavior. However, large state spaces of concurrent systems, due to non-deterministic scheduling, result in a slow automated verification process. Therefore, verification introduces a large delay between completion and deployment of concurrent software. This paper introduces a novel iterative approach to verification of concurrent programs that drastically reduces this delay. By restricting the execution of concurrent programs to a small set of admissible schedules, verification complexity and time is drastically reduced. Iteratively adding admissible schedules after their verification eventually restores non-deterministic scheduling. Thereby, our framework allows to find a sweet spot between a low verification delay and sufficient execution time performance. Our evaluation of a prototype implementation on well-known benchmark programs shows that after verifying only few schedules of the program, execution time overhead is competitive to existing deterministic multi-threading frameworks.","conference":"IEEE","terms":"Schedules;Delays;Concurrent computing;Programming;Model checking;Software,concurrency control;iterative methods;multi-threading;program verification;scheduling,quick verification;iteratively relaxed scheduling;software verification;rigorous check;concurrent systems;nondeterministic scheduling;slow automated verification process;concurrent software;iterative approach;admissible schedules;verification complexity;low verification delay;sufficient execution time performance;software behavior;concurrent program verification;execution time overhead;deterministic multithreading frameworks","keywords":"","startPage":"776","endPage":"781","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115688","citationCount":0,"referenceCount":19,"year":2017,"authors":"P. Metzler; H. Saissi; P. Bokor; N. Suri","affiliations":"Technische Univeristät Darmstadt, Germany; Technische Univeristät Darmstadt, Germany; Technische Univeristät Darmstadt, Germany; Technische Univeristät Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3804"},"title":"Parsimony: An IDE for example-guided synthesis of lexers and parsers","abstract":"We present Parsimony, a programming-by-example development environment for synthesizing lexers and parsers by example. Parsimony provides a graphical interface in which the user presents examples simply by selecting and labeling sample text in a text editor. An underlying synthesis engine then constructs syntactic rules to solve the system of constraints induced by the supplied examples. Parsimony is more expressive and usable than prior programming-by-example systems for parsers in several ways: Parsimony can (1) synthesize lexer rules in addition to productions, (2) solve for much larger constraint systems over multiple examples, rather than handling examples one-at-a-time, and (3) infer much more complex sets of productions, such as entire algebraic expression grammars, by detecting instances of well-known grammar design patterns. The results of a controlled user study across 18 participants show that users are able to perform lexing and parsing tasks faster and with fewer mistakes when using Parsimony as compared to a traditional parsing workflow.","conference":"IEEE","terms":"Automata;Grammar;Syntactics;Data structures;Standards,computational linguistics;grammars;graphical user interfaces;program compilers,supplied examples;Parsimony;programming-by-example systems;example-guided synthesis;programming-by-example development environment;synthesize lexer rules;synthesis engine","keywords":"Lexer;parser;program synthesis;programming-by-example","startPage":"815","endPage":"825","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115692","citationCount":0,"referenceCount":28,"year":2017,"authors":"A. Leung; S. Lerner","affiliations":"University of California, San Diego, USA; University of California, San Diego, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3805"},"title":"Sketch-guided GUI test generation for mobile applications","abstract":"Mobile applications with complex GUIs are very popular today. However, generating test cases for these applications is often tedious professional work. On the one hand, manually designing and writing elaborate GUI scripts requires expertise. On the other hand, generating GUI scripts with record and playback techniques usually depends on repetitive work that testers need to interact with the application over and over again, because only one path is recorded in an execution. Automatic GUI testing focuses on exploring combinations of GUI events. As the number of combinations is huge, it is still necessary to introduce a test interface for testers to reduce its search space. This paper presents a sketch-guided GUI test generation approach for testing mobile applications, which provides a simple but expressive interface for testers to specify their testing purposes. Testers just need to draw a few simple strokes on the screenshots. Then our approach translates the strokes to a testing model and initiates a model-based automatic GUI testing. We evaluate our sketch-guided approach on a few real-world Android applications collected from the literature. The results show that our approach can achieve higher coverage than existing automatic GUI testing techniques with just 10-minute sketching for an application.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Layout;Shape;Connectors;Mobile applications;Grammar,graphical user interfaces;mobile computing;program testing,mobile applications;professional work;elaborate GUI scripts;playback techniques;GUI events;test interface;GUI test generation approach;real-world Android applications;automatic GUI testing techniques","keywords":"","startPage":"38","endPage":"43","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115616","citationCount":2,"referenceCount":23,"year":2017,"authors":"C. Zhang; H. Cheng; E. Tang; X. Chen; L. Bu; X. Li","affiliations":"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3806"},"title":"Detecting and fixing emergent behaviors in Distributed Software Systems using a message content independent method","abstract":"This research is intended to automatically detect emergent behaviors of scenario based Distributed Software Systems (DSS) in design phase. The direct significance of our work is reducing the cost of verifying DSS for unexpected behavior in execution time. Existing approaches have some drawbacks which we try to cover in our work. The main contributions are modeling the DSS components as a social network and not using behavioral modeling, detecting components with no emergent behavior, and investigating the interactions of instances of one type.","conference":"IEEE","terms":"Decision support systems;Model checking;Multi-agent systems;Conferences;Unified modeling language;Social network services;Educational institutions,distributed processing;formal verification;social networking (online),emergent behavior;distributed software system;message content independent method;DSS component;social network","keywords":"Emergent behavior;distributed software system;multiagent system","startPage":"746","endPage":"749","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693148","citationCount":0,"referenceCount":27,"year":2013,"authors":"F. Hendijani Fard","affiliations":"Department of Electrical and Computer Engineering, University of Calgary, 2500 University DR., N.W., Alberta, Canada (T2N 1N4)","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3807"},"title":"Perceived language complexity in GitHub issue discussions and their effect on issue resolution","abstract":"Modern software development is increasingly collaborative. Open Source Software (OSS) are the bellwether; they support dynamic teams, with tools for code sharing, communication, and issue tracking. The success of an OSS project is reliant on team communication. E.g., in issue discussions, individuals rely on rhetoric to argue their position, but also maintain technical relevancy. Rhetoric and technical language are on opposite ends of a language complexity spectrum: the former is stylistically natural; the latter is terse and concise. Issue discussions embody this duality, as developers use rhetoric to describe technical issues. The style mix in any discussion can define group culture and affect performance, e.g., issue resolution times may be longer if discussion is imprecise. Using GitHub, we studied issue discussions to understand whether project-specific language differences exist, and to what extent users conform to a language norm. We built project-specific and overall GitHub language models to study the effect of perceived language complexity on multiple responses. We find that experienced users conform to project-specific language norms, popular individuals use overall GitHub language rather than project-specific language, and conformance to project-specific language norms reduces issue resolution times. We also provide a tool to calculate project-specific perceived language complexity.","conference":"IEEE","terms":"Complexity theory;Pragmatics;Rhetoric;Speech;Software;Standards;Employment,Internet;public domain software;software maintenance;specification languages,GitHub issue discussions;modern software development;Open Source Software;issue tracking;OSS project;team communication;technical language;language complexity spectrum;technical issues;issue resolution times;project-specific language differences;language norm;GitHub language;project-specific perceived language complexity;code sharing;technical relevancy;rhetoric language","keywords":"","startPage":"72","endPage":"83","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115620","citationCount":1,"referenceCount":50,"year":2017,"authors":"D. Kavaler; S. Sirovica; V. Hellendoorn; R. Aranovich; V. Filkov","affiliations":"Department of Computer Science, University of California at Davis, USA; Department of Computer Science, University of California at Davis, USA; Department of Computer Science, University of California at Davis, USA; Department of Linguistics, University of California at Davis, USA; Department of Computer Science, University of California at Davis, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3808"},"title":"Preventing erosion of architectural tactics through their strategic implementation, preservation, and visualization","abstract":"Nowadays, a successful software production is increasingly dependent on how the final deployed system addresses customers' and users' quality concerns such as security, reliability, availability, interoperability, performance and many other types of such requirements. In order to satisfy such quality concerns, software architects are accountable for devising and comparing various alternate solutions, assessing the trade-offs, and finally adopting strategic design decisions which optimize the degree to which each of the quality concerns is satisfied. Although designing and implementing a good architecture is necessary, it is not usually enough. Even a good architecture can deteriorate in subsequent releases and then fail to address those concerns for which it was initially designed. In this work, we present a novel traceability approach for automating the construction of traceabilty links for architectural tactics and utilizing those links to implement a change impact analysis infrastructure to mitigate the problem of architecture degradation. Our approach utilizes machine learning methods to detect tactic-related classes. The detected tactic-related classes are then mapped to a Tactic Traceability Pattern. We train our trace algorithm using code extracted from fifty performance-centric and safety-critical open source software systems and then evaluate it against a real case study.","conference":"IEEE","terms":"Computer architecture;Software;Heart beat;Software architecture;Software reliability,learning (artificial intelligence);safety-critical software;software architecture;software quality,architectural tactic;software production;security;reliability;interoperability;software architect;traceabilty link;change impact analysis infrastructure;machine learning;tactic-related class;tactic traceability pattern;performance-centric open source software system;safety-critical open source software system","keywords":"Architecture;traceability;tactics;traceability patterns;machine learning","startPage":"762","endPage":"765","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693152","citationCount":1,"referenceCount":25,"year":2013,"authors":"M. Mirakhorli","affiliations":"DePaul University, School of Computing, Chicago, IL 60604, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3809"},"title":"Automated Tagging of Software Projects Using Bytecode and Dependencies (N)","abstract":"Several open and closed source repositories group software systems and libraries to allow members of particular organizations or the open source community to take advantage of them. However, to make this possible, it is necessary to have effective ways of searching and browsing the repositories. Software tagging is the process of assigning terms (i.e., tags or labels) to software assets in order to describe features and internal details, making the task of understanding software easier and potentially browsing and searching through a repository more effective. We present Sally, an automatic software tagging approach that is able to produce meaningful tags for Maven-based software projects by analyzing their bytecode and dependency relations without any special requirements from developers. We compared tags generated by Sally to the ones in two widely used online repositories, and the tags generated by a state-of-the-art categorization approach. The results suggest that Sally is able to generate expressive tags without relying on machine learning-based models.","conference":"IEEE","terms":"Feature extraction;Tagging;Data mining;Software systems;Software algorithms;Support vector machines,information retrieval;Internet;project management;public domain software;software management,closed source repository group software systems;open source repository group software systems;open source community;term assignment;software tagging;software assets;automatic software tagging approach;Maven-based software projects;bytecode;dependency relations;online repositories;categorization approach;Sally","keywords":"","startPage":"289","endPage":"294","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372018","citationCount":5,"referenceCount":21,"year":2015,"authors":"S. Vargas-Baldrich; M. Linares-Vásquez; D. Poshyvanyk","affiliations":"Univ. Nac. de Colombia, Bogota, Colombia; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d380a"},"title":"Synthesizing Web Element Locators (T)","abstract":"To programmatically interact with the user interface of a web application, element locators are used to select and retrieve elements from the Document Object Model (DOM). Element locators are used in JavaScript code, Cascading stylesheets, and test cases to interact with the runtime DOM of the webpage. Constructing these element locators is, however, challenging due to the dynamic nature of the DOM. We find that locators written by web developers can be quite complex, and involve selecting multiple DOM elements. We present an automated technique for synthesizing DOM element locators using examples provided interactively by the developer. The main insight in our approach is that the problem of synthesizing complex multi-element locators can be expressed as a constraint solving problem over the domain of valid DOM states in a web application. We implemented our synthesis technique in a tool called LED, which provides an interactive drag and drop support inside the browser for selecting positive and negative examples. We find that LED supports at least 86% of the locators used in the JavaScript code of deployed web applications, and that the locators synthesized by LED have a recall of 98% and a precision of 63%. LED is fast, taking only 0.23 seconds on average to synthesize a locator.","conference":"IEEE","terms":"Light emitting diodes;Cascading style sheets;Mathematical model;Writing;HTML;Navigation;Programming,Internet;Java;online front-ends;program testing;user interfaces,Web element locator synthesis;user interface;document object model;DOM;JavaScript code;Webpage;DOM element locator synthesis;Web application;LED;program synthesis;live editor-for-DOM","keywords":"Program synthesis;Programming by example;Element locators;CSS selectors;Web applications","startPage":"331","endPage":"341","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372022","citationCount":2,"referenceCount":36,"year":2015,"authors":"K. Bajaj; K. Pattabiraman; A. Mesbah","affiliations":"Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada; Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d380b"},"title":"Improved query reformulation for concept location using CodeRank and document structures","abstract":"During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique.","conference":"IEEE","terms":"Natural languages;Periodic structures;Measurement;Java;Software maintenance,learning (artificial intelligence);query formulation;query processing;software maintenance;source code (software);text analysis,initial query;request texts;relevant source code locations;software system;concept location;source document structures;query quality analysis;machine learning;improved query reformulation;software maintenance;software change requests;baseline queries;weight-CodeRank;search terms","keywords":"Query reformulation;CodeRank;term weighting;query quality analysis;concept location;data resampling","startPage":"428","endPage":"439","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115655","citationCount":5,"referenceCount":55,"year":2017,"authors":"M. M. Rahman; C. K. Roy","affiliations":"Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d380c"},"title":"Multi-user variability configuration: A game theoretic approach","abstract":"Multi-user configuration is a neglected problem in variability-intensive systems area. The appearance of conflicts among user configurations is a main concern. Current approaches focus on avoiding such conflicts, applying the mutual exclusion principle. However, this perspective has a negative impact on users satisfaction, who cannot make any decision fairly. In this work, we propose an interpretation of multi-user configuration as a game theoretic problem. Game theory is a well-known discipline which analyzes conflicts and cooperation among intelligent rational decision-makers. We present a taxonomy of multi-user configuration approaches, and how they can be interpreted as different problems of game theory. We focus on cooperative game theory to propose and automate a tradeoff-based bargaining approach, as a way to solve the conflicts and maximize user satisfaction at the same time.","conference":"IEEE","terms":"Games;Frequency modulation;Game theory;Smart homes;Video on demand;Internet;Proposals,game theory;human computer interaction,multiuser variability configuration;game theoretic approach;intelligent rational decision-makers;multiuser configuration taxonomy;cooperative game theory;tradeoff-based bargaining approach","keywords":"","startPage":"574","endPage":"579","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693115","citationCount":1,"referenceCount":17,"year":2013,"authors":"J. García-Galán; P. Trinidad; A. Ruiz-Cortés","affiliations":"Dept. Languages and Computer Systems, University of Seville, Spain; Dept. Languages and Computer Systems, University of Seville, Spain; Dept. Languages and Computer Systems, University of Seville, Spain","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d380d"},"title":"Development History Granularity Transformations (N)","abstract":"Development histories can simplify some software engineering tasks, butdifferent tasks require different history granularities. For example, a history that includes every edit that resulted in compiling code is needed when searching for the cause of a regression, whereas a history that contains only changes relevant to a feature is needed for understanding the evolution of the feature. Unfortunately, today, both manual and automated history generation result in a single-granularity history. This paper introduces the concept of multi-grained development history views and the architecture of Codebase Manipulation, a tool that automatically records a fine-grained history and manages its granularity by applying granularity transformations.","conference":"IEEE","terms":"History;Compounds;Manuals;Maintenance engineering;Software;Computer architecture;Transforms,regression analysis;software engineering,development history granularity transformations;software engineering tasks;compiling code;regression;automated history generation;single-granularity history;multigrained development history views;Codebase Manipulation","keywords":"automated version control;fine-grained development history;history rewriting;history transformation;Codebase Manipulation","startPage":"697","endPage":"702","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372057","citationCount":6,"referenceCount":50,"year":2015,"authors":"K. Muslu; L. Swart; Y. Brun; M. D. Ernst","affiliations":"Univ. of Washington, Seattle, WA, USA; Univ. of Washington, Seattle, WA, USA; Univ. of Washington, Seattle, WA, USA; Univ. of Massachusetts, Amherst, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d380e"},"title":"Generating Qualifiable Avionics Software: An Experience Report (E)","abstract":"We report on our experience with enhancing the data-management component in the avionics software of the NH90 helicopter at Airbus Helicopters. We describe challenges regarding the evolution of avionics software by means of real-world evolution scenarios that arise in industrial practice. A key role plays a legally-binding certification process, called qualification, which is responsible for most of the development effort and cost. To reduce effort and cost, we propose a novel generative approach to develop qualifiable avionics software by combining model-based and product-line technology. Using this approach, we have already generated code that is running on the NH90 helicopter and that is in the process of replacing the current system code. Based on an interview with two professional developers at Airbus and an analysis of the software repository of the NH90, we systematically compare our approach with established development approaches in the avionics domain, in terms of implementation and qualification effort.","conference":"IEEE","terms":"Aerospace electronics;Helicopters;System software;Interviews;Hardware;Encoding,avionics;helicopters;program compilers;software engineering,qualifiable avionics software;data-management component;NH90 helicopter;Airbus helicopter;legally-binding certification process;model-based technology;product-line technology;software repository analysis","keywords":"","startPage":"726","endPage":"736","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372061","citationCount":0,"referenceCount":26,"year":2015,"authors":"A. Wölfl; N. Siegmund; S. Apel; H. Kosch; J. Krautlager; G. Weber-Urbina","affiliations":"Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Univ. of Passau, Passau, Germany; Airbus Helicopters S.A.S., Germany; Airbus Helicopters S.A.S., Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d380f"},"title":"Consistency-preserving edit scripts in model versioning","abstract":"In model-based software development, models are iteratively evolved. To optimally support model evolution, developers need adequate tools for model versioning tasks, including comparison, patching, and merging of models. A significant disadvantage of tools currently available is that they display, and operate with, low-level model changes which refer to internal model representations and which can lead to intermediate inconsistent states. Higher-level consistency-preserving edit operations including refactorings are better suited to explain changes or to resolve conflicts. This paper presents an automatic procedure which transforms a low-level difference into an executable edit script which uses consistency-preserving edit operations only. Edit scripts support consistent model patching and merging on a higher abstraction level. Our approach to edit script generation has been evaluated in a larger real-world case study.","conference":"IEEE","terms":"Unified modeling language;Merging;Semantics;Concrete;Adaptation models;Abstracts;Syntactics,configuration management;software tools,consistency preserving edit scripts;model versioning;software development;internal model representations;automatic procedure;model merging;model comparison;model patching;software tools","keywords":"","startPage":"191","endPage":"201","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693079","citationCount":21,"referenceCount":41,"year":2013,"authors":"T. Kehrer; U. Kelter; G. Taentzer","affiliations":"Software Engineering Group, University of Siegen, Germany; Software Engineering Group, University of Siegen, Germany; Philipps-Universität Marburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3810"},"title":"SEDGE: Symbolic example data generation for dataflow programs","abstract":"Exhaustive, automatic testing of dataflow (esp. mapreduce) programs has emerged as an important challenge. Past work demonstrated effective ways to generate small example data sets that exercise operators in the Pig platform, used to generate Hadoop map-reduce programs. Although such prior techniques attempt to cover all cases of operator use, in practice they often fail. Our SEDGE system addresses these completeness problems: for every dataflow operator, we produce data aiming to cover all cases that arise in the dataflow program (e.g., both passing and failing a filter). SEDGE relies on transforming the program into symbolic constraints, and solving the constraints using a symbolic reasoning engine (a powerful SMT solver), while using input data as concrete aids in the solution process. The approach resembles dynamic-symbolic (a.k.a. “concolic”) execution in a conventional programming language, adapted to the unique features of the dataflow domain. In third-party benchmarks, SEDGE achieves higher coverage than past techniques for 5 out of 20 PigMix benchmarks and 7 out of 11 SDSS benchmarks and (with equal coverage for the rest of the benchmarks). We also show that our targeting of the high-level dataflow language pays off: for complex programs, state-of-the-art dynamic-symbolic execution at the level of the generated map-reduce code (instead of the original dataflow program) requires many more test cases or achieves much lower coverage than our approach.","conference":"IEEE","terms":"Concrete;Cognition;Benchmark testing;Programming;Educational institutions;Data processing;Extraterrestrial measurements,data flow analysis;program testing;programming languages;reasoning about programs;specification languages,symbolic example data generation;dataflow programs;automatic testing;mapreduce programs;pig platform;Hadoop map-reduce programs;operator use;SEDGE system;dataflow operator;symbolic constraints;symbolic reasoning engine;SMT solver;concolic execution;conventional programming language;dataflow domain;SDSS benchmarks;high-level dataflow language;complex programs;state-of-the-art dynamic-symbolic execution;map-reduce code;test cases","keywords":"","startPage":"235","endPage":"245","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693083","citationCount":5,"referenceCount":26,"year":2013,"authors":"K. Li; C. Reichenbach; Y. Smaragdakis; Y. Diao; C. Csallner","affiliations":"Computer Science Department, University of Massachusetts, Amherst, USA; Institute of Informatics, Goethe University Frankfurt, Germany; Department of Informatics, University of Athens, Greece; Computer Science Department, University of Massachusetts, Amherst, USA; Computer Science and Engineering, University of Texas at Arlington, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3811"},"title":"Programming bots by synthesizing natural language expressions into API invocations","abstract":"At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code - all made possible by APIs. Yet, developers today are not as empowered to program bots in much the same way. To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. Our solution is two faceted: Firstly, we construct an API knowledge graph to encode and evolve APIs; secondly, leveraging the above we apply techniques in NLP, ML and Entity Recognition to perform the required synthesis from natural language user expressions into API calls.","conference":"IEEE","terms":"Natural languages;Programming;Computational modeling;Concrete;Meteorology;Business;Machine learning,application program interfaces;learning (artificial intelligence);natural language processing;object-oriented programming,API;natural language user expression synthesis;BotBase;API knowledge graph;bot programming platform;users expressions;world conversations;user utterances;fixed set;machine-learning classifiers;specific use-case;API invocations;API calls","keywords":"","startPage":"832","endPage":"837","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115694","citationCount":1,"referenceCount":27,"year":2017,"authors":"S. Zamanirad; B. Benatallah; M. C. Barukh; F. Casati; C. Rodriguez","affiliations":"School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW 2052; School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW 2052; School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW 2052; University of Trento, Italy / Tomsk Polytechnic University, Russia; School of Computer Science and Engineering, University of New South Wales (UNSW), Sydney, NSW 2052","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3812"},"title":"Learn\u0026Fuzz: Machine learning for input fuzzing","abstract":"Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss and measure the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn\u0026fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs.","conference":"IEEE","terms":"Portable document format;Grammar;Training;Probability distribution;Recurrent neural networks,fuzzy set theory;grammars;learning (artificial intelligence);neural nets;probability;program debugging;program testing;security of data,machine learning;input fuzzing;security vulnerabilities;input-parsing code;complex input format;complex security-critical parser;conflicting learning;learnt input probability distribution;neural-network-based statistical machine-learning techniques;PDF parser;Microsoft's new Edge browser","keywords":"Fuzzing;Deep Learning;Grammar-based Fuzzing;Grammar Learning","startPage":"50","endPage":"59","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115618","citationCount":25,"referenceCount":31,"year":2017,"authors":"P. Godefroid; H. Peleg; R. Singh","affiliations":"Microsoft Research, USA; Technion, Israel; Microsoft Research, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3813"},"title":"Are developers aware of the architectural impact of their changes?","abstract":"Although considered one of the most important decisions in a software development lifecycle, empirical evidence on how developers perform and perceive architectural changes is still scarce. Given the large implications of architectural decisions, we do not know whether developers are aware of their changes' impact on the software's architecture, whether awareness leads to better changes, and whether automatically making developers aware would prevent degradation. Therefore, we use code review data of 4 open source systems to investigate the intent and awareness of developers when performing changes. We extracted 8,900 reviews for which the commits are available. 2,152 of the commits have changes in their computed architectural metrics, and 338 present significant changes to the architecture. We manually inspected all reviews for commits with significant changes and found that only in 38% of the time developers are discussing the impact of their changes on the architectural structure, suggesting a lack of awareness. Finally, we observed that developers tend to be more aware of the architectural impact of their changes when the architectural structure is improved, suggesting that developers should be automatically made aware when their changes degrade the architectural structure.","conference":"IEEE","terms":"Couplings;Computer architecture;Measurement;Java;History;Degradation;Software systems,software architecture;software engineering;software maintenance;software metrics;software quality,architectural changes;architectural decisions;computed architectural metrics;time developers;architectural structure;architectural impact;software development lifecycle","keywords":"Software Architecture;Code Reviews","startPage":"95","endPage":"105","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115622","citationCount":8,"referenceCount":44,"year":2017,"authors":"M. Paixao; J. Krinke; D. Han; C. Ragkhitwetsagul; M. Harman","affiliations":"University College London, United Kingdom; University College London, United Kingdom; University College London, United Kingdom; University College London, United Kingdom; University College London, United Kingdom","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3814"},"title":"Predicting Delays in Software Projects Using Networked Classification (T)","abstract":"Software projects have a high risk of cost and schedule overruns, which has been a source of concern for the software engineering community for a long time. One of the challenges in software project management is to make reliable prediction of delays in the context of constant and rapid changes inherent in software projects. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether a subset of software tasks (among the hundreds to thousands of ongoing tasks) in a software project have a risk of being delayed. Our approach makes use of not only features specific to individual software tasks (i.e. local data) -- as done in previous work -- but also their relationships (i.e. networked data). In addition, using collective classification, our approach can simultaneously predict the degree of delay for a group of related tasks. Our evaluation results show a significant improvement over traditional approaches which perform classification on each task independently: achieving 46% -- 97% precision (49% improved), 46% -- 97% recall (28% improved), 56% -- 75% F-measure (39% improved), and 78% -- 95% Area Under the ROC Curve (16% improved).","conference":"IEEE","terms":"Software;Delays;Predictive models;Risk management;Software engineering;Data mining;Information technology,pattern classification;project management;software engineering;software management,Networked Classification;software engineering community;software project management;ROC Curve;F-measure","keywords":"Networked classification;Software analytics;Risk management;Machine Learning","startPage":"353","endPage":"364","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372024","citationCount":5,"referenceCount":57,"year":2015,"authors":"M. Choetkiertikul; H. K. Dam; T. Tran; A. Ghose","affiliations":"Sch. of Comput. \u0026 Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia; Sch. of Comput. \u0026 Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia; Sch. of Inf. Technol., Deakin Univ., Melbourne, VIC, Australia; Sch. of Comput. \u0026 Inf. Technol., Univ. of Wollongong, Wollongong, NSW, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3815"},"title":"O2O service composition with social collaboration","abstract":"In Online-to-Offline (O2O) commerce, customer services may need to be composed from online and offline services. Such composition is challenging, as it requires effective selection of appropriate services that, in turn, support optimal combination of both online and offline services. In this paper, we address this challenge by proposing an approach to O2O service composition which combines offline route planning and social collaboration to optimize service selection. We frame general O2O service composition problems using timed automata and propose an optimization procedure that incorporates: (1) a Markov Chain Monte Carlo (MCMC) algorithm to stochastically select a concrete composite service, and (2) a model checking approach to searching for an optimal collaboration plan with the lowest cost given certain time constraint. Our procedure has been evaluated using the simulation of a rich scenario on effectiveness and scalability.","conference":"IEEE","terms":"Collaboration;Concrete;Libraries;Printing;Optimization;Quality of service;Planning,automata theory;customer services;electronic commerce;formal verification;Markov processes;Monte Carlo methods,O2O service composition;social collaboration;customer services;offline route planning;service selection;concrete composite service;optimal collaboration plan;Markov chain Monte Carlo algorithm;model checking;timed automata;online-to-offline coomerce;optimal combination","keywords":"","startPage":"451","endPage":"461","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115657","citationCount":2,"referenceCount":31,"year":2017,"authors":"W. Qian; X. Peng; J. Sun; Y. Yu; B. Nuseibeh; W. Zhao","affiliations":"School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Singapore University of Technology and Design, Singapore; School of Computing and Communications, The Open University, UK; School of Computing and Communications, The Open University, UK; School of Computer Science, Fudan University, Shanghai, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3816"},"title":"Transfer learning for performance modeling of configurable systems: An exploratory analysis","abstract":"Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space.","conference":"IEEE","terms":"Hardware;Software systems;Analytical models;Predictive models;Mobile communication;Reliability,learning (artificial intelligence);program diagnostics;software engineering;software performance evaluation,transfer learning;performance modeling;configurable systems;highly dimensional configuration space;performance model;performance behavior;software configurations;software systems","keywords":"Performance analysis;transfer learning","startPage":"497","endPage":"508","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115661","citationCount":10,"referenceCount":64,"year":2017,"authors":"P. Jamshidi; N. Siegmund; M. Velez; C. Kästner; A. Patel; Y. Agarwal","affiliations":"Carnegie Mellon University, USA; Bauhaus-University Weimar, Germany; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9125eee8435e8e7d3817"},"title":"Learning effective query transformations for enhanced requirements trace retrieval","abstract":"In automated requirements traceability, significant improvements can be realized through incorporating user feedback into the trace retrieval process. However, existing feedback techniques are designed to improve results for individual queries. In this paper we present a novel technique designed to extend the benefits of user feedback across multiple trace queries. Our approach, named Trace Query Transformation (TQT), utilizes a novel form of Association Rule Mining to learn a set of query transformation rules which are used to improve the efficacy of future trace queries. We evaluate TQT using two different kinds of training sets. The first represents an initial set of queries directly modified by human analysts, while the second represents a set of queries generated by applying a query optimization process based on initial relevance feedback for trace links between a set of source and target documents. Both techniques are evaluated using requirements from theWorldVista Healthcare system, traced against certification requirements for the Commission for Healthcare Information Technology. Results show that the TQT technique returns significant improvements in the quality of generated trace links.","conference":"IEEE","terms":"Training;Association rules;Itemsets;Medical services;Standards;Manuals;Educational institutions,data mining;formal verification;health care;learning (artificial intelligence);medical computing;program diagnostics;query processing;relevance feedback;text analysis,effective query transformation learning;requirement trace retrieval enhancement process;automated requirements traceability;user feedback;trace query transformation;association rule mining;training sets;query optimization process;relevance feedback;source documents;target documents;WorldVista Healthcare system;certification requirements;Commission for Healthcare Information Technology;TQT technique;machine learning;text mining;software engineering activities","keywords":"requirements traceability;query replacement;contractual requirements;text mining;machine learning;association rules","startPage":"586","endPage":"591","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693117","citationCount":10,"referenceCount":23,"year":2013,"authors":"T. Dietrich; J. Cleland-Huang; Y. Shin","affiliations":"Center of Excellence for Software Traceability (CoEST), School of Computing, DePaul University, Chicago, IL, USA; Center of Excellence for Software Traceability (CoEST), School of Computing, DePaul University, Chicago, IL, USA; Center of Excellence for Software Traceability (CoEST), School of Computing, DePaul University, Chicago, IL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3818"},"title":"PYTHIA: Generating test cases with oracles for JavaScript applications","abstract":"Web developers often write test cases manually using testing frameworks such as Selenium. Testing JavaScript-based applications is challenging as manually exploring various execution paths of the application is difficult. Also JavaScript's highly dynamic nature as well as its complex interaction with the DOM make it difficult for the tester to achieve high coverage. We present a framework to automatically generate unit test cases for individual JavaScript functions. These test cases are strengthened by automatically generated test oracles capable of detecting faults in JavaScript code. Our approach is implemented in a tool called Pythia. Our preliminary evaluation results point to the efficacy of the approach in achieving high coverage and detecting faults.","conference":"IEEE","terms":"Testing;Instruments;Runtime;Browsers;Measurement;Java;Reactive power,automatic test software;Java;program testing;software fault tolerance;software tools;Web services,software tool;document object model;PYTHIA;JavaScript code;faults detection;automatic test oracles generation;JavaScript functions;DOM;complex interaction;manually execution path exploration;software testing;Web developer;JavaScript applications;automatic unit test case generation","keywords":"test generation;oracles;JavaScript;DOM","startPage":"610","endPage":"615","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693121","citationCount":7,"referenceCount":20,"year":2013,"authors":"S. Mirshokraie; A. Mesbah; K. Pattabiraman","affiliations":"University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3819"},"title":"Automatic Detection of Potential Layout Faults Following Changes to Responsive Web Pages (N)","abstract":"Due to the exponential increase in the number ofmobile devices being used to access the World Wide Web, it iscrucial that Web sites are functional and user-friendly across awide range of Web-enabled devices. This necessity has resulted in the introduction of responsive Web design (RWD), which usescomplex cascading style sheets (CSS) to fluidly modify a Web site's appearance depending on the viewport width of the device in use. Although existing tools may support the testing of responsive Web sites, they are time consuming and error-prone to use because theyrequire manual screenshot inspection at specified viewport widths. Addressing these concerns, this paper presents a method thatcan automatically detect potential layout faults in responsively designed Web sites. To experimentally evaluate this approach, weimplemented it as a tool, called ReDeCheck, and applied itto 5 real-world web sites that vary in both their approach toresponsive design and their complexity. The experiments revealthat ReDeCheck finds 91% of the inserted layout faults.","conference":"IEEE","terms":"Layout;Cascading style sheets;Web pages;HTML;Mobile handsets,fault diagnosis;human computer interaction;program testing;Web design,automatic layout fault detection;Web pages;mobile devices;World Wide Web;functional user-friendly Web sites;Web-enabled devices;responsive Web design;RWD;complex cascading style sheets;CSS;Web site appearance;responsive Web site testing;manual screenshot inspection;viewport widths;REDECHECK tool;inserted layout faults","keywords":"software testing;responsive web;empirical studies","startPage":"709","endPage":"714","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372059","citationCount":13,"referenceCount":22,"year":2015,"authors":"T. A. Walsh; P. McMinn; G. M. Kapfhammer","affiliations":"Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Allegheny Coll., USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d381a"},"title":"Mining User Opinions in Mobile App Reviews: A Keyword-Based Approach (T)","abstract":"User reviews of mobile apps often contain complaints or suggestions which are valuable for app developers to improve user experience and satisfaction. However, due to the large volume and noisy-nature of those reviews, manually analyzing them for useful opinions is inherently challenging. To address this problem, we propose MARK, a keyword-based framework for semi-automated review analysis. MARK allows an analyst describing his interests in one or some mobile apps by a set of keywords. It then finds and lists the reviews most relevant to those keywords for further analysis. It can also draw the trends over time of those keywords and detect their sudden changes, which might indicate the occurrences of serious issues. To help analysts describe their interests more effectively, MARK can automatically extract keywords from raw reviews and rank them by their associations with negative reviews. In addition, based on a vector-based semantic representation of keywords, MARK can divide a large set of keywords into more cohesive subsets, or suggest keywords similar to the selected ones.","conference":"IEEE","terms":"Batteries;Mobile communication;Dictionaries;Energy consumption;Data mining;Facebook,data mining;mobile computing,mining user opinion;mobile app review;keyword-based approach;app developer;user experience;user satisfaction;keyword-based framework;semiautomated review analysis;vector-based semantic representation;MARK","keywords":"Opinion Mining;Review Analysis;Keyword","startPage":"749","endPage":"759","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372063","citationCount":27,"referenceCount":32,"year":2015,"authors":"P. M. Vu; T. T. Nguyen; H. V. Pham; T. T. Nguyen","affiliations":"NA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d381b"},"title":"Entropy-based test generation for improved fault localization","abstract":"Spectrum-based Bayesian reasoning can effectively rank candidate fault locations based on passing/failing test cases, but the diagnostic quality highly depends on the size and diversity of the underlying test suite. As test suites in practice often do not exhibit the necessary properties, we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality. We apply probability theory concepts to guide test case generation using entropy, such that the amount of uncertainty in the diagnostic ranking is minimized. Our ENTBUG prototype extends the search-based test generation tool EVOSUITE to use entropy in the fitness function of its underlying genetic algorithm, and we applied it to seven real faults. Empirical results show that our approach reduces the entropy of the diagnostic ranking by 49% on average (compared to using the original test suite), leading to a 91% average reduction of diagnosis candidates needed to inspect to find the true faulty one.","conference":"IEEE","terms":"Entropy;Cognition;Uncertainty;Debugging;Genetic algorithms;Sociology;Statistics,belief networks;entropy;inference mechanisms;probability;program diagnostics;program testing;software fault tolerance,fitness function;EVOSUITE;search-based test generation tool;ENTBUG prototype;diagnostic ranking;entropy;test case generation;probability theory concepts;diagnostic quality;test suite;diagnostic quality;passing-failing test cases;spectrum-based Bayesian reasoning;improved fault localization;entropy-based test generation","keywords":"Fault localization;Test case generation","startPage":"257","endPage":"267","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693085","citationCount":23,"referenceCount":40,"year":2013,"authors":"J. Campos; R. Abreu; G. Fraser; M. d'Amorim","affiliations":"Faculty of Engineering of University of Porto, Portugal; Faculty of Engineering of University of Porto, Portugal; University of Sheffield, United Kingdom; Federal University of Pernambuco, Brazil","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d381c"},"title":"Systematic reduction of GUI test sequences","abstract":"Graphic user interface (GUI) is an integral part of many software applications. However, GUI testing remains a challenging task. The main problem is to generate a set of high-quality test cases, i.e., sequences of user events to cover the often large input space. Since manually crafting event sequences is labor-intensive and automated testing tools often have poor performance, we propose a new GUI testing framework to efficiently generate progressively longer event sequences while avoiding redundant sequences. Our technique for identifying the redundancy among these sequences relies on statically checking a set of simple and syntactic-level conditions, whose reduction power matches and sometimes exceeds that of classic techniques based on partial order reduction. We have evaluated our method on 17 Java Swing applications. Our experimental results show the new technique, while being sound and systematic, can achieve more than 10X reduction in the number of test sequences compared to the state-of-the-art GUI testing tools.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Tools;Systematics;Java;Redundancy,graphical user interfaces;Java;program testing,redundant sequences;partial order reduction;systematic reduction;GUI test sequences;graphic user interface;software applications;high-quality test cases;automated testing tools;Java Swing applications;syntactic-level condition","keywords":"","startPage":"849","endPage":"860","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115696","citationCount":0,"referenceCount":60,"year":2017,"authors":"L. Cheng; Z. Yang; C. Wang","affiliations":"Western Michigan University, Kalamazoo, MI, USA; Western Michigan University, Kalamazoo, MI, USA; University of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d381d"},"title":"Detecting fragile comments","abstract":"Refactoring is a common software development practice and many simple refactorings can be performed automatically by tools. Identifier renaming is a widely performed refactoring activity. With tool support, rename refactorings can rely on the program structure to ensure correctness of the code transformation. Unfortunately, the textual references to the renamed identifier present in the unstructured comment text cannot be formally detected through the syntax of the language, and are thus fragile with respect to identifier renaming. We designed a new rule-based approach to detect fragile comments. Our approach, called Fraco, takes into account the type of identifier, its morphology, the scope of the identifier and the location of comments. We evaluated the approach by comparing its precision and recall against hand-annotated benchmarks created for six target Java systems, and compared the results against the performance of Eclipse's automated in-comment identifier replacement feature. Fraco performed with near-optimal precision and recall on most components of our evaluation data set, and generally outperformed the baseline Eclipse feature. As part of our evaluation, we also noted that more than half of the total number of identifiers in our data set had fragile comments after renaming, which further motivates the need for research on automatic comment refactoring.","conference":"IEEE","terms":"Tools;Java;Benchmark testing;Syntactics;Morphology;Semantics,Java;object-oriented programming;software maintenance;text analysis,in-comment identifier replacement feature;fragile comments;automatic comment refactoring;rename refactorings;program structure;renamed identifier present;unstructured comment text;Fraco;software development practice","keywords":"Software evolution;refactoring;source code comments;inconsistent code;fragile comments","startPage":"112","endPage":"122","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115624","citationCount":5,"referenceCount":30,"year":2017,"authors":"I. K. Ratol; M. P. Robillard","affiliations":"School of Computer Science, McGill University, Montreal, QC, Canada; School of Computer Science, McGill University, Montreal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d381e"},"title":"Model-Driven Allocation Engineering (T)","abstract":"Cyber-physical systems (CPSs) provide sophisticated functionality and are controlled by networked electronic control units (ECUs). Nowadays, software engineers use component-based development approaches to develop their software. Moreover, software components have to be allocated to an ECU to be executed. Engineers have to cope with topology-, software-, and timing-dependencies and memory-, scheduling-, and routing-constraints. Currently, engineers use linear programs to specify allocation constraints and to derive a feasible allocation automatically. However, encoding the allocation problem as a linear program is a complex and error-prone task. This paper contributes a model-driven, OCL-based allocation engineering approach for reducing the engineering effort and to avoid failures. We validate our approach with an automotive case study modeled with MechatronicUML. Our validation shows that we can specify allocation constraints with less engineering effort and are able to derive feasible allocations automatically.","conference":"IEEE","terms":"Resource management;Software;Hardware;Actuators;Computer architecture;Brakes;Automotive engineering,constraint handling;cyber-physical systems;linear programming;object-oriented programming;resource allocation;Unified Modeling Language,model-driven allocation engineering;cyber-physical systems;CPS;networked electronic control units;ECU;software engineers;component-based development approach;software component allocation;topology-dependencies;software-dependencies;timing-dependencies;memory-constraints;routing-constraints;scheduling-constraints;linear programs;constraint allocation problem;model-driven OCL-based allocation engineering approach;MechatronicUML","keywords":"Cyber-physical systems;Allocation;Deployment;Automotive;Constraints;MechatronicUML","startPage":"374","endPage":"384","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372026","citationCount":4,"referenceCount":27,"year":2015,"authors":"U. Pohlmann; M. Hüwe","affiliations":"Project Group Mechatron. Syst. Design, Fraunhofer IPT, Paderborn, Germany; Project Group Mechatron. Syst. Design, Fraunhofer IPT, Paderborn, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d381f"},"title":"Executing Model-Based Tests on Platform-Specific Implementations (T)","abstract":"Model-based testing of embedded real-time systems is challenging because platform-specific details are often abstracted away to make the models amenable to various analyses. Testing an implementation to expose non-conformance to such a model requires reconciling differences arising from these abstractions. Due to stateful behavior, naive comparisons of model and system behaviors often fail causing numerous false positives. Previously proposed approaches address this by being reactively permissive: passing criteria are relaxed to reduce false positives, but may increase false negatives, which is particularly bothersome for safety-critical systems. To address this concern, we propose an automated approach that is proactively adaptive: test stimuli and system responses are suitably modified taking into account platform-specific aspects so that the modified test when executed on the platform-specific implementation exercises the intended scenario captured in the original model-based test. We show that the new framework eliminates false negatives while keeping the number of false positives low for a variety of platform-specific configurations.","conference":"IEEE","terms":"Testing;Real-time systems;Computational modeling;Timing;Software;Monitoring;Hardware,embedded systems;program testing;safety-critical software,model-based testing;platform-specific configuration;embedded real-time system;safety-critical system","keywords":"Model-based testing;platform-specific implementation","startPage":"418","endPage":"428","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372030","citationCount":0,"referenceCount":25,"year":2015,"authors":"D. You; S. Rayadurgam; M. P. E. Heimdahl; J. Komp; B. Kim; O. Sokolsky","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Univ. of Minnesota, Minneapolis, MN, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Minnesota, Minneapolis, MN, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Minnesota, Minneapolis, MN, USA; Medtronic PLC, USA; Dept. of Comput. \u0026 Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA; Dept. of Comput. \u0026 Inf. Sci., Univ. of Pennsylvania, Philadelphia, PA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3820"},"title":"Diagnosing assumption problems in safety-critical products","abstract":"Problems with the correctness and completeness of environmental assumptions contribute to many accidents in safety-critical systems. The problem is exacerbated when products are modified in new releases or in new products of a product line. In such cases existing sets of environmental assumptions are often carried forward without sufficiently rigorous analysis. This paper describes a new technique that exploits the traceability required by many certifying bodies to reason about the likelihood that environmental assumptions are omitted or incorrectly retained in new products. An analysis of over 150 examples of environmental assumptions in historical systems informs the approach. In an evaluation on three safety-related product lines the approach caught all but one of the assumption-related problems. It also provided clearly defined steps for mitigating the identified issues. The contribution of the work is to arm the safety analyst with useful information for assessing the validity of environmental assumptions for a new product.","conference":"IEEE","terms":"Software;Accidents;Computer science;Hazards;Software product lines;Inspection,production engineering computing;safety-critical software,environmental assumptions;safety-related product lines;assumption-related problems;safety-critical products;safety-critical systems","keywords":"Environmental assumptions;Safety-critical systems;Product lines;Software traceability","startPage":"473","endPage":"484","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115659","citationCount":1,"referenceCount":62,"year":2017,"authors":"M. Rahimi; W. Xiong; J. Cleland-Huang; R. Lutz","affiliations":"School of Computing, Depaul University, Chicago IL, USA; Computer Science, Iowa State University, Ames, IA, USA; Computer Science and Eng., University of Notre Dame, South Bend IN, USA; Computer Science, Iowa State University, Ames, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3821"},"title":"A comprehensive study on real world concurrency bugs in Node.js","abstract":"Node.js becomes increasingly popular in building server-side JavaScript applications. It adopts an event-driven model, which supports asynchronous I/O and non-deterministic event processing. This asynchrony and non-determinism can introduce intricate concurrency bugs, and leads to unpredictable behaviors. An in-depth understanding of real world concurrency bugs in Node.js applications will significantly promote effective techniques in bug detection, testing and fixing for Node.js. In this paper, we present NodeCB, a comprehensive study on real world concurrency bugs in Node.js applications. Specifically, we have carefully studied 57 real bug cases from open-source Node.js applications, and have analyzed their bug characteristics, e.g., bug patterns and root causes, bug impacts, bug manifestation, and fix strategies. Through this study, we obtain several interesting findings, which may open up many new research directions in combating concurrency bugs in Node.js. For example, one finding is that two thirds of the bugs are caused by atomicity violation. However, due to lack of locks and transaction mechanism, Node.js cannot easily express and guarantee the atomic intention.","conference":"IEEE","terms":"Computer bugs;Concurrent computing;Instruction sets;Open source software;Databases;Testing,concurrency control;Java;program debugging;program diagnostics;program testing,concurrency bugs;NodeCB;bug manifestation;bug impacts;bug patterns;open-source Node.js applications;bug detection;building server-side JavaScript applications","keywords":"JavaScript;Node.js;event-driven;concurrency bug;empirical study","startPage":"520","endPage":"531","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115663","citationCount":5,"referenceCount":65,"year":2017,"authors":"J. Wang; W. Dou; Y. Gao; C. Gao; F. Qin; K. Yin; J. Wei","affiliations":"State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China; Dept. of Computer Science and Engineering, The Ohio State University, United States; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3822"},"title":"Cloud Twin: Native execution of android applications on the Windows Phone","abstract":"To successfully compete in the software marketplace, modern mobile applications must run on multiple competing platforms, such as Android, iOS, and Windows Phone. Companies producing mobile applications spend substantial amounts of time, effort, and money to port applications across platforms. Creating individual program versions for different platforms further exacerbates the maintenance burden. This paper presents Cloud Twin, a novel approach to natively executing the functionality of a mobile application written for another platform. The functionality is accessed by means of dynamic cross-platform replay, in which the source application's execution in the cloud is mimicked natively on the target platform. The reference implementation of Cloud Twin natively emulates the behavior of Android applications on a Windows Phone. Specifically, Cloud Twin transmits, via web sockets, the UI actions performed on the Windows Phone to the cloud server, which then mimics the received actions on the Android emulator. The UI updates on the emulator are efficiently captured by means of Aspect Oriented Programming and sent back to be replayed on the Windows Phone. Our case studies with third-party applications indicate that the Cloud Twin approach can become a viable solution to the heterogeneity of the mobile application market.","conference":"IEEE","terms":"Mobile communication;Smart phones;XML;Layout;Sockets;Servers;Androids,aspect-oriented programming;cloud computing;DP industry;market opportunities;mobile computing;operating systems (computers),cloud twin;Android applications;Windows phone;software marketplace;mobile applications;dynamic cross-platform replay;cloud server;aspect oriented programming","keywords":"","startPage":"598","endPage":"603","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693119","citationCount":2,"referenceCount":19,"year":2013,"authors":"E. Holder; E. Shah; M. Davoodi; E. Tilevich","affiliations":"Dept. of Computer Science, Virginia Tech, Blacksburg, 24061, USA; Dept. of Computer Science, Virginia Tech, Blacksburg, 24061, USA; Dept. of Computer Science, Virginia Tech, Blacksburg, 24061, USA; Dept. of Computer Science, Virginia Tech, Blacksburg, 24061, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3823"},"title":"Automated inference of classifications and dependencies for combinatorial testing","abstract":"Even for small programs, the input space is huge - often unbounded. Partition testing divides the input space into disjoint equivalence classes and combinatorial testing selects a subset of all possible input class combinations, according to criteria such as pairwise coverage. The down side of this approach is that the partitioning of the input space into equivalence classes (input classification) is done manually. It is expensive and requires deep domain and implementation understanding. In this paper, we propose a novel approach to classify test inputs and their dependencies automatically. Firstly, random (or automatically generated) input vectors are sent to the system under test (SUT). For each input vector, an observed “hit vector” is produced by monitoring the execution of the SUT. Secondly, hit vectors are grouped into clusters using machine learning. Each cluster contains similar hit vectors, i.e., similar behaviors, and from them we obtain corresponding clusters of input vectors. Input classes are then extracted for each input parameter straightforwardly. Our experiments with a number of subjects show good results as the automatically generated classifications are the same or very close to the expected ones.","conference":"IEEE","terms":"Vectors;Testing;Concrete;Support vector machine classification;Systematics;Servers;Clustering algorithms,inference mechanisms;learning (artificial intelligence);pattern classification;program testing;reverse engineering,automated inference;combinatorial testing;partition testing;disjoint equivalence classes;input class combinations;pairwise coverage;input classification;deep domain understanding;implementation understanding;system under test;SUT;hit vector;machine learning;input parameter","keywords":"Automated input classifications;combinatorial testing;invariant inference","startPage":"622","endPage":"627","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693123","citationCount":0,"referenceCount":13,"year":2013,"authors":"Cu Duy Nguyen; P. Tonella","affiliations":"Fondazione Bruno Kessler, Trento, ITALY; Fondazione Bruno Kessler, Trento, ITALY","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3824"},"title":"Ensemble Methods for App Review Classification: An Approach for Software Evolution (N)","abstract":"App marketplaces are distribution platforms for mobile applications that serve as a communication channel between users and developers. These platforms allow users to write reviews about downloaded apps. Recent studies found that such reviews include information that is useful for software evolution. However, the manual analysis of a large amount of user reviews is a tedious and time consuming task. In this work we propose a taxonomy for classifying app reviews into categories relevant for software evolution. Additionally, we describe an experiment that investigates the performance of individual machine learning algorithms and its ensembles for automatically classifying the app reviews. We evaluated the performance of the machine learning techniques on 4550 reviews that were systematically labeled using content analysis methods. Overall, the ensembles had a better performance than the individual classifiers, with an average precision of 0.74 and 0.59 recall.","conference":"IEEE","terms":"Software;Taxonomy;Support vector machines;Manuals;Google;Labeling;Prediction algorithms,learning (artificial intelligence);mobile computing;pattern classification;software engineering,automatic application review classification;software evolution;mobile applications;communication channel;machine learning algorithms;content analysis methods;individual classifiers;ensemble methods","keywords":"User Feedback;Software Evolution;App Reviews;Text Classification","startPage":"771","endPage":"776","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372065","citationCount":33,"referenceCount":27,"year":2015,"authors":"E. Guzman; M. El-Haliby; B. Bruegge","affiliations":"Tech. Univ. Munchen, Garching, Germany; Tech. Univ. Munchen, Garching, Germany; Tech. Univ. Munchen, Garching, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3825"},"title":"Personalized defect prediction","abstract":"Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction-building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java-the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification.","conference":"IEEE","terms":"Predictive models;Vectors;Mars;Syntactics;Computer bugs;Training;Feature extraction,Java;Linux;program compilers,personalized defect prediction;separate prediction model;coding styles;commit frequencies;experience levels;different defect patterns;software defect prediction;C software projects;java software projects;Linux kernel;PostgreSQL;Xorg;Eclipse;Lucene;Jackrabbit","keywords":"Change classification;machine learning;personalized defect prediction;software reliability","startPage":"279","endPage":"289","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693087","citationCount":61,"referenceCount":59,"year":2013,"authors":"T. Jiang; L. Tan; S. Kim","affiliations":"University of Waterloo, ON, Canada; University of Waterloo, ON, Canada; Hong Kong University of Science and Technology, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3826"},"title":"Automatically partition software into least privilege components using dynamic data dependency analysis","abstract":"The principle of least privilege requires that software components should be granted only necessary privileges, so that compromising one component does not lead to compromising others. However, writing privilege separated software is difficult and as a result, a large number of software is monolithic, i.e., it runs as a whole without separation. Manually rewriting monolithic software into privilege separated software requires significant effort and can be error prone. We propose ProgramCutter, a novel approach to automatically partitioning monolithic software using dynamic data dependency analysis. ProgramCutter works by constructing a data dependency graph whose nodes are functions and edges are data dependencies between functions. The graph is then partitioned into subgraphs where each subgraph represents a least privilege component. The privilege separated software runs each component in a separated process with confined system privileges. We evaluate it by applying it on four open source software. We can reduce the privileged part of the program from 100% to below 22%, while having a reasonable execution time overhead. Since ProgramCutter does not require any expert knowledge of the software, it not only can be used by its developers for software refactoring, but also by end users or system administrators. Our contributions are threefold: (i) we define a quantitative measure of the security and performance of privilege separation; (ii) we propose a graph-based approach to compute the optimal separation based on dynamic information flow analysis; and (iii) the separation process is automatic and does not require expert knowledge of the software.","conference":"IEEE","terms":"Software;Authentication;Educational institutions;Writing;Databases;Performance analysis,data analysis;data flow graphs;program slicing,least privilege components;dynamic data dependency analysis;ProgramCutter;monolithic software automatic partitioning;data dependency graph;open source software;software refactoring;dynamic information flow analysis","keywords":"","startPage":"323","endPage":"333","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693091","citationCount":8,"referenceCount":31,"year":2013,"authors":"Y. Wu; J. Sun; Y. Liu; J. S. Dong","affiliations":"Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Nanyang Technological University, Singapore; School of Computing, National University of Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3827"},"title":"Synthetic data generation for statistical testing","abstract":"Usage-based statistical testing employs knowledge about the actual or anticipated usage profile of the system under test for estimating system reliability. For many systems, usage-based statistical testing involves generating synthetic test data. Such data must possess the same statistical characteristics as the actual data that the system will process during operation. Synthetic test data must further satisfy any logical validity constraints that the actual data is subject to. Targeting data-intensive systems, we propose an approach for generating synthetic test data that is both statistically representative and logically valid. The approach works by first generating a data sample that meets the desired statistical characteristics, without taking into account the logical constraints. Subsequently, the approach tweaks the generated sample to fix any logical constraint violations. The tweaking process is iterative and continuously guided toward achieving the desired statistical characteristics. We report on a realistic evaluation of the approach, where we generate a synthetic population of citizens' records for testing a public administration IT system. Results suggest that our approach is scalable and capable of simultaneously fulfilling the statistical representativeness and logical validity requirements.","conference":"IEEE","terms":"Statistical analysis;Data models;Probabilistic logic;Generators;Unified modeling language;Reliability;Histograms,data analysis;formal logic;program testing;public administration;software reliability;statistical testing,synthetic test data;statistical representativeness;synthetic data generation;system reliability;data-intensive systems;usage profile;usage-based statistical testing;logical constraints;public administration IT system;logical validity requirements","keywords":"Test Data Generation;Usage-based Statistical Testing;Model-Driven Engineering;UML;OCL","startPage":"872","endPage":"882","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115698","citationCount":2,"referenceCount":35,"year":2017,"authors":"G. Soltana; M. Sabetzadeh; L. C. Briand","affiliations":"SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3828"},"title":"Automatically generating commit messages from diffs using neural machine translation","abstract":"Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically \"translate\" diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.","conference":"IEEE","terms":"Algorithm design and analysis;Software;Natural languages;Software algorithms;Machine learning;Computer bugs;Prediction algorithms,configuration management;language translation;program debugging;public domain software;software maintenance,diffs;neural machine translation;NMT;software evolution;automatic commit message generation","keywords":"","startPage":"135","endPage":"146","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115626","citationCount":15,"referenceCount":59,"year":2017,"authors":"S. Jiang; A. Armaly; C. McMillan","affiliations":"Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3829"},"title":"ICoq: Regression proof selection for large-scale verification projects","abstract":"Proof assistants such as Coq are used to construct and check formal proofs in many large-scale verification projects. As proofs grow in number and size, the need for tool support to quickly find failing proofs after revising a project increases. We present a technique for large-scale regression proof selection, suitable for use in continuous integration services, e.g., Travis CI. We instantiate the technique in a tool dubbed ICOQ. ICOQ tracks fine-grained dependencies between Coq definitions, propositions, and proofs, and only checks those proofs affected by changes between two revisions. ICOQ additionally saves time by ignoring changes with no impact on semantics. We applied ICOQ to track dependencies across many revisions in several large Coq projects and measured the time savings compared to proof checking from scratch and when using Coq's timestamp-based toolchain for incremental checking. Our results show that proof checking with ICOQ is up to 10 times faster than the former and up to 3 times faster than the latter.","conference":"IEEE","terms":"Tools;Java;Time measurement;Software;Standards;Semantics,program testing;program verification;project management;regression analysis;theorem proving,Coq projects;proof checking;large-scale verification projects;proof assistants;formal proofs;large-scale regression proof selection;tool dubbed ICOQ;ICOQ additionally;incremental checking","keywords":"","startPage":"171","endPage":"182","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115630","citationCount":1,"referenceCount":64,"year":2017,"authors":"A. Celik; K. Palmskog; M. Gligoric","affiliations":"University of Texas at Austin, Austin, TX-78712, USA; University of Illinois at Urbana-Champaign, Urbana, IL-61801, USA; University of Texas at Austin, Austin, TX-78712, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d382a"},"title":"Automating the Extraction of Model-Based Software Product Lines from Model Variants (T)","abstract":"We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVaPL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVaPL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an In-flight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants We address the problem of automating 1) the analysis of existing similar model variants and 2) migrating them into a software product line. Our approach, named MoVaPL, considers the identification of variability and commonality in model variants, as well as the extraction of a CVL-compliant Model-based Software Product Line (MSPL) from the features identified on these variants. MoVaPL builds on a generic representation of models making it suitable to any MOF-based models. We apply our approach on variants of the open source ArgoUML UML modeling tool as well as on variants of an In-flight Entertainment System. Evaluation with these large and complex case studies contributed to show how our feature identification with structural constraints discovery and the MSPL generation process are implemented to make the approach valid (i.e., the extracted software product line can be used to regenerate all variants considered) and sound (i.e., derived variants which did not exist are at least structurally valid).which did not exist are at least structurally valid).","conference":"IEEE","terms":"Unified modeling language;Feature extraction;Computational modeling;Software product lines;Analytical models;Upper bound;Buildings,public domain software;software product lines;Unified Modeling Language,model variant analysis;MoVaPL approach;CVL-compliant model-based software product lines;common variability language;MOF-based models;open source ArgoUML UML modeling tool;in-flight entertainment system;MSPL generation process","keywords":"software product lines;model driven engineering;reverse engineering;model variants","startPage":"396","endPage":"406","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372028","citationCount":19,"referenceCount":39,"year":2015,"authors":"J. Martinez; T. Ziadi; T. F. Bissyandé; J. Klein; Y. l. Traon","affiliations":"SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; UPMC Univ. Paris 06, Paris, France; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; SnT, Univ. of Luxembourg, Luxembourg City, Luxembourg; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d382b"},"title":"Testing Cross-Platform Mobile App Development Frameworks (T)","abstract":"Mobile app developers often wish to make their apps available on a wide variety of platforms, e.g., Android, iOS, and Windows devices. Each of these platforms uses a different programming environment, each with its own language and APIs for app development. Small app development teams lack the resources and the expertise to build and maintain separate code bases of the app customized for each platform. As a result, we are beginning to see a number of cross-platform mobile app development frameworks. These frameworks allow the app developers to specify the business logic of the app once, using the language and APIs of a home platform (e.g., Windows Phone), and automatically produce versions of the app for multiple target platforms (e.g., iOS and Android). In this paper, we focus on the problem of testing cross-platform app development frameworks. Such frameworks are challenging to develop because they must correctly translate the home platform API to the (possibly disparate) target platform API while providing the same behavior. We develop a differential testing methodology to identify inconsistencies in the way that these frameworks handle the APIs of the home and target platforms. We have built a prototype testing tool, called X-Checker, and have applied it to test Xamarin, a popular framework that allows Windows Phone apps to be cross-compiled into native Android (and iOS) apps. To date, X-Checker has found 47 bugs in Xamarin, corresponding to inconsistencies in the way that Xamarin translates between the semantics of the Windows Phone and the Android APIs. We have reported these bugs to the Xamarin developers, who have already committed patches for twelve of them.","conference":"IEEE","terms":"Mobile communication;Smart phones;Computer bugs;Testing;Software;Libraries,Android (operating system);application program interfaces;mobile computing,cross-platform mobile app development frameworks;mobile app developers;Windows devices;programming environment;app development teams;multiple target platforms;home platform API;differential testing methodology;prototype testing tool;X-Checker;Windows Phone apps;Android apps;iOS apps;Android API;Xamarin developers","keywords":"Mobile apps;Cross-platform;Porting","startPage":"441","endPage":"451","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372032","citationCount":4,"referenceCount":43,"year":2015,"authors":"N. Boushehrinejadmoradi; V. Ganapathy; S. Nagarakatte; L. Iftode","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d382c"},"title":"A demonstration of simultaneous execution and editing in a development environment","abstract":"We introduce a tool within the Code Bubbles development environment that allows for continuous execution as the programmer edits. The tool, SEEDE, shows both the intermediate and final results of execution in terms of variables, control flow, output, and graphics. These results are updated as the user edits. The user can explore the execution to find or fix bugs or use the intermediate values to help write appropriate code. A demonstration video is available at https://www.you-tube.com/watch?v=GpibSxX3Wlw.","conference":"IEEE","terms":"Java;Tools;Debugging;Encoding;Writing;Data structures;Navigation,program debugging;public domain software,SEEDE;control flow;user edits;intermediate values;editing;Code Bubbles development environment;continuous execution;programmer edits","keywords":"Continuous execution;integrated development environments;debugging;live coding","startPage":"895","endPage":"900","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115701","citationCount":0,"referenceCount":36,"year":2017,"authors":"S. P. Reiss; Q. Xin","affiliations":"Department of Computer Science, Brown University, Providence, RI 02912, USA; Department of Computer Science, Brown University, Providence, RI 02912, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d382d"},"title":"Renaming and shifted code in structured merging: Looking ahead for precision and performance","abstract":"Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax-specific lookahead. Our experiments with 48 real-world open-source projects (4,878 merge scenarios with over 400 million lines of code) demonstrate that we can significantly improve matching precision in 28 percent of cases while maintaining performance.","conference":"IEEE","terms":"Merging;Tools;Syntactics;Optimization;Open source software,merging;optimisation;public domain software;software maintenance;trees (mathematics),shifted code;source-code artifacts;software versions;programming language;abstract syntax trees;syntax-specific lookahead;renaming code;structured merging;language-aware structured approach;NP hard problem;syntax-aware heuristic optimization;lookahead mechanism;tree matching","keywords":"","startPage":"543","endPage":"553","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115665","citationCount":3,"referenceCount":30,"year":2017,"authors":"O. Leßenich; S. Apel; C. Kästner; G. Seibt; J. Siegmund","affiliations":"Univ. of Passau, Passau, Germany; University of Passau, Germany; Carnegie Mellon University, USA; University of Passau, Germany; University of Passau, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d382e"},"title":"Using automatically generated invariants for regression testing and bug localization","abstract":"We present Preambl, an approach that applies automatically generated invariants to regression testing and bug localization. Our invariant generation methodology is Precis, an automatic and scalable engine that uses program predicates to guide clustering of dynamically obtained path information. In this paper, we apply it for regression testing and for capturing program predicates information to guide statistical analysis based bug localization. We present a technique to localize bugs in paths of variable lengths. We are able to map the localized post-deployment bugs on a path to pre-release invariants generated along that path. Our experimental results demonstrate the efficacy of the use of PRECIS for regression testing, as well as the ability of Preambl to zone in on relevant segments of program paths.","conference":"IEEE","terms":"Testing;Instruments;Statistical analysis;Computer bugs;Software;Measurement units;Target tracking,program debugging;regression analysis,automatically generated invariants;regression testing;Preambl;invariant generation methodology;scalable engine;path information;statistical analysis based bug localization;localized post deployment bugs;PRECIS","keywords":"","startPage":"634","endPage":"639","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693125","citationCount":0,"referenceCount":17,"year":2013,"authors":"P. Sagdeo; N. Ewalt; D. Pal; S. Vasudevan","affiliations":"University of Illinois at Urbana-Champaign, 61801, USA; University of Illinois at Urbana-Champaign, 61801, USA; University of Illinois at Urbana-Champaign, 61801, USA; University of Illinois at Urbana-Champaign, 61801, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d382f"},"title":"DRIVER -- A Platform for Collaborative Framework Understanding","abstract":"Application frameworks are a powerful technique for large-scale reuse but often very hard to learn from scratch. Although good documentation helps on reducing the learning curve, it is often found lacking, and costly, as it needs to attend different audiences with disparate learning needs. When code and documentation prove insufficient, developers turn to their network of experts. The lack of awareness about the experts, interrupting the wrong people, and experts unavailability are well known hindrances to effective collaboration. This paper presents the DRIVER platform, a collaborative learning environment for framework users to share their knowledge. It provides the documentation on a wiki, where the learning paths of the community of learners can be captured, shared, rated, and recommended, thus tapping into the collective knowledge of the community of framework users. The tool can be obtained at http://bit.ly/driverTool.","conference":"IEEE","terms":"Documentation;Knowledge based systems;Information services;Electronic publishing;Internet;Collaboration;Best practices,computer aided instruction;computer science education;groupware;software reusability,DRIVER;collaborative framework understanding;large-scale reuse;learning curve;learning needs;collaborative learning environment","keywords":"framework;learning;tool;collaborative","startPage":"783","endPage":"788","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372067","citationCount":1,"referenceCount":23,"year":2015,"authors":"N. Flores; A. Aguiar","affiliations":"Fac. of Eng., Dept. of Inf. Eng., Univ. of Porto, Porto, Portugal; Fac. of Eng., Dept. of Inf. Eng., Univ. of Porto, Porto, Portugal","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3830"},"title":"Lazy-CSeq: A Context-Bounded Model Checking Tool for Multi-threaded C-Programs","abstract":"Lazy-CSeq is a context-bounded verification tool for sequentially consistent C programs using POSIX threads. It first translates a multi-threaded C program into a bounded nondeterministic sequential C program that preserves bounded reachability for all round-robin schedules up to a given number of rounds. It then reuses existing high-performance bounded model checkers as sequential verification backends. Lazy-CSeq handles the full C language and the main parts of the POSIX thread API, such as dynamic thread creation and deletion, and synchronization via thread join, locks, and condition variables. It supports assertion checking and deadlock detection, and returns counterexamples in case of errors. Lazy-CSeq outperforms other concurrency verification tools and has won the concurrency category of the last two SV-COMP verification competitions.","conference":"IEEE","terms":"System recovery;Arrays;Concurrent computing;Context;Programming;Schedules,application program interfaces;C language;formal verification;multi-threading,Lazy-CSeq;context-bounded model checking tool;multithreaded C-programs;context-bounded verification tool;bounded nondeterministic sequential C program;bounded reachability;round-robin schedules;sequential verification backends;model checkers;POSIX thread API;assertion checking;deadlock detection;concurrency verification tools;SV-COMP verification competitions","keywords":"C programs;concurrency;multi-thread;multi-threaded;bounded model-checking;context-bounded","startPage":"807","endPage":"812","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372071","citationCount":20,"referenceCount":39,"year":2015,"authors":"O. Inverso; T. L. Nguyen; B. Fischer; S. L. Torre; G. Parlato","affiliations":"Electron. \u0026 Comput. Sci., Univ. of Southampton, Southampton, UK; Electron. \u0026 Comput. Sci., Univ. of Southampton, Southampton, UK; Div. of Comput. Sci., Stellenbosch Univ., Stellenbosch, South Africa; NA; Electron. \u0026 Comput. Sci., Univ. of Southampton, Southampton, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3831"},"title":"Variability-aware performance prediction: A statistical learning approach","abstract":"Configurable software systems allow stakeholders to derive program variants by selecting features. Understanding the correlation between feature selections and performance is important for stakeholders to be able to derive a program variant that meets their requirements. A major challenge in practice is to accurately predict performance based on a small sample of measured variants, especially when features interact. We propose a variability-aware approach to performance prediction via statistical learning. The approach works progressively with random samples, without additional effort to detect feature interactions. Empirical results on six real-world case studies demonstrate an average of 94% prediction accuracy based on small random samples. Furthermore, we investigate why the approach works by a comparative analysis of performance distributions. Finally, we compare our approach to an existing technique and guide users to choose one or the other in practice.","conference":"IEEE","terms":"Predictive models;Feature extraction;Silicon;Software systems;Correlation;Measurement;Accuracy,configuration management;learning (artificial intelligence);software performance evaluation;statistical analysis,variability-aware performance prediction;statistical learning;configurable software systems;program variants","keywords":"","startPage":"301","endPage":"311","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693089","citationCount":36,"referenceCount":27,"year":2013,"authors":"J. Guo; K. Czarnecki; S. Apel; N. Siegmund; A. Wąsowski","affiliations":"University of Waterloo, Canada; University of Waterloo, Canada; University of Passau, Germany; University of Passau, Germany; IT University of Copenhagen, Denmark","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3832"},"title":"Improving bug localization using structured information retrieval","abstract":"Locating bugs is important, difficult, and expensive, particularly for large-scale systems. To address this, natural language information retrieval techniques are increasingly being used to suggest potential faulty source files given bug reports. While these techniques are very scalable, in practice their effectiveness remains low in accurately localizing bugs to a small number of files. Our key insight is that structured information retrieval based on code constructs, such as class and method names, enables more accurate bug localization. We present BLUiR, which embodies this insight, requires only the source code and bug reports, and takes advantage of bug similarity data if available. We build BLUiR on a proven, open source IR toolkit that anyone can use. Our work provides a thorough grounding of IR-based bug localization research in fundamental IR theoretical and empirical knowledge and practice. We evaluate BLUiR on four open source projects with approximately 3,400 bugs. Results show that BLUiR matches or outperforms a current state-of-the-art tool across applications considered, even when BLUiR does not use bug similarity data used by the other tool.","conference":"IEEE","terms":"Computer bugs;Measurement;Accuracy;Information retrieval;Indexing;Java;Mathematical model,information retrieval;natural language processing;program debugging;public domain software,bug localization;structured information retrieval;large-scale systems;natural language information retrieval;code constructs;BLUiR;open source IR toolkit;source code;bug reports;bug similarity data","keywords":"Bug localization;information retrieval;search","startPage":"345","endPage":"355","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693093","citationCount":95,"referenceCount":46,"year":2013,"authors":"R. K. Saha; M. Lease; S. Khurshid; D. E. Perry","affiliations":"Department of Electrical and Computer Engineering, The University of Texas at Austin, USA; School of Information, The University of Texas at Austin, USA; Department of Electrical and Computer Engineering, The University of Texas at Austin, USA; Department of Electrical and Computer Engineering, The University of Texas at Austin, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3833"},"title":"APIBot: Question answering bot for API documentation","abstract":"As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706.","conference":"IEEE","terms":"Documentation;Knowledge discovery;Natural languages;Probabilistic logic;Training;Software engineering;Software,application program interfaces;Java;question answering (information retrieval);software engineering,API documentation;APIBot;question answering bot;application programming interface knowledge;API-related question answering;SiriusQA;intelligent personal assistant;software engineering scenario;domain specific knowledge;Java 8 documentation;Hit@5 score","keywords":"API Documentation;Question Answering;Bot","startPage":"153","endPage":"158","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115628","citationCount":2,"referenceCount":21,"year":2017,"authors":"Y. Tian; F. Thung; A. Sharma; D. Lo","affiliations":"School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3834"},"title":"Proof-based coverage metrics for formal verification","abstract":"When using formal verification on critical software, an important question involves whether we have we specified enough properties for a given implementation model. To address this question, coverage metrics for property-based formal verification have been proposed. Existing metrics are usually based on mutation, where the implementation model is repeatedly modified and re-analyzed to determine whether mutant models are \"killed\" by the property set. These metrics tend to be very expensive to compute, as they involve many additional verification problems. This paper proposes an alternate family of metrics that can be computed using the recently introduced idea of Inductive Validity Cores (IVCs). IVCs determine a minimal set of model elements necessary to establish a proof. One of the proposed metrics is both rigorous and substantially cheaper to compute than mutation-based metrics. In addition, unlike the mutation-based techniques, the design elements marked as necessary by the metric are guaranteed to preserve provability. We demonstrate the metrics on a large corpus of examples.","conference":"IEEE","terms":"Measurement;Computational modeling;Software;Safety;Mathematical model;Testing;Analytical models,formal verification,formal verification;mutant models;additional verification problems;mutation-based techniques;proof-based coverage metrics;critical software;inductive validity cores;IVC","keywords":"Coverage;requirements completeness;formal verification;inductive proofs;inductive validity cores","startPage":"194","endPage":"199","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115632","citationCount":2,"referenceCount":33,"year":2017,"authors":"E. Ghassabani; A. Gacek; M. W. Whalen; M. P. E. Heimdahl; L. Wagner","affiliations":"Department of Computer Science \u0026 Engineering, University of Minnesota, MN, USA; Rockwell Collins, Advanced Technology Center, IA, USA; Department of Computer Science \u0026 Engineering, University of Minnesota, MN, USA; Department of Computer Science \u0026 Engineering, University of Minnesota, MN, USA; Rockwell Collins, Advanced Technology Center, IA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3835"},"title":"Mutation-Based Fault Localization for Real-World Multilingual Programs (T)","abstract":"Programmers maintain and evolve their software in a variety of programming languages to take advantage of various control/data abstractions and legacy libraries. The programming language ecosystem has diversified over the last few decades, and non-trivial programs are likely to be written in more than a single language. Unfortunately, language interfaces such as Java Native Interface and Python/C are difficult to use correctly and the scope of fault localization goes beyond language boundaries, which makes debugging multilingual bugs challenging. To overcome the aforementioned limitations, we propose a mutation-based fault localization technique for real-world multilingual programs. To improve the accuracy of locating multilingual bugs, we have developed and applied new mutation operators as well as conventional mutation operators. The results of the empirical evaluation for six non-trivial real-world multilingual bugs are promising in that the proposed technique identifies the buggy statements as the most suspicious statements for all six bugs.","conference":"IEEE","terms":"Computer bugs;Debugging;Java;Safety;Testing;Libraries;Programming,program debugging;program testing,programming languages;control abstraction;legacy libraries;programming language ecosystem;fault localization;multilingual bug debugging;mutation-based fault localization technique;real-world multilingual programs;multilingual bug location;mutation operators;empirical evaluation;nontrivial real-world multilingual bugs;buggy statement identification;suspicious statement identification;data abstraction","keywords":"Multilingual Programs;Debugging;Fault Localization;Mutation Analysis;Mutation Based Fault Localization","startPage":"464","endPage":"475","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372034","citationCount":15,"referenceCount":51,"year":2015,"authors":"S. Hong; B. Lee; T. Kwak; Y. Jeon; B. Ko; Y. Kim; M. Kim","affiliations":"KAIST, Daejeon, South Korea; GIST, Gwangju, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; GIST, Gwangju, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3836"},"title":"ModelWriter: Text and model-synchronized document engineering platform","abstract":"The ModelWriter platform provides a generic framework for automated traceability analysis. In this paper, we demonstrate how this framework can be used to trace the consistency and completeness of technical documents that consist of a set of System Installation Design Principles used by Airbus to ensure the correctness of aircraft system installation. We show in particular, how the platform allows the integration of two types of reasoning: reasoning about the meaning of text using semantic parsing and description logic theorem proving; and reasoning about document structure using first-order relational logic and finite model finding for traceability analysis.","conference":"IEEE","terms":"Semantics;Grammar;Cognition;Atmospheric modeling;Hydraulic systems;Analytical models;Tools,aerospace engineering;document handling;formal logic;inference mechanisms;program diagnostics;text analysis;theorem proving,Airbus;aircraft system installation;semantic parsing;document structure;finite model;document engineering platform;ModelWriter platform;automated traceability analysis;description logic theorem;system installation design principles","keywords":"","startPage":"907","endPage":"912","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115703","citationCount":1,"referenceCount":32,"year":2017,"authors":"F. Erata; C. Gardent; B. Gyawali; A. Shimorina; Y. Lussaud; B. Tekinerdogan; G. Kardas; A. Monceaux","affiliations":"Information Technology Group, Wageningen University and Research Centre, The Netherlands; CNRS, LORIA, UMR 7503 Vandoeuvre-les-Nancy, F-54500, Nancy, France; CNRS, LORIA, UMR 7503 Vandoeuvre-les-Nancy, F-54500, Nancy, France; CNRS, LORIA, UMR 7503 Vandoeuvre-les-Nancy, F-54500, Nancy, France; System Engineering Platforms, Airbus Group Innovations, Toulouse, France; Information Technology Group, Wageningen University and Research Centre, The Netherlands; Ege University, International Computer Institute, Izmir, Turkey; UNIT Information Technologies R\u0026D Ltd., Izmir, Turkey","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3837"},"title":"Detecting unknown inconsistencies in web applications","abstract":"Although there has been increasing demand for more reliable web applications, JavaScript bugs abound in web applications. In response to this issue, researchers have proposed automated fault detection tools, which statically analyze the web application code to find bugs. While useful, these tools either only target a limited set of bugs based on predefined rules, or they do not detect bugs caused by cross-language interactions, which occur frequently in web application code. To address this problem, we present an anomaly-based inconsistency detection approach, implemented in a tool called HOLOCRON. The main novelty of our approach is that it does not look for hard-coded inconsistency classes. Instead, it applies subtree pattern matching to infer inconsistency classes and association rule mining to detect inconsistencies that occur both within a single language, and between two languages. We evaluated HOLOCRON, and it successfully detected 51 previously unreported inconsistencies - including 18 bugs and 33 code smells - in 12 web applications.","conference":"IEEE","terms":"Computer bugs;Tools;Encoding;Transforms;Computer languages;Testing,data mining;fault diagnosis;program debugging;program diagnostics,JavaScript bugs;fault detection tools;inconsistency detection approach;hard-coded inconsistency classes;association rule mining;Web application code;unreported inconsistencies","keywords":"JavaScript;fault detection;cross-language interactions","startPage":"566","endPage":"577","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115667","citationCount":1,"referenceCount":55,"year":2017,"authors":"F. S. Ocariza; K. Pattabiraman; A. Mesbah","affiliations":"University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3838"},"title":"Rethinking pointer reasoning in symbolic execution","abstract":"Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors concretize symbolic addresses that span memory intervals larger than some threshold. Unfortunately, this could result in missing interesting execution states, e.g., where a bug arises. In this paper we introduce MEMSIGHT, a new approach to symbolic memory that reduces the need for concretization, hence offering the opportunity for broader state explorations and more precise pointer reasoning. Rather than mapping address instances to data as previous tools do, our technique maps symbolic address expressions to data, maintaining the possible alternative states resulting from the memory referenced by a symbolic address in a compact, implicit form. A preliminary experimental investigation on prominent benchmarks from the DARPA Cyber Grand Challenge shows that MemSight enables the exploration of states unreachable by previous techniques.","conference":"IEEE","terms":"Concrete;Weapons;Cognition;Indexes;Merging;Load modeling;Computer bugs,program debugging;program diagnostics;program testing;program verification,symbolic execution;symbolic executor;memory access;span memory intervals;symbolic memory;address instances;bugs;address space;execution states;program analysis technique;pointer reasoning;symbolic address expressions","keywords":"","startPage":"613","endPage":"618","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115671","citationCount":1,"referenceCount":12,"year":2017,"authors":"E. Coppa; D. C. D'Elia; C. Demetrescu","affiliations":"Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Italy; Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Italy; Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3839"},"title":"ExPort: Detecting and visualizing API usages in large source code repositories","abstract":"This paper presents a technique for automatically mining and visualizing API usage examples. In contrast to previous approaches, our technique is capable of finding examples of API usage that occur across several functions in a program. This distinction is important because of a gap between what current API learning tools provide and what programmers need: current tools extract relatively small examples from single files/functions, even though programmers use APIs to build large software. The small examples are helpful in the initial stages of API learning, but leave out details that are helpful in later stages. Our technique is intended to fill this gap. It works by representing software as a Relational Topic Model, where API calls and the functions that use them are modeled as a document network. Given a starting API, our approach can recommend complex API usage examples mined from a repository of over 14 million Java methods.","conference":"IEEE","terms":"Visualization;Software;Java;Portfolios;Databases;Prototypes;Concrete,application program interfaces;data mining;learning (artificial intelligence);program visualisation;source code (software),ExPort;API usage visualization;API usage detection;large source code repositories;automatic API usage mining;API learning tools;relational topic model;document network;Java methods","keywords":"API usage;visualization;call graph;code search","startPage":"646","endPage":"651","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693127","citationCount":26,"referenceCount":27,"year":2013,"authors":"E. Moritz; M. Linares-Vásquez; D. Poshyvanyk; M. Grechanik; C. McMillan; M. Gethers","affiliations":"The College of William and Mary, Williamsburg, VA, USA; The College of William and Mary, Williamsburg, VA, USA; The College of William and Mary, Williamsburg, VA, USA; University of Illinois at Chicago, USA; University of Notre Dame, IN, USA; University of Maryland Baltimore County, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d383a"},"title":"Assessing the maturity of requirements through argumentation: A good enough approach","abstract":"Requirements engineers need to be confident that enough requirements analysis has been done before a project can move forward. In the context of KAOS, this information can be derived from the soundness of the refinements: sound refinements indicate that the requirements in the goal-graph are mature enough or good enough for implementation. We can estimate how close we are to `good enough' requirements using the judgments of experts and other data from the goals. We apply Toulmin's model of argumentation to evaluate how sound refinements are. We then implement the resulting argumentation model using Bayesian Belief Networks and provide a semi-automated way aided by Natural Language Processing techniques to carry out the proposed evaluation. We have performed an initial validation on our work using a small case-study involving an electronic document management system.","conference":"IEEE","terms":"Natural language processing;Context;Bayes methods;Probability distribution;Educational institutions;Tagging;Reliability,Bayes methods;document handling;formal verification;natural language processing,requirements engineers;requirements analysis;KAOS context;sound refinements;goal graph;Bayesian belief networks;natural language processing techniques;electronic document management system","keywords":"Soundness of refinements;Bayesian belief networks;maturity of requirements","startPage":"670","endPage":"675","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693131","citationCount":1,"referenceCount":32,"year":2013,"authors":"V. Veerappa; R. Harrison","affiliations":"Oxford Brookes University, UK; Oxford Brookes University, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d383b"},"title":"Recommending API Usages for Mobile Apps with Hidden Markov Model","abstract":"Mobile apps often rely heavily on standard API frameworks and libraries. However, learning to use those APIs is often challenging due to the fast-changing nature of API frameworks and the insufficiency of documentation and code examples. This paper introduces DroidAssist, a recommendation tool for API usages of Android mobile apps. The core of DroidAssist is HAPI, a statistical, generative model of API usages based on Hidden Markov Model. With HAPIs trained from existing mobile apps, DroidAssist could perform code completion for method calls. It can also check existing call sequences to detect and repair suspicious (i.e. unpopular) API usages.","conference":"IEEE","terms":"Hidden Markov models;Androids;Humanoid robots;Mobile communication;Data mining;Maintenance engineering;Documentation,application program interfaces;hidden Markov models;mobile computing;recommender systems;statistical analysis,API usage recommendation;hidden Markov model;DroidAssist recommendation tool;Android mobile applications;HAPI;statistical generative model;code completion;method calls;call sequences;suspicious API usage detection;suspicious API usage repair","keywords":"Statistical code completion;API usage","startPage":"795","endPage":"800","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372069","citationCount":13,"referenceCount":22,"year":2015,"authors":"T. T. Nguyen; H. V. Pham; P. M. Vu; T. T. Nguyen","affiliations":"Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA; Comput. Sci. Dept., Utah State Univ., Logan, UT, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d383c"},"title":"Detecting and characterizing semantic inconsistencies in ported code","abstract":"Adding similar features and bug fixes often requires porting program patches from reference implementations and adapting them to target implementations. Porting errors may result from faulty adaptations or inconsistent updates. This paper investigates (1) the types of porting errors found in practice, and (2) how to detect and characterize potential porting errors. Analyzing version histories, we define five categories of porting errors, including incorrect control- and data-flow, code redundancy, inconsistent identifier renamings, etc. Leveraging this categorization, we design a static control- and data-dependence analysis technique, SPA, to detect and characterize porting inconsistencies. Our evaluation on code from four open-source projects shows that SPA can detect porting inconsistencies with 65% to 73% precision and 90% recall, and identify inconsistency types with 58% to 63% precision and 92% to 100% recall. In a comparison with two existing error detection tools, SPA improves precision by 14 to 17 percentage points.","conference":"IEEE","terms":"Context;Linux;Semantics;Cloning;Syntactics;OFDM;History,error detection;program debugging;program diagnostics;public domain software,semantic inconsistency detection;semantic inconsistency characterization;ported code;bug fixes;porting errors;incorrect control;data-flow;code redundancy;inconsistent identifier renamings;static control;data-dependence analysis technique;open-source projects;error detection tools","keywords":"","startPage":"367","endPage":"377","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693095","citationCount":14,"referenceCount":22,"year":2013,"authors":"B. Ray; M. Kim; S. Person; N. Rungta","affiliations":"The University of Texas at Austin, USA; The University of Texas at Austin, USA; NASA Langley Research Center, Hampton, USA; NASA Ames Research Center, Mountain View, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d383d"},"title":"Synergizing Specification Miners through Model Fissions and Fusions (T)","abstract":"Software systems are often developed and released without formal specifications. For those systems that are formally specified, developers have to continuously maintain and update the specifications or have them fall out of date. To deal with the absence of formal specifications, researchers have proposed techniques to infer the missing specifications of an implementation in a variety of forms, such as finite state automaton (FSA). Despite the progress in this area, the efficacy of the proposed specification miners needs to improve if these miners are to be adopted. We propose SpecForge, a new specification mining approach that synergizes many existing specification miners. SpecForge decomposes FSAs that are inferred by existing miners into simple constraints, through a process we refer to as model fission. It then filters the outlier constraints and fuses the constraints back together into a single FSA (i.e., model fusion). We have evaluated SpecForge on execution traces of 10 programs, which includes 5 programs from DaCapo benchmark, to infer behavioral models of 13 library classes. Our results show that SpecForge achieves an average precision, recall and F-measure of 90.57%, 54.58%, and 64.21% respectively. SpecForge outperforms the best performing baseline by 13.75% in terms of F-measure.","conference":"IEEE","terms":"Inference algorithms;Automata;Libraries;Benchmark testing;Software;Manuals;Software engineering,data mining;finite state machines;formal specification,specification miners;model fissions;model fusions;software systems;formal specifications;specification maintenance;finite state automaton;FSA;SpecForge;specification mining approach;outlier constraints;DaCapo benchmark;behavioral models;F-measure","keywords":"Specification Mining;Synergizing Miners;Model Fission;Model Fusion","startPage":"115","endPage":"125","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372001","citationCount":16,"referenceCount":37,"year":2015,"authors":"T. B. Le; X. D. Le; D. Lo; I. Beschastnikh","affiliations":"Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Sch. of Inf. Syst., Singapore Manage. Univ., Singapore, Singapore; Dept. of Comput. Sci., Univ. of British Columbia, Vancouver, BC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d383e"},"title":"Modular verification of interrupt-driven software","abstract":"Interrupts have been widely used in safety-critical computer systems to handle outside stimuli and interact with the hardware, but reasoning about interrupt-driven software remains a difficult task. Although a number of static verification techniques have been proposed for interrupt-driven software, they often rely on constructing a monolithic verification model. Furthermore, they do not precisely capture the complete execution semantics of interrupts such as nested invocations of interrupt handlers. To overcome these limitations, we propose an abstract interpretation framework for static verification of interrupt-driven software that first analyzes each interrupt handler in isolation as if it were a sequential program, and then propagates the result to other interrupt handlers. This iterative process continues until results from all interrupt handlers reach a fixed point. Since our method never constructs the global model, it avoids the up-front blowup in model construction that hampers existing, non-modular, verification techniques. We have evaluated our method on 35 interrupt-driven applications with a total of 22,541 lines of code. Our results show the method is able to quickly and more accurately analyze the behavior of interrupts.","conference":"IEEE","terms":"Tools;Computer bugs;Model checking;Semantics;Instruction sets,formal verification;interrupts;program diagnostics;program verification,interrupt-driven software;static verification techniques;interrupt handlers;interrupt-driven applications;safety-critical computer systems","keywords":"","startPage":"206","endPage":"216","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115634","citationCount":1,"referenceCount":45,"year":2017,"authors":"C. Sung; M. Kusano; C. Wang","affiliations":"University of Southern, California Los Angeles, CA, USA; Virginia Tech Blacksburg, VA, USA; University of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d383f"},"title":"Fuzzing the Rust Typechecker Using CLP (T)","abstract":"Language fuzzing is a bug-finding technique for testing compilers and interpreters, its effectiveness depends upon the ability to automatically generate valid programs in the language under test. Despite the proven success of language fuzzing, there is a severe lack of tool support for fuzzing statically-typed languages with advanced type systems because existing fuzzing techniques cannot effectively and automatically generate well-typed programs that use sophisticated types. In this work we describe how to automatically generate well-typed programs that use sophisticated type systems by phrasing the problem of well-typed program generation in terms of Constraint Logic Programming (CLP). In addition, we describe how to specifically target the typechecker implementation for testing, unlike all existing work which ignores the typechecker. We focus on typechecker precision bugs, soundness bugs, and consistency bugs. We apply our techniques to Rust, a complex, industrial-strength language with a sophisticated type system.","conference":"IEEE","terms":"Testing;Computer bugs;Grammar;Java;Engines;Logic programming;Syntactics,logic programming;program debugging;program testing,rust typechecker;language fuzzing;CLP;constraint logic programming;bug-finding technique;compiler testing;interpreter testing;well-typed program generation;typechecker precision bugs;soundness bugs;consistency bugs;Rust language","keywords":"","startPage":"482","endPage":"493","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372036","citationCount":5,"referenceCount":61,"year":2015,"authors":"K. Dewey; J. Roesch; B. Hardekopf","affiliations":"Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Univ. of California, Santa Barbara, Santa Barbara, CA, USA; Univ. of California, Santa Barbara, Santa Barbara, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3840"},"title":"Optimistic Shared Memory Dependence Tracing (T)","abstract":"Inter-thread shared memory dependences are crucial to understanding the behavior of concurrent systems, as such dependences are the cornerstone of time-travel debugging and further predictive trace analyses. To enable effective and efficient shared memory dependence tracing, we present an optimistic scheme addressing the challenge of capturing exact dependences between unsynchronized events to reduce the probe effect of program instrumentation. Specifically, our approach achieved a wait-free fast path for thread-local reads on x86-TSO relaxed memory systems, and simultaneously achieved precise tracing of exact read-after-write, write-after-write and write-after-read dependences on the fly. We implemented an open-source RWTrace tool, and evaluation results show that our approach not only achieves efficient shared memory dependence tracing, but also scales well on a multi-core computer system.","conference":"IEEE","terms":"Instruction sets;Memory management;Synchronization;Algorithm design and analysis;Benchmark testing;Computer science;Debugging,program debugging;shared memory systems,optimistic shared memory dependence tracing;interthread shared memory dependence;time-travel debugging;predictive trace analysis;program instrumentation;x86-TSO relaxed memory system;open-source RWTrace tool;multicore computer system","keywords":"concurrency;shared memory dependence;dynamic analysis","startPage":"524","endPage":"534","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372040","citationCount":2,"referenceCount":35,"year":2015,"authors":"Y. Jiang; D. Li; C. Xu; X. Ma; J. Lu","affiliations":"Dept. of Comput. Sci. \u0026 Technol., Nanjing Univ., Nanjing, China; Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA; Dept. of Comput. Sci. \u0026 Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci. \u0026 Technol., Nanjing Univ., Nanjing, China; Dept. of Comput. Sci. \u0026 Technol., Nanjing Univ., Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3841"},"title":"DSSynth: An automated digital controller synthesis tool for physical plants","abstract":"We present an automated MATLAB Toolbox, named DSSynth (Digital-System Synthesizer), to synthesize sound digital controllers for physical plants that are represented as linear timeinvariant systems with single input and output. In particular, DSSynth synthesizes digital controllers that are sound w.r.t. stability and safety specifications. DSSynth considers the complete range of approximations, including time discretization, quantization effects and finite-precision arithmetic (and its rounding errors). We demonstrate the practical value of this toolbox by automatically synthesizing stable and safe controllers for intricate physical plant models from the digital control literature. The resulting toolbox enables the application of program synthesis to real-world control engineering problems. A demonstration can be found at https://youtu.be_hLQslRcee8.","conference":"IEEE","terms":"MATLAB;Mathematical model;Transfer functions;Tools;Digital control;Engines,control engineering computing;control system synthesis;digital control;linear systems;stability,automated digital controller synthesis tool;physical plants;linear timeinvariant systems;intricate physical plant models;digital control literature;real-world control engineering problems;automated MATLAB toolbox;digital-system synthesizer;DSSynth;sound digital controller synthesis","keywords":"Formal Synthesis;Digital Control Systems;MATLAB Toolbox;Finite-Word Length;Verification","startPage":"919","endPage":"924","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115705","citationCount":0,"referenceCount":30,"year":2017,"authors":"A. Abate; I. Bessa; D. Cattaruzza; L. Chaves; L. Cordeiro; C. David; P. Kesseli; D. Kroening; E. Polgreen","affiliations":"University of Oxford, Oxford, United Kingdom; Federal University of Amazonas, Manaus, Brazil; University of Oxford, Oxford, United Kingdom; Federal University of Amazonas, Manaus, Brazil; University of Oxford, Oxford, United Kingdom; University of Oxford, Oxford, United Kingdom; University of Oxford, Oxford, United Kingdom; University of Oxford, Oxford, United Kingdom; University of Oxford, Oxford, United Kingdom","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3842"},"title":"Constraint-based automatic symmetry detection","abstract":"We present an automatic approach to detecting symmetry relations for general concurrent models. Despite the success of symmetry reduction in mitigating state explosion problem, one essential step towards its soundness and effectiveness, i.e., how to discover sufficient symmetries with least human efforts, is often either overlooked or oversimplified. In this work, we show how a concurrent model can be viewed as a constraint satisfaction problem (CSP), and present an algorithm capable of detecting symmetries arising from the CSP which induce automorphisms of the model. To the best of our knowledge, our method is the first approach that can automatically detect both process and data symmetries as demonstrated via a number of systems.","conference":"IEEE","terms":"Arrays;Protocols;Color;Space exploration;Transforms;Cost accounting;Lead,concurrency theory;constraint satisfaction problems,constraint-based automatic symmetry detection;general concurrent models;symmetry reduction;state explosion problem;human efforts;constraint satisfaction problem;CSP;automorphisms;data symmetries","keywords":"","startPage":"15","endPage":"25","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693062","citationCount":1,"referenceCount":37,"year":2013,"authors":"S. J. Zhang; J. Sun; C. Sun; Y. Liu; J. Ma; J. S. Dong","affiliations":"Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; National University of Singapore, Singapore; Nanyang Technological University, Singapore; Singapore University of Technology and Design, Singapore; National University of Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3843"},"title":"Automatic testing of symbolic execution engines via program generation and differential testing","abstract":"Symbolic execution has attracted significant attention in recent years, with applications in software testing, security, networking and more. Symbolic execution tools, like CREST, KLEE, FuzzBALL, and Symbolic PathFinder, have enabled researchers and practitioners to experiment with new ideas, scale the technique to larger applications and apply it to new application domains. Therefore, the correctness of these tools is of critical importance. In this paper, we present our experience extending compiler testing techniques to find errors in both the concrete and symbolic execution components of symbolic execution engines. The approach used relies on a novel way to create program versions, in three different testing modes-concrete, single-path and multi-path-each exercising different features of symbolic execution engines. When combined with existing program generation techniques and appropriate oracles, this approach enables differential testing within a single symbolic execution engine. We have applied our approach to the KLEE, CREST and FuzzBALL symbolic execution engines, where it has discovered 20 different bugs exposing a variety of important errors having to do with the handling of structures, division, modulo, casting, vector instructions and more, as well as issues related to constraint solving, compiler optimisations and test input replay.","conference":"IEEE","terms":"Testing;Engines;Tools;Computer bugs;Concrete;Program processors;Instruments,optimising compilers;program diagnostics;program testing;program verification,differential testing;single symbolic execution engine;test input replay;automatic testing;software testing;Symbolic PathFinder;symbolic execution components;symbolic execution tools;program generation techniques;compiler testing techniques;concrete testing mode;single-path testing mode;multipath testing mode;KLEE execution engine;CREST symbolic execution engine;FuzzBALL symbolic execution engine;compiler optimisations;constraint solving","keywords":"","startPage":"590","endPage":"600","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115669","citationCount":1,"referenceCount":34,"year":2017,"authors":"T. Kapus; C. Cadar","affiliations":"Imperial College London, United Kingdom; Imperial College London, United Kingdom","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3844"},"title":"Tortoise: Interactive system configuration repair","abstract":"System configuration languages provide powerful abstractions that simplify managing large-scale, networked systems. Thousands of organizations now use configuration languages, such as Puppet. However, specifications written in configuration languages can have bugs and the shell remains the simplest way to debug a misconfigured system. Unfortunately, it is unsafe to use the shell to fix problems when a system configuration language is in use: a fix applied from the shell may cause the system to drift from the state specified by the configuration language. Thus, despite their advantages, configuration languages force system administrators to give up the simplicity and familiarity of the shell. This paper presents a synthesis-based technique that allows administrators to use configuration languages and the shell in harmony. Administrators can fix errors using the shell and the technique automatically repairs the higher-level specification written in the configuration language. The approach (1) produces repairs that are consistent with the fix made using the shell; (2) produces repairs that are maintainable by minimizing edits made to the original specification; (3) ranks and presents multiple repairs when relevant; and (4) supports all shells the administrator may wish to use. We implement our technique for Puppet, a widely used system configuration language, and evaluate it on a suite of benchmarks under 42 repair scenarios. The top-ranked repair is selected by humans 76% of the time and the human-equivalent repair is ranked 1.31 on average.","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;Web servers;Organizations;Benchmark testing,formal specification;large-scale systems;program debugging;specification languages,configuration languages force system administrators;system configuration language;interactive system configuration repair;misconfigured system;large-scale networked systems;synthesis-based technique;Puppet;human-equivalent repair;Tortoise","keywords":"","startPage":"625","endPage":"636","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115673","citationCount":6,"referenceCount":63,"year":2017,"authors":"A. Weiss; A. Guha; Y. Brun","affiliations":"Northeastern University, Boston, MA, USA 02115; University of Massachusetts, Amherst, Amherst, MA, USA 01003; University of Massachusetts, Amherst, Amherst, MA, USA 01003","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3845"},"title":"A pattern-based approach to parametric specification mining","abstract":"This paper presents a technique for using execution traces to mine parametric temporal specifications in the form of quantified event automata (QEA) - previously introduced as an expressive and efficient formalism for runtime verification. We consider a pattern-based mining approach that uses a pattern library to generate and check potential properties over given traces, and then combines successful patterns. By using predefined models to measure the tool's precision and recall we demonstrate that our approach can effectively and efficiently extract specifications in realistic scenarios.","conference":"IEEE","terms":"Libraries;Automata;Training;Satellites;Runtime;Pattern matching;Complexity theory,automata theory;data mining;formal specification;program verification;software libraries,pattern-based approach;parametric specification mining;execution traces;parametric temporal specifications;quantified event automata;QEA;runtime verification;pattern-based mining approach;pattern library","keywords":"","startPage":"658","endPage":"663","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693129","citationCount":13,"referenceCount":18,"year":2013,"authors":"G. Reger; H. Barringer; D. Rydeheard","affiliations":"University of Manchester, United Kingdom; University of Manchester, United Kingdom; University of Manchester, United Kingdom","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3846"},"title":"Model/code co-refactoring: An MDE approach","abstract":"Model-driven engineering suggests that models are the primary artefacts of software development. This means that models may be refactored even after code has been generated from them, in which case the code must be changed to reflect the refactoring. However, as we show neither re-generating the code from the refactored model nor applying an equivalent refactoring to the generated code is sufficient to keep model and code in sync - rather, model and code need to be refactored jointly. To enable this, we investigate the technical requirements of model/code co-refactoring, and implement a model-driven solution that we evaluate using a set of open-source programs and their structural models. Results suggest that our approach is feasible.","conference":"IEEE","terms":"Unified modeling language;Java;DSL;Adaptation models;Biological system modeling;Synchronization,program compilers;software engineering,code corefactoring;MDE;model driven engineering;software development;code regeneration;refactored model;equivalent refactoring;model driven solution;open source programs;structural models","keywords":"Model-driven engineering;refactoring;constraints","startPage":"682","endPage":"687","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693133","citationCount":7,"referenceCount":19,"year":2013,"authors":"J. von Pilgrim; B. Ulke; A. Thies; F. Steimann","affiliations":"Lehrgebiet Programmiersysteme, Fernuniversität in Hagen, Germany; Lehrgebiet Programmiersysteme, Fernuniversität in Hagen, Germany; Lehrgebiet Programmiersysteme, Fernuniversität in Hagen, Germany; Lehrgebiet Programmiersysteme, Fernuniversität in Hagen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3847"},"title":"CIVL: Formal Verification of Parallel Programs","abstract":"CIVL is a framework for static analysis and verification of concurrent programs. One of the main challenges to practical application of these techniques is the large number of ways to express concurrency: MPI, OpenMP, CUDA, and Pthreads, for example, are just a few of many \"concurrency dialects\" in wide use today. These dialects are constantly evolving and it is increasingly common to use several of them in a single \"hybrid\" program. CIVL addresses these problems by providing a concurrency intermediate verification language, CIVL-C, as well as translators that consume C programs using these dialects and produce CIVL-C. Analysis and verification tools which operate on CIVL-C can then be applied easily to a wide variety of concurrent C programs. We demonstrate CIVL's error detection and verification capabilities on (1) an MPI+OpenMP program that estimates π and contains a subtle race condition, and (2) an MPI-based 1d-wave simulator that fails to conform to a simple sequential implementation.","conference":"IEEE","terms":"Concurrent computing;Libraries;Standards;Graphics processing units;Government;Electronic mail;Maintenance engineering,C language;parallel programming;program diagnostics;program verification,CIVL;formal verification;parallel program;static analysis;concurrent program;CUDA;Pthread;hybrid program;concurrency intermediate verification language;C program;MPI-plus-OpenMP program;MPI-based 1d-wave simulator","keywords":"software verification;parallel programs;MPI;OpenMP;function equivalence;symbolic execution;model checking","startPage":"830","endPage":"835","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372075","citationCount":8,"referenceCount":34,"year":2015,"authors":"M. Zheng; M. S. Rogers; Z. Luo; M. B. Dwyer; S. F. Siegel","affiliations":"Dept. of Comput. \u0026 Inf. Sci., Univ. of Delaware, Newark, DE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska - Lincoln, Lincoln, NE, USA; Dept. of Comput. \u0026 Inf. Sci., Univ. of Delaware, Newark, DE, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Nebraska - Lincoln, Lincoln, NE, USA; Dept. of Comput. \u0026 Inf. Sci., Univ. of Delaware, Newark, DE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3848"},"title":"Characterizing and detecting resource leaks in Android applications","abstract":"Android phones come with a host of hardware components embedded in them, such as Camera, Media Player and Sensor. Most of these components are exclusive resources or resources consuming more memory/energy than general. And they should be explicitly released by developers. Missing release operations of these resources might cause serious problems such as performance degradation or system crash. These kinds of defects are called resource leaks. This paper focuses on resource leak problems in Android apps, and presents our lightweight static analysis tool called Relda, which can automatically analyze an application's resource operations and locate the resource leaks. We propose an automatic method for detecting resource leaks based on a modified Function Call Graph, which handles the features of event-driven mobile programming by analyzing the callbacks defined in Android framework. Our experimental data shows that Relda is effective in detecting resource leaks in real Android apps.","conference":"IEEE","terms":"Androids;Humanoid robots;Cameras;Smart phones;Media;Java;Mobile communication,Android (operating system);mobile computing;program diagnostics;resource allocation,resource leaks characterization;resource leaks detection;Android applications;camera component;media player component;sensor component;resource leak problems;Relda tool;lightweight static analysis tool;application resource operations;callbacks;event-driven mobile programming;function call graph","keywords":"Android apps;resource leak;static analysis","startPage":"389","endPage":"398","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693097","citationCount":36,"referenceCount":49,"year":2013,"authors":"C. Guo; J. Zhang; J. Yan; Z. Zhang; Y. Zhang","affiliations":"State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China; Technology Center of Software Engineering Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3849"},"title":"Dynamically Testing GUIs Using Ant Colony Optimization (T)","abstract":"In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Ant colony optimization;Generators;Context;Heuristic algorithms;Computer architecture,ant colony optimisation;graphical user interfaces;learning (artificial intelligence);program testing,dynamic GUI test generator;ant colony optimization algorithm;AntQ;Q-Learning;behavioral reinforcement learning technique;fitness function;code coverage;fault finding abilities","keywords":"","startPage":"138","endPage":"148","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372003","citationCount":5,"referenceCount":38,"year":2015,"authors":"S. Carino; J. H. Andrews","affiliations":"Dept. of Comput. Sci., Univ. of Western Ontario, London, ON, Canada; Dept. of Comput. Sci., Univ. of Western Ontario, London, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d384a"},"title":"Static detection of asymptotic resource side-channel vulnerabilities in web applications","abstract":"Web applications can leak confidential user information due to the presence of unintended side-channel vulnerabilities in code. One particularly subtle class of side-channel vulnerabilities arises due to resource usage imbalances along different execution paths of a program. Such side-channel vulnerabilities are especially severe if the resource usage imbalance is asymptotic. This paper formalizes the notion of asymptotic resource side-channels and presents a lightweight static analysis algorithm for automatically detecting them. Based on these ideas, we have developed a tool called SCANNER that detects resource-related side-channel vulnerabilities in PHP applications. SCANNER has found 18 zero-day security vulnerabilities in 10 different web applications and reports only 2 false positives. The vulnerabilities uncovered by SCANNER can be exploited using cross-site search attacks to extract various kinds of confidential information, such as a user's medications or purchase history.","conference":"IEEE","terms":"Security;Timing;Databases;Algorithm design and analysis;Tools;Time factors,cryptography;Internet;program diagnostics,resource usage imbalance;asymptotic resource side-channels;SCANNER;PHP applications;zero-day security vulnerabilities;static detection;asymptotic resource side-channel vulnerabilities;confidential user information;unintended side-channel vulnerabilities;execution paths;Web applications;lightweight static analysis algorithm","keywords":"","startPage":"229","endPage":"239","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115636","citationCount":0,"referenceCount":48,"year":2017,"authors":"J. Chen; O. Olivo; I. Dillig; C. Lin","affiliations":"The University of Texas at Austin, United States; The University of Texas at Austin, United States; The University of Texas at Austin, United States; The University of Texas at Austin, United States","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d384b"},"title":"Automatically assessing crashes from heap overflows","abstract":"Heap overflow is one of the most widely exploited vulnerabilities, with a large number of heap overflow instances reported every year. It is important to decide whether a crash caused by heap overflow can be turned into an exploit. Efficient and effective assessment of exploitability of crashes facilitates to identify severe vulnerabilities and thus prioritize resources. In this paper, we propose the first metrics to assess heap overflow crashes based on both the attack aspect and the feasibility aspect. We further present HCSIFTER, a novel solution to automatically assess the exploitability of heap overflow instances under our metrics. Given a heap-based crash, HCSIFTER accurately detects heap overflows through dynamic execution without any source code or debugging information. Then it uses several novel methods to extract program execution information needed to quantify the severity of the heap overflow using our metrics. We have implemented a prototype HCSIFTER and applied it to assess nine programs with heap overflow vulnerabilities. HCSIFTER successfully reports that five heap overflow vulnerabilities are highly exploitable and two overflow vulnerabilities are unlikely exploitable. It also gave quantitatively assessments for other two programs. On average, it only takes about two minutes to assess one heap overflow crash. The evaluation result demonstrates both effectiveness and efficiency of HC Sifter.","conference":"IEEE","terms":"Computer crashes;Measurement;Tools;Payloads;Indexes;Data mining;Layout,program debugging;program diagnostics;security of data,HCSIFTER;metrics;heap overflow vulnerabilities;heap overflow crash;HC Sifter;widely exploited vulnerabilities;crashes facilitates","keywords":"Memory error;Heap overflow;Vulnerability assessment","startPage":"274","endPage":"279","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115640","citationCount":0,"referenceCount":25,"year":2017,"authors":"L. He; Y. Cai; H. Hu; P. Su; Z. Liang; Y. Yang; H. Huang; J. Yan; X. Jia; D. Feng","affiliations":"Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, National University of Singapore; Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Trusted Computing and Information Assurance Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d384c"},"title":"Development Emails Content Analyzer: Intention Mining in Developer Discussions (T)","abstract":"Written development communication (e.g. mailing lists, issue trackers) constitutes a precious source of information to build recommenders for software engineers, for example aimed at suggesting experts, or at redocumenting existing source code. In this paper we propose a novel, semi-supervised approach named DECA (Development Emails Content Analyzer) that uses Natural Language Parsing to classify the content of development emails according to their purpose (e.g. feature request, opinion asking, problem discovery, solution proposal, information giving etc), identifying email elements that can be used for specific tasks. A study based on data from Qt and Ubuntu, highlights a high precision (90%) and recall (70%) of DECA in classifying email content, outperforming traditional machine learning strategies. Moreover, we successfully used DECA for re-documenting source code of Eclipse and Lucene, improving the recall, while keeping high precision, of a previous approach based on ad-hoc heuristics.","conference":"IEEE","terms":"Electronic mail;Proposals;Natural languages;Taxonomy;Bandwidth;Pragmatics;Software,data mining;electronic mail;grammars;learning (artificial intelligence);natural language processing;pattern classification;program compilers;recommender systems;software engineering;source code (software),development emails content analyzer;DECA;intention mining;software development;semisupervised approach;natural language parsing;content classification;source code;recommender system","keywords":"Unstructured Data Mining;Natural Language Processing;Empirical Study","startPage":"12","endPage":"23","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371991","citationCount":24,"referenceCount":59,"year":2015,"authors":"A. D. Sorbo; S. Panichella; C. A. Visaggio; M. D. Penta; G. Canfora; H. C. Gall","affiliations":"NA; Univ. of Zurich, Zurich, Switzerland; Univ. of Sannio, Benevento, Italy; NA; Univ. of Sannio, Benevento, Italy; Univ. of Zurich, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d384d"},"title":"Towards precise metrics for predicting graph query performance","abstract":"Queries are the foundations of data intensive applications. In model-driven software engineering (MDSE), model queries are core technologies of tools and transformations. As software models are rapidly increasing in size and complexity, most MDSE tools frequently exhibit scalability issues that decrease developer productivity and increase costs. As a result, choosing the right model representation and query evaluation approach is a significant challenge for tool engineers. In the current paper, we aim to provide a benchmarking framework for the systematic investigation of query evaluation performance. More specifically, we experimentally evaluate (existing and novel) query and instance model metrics to highlight which provide sufficient performance estimates for different MDSE scenarios in various model query tools. For that purpose, we also present a comparative benchmark, which is designed to differentiate model representation and graph query evaluation approaches according to their performance when using large models and complex queries.","conference":"IEEE","terms":"Measurement;Benchmark testing;Query processing;Resource description framework;Unified modeling language;Scalability;Engines,data models;graph theory;query processing;software reliability,graph query performance prediction;data intensive applications;model-driven software engineering;model queries;software models;MDSE tools;query evaluation approach;instance model metrics;model representation","keywords":"Performance benchmark;Model queries;Model metrics;Query metrics","startPage":"421","endPage":"431","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693100","citationCount":5,"referenceCount":39,"year":2013,"authors":"B. Izsó; Z. Szatmári; G. Bergmann; Á. Horváth; I. Ráth","affiliations":"Department of Measurement and Information Systems, Budapest University of Technology and Economics, Hungary; Department of Measurement and Information Systems, Budapest University of Technology and Economics, Hungary; Department of Measurement and Information Systems, Budapest University of Technology and Economics, Hungary; Department of Measurement and Information Systems, Budapest University of Technology and Economics, Hungary; Department of Measurement and Information Systems, Budapest University of Technology and Economics, Hungary","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d384e"},"title":"Automatically Generating Test Templates from Test Names (N)","abstract":"Existing specification-based testing techniques require specifications that either do not exist or are too difficult to create. As a result, they often fall short of their goal of helping developers test expected behaviors. In this paper we present a novel, natural language-based approach that exploits the descriptive nature of test names to generate test templates. Similar to how modern IDEs simplify development by providing templates for common constructs such as loops, test templates can save time and lower the cognitive barrier for writing tests. The results of our evaluation show that the approach is feasible: despite the difficulty of the task, when test names contain a sufficient amount of information, the approach's accuracy is over 80% when parsing the relevant information from the test name and generating the template.","conference":"IEEE","terms":"Testing;Semantics;Concrete;Writing;Software;Speech;Encoding,natural language processing;program testing,natural language-based approach;descriptive test names;cognitive barrier;writing tests;test names;automatic test template generation","keywords":"","startPage":"506","endPage":"511","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372038","citationCount":3,"referenceCount":21,"year":2015,"authors":"B. Zhang; E. Hill; J. Clause","affiliations":"Univ. of Delaware, Newark, DE, USA; Drew Univ., Madison, NJ, USA; Univ. of Delaware, Newark, DE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d384f"},"title":"Practically Tunable Static Analysis Framework for Large-Scale JavaScript Applications (T)","abstract":"We present a novel approach to analyze large-scale JavaScript applications statically by tuning the analysis scalability possibly giving up its soundness. For a given sound static baseline analysis of JavaScript programs, our framework allows users to define a sound approximation of selected executions that they are interested in analyzing, and it derives a tuned static analysis that can analyze the selected executions practically. The selected executions serve as parameters of the framework by taking trade-off between the scalability and the soundness of derived analyses. We formally describe our framework in abstract interpretation, and implement two instances of the framework. We evaluate them by analyzing large-scale real-world JavaScript applications, and the evaluation results show that the framework indeed empowers users to experiment with different levels of scalability and soundness. Our implementation provides an extra level of scalability by deriving sparse versions of derived analyses, and the implementation is publicly available.","conference":"IEEE","terms":"Scalability;Libraries;Reactive power;Sensitivity;Tuning;Approximation methods;Concrete,Java;program diagnostics,practically tunable static analysis framework;large-scale JavaScript applications;JavaScript programs;tuned static analysis;selected executions approximation;abstract interpretation;sparse versions","keywords":"","startPage":"541","endPage":"551","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372042","citationCount":6,"referenceCount":34,"year":2015,"authors":"Y. Ko; H. Lee; J. Dolby; S. Ryu","affiliations":"NA; NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3850"},"title":"CogniCrypt: Supporting developers in using cryptography","abstract":"Previous research suggests that developers often struggle using low-level cryptographic APIs and, as a result, produce insecure code. When asked, developers desire, among other things, more tool support to help them use such APIs. In this paper, we present CogniCrypt, a tool that supports developers with the use of cryptographic APIs. CogniCrypt assists the developer in two ways. First, for a number of common cryptographic tasks, CogniCrypt generates code that implements the respective task in a secure manner. Currently, CogniCrypt supports tasks such as data encryption, communication over secure channels, and long-term archiving. Second, CogniCrypt continuously runs static analyses in the background to ensure a secure integration of the generated code into the developer's workspace. This video demo showcases the main features of CogniCrypt: youtube.com/watch?v=JUq5mRHfAWY.","conference":"IEEE","terms":"Tools;Ciphers;Encryption;Java,application program interfaces;cryptography,CogniCrypt;cryptographic tasks;data encryption;long-term archiving;static analysis;low-level cryptographic API","keywords":"Cryptography;Code Generation;Variability Modeling;Code Analysis","startPage":"931","endPage":"936","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115707","citationCount":6,"referenceCount":28,"year":2017,"authors":"S. Krüger; S. Nadi; M. Reif; K. Ali; M. Mezini; E. Bodden; F. Göpfert; F. Günther; C. Weinert; D. Demmler; R. Kamath","affiliations":"Paderborn University, Germany; University of Alberta, Canada; Technische Universität Darmstadt, Germany; University of Alberta, Canada; Technische Universität Darmstadt, Germany; Paderborn University, Germany; Technische Universität Darmstadt, Germany; Technische Universität Darmstadt, Germany; Technische Universität Darmstadt, Germany; Technische Universität Darmstadt, Germany; Technische Universität Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3851"},"title":"EventFlowSlicer: A tool for generating realistic goal-driven GUI tests","abstract":"Most automated testing techniques for graphical user interfaces (GUIs) produce test cases that are only concerned with covering the elements (widgets, menus, etc.) on the interface, or the underlying program code, with little consideration of test case semantics. This is effective for functional testing where the aim is to find as many faults as possible. However, when one wants to mimic a real user for evaluating usability, or when it is necessary to extensively test important end-user tasks of a system, or to generate examples of how to use an interface, this generation approach fails. Capture and replay techniques can be used, however there are often multiple ways to achieve a particular goal, and capturing all of these is usually too time consuming and unrealistic. Prior work on human performance regression testing introduced a constraint based method to filter test cases created by a functional test case generator, however that work did not capture the specifications, or directly generate only the required tests and considered only a single type of test goal. In this paper we present EventFlowSlicer, a tool that allows the GUI tester to specify and generate all realistic test cases relevant to achieve a stated goal. The user first captures relevant events on the interface, then adds constraints to provide restrictions on the task. An event flow graph is extracted containing only the widgets of interest for that goal. Next all test cases are generated for edges in the graph which respect the constraints. The test cases can then be replayed using a modified version of GUITAR. A video demonstration of EventFlowSlicer can be found at https://youtu.be/hw7WYz8WYVU.","conference":"IEEE","terms":"Color;Tools;Testing;Graphical user interfaces;Keyboards;Java,graphical user interfaces;program testing,realistic goal-driven GUI tests;automated testing techniques;graphical user interfaces;test case semantics;important end-user tasks;human performance regression testing;functional test case generator","keywords":"Software test generation;graphical user interfaces;goal-based testing","startPage":"955","endPage":"960","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115711","citationCount":1,"referenceCount":21,"year":2017,"authors":"J. A. Saddler; M. B. Cohen","affiliations":"Department of Computer Science \u0026 Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115, USA; Department of Computer Science \u0026 Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3852"},"title":"Efficient data race prediction with incremental reasoning on time-stamped lock history","abstract":"We present an efficient data race prediction algorithm that uses lock-reordering based incremental search on time-stamped lock histories for solving multiple races effectively. We balance prediction accuracy, coverage, and performance with a specially designed pairwise reachability algorithm that can store and re-use past search results, thereby, amortizing the cost of reasoning over redundant and overlapping search space. Compared to graph-based search algorithms, our algorithm incurs much smaller overhead due to amortization, and can potentially be used while a program under test is executing. To demonstrate such a possibility, we implemented our approach as an incremental Predictive Analysis (iPA) module in a predictive testing framework. Our approach can handle traces with a few hundreds to half a million events, and predict known/unknown real data races with a performance penalty of less than 4% in addition to what is incurred by runtime race detectors.","conference":"IEEE","terms":"Instruction sets;Testing;History;Synchronization;Cognition;Accuracy;Vectors,multi-threading;prediction theory;program debugging;program testing;reachability analysis;reasoning about programs;redundancy;search problems;software reliability,efficient data race prediction algorithm;incremental reasoning;time stamped lock history;lock reordering based incremental search;pairwise reachability algorithm;redundant search space;overlapping search space;amortization;program under test execution;incremental predictive analysis;iPA module;predictive testing;runtime race detector;multi-threaded programs","keywords":"","startPage":"37","endPage":"47","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693064","citationCount":1,"referenceCount":37,"year":2013,"authors":"M. K. Ganai","affiliations":"NEC Labs America, Princeton, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3853"},"title":"Elixir: Effective object-oriented program repair","abstract":"This work is motivated by the pervasive use of method invocations in object-oriented (OO) programs, and indeed their prevalence in patches of OO-program bugs. We propose a generate-and-validate repair technique, called ELIXIR designed to be able to generate such patches. ELIXIR aggressively uses method calls, on par with local variables, fields, or constants, to construct more expressive repair-expressions, that go into synthesizing patches. The ensuing enlargement of the repair space, on account of the wider use of method calls, is effectively tackled by using a machine-learnt model to rank concrete repairs. The machine-learnt model relies on four features derived from the program context, i.e., the code surrounding the potential repair location, and the bug report. We implement ELIXIR and evaluate it on two datasets, the popular Defects4J dataset and a new dataset Bugs.jar created by us, and against 2 baseline versions of our technique, and 5 other techniques representing the state of the art in program repair. Our evaluation shows that ELIXIR is able to increase the number of correctly repaired bugs in Defects4J by 85% (from 14 to 26) and by 57% in Bugs.jar (from 14 to 22), while also significantly out-performing other state-of-the-art repair techniques including ACS, HD-Repair, NOPOL, PAR, and jGenProg.","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;Java;Tools;Software;Object oriented modeling;Concrete,learning (artificial intelligence);object-oriented programming;program debugging,OO-program bugs;generate- validate repair technique;-validate repair technique;called ELIXIR;expressive repair-expressions;synthesizing patches;repair space;machine-learnt model;concrete repairs;program context;potential repair location;bug report;popular Defects4J dataset;dataset Bugs.jar;correctly repaired bugs;state-of-the-art repair techniques;HD-Repair;PAR;object-oriented program repair","keywords":"","startPage":"648","endPage":"659","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115675","citationCount":17,"referenceCount":45,"year":2017,"authors":"R. K. Saha; Y. Lyu; H. Yoshida; M. R. Prasad","affiliations":"Fujitsu Laboratories of America, Sunnyvale, CA, USA; University of Southern California, Los Angeles, CA, USA; Fujitsu Laboratories of America, Sunnyvale, CA, USA; Fujitsu Laboratories of America, Sunnyvale, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3854"},"title":"Model repair and transformation with Echo","abstract":"Models are paramount in model-driven engineering. In a software project many models may coexist, capturing different views of the system or different levels of abstraction. A key and arduous task in this development method is to keep all such models consistent, both with their meta-models (and the respective constraints) and among themselves. This paper describes Echo, a tool that aims at simplifying this task by automating inconsistency detection and repair using a solver based engine. Consistency between different models can be specified by bidirectional model transformations, and is guaranteed to be recovered by minimal updates on the inconsistent models. The tool is freely available as an Eclipse plugin, developed on top of the popular EMF framework, and supports constraints and transformations specified in the OMG standard languages OCL and QVT-R, respectively.","conference":"IEEE","terms":"Unified modeling language;Maintenance engineering;Object oriented modeling;Metals;Standards;Computational modeling;Semantics,software maintenance,model repair;Echo;model-driven engineering;software project;development method;meta-models;inconsistency detection automation;solver based engine;bidirectional model transformations;Eclipse plugin;EMF framework;OMG standard languages;OCL;QVT-R","keywords":"","startPage":"694","endPage":"697","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693135","citationCount":11,"referenceCount":15,"year":2013,"authors":"N. Macedo; T. Guimarães; A. Cunha","affiliations":"HASLAB - High Assurance Software Laboratory, INESC TEC \u0026 Universidade do Minho, Braga, Portugal; HASLAB - High Assurance Software Laboratory, INESC TEC \u0026 Universidade do Minho, Braga, Portugal; HASLAB - High Assurance Software Laboratory, INESC TEC \u0026 Universidade do Minho, Braga, Portugal","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3855"},"title":"GRT: An Automated Test Generator Using Orchestrated Program Analysis","abstract":"While being highly automated and easy to use, existing techniques of random testing suffer from low code coverage and defect detection ability for practical software applications. Most tools use a pure black-box approach, which does not use knowledge specific to the software under test. Mining and leveraging the information of the software under test can be promising to guide random testing to overcome such limitations. Guided Random Testing (GRT) implements this idea. GRT performs static analysis on software under test to extract relevant knowledge and further combines the information extracted at run-time to guide the whole test generation procedure. GRT is highly configurable, with each of its six program analysis components implemented as a pluggable module whose parameters can be adjusted. Besides generating test cases, GRT also automatically creates a test coverage report. We show our experience in GRT tool development and demonstrate its practical usage using two concrete application scenarios.","conference":"IEEE","terms":"Testing;Data mining;Generators;Libraries;Impurities;Software systems,program diagnostics;program testing,GRT;automated test generator;orchestrated program analysis;random testing;low code coverage;defect detection ability;black-box approach;software under test;static analysis;program analysis components;test coverage report","keywords":"Automatic test generation;random testing;bug detection;static analysis;dynamic analysis","startPage":"842","endPage":"847","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372077","citationCount":8,"referenceCount":38,"year":2015,"authors":"L. Ma; C. Artho; C. Zhang; H. Sato; J. Gmeiner; R. Ramler","affiliations":"Univ. of Tokyo, Tokyo, Japan; ITRI, AIST, Tsukuba, Japan; Univ. of Waterloo, Waterloo, ON, Canada; Univ. of Tokyo, Tokyo, Japan; Software Competence Center, Hagenberg, Austria; Software Competence Center, Hagenberg, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3856"},"title":"ActivitySpace: A Remembrance Framework to Support Interapplication Information Needs","abstract":"Developers' daily work produces, transforms, and communicates cross-cutting information across applications, including IDEs, emails, Q\u0026A sites, Twitter, and many others. However, these applications function independently of one another. Even though each application has their own effective information management mechanisms, cross-cutting information across separate applications creates a problem of information fragmentation, forcing developers to manually track, correlate, and re-find cross-cutting information across applications. In this paper, we present ActivitySpace, a remembrance framework that unobtrusively tracks and analyze a developer's daily work in separate applications, and provides various semantic and episodic UIs that help developers correlate and re-find cross-cutting information across applications based on information content, time and place of his/her activities. Through a user study of 8 participants, we demonstrate how ActivitySpace helps to tackle information fragmentation problem in developers' daily work.","conference":"IEEE","terms":"History;Semantics;Software;Mice;Databases;Computers;Image color analysis,software development management;user interfaces,ActivitySpace;remembrance framework;interapplication information needs;information management mechanism;information fragmentation;semantic UI;episodic UI","keywords":"","startPage":"864","endPage":"869","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372081","citationCount":7,"referenceCount":21,"year":2015,"authors":"L. Bao; D. Ye; Z. Xing; X. Xia; X. Wang","affiliations":"Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Eng., Nanyang Technol. Univ., Singapore, Singapore; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China; Coll. of Comput. Sci., Zhejiang Univ., Hangzhou, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3857"},"title":"Dynamically transforming data structures","abstract":"Fine-tuning which data structure implementation to use for a given problem is sometimes tedious work since the optimum solution depends on the context, i.e., on the operation sequences, actual parameters as well as on the hardware available at run time. Sometimes a data structure with higher asymptotic time complexity performs better in certain contexts because of lower constants. The optimal solution may not even be possible to determine at compile time. We introduce transformation data structures that dynamically change their internal representation variant based on a possibly changing context. The most suitable variant is selected at run time rather than at compile time. We demonstrate the effect on performance with a transformation ArrayList data structure using an array variant and a linked hash bag variant as alternative internal representations. Using our transformation ArrayList, the standard DaCapo benchmark suite shows a performance gain of 5.19% in average.","conference":"IEEE","terms":"Context;Abstracts;Arrays;Algorithm design and analysis;Switches;Complexity theory,data structures,transformation ArrayList data structure;hash bag variant;DaCapo benchmark suite;internal representation variant;possibly changing context","keywords":"","startPage":"410","endPage":"420","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693099","citationCount":1,"referenceCount":15,"year":2013,"authors":"E. Österlund; W. Löwe","affiliations":"Software Technology Group, Linnaeus University, Växjö, Sweden; Software Technology Group, Linnaeus University, Växjö, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3858"},"title":"Array Shadow State Compression for Precise Dynamic Race Detection (T)","abstract":"Precise dynamic race detectors incur significant time and space overheads, particularly for array-intensive programs, due to the need to store and manipulate analysis (or shadow) state for every element of every array. This paper presents SlimState, a precise dynamic race detector that uses an adaptive, online algorithm to optimize array shadow state representations. SlimState is based on the insight that common array access patterns lead to analogous patterns in array shadow state, enabling optimized, space efficient representations of array shadow state with no loss in precision. We have implemented SlimState for Java. Experiments on a variety of benchmarks show that array shadow compression reduces the space and time overhead of race detection by 27% and 9%, respectively. It is particularly effective for array-intensive programs, reducing space and time overheads by 35% and 17%, respectively, on these programs.","conference":"IEEE","terms":"Arrays;Clocks;Instruction sets;Detectors;Synchronization;Heuristic algorithms;Java,Java;program testing;system monitoring,array shadow state compression;precise dynamic race detection;SLIMSTATE;adaptive online algorithm;array shadow state representations;array access patterns;analogous patterns;space efficient representations;Java;space overhead;time overhead;array-intensive programs","keywords":"concurrency;data race detection;dynamic analysis","startPage":"155","endPage":"165","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372005","citationCount":11,"referenceCount":46,"year":2015,"authors":"J. R. Wilcox; P. Finch; C. Flanagan; S. N. Freund","affiliations":"Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA; Cognius, Boston, MA, USA; Comput. Sci. Dept., Univ. of California, Santa Cruz, Santa Cruz, CA, USA; Comput. Sci. Dept., Williams Coll., Williamstown, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3859"},"title":"All about activity injection: Threats, semantics, and detection","abstract":"Android supports seamless user experience by maintaining activities from different apps in the same activity stack. While such close inter-app communication is essential in the Android framework, the powerful inter-app communication contains vulnerabilities that can inject malicious activities into a victim app's activity stack to hijack user interaction flows. In this paper, we demonstrate activity injection attacks with a simple malware, and formally specify the activity activation mechanism using operational semantics. Based on the operational semantics, we develop a static analysis tool, which analyzes Android apps to detect activity injection attacks. Our tool is fast enough to analyze real-world Android apps in 6 seconds on average, and our experiments found that 1,761 apps out of 129,756 real-world Android apps inject their activities into other apps' tasks.","conference":"IEEE","terms":"Androids;Humanoid robots;Electronic mail;Smart phones;Semantics;Malware,Android (operating system);invasive software;mobile computing;program diagnostics,Android framework;malicious activities;user interaction;activity injection attacks;activity activation mechanism;operational semantics;victim application activity stack;real-world Android application;interapplication communication","keywords":"","startPage":"252","endPage":"262","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115638","citationCount":0,"referenceCount":55,"year":2017,"authors":"S. Lee; S. Hwang; S. Ryu","affiliations":"KAIST, Korea; LG Electronics, Korea; KAIST, Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d385a"},"title":"UI driven Android application reduction","abstract":"While smartphones and mobile apps have been an integral part of our life, modern mobile apps tend to contain a lot of rarely used functionalities. For example, applications contain advertisements and offer extra features such as recommended news stories in weather apps. While these functionalities are not essential to an app, they nonetheless consume power, CPU cycles and bandwidth. In this paper, we design a UI driven approach that allows customizing an Android app by removing its unwanted functionalities. In particular, our technique displays the UI and allows the user to select elements denoting functionalities that she wants to remove. Using this information, our technique automatically removes all the code elements related to the selected functionalities, including all the relevant background tasks. The underlying analysis is a type system, in which each code element is tagged with a type indicating if it should be removed. From the UI hints, our technique infers types for all other code elements and reduces the app accordingly. We implement a prototype and evaluate it on 10 real-world Android apps. The results show that our approach can accurately discover the removable code elements and lead to substantial resource savings in the reduced apps.","conference":"IEEE","terms":"Meteorology;Androids;Humanoid robots;Mobile communication;Real-time systems;Prototypes,Android (operating system);mobile computing;smart phones;user interfaces,UI driven Android application reduction;smartphones;integral part;modern mobile apps;advertisements;offer extra features;recommended news stories;UI driven approach;code element;UI hints;real-world Android apps;removable code elements;UI;CPU","keywords":"","startPage":"286","endPage":"296","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115642","citationCount":0,"referenceCount":38,"year":2017,"authors":"J. Huang; Y. Aafer; D. Perry; X. Zhang; C. Tian","affiliations":"Department of Computer Science, Purdue University, USA; Department of Computer Science, Purdue University, USA; Department of Computer Science, Purdue University, USA; Department of Computer Science, Purdue University, USA; Huawei R\u0026D, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d385b"},"title":"How do Developers Document Database Usages in Source Code? (N)","abstract":"Database-centric applications (DCAs) usually contain a large number of tables, attributes, and constraints describing the underlying data model. Understanding how database tables and attributes are used in the source code along with the constraints related to these usages is an important component of DCA maintenance. However, documenting database-related operations and their constraints in the source code is neither easy nor common in practice. In this paper, we present a two-fold empirical study aimed at identifying how developers document database usages at source code method level. In particular, (i) we surveyed open source developers to understand their practices on documenting database usages in source code, and (ii) we mined a large set of open source projects to measure to what extent database-related methods are commented and if these comments are updated during evolution. Although 58% of the developers claimed to find value in method comments describing database usages, our findings suggest that 77% of 33K+ methods in 3.1K+ open-source Java projects with database accesses were completely undocumented.","conference":"IEEE","terms":"Databases;Documentation;Java;Data mining;Electronic mail;Software;Data models,database management systems;document handling;public domain software;software maintenance;source code (software),database centric applications;underlying data model;database tables;DCA maintenance;documenting database related operations;source code method level;open source projects;database related methods","keywords":"","startPage":"36","endPage":"41","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371993","citationCount":7,"referenceCount":31,"year":2015,"authors":"M. Linares-Vásquez; B. Li; C. Vendome; D. Poshyvanyk","affiliations":"Coll. of William \u0026 Mary, Williamsburg, VA, USA; Coll. of William \u0026 Mary, Williamsburg, VA, USA; Coll. of William \u0026 Mary, Williamsburg, VA, USA; Coll. of William \u0026 Mary, Williamsburg, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d385c"},"title":"Mining branching-time scenarios","abstract":"Specification mining extracts candidate specification from existing systems, to be used for downstream tasks such as testing and verification. Specifically, we are interested in the extraction of behavior models from execution traces. In this paper we introduce mining of branching-time scenarios in the form of existential, conditional Live Sequence Charts, using a statistical data-mining algorithm. We show the power of branching scenarios to reveal alternative scenario-based behaviors, which could not be mined by previous approaches. The work contrasts and complements previous works on mining linear-time scenarios. An implementation and evaluation over execution trace sets recorded from several real-world applications shows the unique contribution of mining branching-time scenarios to the state-of-the-art in specification mining.","conference":"IEEE","terms":"Semantics;Context;Educational institutions;Data mining;Abstracts;Weight measurement;Testing,data mining;formal verification;program testing;statistical analysis,behavior models extraction;branching-time scenarios mining;conditional live sequence charts;statistical data-mining algorithm","keywords":"","startPage":"443","endPage":"453","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693102","citationCount":17,"referenceCount":42,"year":2013,"authors":"D. Fahland; D. Lo; S. Maoz","affiliations":"Eindhoven University of Technology, The Netherlands; Singapore Management University, Singapore; Tel Aviv University, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d385d"},"title":"Variable Feature Usage Patterns in PHP (T)","abstract":"PHP allows the names of variables, classes, functions, methods, and properties to be given dynamically, as expressions that, when evaluated, return an identifier as a string. While this provides greater flexibility for programmers, it also makes PHP programs harder to precisely analyze and understand. In this paper we present a number of patterns designed to recognize idiomatic uses of these features that can be statically resolved to a precise set of possible names. We then evaluate these patterns across a corpus of 20 open-source systems totaling more than 3.7 million lines of PHP, showing how often these patterns occur in actual PHP code, demonstrating their effectiveness at statically determining the names that can be used at runtime, and exploring anti-patterns that indicate when the identifier computation is truly dynamic.","conference":"IEEE","terms":"Reactive power;Arrays;Runtime;Feature extraction;Pattern recognition;Open source software;Engines,object-oriented languages;object-oriented methods;public domain software,variable feature usage patterns;PHP programs;open-source systems;PHP code;anti-patterns;object-oriented language","keywords":"Static analysis;dynamic language features;usage patterns;PHP","startPage":"563","endPage":"573","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372044","citationCount":6,"referenceCount":22,"year":2015,"authors":"M. Hills","affiliations":"East Carolina Univ., Greenville, NC, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d385e"},"title":"Taco: A tool to generate tensor algebra kernels","abstract":"Tensor algebra is an important computational abstraction that is increasingly used in data analytics, machine learning, engineering, and the physical sciences. However, the number of tensor expressions is unbounded, which makes it hard to develop and optimize libraries. Furthermore, the tensors are often sparse (most components are zero), which means the code has to traverse compressed formats. To support programmers we have developed taco, a code generation tool that generates dense, sparse, and mixed kernels from tensor algebra expressions. This paper describes the taco web and command-line tools and discusses the benefits of a code generator over a traditional library. See also the demo video at tensor-compiler.org/ase2017.","conference":"IEEE","terms":"Tensile stress;Tools;Kernel;Indexes;Libraries;Linear algebra,data analysis;learning (artificial intelligence);mathematics computing;program compilers;software libraries;tensors,tensor algebra kernels;data analytics;machine learning;physical sciences;tensor expressions;compressed formats;code generation tool;dense kernels;mixed kernels;tensor algebra expressions;taco web;command-line tools;code generator;computational abstraction;sparse kernels","keywords":"Tensor algebra;linear algebra;sparse;compiler","startPage":"943","endPage":"948","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115709","citationCount":2,"referenceCount":25,"year":2017,"authors":"F. Kjolstad; S. Chou; D. Lugato; S. Kamil; S. Amarasinghe","affiliations":"MIT CSAIL, USA; MIT CSAIL, USA; CEA, France; Adobe, USA; MIT CSAIL, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d385f"},"title":"FEMIR: A tool for recommending framework extension examples","abstract":"Software frameworks enable developers to reuse existing well tested functionalities instead of taking the burden of implementing everything from scratch. However, to meet application specific requirements, the frameworks need to be customized via extension points. This is often done by passing a framework related object as an argument to an API call. To enable such customizations, the object can be created by extending a framework class, implementing an interface, or changing the properties of the object via API calls. However, it is both a common and non-trivial task to find all the details related to the customizations. In this paper, we present a tool, called FEMIR, that utilizes partial program analysis and graph mining technique to detect, group, and rank framework extension examples. The tool extends existing code completion infrastructure to inform developers about customization choices, enabling them to browse through extension points of a framework, and frequent usages of each point in terms of code examples. A video demo is made available at https://asaduzzamanparvez.wordpress.com/femir.","conference":"IEEE","terms":"Tools;Receivers;Proposals;Java;Indexes;Tree data structures,application program interfaces;data mining;graph theory;program diagnostics,software frameworks;tested functionalities;application specific requirements;framework related object;API call;framework class;partial program analysis;graph mining technique;rank framework extension examples;customization choices;code examples;FEMIR","keywords":"API;framework;reuse;extension point;extension;partial program analysis;graph mining","startPage":"967","endPage":"972","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115713","citationCount":1,"referenceCount":8,"year":2017,"authors":"M. Asaduzzaman; C. K. Roy; K. A. Schneider; D. Hou","affiliations":"Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada; Electrical and Computer Engineering Department, Clarkson University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3860"},"title":"Improving efficiency of dynamic analysis with dynamic dependence summaries","abstract":"Modern applications make heavy use of third-party libraries and components, which poses new challenges for efficient dynamic analysis. To perform such analyses, transitive dependent components at all layers of the call stack must be monitored and analyzed, and as such may be prohibitively expensive for systems with large libraries and components. As an approach to address such expenses, we record, summarize, and reuse dynamic dataflows between inputs and outputs of components, based on dynamic control and data traces. These summarized dataflows are computed at a fine-grained instruction level; the result of which, we call “dynamic dependence summaries.” Although static summaries have been proposed, to the best of our knowledge, this work presents the first technique for dynamic dependence summaries. The benefits to efficiency of such summarization may be afforded with losses of accuracy. As such, we evaluate the degree of accuracy loss and the degree of efficiency gain when using dynamic dependence summaries of library methods. On five large programs from the DaCapo benchmark (for which no existing whole-program dynamic dependence analyses have been shown to scale) and 21 versions of NANOXML, the summarized dependence analysis provided 90% accuracy and a speed-up of 100% (i.e., ×2), on average, when compared to traditional exhaustive dynamic dependence analysis.","conference":"IEEE","terms":"Arrays;Concrete;Abstracts;Libraries;Indexes;Java;Accuracy,instruction sets;program diagnostics;software libraries,dynamic program analysis;third-party libraries;transitive dependent components;dynamic dataflows;dynamic control;data traces;fine-grained instruction level;dynamic dependence summaries;degree of accuracy loss evaluation;degree of efficiency gain evaluation;library methods;DaCapo benchmark;NANOXML;summarized dependence analysis;exhaustive dynamic dependence analysis;large object-oriented libraries","keywords":"Program analysis;Dynamic analysis;Dynamic Slicing;Data-flow;Summarization","startPage":"59","endPage":"69","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693066","citationCount":6,"referenceCount":34,"year":2013,"authors":"V. K. Palepu; G. Xu; J. A. Jones","affiliations":"University of California, Irvine, USA; University of California, Irvine, USA; University of California, Irvine, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3861"},"title":"Operator-based and random mutant selection: Better together","abstract":"Mutation testing is a powerful methodology for evaluating the quality of a test suite. However, the methodology is also very costly, as the test suite may have to be executed for each mutant. Selective mutation testing is a well-studied technique to reduce this cost by selecting a subset of all mutants, which would otherwise have to be considered in their entirety. Two common approaches are operator-based mutant selection, which only generates mutants using a subset of mutation operators, and random mutant selection, which selects a subset of mutants generated using all mutation operators. While each of the two approaches provides some reduction in the number of mutants to execute, applying either of the two to medium-sized, real-world programs can still generate a huge number of mutants, which makes their execution too expensive. This paper presents eight random sampling strategies defined on top of operator-based mutant selection, and empirically validates that operator-based selection and random selection can be applied in tandem to further reduce the cost of mutation testing. The experimental results show that even sampling only 5% of mutants generated by operator-based selection can still provide precise mutation testing results, while reducing the average mutation testing time to 6.54% (i.e., on average less than 5 minutes for this study).","conference":"IEEE","terms":"Testing;Standards;Java;Power measurement;Correlation;Educational institutions;Libraries,cost reduction;program testing;software quality,operator-based mutant selection;random mutant selection;mutation testing;test suite quality evaluation;cost reduction;random sampling strategies","keywords":"","startPage":"92","endPage":"102","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693070","citationCount":27,"referenceCount":65,"year":2013,"authors":"L. Zhang; M. Gligoric; D. Marinov; S. Khurshid","affiliations":"University of Texas, Austin, 78712, USA; University of Illinois, Urbana, 61801, USA; University of Illinois, Urbana, 61801, USA; University of Texas, Austin, 78712, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3862"},"title":"Boosting complete-code tool for partial program","abstract":"To improve software quality, researchers and practitioners have proposed static analysis tools for various purposes (e.g., detecting bugs, anomalies, and vulnerabilities). Although many such tools are powerful, they typically need complete programs where all the code names (e.g., class names, method names) are resolved. In many scenarios, researchers have to analyze partial programs in bug fixes (the revised source files can be viewed as a partial program), tutorials, and code search results. As a partial program is a subset of a complete program, many code names in partial programs are unknown. As a result, despite their syntactical correctness, existing complete-code tools cannot analyze partial programs, and existing partial-code tools are limited in both their number and analysis capability. Instead of proposing another tool for analyzing partial programs, we propose a general approach, called GRAPA, that boosts existing tools for complete programs to analyze partial programs. Our major insight is that after unknown code names are resolved, tools for complete programs can analyze partial programs with minor modifications. In particular, GRAPA locates Java archive files to resolve unknown code names, and resolves the remaining unknown code names from resolved code names. To illustrate GRAPA, we implement a tool that leverages the state-of-the-art tool, WALA, to analyze Java partial programs. We thus implemented the first tool that is able to build system dependency graphs for partial programs, complementing existing tools. We conduct an evaluation on 8,198 partial-code commits from four popular open source projects. Our results show that GRAPA fully resolved unknown code names for 98.5% bug fixes, with an accuracy of 96.1% in total. Furthermore, our results show the significance of GRAPA's internal techniques, which provides insights on how to integrate with more complete-code tools to analyze partial programs.","conference":"IEEE","terms":"Tools;Computer bugs;Boosting;Java;Data mining;Software;Syntactics,graph theory;Java;program debugging;program diagnostics;public domain software;software quality,unknown code names;partial program;complete-code tool;partial-code tools;software quality improvement;GRAPA;Java archive files;Java partial program analysis","keywords":"Partial program;program analysis;boosting complete-code tool","startPage":"671","endPage":"681","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115677","citationCount":3,"referenceCount":44,"year":2017,"authors":"H. Zhong; X. Wang","affiliations":"Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science, University of Texas at San Antonio, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3863"},"title":"AnswerBot: Automated generation of answer summary to developers' technical questions","abstract":"The prevalence of questions and answers on domain-specific Q\u0026A sites like Stack Overflow constitutes a core knowledge asset for software engineering domain. Although search engines can return a list of questions relevant to a user query of some technical question, the abundance of relevant posts and the sheer amount of information in them makes it difficult for developers to digest them and find the most needed answers to their questions. In this work, we aim to help developers who want to quickly capture the key points of several answer posts relevant to a technical question before they read the details of the posts. We formulate our task as a query-focused multi-answer-posts summarization task for a given technical question. Our proposed approach AnswerBot contains three main steps : 1) relevant question retrieval, 2) useful answer paragraph selection, 3) diverse answer summary generation. To evaluate our approach, we build a repository of 228,817 Java questions and their corresponding answers from Stack Overflow. We conduct user studies with 100 randomly selected Java questions (not in the question repository) to evaluate the quality of the answer summaries generated by our approach, and the effectiveness of its relevant question retrieval and answer paragraph selection components. The user study results demonstrate that answer summaries generated by our approach are relevant, useful and diverse; moreover, the two components are able to effectively retrieve relevant questions and select salient answer paragraphs for summarization.","conference":"IEEE","terms":"Synchronization;Java;Tools;Knowledge discovery;Google;Search engines,Java;query processing;question answering (information retrieval);search engines,Stack Overflow;question repository;answer summary;relevant question retrieval;answer paragraph selection components;salient answer paragraphs;automated generation;core knowledge asset;software engineering domain;search engines;user query;relevant posts;answer posts;multianswer-posts summarization task;domain-specific Q\u0026A;randomly selected Java questions;AnswerBot","keywords":"Summary generation;question retrieval","startPage":"706","endPage":"716","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115681","citationCount":7,"referenceCount":30,"year":2017,"authors":"B. Xu; Z. Xing; X. Xia; D. Lo","affiliations":"College of Computer Science and Technology, Zhejiang University, China; School of Engineering and Computer Science, Australian National University, Australia; Department of Computer Science, University of British Columbia, Canada; School of Information Systems, Singapore Management University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9126eee8435e8e7d3864"},"title":"OCRA: A tool for checking the refinement of temporal contracts","abstract":"Contract-based design enriches a component model with properties structured in pairs of assumptions and guarantees. These properties are expressed in term of the variables at the interface of the components, and specify how a component interacts with its environment: the assumption is a property that must be satisfied by the environment of the component, while the guarantee is a property that the component must satisfy in response. Contract-based design has been recently proposed in many methodologies for taming the complexity of embedded systems. In fact, contract-based design enables stepwise refinement, compositional verification, and reuse of components. However, only few tools exist to support the formal verification underlying these methods. OCRA (Othello Contracts Refinement Analysis) is a new tool that provides means for checking the refinement of contracts specified in a linear-time temporal logic. The specification language allows to express discrete as well as metric real-time constraints. The underlying reasoning engine allows checking if the contract refinement is correct. OCRA has been used in different projects and integrated in CASE tools.","conference":"IEEE","terms":"Contracts;Cognition;Model checking;Context;Embedded systems;Unified modeling language;Automata,formal specification;object-oriented programming;program verification;software reusability;software tools;specification languages;temporal logic,OCRA tool;temporal contract refinement checking;component interface model;contract-based design;stepwise refinement;compositional verification;component reuse;formal verification;Othello Contracts Refinement Analysis;linear-time temporal logic;specification language;discrete constraints;metric constraints;reasoning engine;CASE tools","keywords":"","startPage":"702","endPage":"705","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693137","citationCount":44,"referenceCount":26,"year":2013,"authors":"A. Cimatti; M. Dorigatti; S. Tonetta","affiliations":"FBK-irst, Trento, Italy; FBK-irst, Trento, Italy; FBK-irst, Trento, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3865"},"title":"SiPL -- A Delta-Based Modeling Framework for Software Product Line Engineering","abstract":"Model-based development has become a widely-used approach to implement software, e.g. for embedded systems. Models replace source code as primary executable artifacts in these cases. Software product line technologies for these domains must be able to generate models as instances of an SPL. This need is addressed among others by an implementation technology for SPLs known as delta modeling. Current approaches to delta modeling require deltas to be written manually using delta languages, and they offer only very limited support for creating and testing a network of deltas. This paper presents a new approach to delta modeling and a supporting tool suite: the abstract notion of a delta is refined to be a consistency-preserving edit script which is generated by comparing two models. The rich structure of edit scripts allows us to detect conflicts and further relations between deltas statically and to implement restructurings in delta sets such as the merging of two deltas. We illustrate the tooling using a case study.","conference":"IEEE","terms":"Unified modeling language;Visualization;Standards;Graphical user interfaces;Software product lines;Software packages,software product lines,delta-based modeling framework;software product line engineering;SiPL;source code;software product line technology;SPL;delta language","keywords":"software product line engineering;model-based development;delta modeling;model differencing","startPage":"852","endPage":"857","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372079","citationCount":4,"referenceCount":30,"year":2015,"authors":"C. Pietsch; T. Kehrer; U. Kelter; D. Reuling; M. Ohrndorf","affiliations":"Software Eng. Group, Univ. of Siegen, Siegen, Germany; Software Eng. Group, Univ. of Siegen, Siegen, Germany; Software Eng. Group, Univ. of Siegen, Siegen, Germany; Software Eng. Group, Univ. of Siegen, Siegen, Germany; Software Eng. Group, Univ. of Siegen, Siegen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3866"},"title":"The iMPAcT Tool: Testing UI Patterns on Mobile Applications","abstract":"This paper presents the iMPAcT tool that tests recurring behaviour, i.e., UI Patterns, on mobile applications. This tool is implemented in Java and makes use of Android's APIs UI Automator and UiAutomation. The tool automatically explores a mobile application in order to automatically identify and test UI Patterns. Each UI Pattern has a test strategy, Test Patterns, associated, which is applied when an UI Pattern is found. The approach works on top of a catalogue of UI Patterns, which determines which UI Patterns are to be tested, and what should their correct behaviour be, and may be used for any application.","conference":"IEEE","terms":"Testing;Mobile communication;Androids;Humanoid robots;Mobile applications;Layout;Pattern matching,Android (operating system);application program interfaces;data flow analysis;Java;mobile computing,iMPAcT tool;UI pattern testing;mobile applications;recurring behaviour testing;Java;Android API;UI Automator;UiAutomation","keywords":"Mobile Testing;UI Patterns;Pattern-based testing;Reverse Engineering","startPage":"876","endPage":"881","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372083","citationCount":9,"referenceCount":34,"year":2015,"authors":"I. C. Morgado; A. C. R. Paiva","affiliations":"Dept. of Inf. Eng., Univ. of Porto, Porto, Portugal; Dept. of Inf. Eng., Univ. of Porto, Porto, Portugal","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3867"},"title":"JaConTeBe: A Benchmark Suite of Real-World Java Concurrency Bugs (T)","abstract":"Researchers have proposed various approaches to detect concurrency bugs and improve multi-threaded programs, but performing evaluations of the effectiveness of these approaches still remains a substantial challenge. We survey the existing evaluations and find out that they often use code or bugs not representative of real world. To improve representativeness, we have prepared JaConTeBe, a benchmark suite of 47 confirmed concurrency bugs from 8 popular open-source projects, supplemented with test cases for reproducing buggy behaviors. Running three approaches on JaConTeBe shows that our benchmark suite confirms some limitations of the three approaches. We submitted JaConTeBe to the SIR repository (a software-artifact repository for rigorous controlled experiments), and it was included as a part of SIR.","conference":"IEEE","terms":"Computer bugs;Benchmark testing;Concurrent computing;Java;System recovery;Open source software,Java;multi-threading;program debugging;public domain software,JaConTeBe;real-world Java concurrency bugs;benchmark suite;multithreaded programs;open-source projects;test cases;buggy behaviors;SIR repository;software-artifact repository","keywords":"Java concurrency bugs;evaluations;benchmark suite;SIR;JaConTeBe","startPage":"178","endPage":"189","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372007","citationCount":8,"referenceCount":85,"year":2015,"authors":"Z. Lin; D. Marinov; H. Zhong; Y. Chen; J. Zhao","affiliations":"Sch. Of Software, Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Dept. of Comput. Sci. \u0026 Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci. \u0026 Eng., Shanghai Jiao Tong Univ., Shanghai, China; Dept. of Comput. Sci. \u0026 Eng., Shanghai Jiao Tong Univ., Shanghai, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3868"},"title":"Study and Refactoring of Android Asynchronous Programming (T)","abstract":"To avoid unresponsiveness, a core part of mobile development is asynchronous programming. Android providesseveral async constructs that developers can use. However, developers can still use the inappropriate async constructs, which result in memory leaks, lost results, and wasted energy. Fortunately, refactoring tools can eliminate these problems by transforming async code to use the appropriate constructs. In this paper we conducted a formative study on a corpusof 611 widely-used Android apps to map the asynchronouslandscape of Android apps, understand how developers retrofit asynchrony, and learn about barriers encountered by developers. Based on this study, we designed, implemented, and evaluated ASYNCDROID, a refactoring tool which enables Android developers to transform existing improperly-used async constructs into correct constructs. Our empirical evaluation shows that ASYNCDROID is applicable, accurate, and saves developers effort. We submitted 45 refactoring patches, and developers consider that the refactorings are useful.","conference":"IEEE","terms":"Androids;Humanoid robots;Graphical user interfaces;Receivers;Programming;Registers;Transforms,Android (operating system);mobile computing;software maintenance,mobile development;Android asynchronous programming;Android application;asynchronous landscape;ASYNCDROID;refactoring tool;Android developers;improperly-used async constructs;refactoring patches","keywords":"Refactoring;Android;Asynchronous","startPage":"224","endPage":"235","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372011","citationCount":23,"referenceCount":51,"year":2015,"authors":"Y. Lin; S. Okur; D. Dig","affiliations":"Comput. Sci. Dept., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Comput. Sci. Dept., Univ. of Illinois at Urbana-Champaign, Urbana, IL, USA; Sch. of Electr. Eng. \u0026 Comput. Sci., Oregon State Univ., Corvallis, OR, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3869"},"title":"Automated cross-platform inconsistency detection for mobile apps","abstract":"Testing of Android apps is particularly challenging due to the fragmentation of the Android ecosystem in terms of both devices and operating system versions. Developers must in fact ensure not only that their apps behave as expected, but also that the apps' behavior is consistent across platforms. To support this task, we propose DiffDroid, a new technique that helps developers automatically find cross-platform inconsistencies (CPIs) in mobile apps. DiffDroid combines input generation and differential testing to compare the behavior of an app on different platforms and identify possible inconsistencies. Given an app, DiffDroid (1) generates test inputs for the app, (2) runs the app with these inputs on a reference device and builds a model of the app behavior, (3) runs the app with the same inputs on a set of other devices, and (4) compares the behavior of the app on these different devices with the model of its behavior on the reference device. We implemented DiFFDRoiD and performed an evaluation of our approach on 5 benchmarks and over 130 platforms. our results show that DiFFDRoiD can identify CPis on real apps efficiently and with a limited number of false positives. DiFFDRoiD and our experimental infrastructure are publicly available.","conference":"IEEE","terms":"Testing;Androids;Humanoid robots;Encoding;Performance evaluation;Analytical models,Android (operating system);mobile computing;program testing,mobile apps;DiffDroid;DiFFDRoiD;automated cross-platform inconsistency detection;Android apps;cross-platform inconsistencies","keywords":"","startPage":"308","endPage":"318","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115644","citationCount":6,"referenceCount":45,"year":2017,"authors":"M. Fazzini; A. Orso","affiliations":"Georgia Institute of Technology, USA; Georgia Institute of Technology, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d386a"},"title":"Synthesising Interprocedural Bit-Precise Termination Proofs (T)","abstract":"Proving program termination is key to guaranteeing absence of undesirable behaviour, such as hanging programs and even security vulnerabilities such as denial-of-service attacks. To make termination checks scale to large systems, interprocedural termination analysis seems essential, which is a largely unexplored area of research in termination analysis, where most effort has focussed on difficult single-procedure problems. We present a modular termination analysis for C programs using template-based interprocedural summarisation. Our analysis combines a context-sensitive, over-approximating forward analysis with the inference of under-approximating preconditions for termination. Bit-precise termination arguments are synthesised over lexicographic linear ranking function templates. Our experimental results show that our tool 2LS outperforms state-of-the-art alternatives, and demonstrate the clear advantage of interprocedural reasoning over monolithic analysis in terms of efficiency, while retaining comparable precision.","conference":"IEEE","terms":"Context;Algorithm design and analysis;Encoding;Semantics;Computer crime;Computer bugs,C language;computer network security;inference mechanisms;program diagnostics;theorem proving,synthesising interprocedural bit-precise termination proof;program termination;security vulnerability;denial-of-service attack;large system;interprocedural termination analysis;modular termination analysis;C program;template-based interprocedural summarisation;context-sensitive forward analysis;over-approximating forward analysis;inference;bit-precise termination argument;lexicographic linear ranking function template;interprocedural reasoning;monolithic analysis","keywords":"program analysis;termination analysis;interprocedural analysis;synthesis;abstract interpretation","startPage":"53","endPage":"64","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371995","citationCount":9,"referenceCount":64,"year":2015,"authors":"H. Chen; C. David; D. Kroening; P. Schrammel; B. Wachter","affiliations":"Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK; Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d386b"},"title":"Scalable product line configuration: A straw to break the camel's back","abstract":"Software product lines are hard to configure. Techniques that work for medium sized product lines fail for much larger product lines such as the Linux kernel with 6000+ features. This paper presents simple heuristics that help the Indicator-Based Evolutionary Algorithm (IBEA) in finding sound and optimum configurations of very large variability models in the presence of competing objectives. We employ a combination of static and evolutionary learning of model structure, in addition to utilizing a pre-computed solution used as a “seed” in the midst of a randomly-generated initial population. The seed solution works like a single straw that is enough to break the camel's back -given that it is a feature-rich seed. We show promising results where we can find 30 sound solutions for configuring upward of 6000 features within 30 minutes.","conference":"IEEE","terms":"Biological system modeling;Optimization;Software;Analytical models;Linux;Sociology;Statistics,evolutionary computation;learning (artificial intelligence);Linux;operating system kernels;software product lines,scalable product line configuration;software product lines;Linux kernel;indicator-based evolutionary algorithm;IBEA;competing objectives;static learning;evolutionary learning;model structure;precomputed solution;randomly-generated initial population;seed solution;feature-rich seed","keywords":"Variability models;automated configuration;multiobjective optimization;evolutionary algorithms;SMT solvers","startPage":"465","endPage":"474","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693104","citationCount":44,"referenceCount":34,"year":2013,"authors":"A. S. Sayyad; J. Ingram; T. Menzies; H. Ammar","affiliations":"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, USA; Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d386c"},"title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)","abstract":"Prior research shows that directly applying phrase-based SMT on lexical tokens to migrate Java to C# produces much semantically incorrect code. A key limitation is the use of sequences in phrase-based SMT to model and translate source code with well-formed structures. We propose mppSMT, a divide-and-conquer technique to address that with novel training and migration algorithms using phrase-based SMT in three phases. First, mppSMT treats a program as a sequence of syntactic units and maps/translates such sequences in two languages to one another. Second, with a syntax-directed fashion, it deals with the tokens within syntactic units by encoding them with semantic symbols to represent their data and token types. This encoding via semantic symbols helps better migration of API usages. Third, the lexical tokens corresponding to each sememe are mapped or migrated. The resulting sequences of tokens are merged together to form the final migrated code. Such divide-and-conquer and syntax-direction strategies enable phrase-based SMT to adapt well to syntactical structures in source code, thus, improving migration accuracy. Our empirical evaluation on several real-world systems shows that 84.8 -- 97.9% and 70 -- 83% of the migrated methods are syntactically and semantically correct, respectively. 26.3 -- 51.2% of total migrated methods are exactly matched to the human-written C# code in the oracle. Compared to Java2CSharp, a rule-based migration tool, it achieves higher semantic accuracy from 6.6 -- 57.7% relatively. Importantly, it does not require manual labeling for training data or manual definition of rules.","conference":"IEEE","terms":"Syntactics;Java;Computational modeling;Training;Semantics;Training data;Decoding,divide and conquer methods;language translation;source code (software),divide-and-conquer approach;multiphase statistical migration;source code;mppSMT;migration algorithms;phrase-based SMT;semantic symbols;API usages;lexical tokens;human-written C# code;oracle;statistical machine translation","keywords":"Language Migration;Statistical Machine Translation;Syntax-directed Translation","startPage":"585","endPage":"596","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372046","citationCount":18,"referenceCount":51,"year":2015,"authors":"A. T. Nguyen; T. T. Nguyen; T. N. Nguyen","affiliations":"NA; NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d386d"},"title":"Copy and Paste Redeemed (T)","abstract":"Modern software development relies on code reuse, which software engineers typically realise through handwritten abstractions, such as functions, methods, or classes. However, such abstractions can be challenging to develop and maintain. One alternative form of re-use is copy-paste-modify, a methodology in which developers explicitly duplicate source code to adapt the duplicate for a new purpose. We observe that copy-paste-modify can be substantially faster to use than manual abstraction, and past research strongly suggests that it is a popular technique among software developers. We therefore propose that software engineers should forego hand-written abstractions in favour of copying and pasting. However, empirical evidence also shows that copy-paste-modify complicates software maintenance and increases the frequency of bugs. To address this concern, we propose a software tool that merges together similar pieces of code and automatically creates suitable abstractions. This allows software developers to get the best of both worlds: custom abstraction together with easy re-use. To demonstrate the feasibility of our approach, we have implemented and evaluated a prototype merging tool for C++ on a number of near-miss clones (clones with some modifications) in popular Open Source packages. We found that maintainers find our algorithmically created abstractions to be largely preferable to existing duplicated code.","conference":"IEEE","terms":"Cloning;Software;Manuals;Software algorithms;Merging;Algorithm design and analysis;Face,C++ listings;public domain software;software reusability;software tools;source code (software),modern software development;code reuse;software engineers;handwritten abstractions;copy-paste-modify;source code duplication;software developers;hand-written abstractions;software tool;prototype merging tool;C++;open source packages","keywords":"Refactoring;Clone management;Software Engineering;Static Analysis;Software evolution","startPage":"630","endPage":"640","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372050","citationCount":6,"referenceCount":18,"year":2015,"authors":"K. Narasimhan; C. Reichenbach","affiliations":"Inst. fur Inf., Goethe Univ. Frankfurt, Frankfurt, Germany; Inst. fur Inf., Goethe Univ. Frankfurt, Frankfurt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d386e"},"title":"Opiner: An opinion search and summarization engine for APIs","abstract":"Opinions are key determinants to many of the activities related to software development. The perceptions of developers about an API, and the choices they make about whether and how they should use it, may, to a considerable degree, be conditioned upon how other developers see and evaluate the API. Given the plethora of APIs available for a given development task and the advent of developer forums as the media to share opinions about those APIs, it can be challenging for a developer to make informed decisions about an API to support the task. We introduce Opiner, our opinion search and summarization engine for API reviews. The server side of Opiner collects and summarizes opinions about APIs by crawling online developer forums and by associating the opinions found in the forum posts to the APIs discussed in the posts. The client side of Opiner is a Website that presents different summarized viewpoints of the opinions about the APIs in an online search engine. We evaluated Opiner by asking Industrial developers to select APIs for two development tasks. We found that developers were interested to use our proposed summaries of API reviews and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence. The Opiner online search engine is available at: http://opiner.polymtl.ca. A video demo is available at: https://youtu.be/XAXpfmg5Lqs.","conference":"IEEE","terms":"Databases;Portals;Engines;Detectors;Software;Search engines,application program interfaces;Internet;search engines,online developer forums;development tasks;API reviews;summarization engine;software development;industrial developers;opiner online search engine","keywords":"Opinion mining;API informal documentation;opinion summaries;study;summary quality","startPage":"978","endPage":"983","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115715","citationCount":2,"referenceCount":28,"year":2017,"authors":"G. Uddin; F. Khomh","affiliations":"School of Computer Science, McGill University, Montréal, QC, Canada; SWAT Lab, Polytechnique Montréal, QC, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d386f"},"title":"Bita: Coverage-guided, automatic testing of actor programs","abstract":"Actor programs are concurrent programs where concurrent entities communicate asynchronously by exchanging messages. Testing actor programs is challenging because the order of message receives depends on the non-deterministic scheduler and because exploring all schedules does not scale to large programs. This paper presents Bita, a scalable, automatic approach for testing non-deterministic behavior of actor programs. The key idea is to generate and explore schedules that are likely to reveal concurrency bugs because these schedules increase the schedule coverage. We present three schedule coverage criteria for actor programs, an algorithm to generate feasible schedules that increase coverage, and a technique to force a program to comply with a schedule. Applying Bita to real-world actor programs implemented in Scala reveals eight previously unknown concurrency bugs, of which six have already been fixed by the developers. Furthermore, we show our approach to find bugs 122× faster than random scheduling, on average.","conference":"IEEE","terms":"Schedules;Receivers;Concurrent computing;Computer bugs;Testing;Programming;Postal services,message passing;multiprocessing programs;program debugging;program testing;scheduling,Bita;coverage-guided testing;automatic testing;actor programs;concurrent programs;message exchange;concurrency bugs;scheduling","keywords":"","startPage":"114","endPage":"124","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693072","citationCount":8,"referenceCount":54,"year":2013,"authors":"S. Tasharofi; M. Pradel; Y. Lin; R. Johnson","affiliations":"Department of Computer Science, University of Illinois, Urbana, 61801, USA; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, University of Illinois, Urbana, 61801, USA; Department of Computer Science, University of Illinois, Urbana, 61801, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3870"},"title":"Context-aware integrated development environment command recommender systems","abstract":"Integrated development environments (IDEs) are complex applications that integrate multiple tools for creating and manipulating software project artifacts. To improve users' knowledge and the effectiveness of usage of the available functionality, the inclusion of recommender systems into IDEs has been proposed. We present a novel IDE command recommendation algorithm that, by taking into account the contexts in which a developer works and in which different commands are usually executed, is able to provide relevant recommendations. We performed an empirical comparison of the proposed algorithm with state-of-the-art IDE command recommenders on a real-world data set. The algorithms were evaluated in terms of precision, recall, F1, k-tail, and with a new evaluation metric that is specifically measuring the usefulness of contextual recommendations. The experiments revealed that in terms of the contextual relevance and usefulness of recommendations the proposed algorithm outperforms existing algorithms.","conference":"IEEE","terms":"Prediction algorithms;Context modeling;Algorithm design and analysis;Software;Java;Recommender systems;Software algorithms,project management;recommender systems;software engineering;software management;ubiquitous computing,context-aware integrated development environment command recommender systems;integrated development environments;software project artifacts;IDE command recommenders;available functionality;contextual recommendations;IDE command recommendation algorithm;complex applications","keywords":"","startPage":"688","endPage":"693","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115679","citationCount":2,"referenceCount":17,"year":2017,"authors":"M. Gasparic; T. Gurbanov; F. Ricci","affiliations":"Free University of Bozen-Bolzano, Piazza Domenicani, 3, 39100 Bolzano, Italy; Free University of Bozen-Bolzano, Piazza Domenicani, 3, 39100 Bolzano, Italy; Free University of Bozen-Bolzano, Piazza Domenicani, 3, 39100 Bolzano, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3871"},"title":"The rise of the (modelling) bots: Towards assisted modelling via social networks","abstract":"We are witnessing a rising role of mobile computing and social networks to perform all sorts of tasks. This way, social networks like Twitter or Telegram are used for leisure, and they frequently serve as a discussion media for work-related activities. In this paper, we propose taking advantage of social networks to enable the collaborative creation of models by groups of users. The process is assisted by modelling bots that orchestrate the collaboration and interpret the users' inputs (in natural language) to incrementally build a (meta-)model. The advantages of this modelling approach include ubiquity of use, automation, assistance, natural user interaction, traceability of design decisions, possibility to incorporate coordination protocols, and seamless integration with the user's normal daily usage of social networks. We present a prototype implementation called SOCIO, able to work over several social networks like Twitter and Telegram, and a preliminary evaluation showing promising results.","conference":"IEEE","terms":"Computational modeling;Collaboration;Twitter;Windows;Prototypes,groupware;mobile computing;natural languages;social networking (online);user interfaces,social networks;assisted modelling;natural user interaction;mobile computing;Twitter;Telegram;natural language;SOCIO;modelling bots;collaborative creation","keywords":"Collaborative modelling;meta-modelling;social networks;natural language processing","startPage":"723","endPage":"728","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115683","citationCount":5,"referenceCount":17,"year":2017,"authors":"S. Pérez-Soler; E. Guerra; J. de Lara; F. Jurado","affiliations":"Universidad Autónoma de Madrid (Spain); Universidad Autónoma de Madrid (Spain); Universidad Autónoma de Madrid (Spain); Universidad Autónoma de Madrid (Spain)","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3872"},"title":"CSeq: A concurrency pre-processor for sequential C verification tools","abstract":"Sequentialization translates concurrent programs into equivalent nondeterministic sequential programs so that the different concurrent schedules no longer need to be handled explicitly. It can thus be used as a concurrency preprocessing technique for automated sequential program verification tools. Our CSeq tool implements a novel sequentialization for C programs using pthreads, which extends the Lal/Reps sequentialization to support dynamic thread creation. CSeq now works with three different backend tools, CBMC, ESBMC, and LLBMC, and is competitive with state-of-the-art verification tools for concurrent programs.","conference":"IEEE","terms":"Concurrent computing;Instruction sets;Context;Programming;Benchmark testing,C language;concurrency control;parallel programming;program interpreters;program verification,sequential C verification tools;concurrency preprocessor;concurrent program translation;equivalent nondeterministic sequential programs;concurrency preprocessing technique;automated sequential program verification tools;CSeq tool;C program sequentialization;pthreads;Lal-Reps sequentialization;dynamic thread creation;CBMC;ESBMC;LLBMC;backend tools","keywords":"","startPage":"710","endPage":"713","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693139","citationCount":18,"referenceCount":18,"year":2013,"authors":"B. Fischer; O. Inverso; G. Parlato","affiliations":"Division of Computer Science, Stellenbosch University, South Africa; Electronics and Computer Science, University of Southampton, UK; Electronics and Computer Science, University of Southampton, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3873"},"title":"Stability of Self-Adaptive Software Architectures","abstract":"Stakeholders and organisations are increasingly looking for long-lived software. As architectures have a profound effect on the operational life-time of the software and the quality of the service provision, architectural stability could be considered a primary criterion towards achieving the long-livety of the software. Architectural stability is envisioned as the next step in quality attributes, combining many inter-related qualities. This research suggests the notion of behavioural stability as a primary criterion for evaluating whether the architecture maintains achieving the expected quality attributes, maintaining architecture robustness, and evaluating how well the architecture accommodates run-time evolutionary changes. The research investigates the notion of architecture stability at run-time in the context of self-adaptive software architectures. We expect to define, characterise and analyse this intuitive concept, as well as identify the consequent trade-offs to be dynamically managed and enhance the self-adaptation process for a long-lived software.","conference":"IEEE","terms":"Computer architecture;Stability criteria;Software;Software architecture;Adaptation models;Robustness,software architecture;software quality;stability,self-adaptive software architecture;architectural stability;software operational life-time;service provision quality","keywords":"","startPage":"886","endPage":"889","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372085","citationCount":1,"referenceCount":12,"year":2015,"authors":"M. Salama","affiliations":"Sch. of Comput. Sci., Univ. of Birmingham, Birmingham, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3874"},"title":"Do Automatically Generated Unit Tests Find Real Faults? An Empirical Study of Effectiveness and Challenges (T)","abstract":"Rather than tediously writing unit tests manually, tools can be used to generate them automatically - sometimes even resulting in higher code coverage than manual testing. But how good are these tests at actually finding faults? To answer this question, we applied three state-of-the-art unit test generation tools for Java (Randoop, EvoSuite, and Agitar) to the 357 real faults in the Defects4J dataset and investigated how well the generated test suites perform at detecting these faults. Although the automatically generated test suites detected 55.7% of the faults overall, only 19.9% of all the individual test suites detected a fault. By studying the effectiveness and problems of the individual tools and the tests they generate, we derive insights to support the development of automated unit test generators that achieve a higher fault detection rate. These insights include 1) improving the obtained code coverage so that faulty statements are executed in the first instance, 2) improving the propagation of faulty program states to an observable output, coupled with the generation of more sensitive assertions, and 3) improving the simulation of the execution environment to detect faults that are dependent on external factors such as date and time.","conference":"IEEE","terms":"Java;Testing;Generators;Writing;Manuals;Software,automatic test software;fault diagnosis;Java;program testing,unit test generation tools;Java;Randoop;EvoSuite;Agitar;Defects4J dataset;test suites;automated unit test generators;code coverage;faulty statements;faulty program state propagation;observable output;sensitive assertions;execution environment simulation;fault detection","keywords":"automated test generation;test effectiveness;unit testing;empirical study;regression testing","startPage":"201","endPage":"211","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372009","citationCount":50,"referenceCount":46,"year":2015,"authors":"S. Shamshiri; R. Just; J. M. Rojas; G. Fraser; P. McMinn; A. Arcuri","affiliations":"Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Dept. of Comput. Sci., Univ. of Sheffield, Sheffield, UK; Scienta, Norway","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3875"},"title":"Reverse Engineering Mobile Application User Interfaces with REMAUI (T)","abstract":"When developing the user interface code of a mobile application, in practice a big gap exists between the digital conceptual drawings of graphic artists and working user interface code. Currently, programmers bridge this gap manually, by reimplementing the conceptual drawings in code, which is cumbersome and expensive. To bridge this gap, we introduce the first technique to automatically Reverse Engineer Mobile Application User Interfaces (REMAUI). On a given input bitmap REMAUI identifies user interface elements such as images, texts, containers, and lists, via computer vision and optical character recognition (OCR) techniques. In our experiments on 488 screenshots of over 100 popular third-party Android and iOS applications, REMAUI-generated user interfaces were similar to the originals, both pixel-by-pixel and in terms of their runtime user interface hierarchies. REMAUI's average overall runtime on a standard desktop computer was 9 seconds.","conference":"IEEE","terms":"Graphical user interfaces;Mobile applications;Smart phones;Layout;Optical character recognition software;Containers,Android (operating system);computer vision;iOS (operating system);mobile computing;optical character recognition;reverse engineering;user interfaces,reverse engineering mobile application user interfaces;digital conceptual drawings;graphic artists;working user interface code;input bitmap;user interface elements identification;computer vision;optical character recognition techniques;OCR techniques;third-party Android;iOS applications;REMAUI-generated user interfaces;runtime user interface hierarchies;standard desktop computer","keywords":"","startPage":"248","endPage":"259","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372013","citationCount":15,"referenceCount":55,"year":2015,"authors":"T. A. Nguyen; C. Csallner","affiliations":"Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA; Comput. Sci. \u0026 Eng. Dept., Univ. of Texas at Arlington, Arlington, TX, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3876"},"title":"DSIbin: Identifying dynamic data structures in C/C++ binaries","abstract":"Reverse engineering binary code is notoriously difficult and, especially, understanding a binary's dynamic data structures. Existing data structure analyzers are limited wrt. program comprehension: they do not detect complex structures such as skip lists, or lists running through nodes of different types such as in the Linux kernel's cyclic doubly-linked list. They also do not reveal complex parent-child relationships between structures. The tool DSI remedies these shortcomings but requires source code, where type information on heap nodes is available. We present DSIbin, a combination of DSI and the type excavator Howard for the inspection of C/C++ binaries. While a naive combination already improves upon related work, its precision is limited because Howard's inferred types are often too coarse. To address this we auto-generate candidates of refined types based on speculative nested-struct detection and type merging; the plausibility of these hypotheses is then validated by DSI. We demonstrate via benchmarking that DSIbin detects data structures with high precision.","conference":"IEEE","terms":"Tools;Shape;Data structures;Linux;Kernel;Benchmark testing;Merging,binary codes;C++ language;data structures;excavators;Linux;program diagnostics;reverse engineering,DSIbin;dynamic data structure identification;C-C++ binaries;data structure analysis;Howards inferred types;nested-struct detection;type excavator Howard;source code;tool DSI;complex parent-child relationships;Linux kernel;skip lists;complex structures;program comprehension;reverse engineering binary code","keywords":"Data structure identification;reverse engineering;dynamic data structures;pointer programs;type recovery","startPage":"331","endPage":"341","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115646","citationCount":4,"referenceCount":39,"year":2017,"authors":"T. Rupprecht; X. Chen; D. H. White; J. H. Boockmann; G. Lüttgen; H. Bos","affiliations":"University of Bamberg, Germany; Microsoft, Canada; University of Bamberg, Germany; University of Bamberg, Germany; University of Bamberg, Germany; Vrije Universiteit Amsterdam, The Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3877"},"title":"Crust: A Bounded Verifier for Rust (N)","abstract":"Rust is a modern systems language that provides guaranteed memory safety through static analysis. However, Rust includes an escape hatch in the form of \"unsafe code,\" which the compiler assumes to be memory safe and to preserve crucial pointer aliasing invariants. Unsafe code appears in many data structure implementations and other essential libraries, and bugs in this code can lead to memory safety violations in parts of the program that the compiler otherwise proved safe. We present CRUST, a tool combining exhaustive test generation and bounded model checking to detect memory safety errors, as well as violations of Rust's pointer aliasing invariants within unsafe library code. CRUST requires no programmer annotations, only an indication of the modules to check. We evaluate CRUSTon data structures from the Rust standard library. It detects memory safety bugs that arose during the library's development and remained undetected for several months.","conference":"IEEE","terms":"Arrays;Libraries;Safety;Computer bugs;Indexes;Standards,data structures;formal verification;program compilers;program debugging;program diagnostics;program testing;software libraries,CRUST;Rust;static analysis;compiler;pointer aliasing invariants;data structure implementations;memory safety violations;exhaustive test generation;bounded model checking;unsafe library code;memory safety bugs","keywords":"SMT-based verification;test generation;memory safety","startPage":"75","endPage":"80","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371997","citationCount":5,"referenceCount":10,"year":2015,"authors":"J. Toman; S. Pernsteiner; E. Torlak","affiliations":"Dept. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA; Dept. of Comput. Sci. \u0026 Eng., Univ. of Washington, Seattle, WA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3878"},"title":"TrEKer: Tracing error propagation in operating system kernels","abstract":"Modern operating systems (OSs) consist of numerous interacting components, many of which are developed and maintained independently of one another. In monolithic systems, the boundaries of and interfaces between such components are not strictly enforced at runtime. Therefore, faults in individual components may directly affect other parts of the system in various ways. Software fault injection (SFI) is a testing technique to assess the resilience of a software system in the presence of faulty components. Unfortunately, SFI tests of OSs are inconclusive if they do not lead to observable failures, as corruptions of the internal software state may not be visible at its interfaces and, yet, affect the subsequent execution of the OS beyond the duration of the test. In this paper we present TrEKer, a fully automated approach for identifying how faulty OS components affect other parts of the system. TrEKer combines static and dynamic analyses to achieve efficient tracing on the granularity of memory accesses. We demonstrate TrEKer's ability to support SFI oracles by accurately tracing the effects of faults injected into three widely used Linux kernel modules.","conference":"IEEE","terms":"Kernel;Hardware;Instruments;Testing;Fault diagnosis,fault diagnosis;Linux;operating system kernels;software fault tolerance,faulty components;SFI tests;observable failures;internal software state;fully automated approach;faulty OS components;efficient tracing;SFI oracles;modern operating systems;numerous interacting components;monolithic systems;software fault injection;testing technique;Linux kernel modules;tracing error propagation;operating system kernels;software system resilience;OS;TrEKer","keywords":"Software Fault Injection;Robustness Testing;Test Oracles;Execution Tracing;Operating Systems","startPage":"377","endPage":"387","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115650","citationCount":1,"referenceCount":40,"year":2017,"authors":"N. Coppik; O. Schwahn; S. Winter; N. Suri","affiliations":"DEEDS Group, TU Darmstadt, Darmstadt, Germany; DEEDS Group, TU Darmstadt, Darmstadt, Germany; DEEDS Group, TU Darmstadt, Darmstadt, Germany; DEEDS Group, TU Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3879"},"title":"A comparative analysis of software architecture recovery techniques","abstract":"Many automated techniques of varying accuracy have been developed to help recover the architecture of a software system from its implementation. However, rigorously assessing these techniques has been hampered by the lack of architectural “ground truths”. Over the past several years, we have collected a set of eight architectures that have been recovered from open-source systems and independently, carefully verified. In this paper, we use these architectures as ground truths in performing a comparative analysis of six state-of-the-art software architecture recovery techniques. We use a number of metrics to assess each technique for its ability to identify a system's architectural components and overall architectural structure. Our results suggest that two of the techniques routinely outperform the rest, but even the best of the lot has surprisingly low accuracy. Based on the empirical data, we identify several avenues of future research in software architecture recovery.","conference":"IEEE","terms":"Computer architecture;Accuracy;Vectors;Clustering algorithms;Algorithm design and analysis;Software architecture;Java,formal verification;software architecture;software maintenance,software system architecture recovery techniques;open source systems;ground truths;system architectural components;architectural structure","keywords":"","startPage":"486","endPage":"496","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693106","citationCount":37,"referenceCount":51,"year":2013,"authors":"J. Garcia; I. Ivkovic; N. Medvidovic","affiliations":"Computer Science Department, University of Southern California, Los Angeles, 90089, USA; Wilfrid Laurier University, 75 University Avenue West, Waterloo, ON, N2L 3C5, Canada; Computer Science Department, University of Southern California, Los Angeles, 90089, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d387a"},"title":"Minimizing CPU time shortage risks in integrated embedded software","abstract":"A major activity in many industries is to integrate software artifacts such that the functional and performance requirements are properly taken care of. In this paper, we focus on the problem of minimizing the risk of CPU time shortage in integrated embedded systems. In order to minimize this risk, we manipulate the start time (offset) of the software executables such that the system real-time constraints are satisfied, and further, the maximum CPU time usage is minimized. We develop a number of search-based optimization algorithms, specifically designed to work for large search spaces, to compute offsets for concurrent software executables with the objective of minimizing CPU usage. We evaluated and compared our algorithms by applying them to a large automotive software system. Our experience shows that our algorithms can automatically generate offsets such that the maximum CPU usage is very close to the known lower bound imposed by the domain constraints. Further, our approach finds limits on the maximum CPU usage lower than those found by a random strategy, and is not slower than a random strategy. Finally, our work achieves better results than the CPU usage minimization techniques devised by domain experts.","conference":"IEEE","terms":"Software;Automotive engineering;Real-time systems;Software algorithms;Industries;Synchronization,embedded systems;integrated software;search problems,CPU time shortage risks minimization;integrated embedded software;integrated embedded systems;search-based optimization algorithms;automotive software system;random strategy","keywords":"","startPage":"529","endPage":"539","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693110","citationCount":3,"referenceCount":27,"year":2013,"authors":"S. Nejati; M. Adedjouma; L. C. Briand; J. Hellebaut; J. Begey; Y. Clement","affiliations":"SnT Center, University of Luxembourg, Luxembourg; SnT Center, University of Luxembourg, Luxembourg; SnT Center, University of Luxembourg, Luxembourg; Delphi Automotive Systems, Luxembourg; Delphi Automotive Systems, Luxembourg; Delphi Automotive Systems, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d387b"},"title":"Exploiting Domain and Program Structure to Synthesize Efficient and Precise Data Flow Analyses (T)","abstract":"A key challenge in implementing an efficient and precise data flow analysis is determining how to abstract the domain of values that a program variable can take on and how to update abstracted values to reflect program semantics. Such updates are performed by a transfer function and recent work by Thakur, Elder and Reps defined the bilateral algorithm for computing the most precise transfer function for a given abstract domain. In this paper, we identify and exploit the special case where abstract domains are comprised of disjoint subsets. For such domains, transfer functions computed using a customized algorithm can improve performance and in combination with symbolic modeling of block-level transfer functions improve precision as well. We implemented these algorithms in Soot and used them to perform data flow analysis on more than 100 non-trivial Java methods drawn from open source projects. Our experimental data are promising as they demonstrate that a 25-fold reduction in analysis time can be achieved and precision can be increased relative to existing methods.","conference":"IEEE","terms":"Transfer functions;Algorithm design and analysis;Concrete;Computational modeling;Semantics;Lattices;Computer science,data flow analysis;Java;transfer functions,program structure;domain structure;data flow analysis;program semantics;bilateral algorithm;abstract domains;disjoint subsets;symbolic modeling;block-level transfer functions;Soot;Java methods;open source projects","keywords":"Data flow analysis;transfer function;abstract domain","startPage":"608","endPage":"618","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372048","citationCount":3,"referenceCount":29,"year":2015,"authors":"E. Sherman; M. B. Dwyer","affiliations":"NA; NA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d387c"},"title":"Covert Communication in Mobile Applications (T)","abstract":"This paper studies communication patterns in mobile applications. Our analysis shows that 63% of the external communication made by top-popular free Android applications from Google Play has no effect on the user-observable application functionality. To detect such covert communication in an efficient manner, we propose a highly precise and scalable static analysis technique: it achieves 93% precision and 61% recall compared to the empirically determined \"ground truth\", and runs in a matter of a few minutes. Furthermore, according to human evaluators, in 42 out of 47 cases, disabling connections deemed covert by our analysis leaves the delivered application experience either completely intact or with only insignificant interference. We conclude that our technique is effective for identifying and disabling covert communication. We then use it to investigate communication patterns in the 500 top-popular applications from Google Play.","conference":"IEEE","terms":"Google;Androids;Humanoid robots;Mobile applications;Interference;Servers;Visualization,Android (operating system);mobile computing;program diagnostics,covert communication;mobile applications;communication patterns;top-popular free Android applications;Google Play;user-observable application functionality;scalable static analysis technique;ground truth;human evaluators;application experience","keywords":"communication;mobile applications;Android;application analysis","startPage":"647","endPage":"657","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372052","citationCount":7,"referenceCount":36,"year":2015,"authors":"J. Rubin; M. I. Gordon; N. Nguyen; M. Rinard","affiliations":"Massachusetts Inst. of Technol., Cambridge, MA, USA; Massachusetts Inst. of Technol., Cambridge, MA, USA; Global InfoTek Inc., USA; Massachusetts Inst. of Technol., Cambridge, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d387d"},"title":"Kobold: Web usability as a service","abstract":"While Web applications have become pervasive in today's business, social interaction and information exchange, their usability is often deficient, even being a key factor for a website success. Usability problems repeat across websites, and many of them have been catalogued, but usability evaluation and repair still remains expensive. There are efforts from both the academy and industry to automate usability testing or to provide automatic statistics, but they rarely offer concrete solutions. These solutions appear as guidelines or patterns that developers can follow manually. This paper presents Kobold, a tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solutions. By using the refactoring technique and its associated concept of bad smell, Kobold mines UI events to detect usability smells and applies usability refactorings on the client to correct them. The purpose of Kobold is to deliver usability advice and solutions as a service (SaaS) for developers, allowing them to respond to feedback of the real use of their applications and improve usability incrementally, even when there are no usability experts on the team. Kobold is available at: http://autorefactoring.lifia.info.unlp.edu.ar. A screencast is available at https://youtu.be/c-myYPMUh0Q.","conference":"IEEE","terms":"Usability;Tools;Concrete;Software as a service;Servers;Automation;Business,business data processing;Internet;software maintenance;software quality;Web sites,user interaction events;Kobold mines;usability refactorings;Web usability;Web applications;social interaction;information exchange;usability evaluation;automatic statistics;Web site","keywords":"Web Usability;Software as a Service;Usability Refactoring","startPage":"990","endPage":"995","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115717","citationCount":2,"referenceCount":13,"year":2017,"authors":"J. Grigera; A. Garrido; G. Rossi","affiliations":"LIFIA, Universidad Nacional de La Plata, Argentina, Also at CIC, Argentina; LIFIA, Universidad Nacional de La Plata, Argentina, Also at CONICET, Argentina; LIFIA, Universidad Nacional de La Plata, Argentina, Also at CONICET, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d387e"},"title":"BLITZ: Compositional bounded model checking for real-world programs","abstract":"Bounded Model Checking (BMC) for software is a precise bug-finding technique that builds upon the efficiency of modern SAT and SMT solvers. BMC currently does not scale to large programs because the size of the generated formulae exceeds the capacity of existing solvers. We present a new, compositional and property-sensitive algorithm that enables BMC to automatically find bugs in large programs. A novel feature of our technique is to decompose the behaviour of a program into a sequence of BMC instances and use a combination of satisfying assignments and unsatisfiability proofs to propagate information across instances. A second novelty is to use the control- and data-flow of the program as well as information from proofs to prune the set of variables and procedures considered and hence, generate smaller instances. Our tool BLITZ outperforms existing tools and scales to programs with over 100,000 lines of code. BLITZ automatically and efficiently discovers bugs in widely deployed software including new vulnerabilities in Internet infrastructure software.","conference":"IEEE","terms":"Input variables;Computer bugs;Semantics;Model checking;Software;Indium phosphide;Internet,computability;data flow computing;Internet;program debugging;program verification,BLITZ;compositional bounded model checking;real-world programs;BMC;bug-finding technique;SAT solver;SMT solver;compositional algorithm;property-sensitive algorithm;control-flow;data-flow;Internet infrastructure software","keywords":"","startPage":"136","endPage":"146","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693074","citationCount":9,"referenceCount":21,"year":2013,"authors":"C. Y. Cho; V. D'Silva; D. Song","affiliations":"University of California, Berkeley, USA; DSO National Laboratories, Singapore; DSO National Laboratories, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d387f"},"title":"Promoting secondary orders of event pairs in randomized scheduling using a randomized stride","abstract":"Because of the wide use of randomized scheduling in concurrency testing research, it is important to understand randomized scheduling and its limitations. This work analyzes how randomized scheduling discovers concurrency bugs by focusing on the probabilities of the two possible orders of a pair of events. Analysis shows that the disparity between probabilities can be large for programs that encounter a large number of events during execution. Because sets of ordered event pairs define conditions for discovering concurrency bugs, this disparity can make some concurrency bugs highly unlikely. The complementary nature of the two possible orders also indicates a potential trade-off between the probability of discovering frequently-occurring and infrequently-occurring concurrency bugs. To help address this trade-off in a more balanced way, randomized-stride scheduling is proposed, where scheduling granularity for each thread is adjusted using a randomized stride calculated based on thread length. With some assumptions, strides can be calculated to allow covering the least likely event pair orders. Experiments confirm the analysis results and also suggest that randomized-stride scheduling is more effective for discovering concurrency bugs compared to the original randomized scheduling implementation, and compared to other algorithms in recent literature.","conference":"IEEE","terms":"Computer bugs;Concurrent computing;Testing;Instruction sets;Scheduling algorithms;Message systems;Scheduling,concurrency control;probability;processor scheduling;program debugging,secondary orders;concurrency testing research;probabilities;ordered event pairs;randomized-stride scheduling;scheduling granularity;event pair orders;infrequently-occurring concurrency bugs;frequently-occurring concurrency bugs","keywords":"Multithreading;software debugging;software quality;parallel programming;scheduling algorithms","startPage":"741","endPage":"752","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115685","citationCount":0,"referenceCount":53,"year":2017,"authors":"M. Abdelrasoul","affiliations":"North Carolina State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3880"},"title":"Systematically testing background services of mobile apps","abstract":"Contrary to popular belief, mobile apps can spend a large fraction of time running \"hidden\" as background services. And, bugs in services can translate into crashes, energy depletion, device slow-down, etc. Unfortunately, without necessary testing tools, developers can only resort to telemetries from user devices in the wild. To this end, Snowdrop is a testing framework that systematically identifies and automates background services in Android apps. Snowdrop realizes a service-oriented approach that does not assume all inter-component communication messages are explicitly coded in the app bytecode. Furthermore, to improve the completeness of test inputs generated, Snowdrop infers field values by exploiting the similarity in how developers name variables. We evaluate Snowdrop by testing 848 commercially available mobile apps. Empirical results show that Snowdrop can achieve 20.91% more code path coverage than pathwise test input generators, and 64.11% more coverage than random test input generators.","conference":"IEEE","terms":"Testing;Androids;Humanoid robots;Tools;Mobile communication;Generators;Telemetry,Android (operating system);mobile computing;program testing;service-oriented architecture,background services;Android apps;Snowdrop;app bytecode;pathwise test input generators;random test input generators;user devices;testing tools;service-oriented approach;intercomponent communication messages;code path coverage","keywords":"App background services;test input generation;Android Intents","startPage":"4","endPage":"15","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115613","citationCount":4,"referenceCount":46,"year":2017,"authors":"L. L. Zhang; C. M. Liang; Y. Liu; E. Chen","affiliations":"University of Science and Technology of China, China; Microsoft Research, China; Microsoft Research, China; University of Science and Technology of China, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3881"},"title":"The interactive verification debugger: Effective understanding of interactive proof attempts","abstract":"The Symbolic Execution Debugger (SED) is an extension of the Eclipse debug platform for interactive symbolic execution. Like a traditional debugger, the SED can be used to locate the origin of a defect and to increase program understanding. However, as it is based on symbolic execution, all execution paths are explored simultaneously. We demonstrate an extension of the SED called Interactive Verification Debugger (IVD) for inspection and understanding of formal verification attempts. By a number of novel views, the IVD allows to quickly comprehend interactive proof situations and to debug the reasons for a proof attempt that got stuck. It is possible to perform interactive proofs completely from within the IVD. It can be experimentally demonstrated that the IVD is more effective in understanding proof attempts than a conventional prover user interface. A screencast explaining proof attempt inspection with the IVD is available at youtu.be/8e-q9Jf1h_w.","conference":"IEEE","terms":"Java;Inspection;Software;Visualization;Indexes;User interfaces;Debugging,interactive systems;program debugging;program verification;theorem proving;user interfaces,interactive verification debugger;IVD;interactive proof attempt;symbolic execution debugger;SED;Eclipse debug platform;interactive symbolic execution;formal verification attempt","keywords":"Symbolic Execution;Debugging;Program Execution Visualization;Verification;Proof Understanding","startPage":"846","endPage":"851","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582826","citationCount":0,"referenceCount":15,"year":2016,"authors":"M. Hentschel; R. Hähnle; R. Bubel","affiliations":"TU Darmstadt, Darmstadt, Germany; TU Darmstadt, Darmstadt, Germany \u0026 Università degli Studi di Torino, Torino, Italy; TU Darmstadt, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3882"},"title":"Automated testing and notification of mobile app privacy leak-cause behaviours","abstract":"I describe the design, implementation and evaluation of a novel hybrid static/dynamic analysis system for automatically uncovering and testing for the user-triggered causes and paths of privacy leaks in Android applications (privacy `leak-causes'). I describe how I plan to further evaluate and demonstrate improvements in accuracy, coverage and testing speed of my hybrid testing approach against other currently available systems. I also show how user privacy is improved by the presentation of information on leak-causes in a field study as privacy notices. I present plans to investigate which of the commonly utilized mobile notification mechanisms is best suited to the presentation of leak-causes, as well as how users may set better privacy control policies with the information provided.","conference":"IEEE","terms":"Privacy;Testing;Data privacy;Androids;Humanoid robots;Smart phones;Mobile communication,data privacy;mobile computing;program diagnostics;program testing,mobile app privacy leak-cause behaviour;mobile application;hybrid static-dynamic analysis system;user-triggered causes;Android applications;user privacy;privacy notices;mobile notification mechanism;privacy control policy","keywords":"","startPage":"880","endPage":"883","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582832","citationCount":0,"referenceCount":16,"year":2016,"authors":"J. C. J. Keng","affiliations":"School of Information Systems, Singapore Management University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3883"},"title":"Statistical analysis of large sets of models","abstract":"Many applications in Model-Driven Engineering involve processing multiple models, e.g. for comparing and merging of model variants into a common domain model. Despite many sophisticated techniques for model comparison, little attention has been given to the initial data analysis and filtering activities. These are hard to ignore especially in the case of a large dataset, possibly with outliers and sub-groupings. We would like to develop a generic approach for model comparison and analysis for large datasets; using techniques from information retrieval, natural language processing and machine learning. We are implementing our approach as an open framework and have so far evaluated it on public datasets involving domain analysis, repository management and model searching scenarios.","conference":"IEEE","terms":"Biological system modeling;Analytical models;Computational modeling;Feature extraction;Merging;Data models;Natural language processing,data analysis;information filtering;learning (artificial intelligence);natural language processing;software engineering;statistical analysis,statistical analysis;model large set;model-driven engineering;filtering activities;generic approach;model comparison;large dataset analysis;information retrieval;natural language processing;machine learning;domain analysis;repository management;model searching scenarios","keywords":"Model-driven engineering;model comparison;vector space model;clustering","startPage":"888","endPage":"891","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582834","citationCount":0,"referenceCount":17,"year":2016,"authors":"Ö. Babur","affiliations":"Eindhoven University of Technology, 5600 MB Eindhoven, The Netherlands","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3884"},"title":"API recommendation system for software development","abstract":"Nowadays, software developers often utilize existing third party libraries and make use of Application Programming Interface (API) to develop a software. However, it is not always obvious which library to use or whether the chosen library will play well with other libraries in the system. Furthermore, developers need to spend some time to understand the API to the point that they can freely use the API methods and putting the right parameters inside them. In this work, I plan to automatically recommend relevant APIs to developers. This API recommendation can be divided into multiple stages. First, we can recommend relevant libraries provided a given task to complete. Second, we can recommend relevant API methods that developer can use to program the required task. Third, we can recommend correct parameters for a given method according to its context. Last but not least, we can recommend how different API methods can be combined to achieve a given task. In effort to realize this API recommendation system, I have published two related papers. The first one deals with recommending additional relevant API libraries given known useful API libraries for the target program. This system can achieve recall rate@5 of 0.852 and recall rate@10 of 0.894 in recommending additional relevant libraries. The second one deals with recommending relevant API methods a given target API and a textual description of the task. This system can achieve recall-rate@5 of 0.690 and recall-rate@10 of 0.779. The results for both system indicate that the systems are useful and capable in recommending the right API/library reasonably well. Currently, I am working on another system which can recommend web APIs (i.e., libraries) given a description of the task. I am also working on a system that recommends correct parameters given an API method. In the future, I also plan to realize API composition recommendation for the given task.","conference":"IEEE","terms":"Libraries;Feature extraction;Training;Software;Databases;History;Context,application program interfaces;recommender systems;software libraries,API recommendation system;software development;application programming interface;API libraries;textual description;recall-rate","keywords":"API;Library;Recommendation System","startPage":"896","endPage":"899","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582836","citationCount":0,"referenceCount":25,"year":2016,"authors":"F. Thung","affiliations":"School of Information Systems, Singapore Management University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3885"},"title":"Changing microsoft's build: Revolution or evolution","abstract":"Tens of thousands of Microsoft engineers build and test hundreds of software products several times a day. It is essential that this continuous integration scales, guarantees short feedback cycles, and functions reliably with minimal human intervention. During the past three years TSE's charter has been to shorten this cycle time. We went after this goal in two ways: Evolution via CloudBuild and Revolution via Concord. CloudBuild is a build service infrastructure, now being used by all major product groups in Microsoft, like Azure, Bing, Office, SQL except for Windows. CloudBuild addresses all aspects of a continuous integration workflow, like builds, test and code analysis, but also drops, package and symbol creation and storage. CloudBuild supports multiple build languages as long as they fulfill a coarse grained IO based contract. CloudBuild uses content based caching to run build-related tasks only when needed. Lastly, it builds on many machines in parallel. The speed ups of build and testing range from 1.2x to 10x. CloudBuild aims to rapidly onboard teams and hence has to support non-deterministic build tools and specification languages that under-declare dependencies. CloudBuild, being a reliable build service in the presence of unreliable components, currently achieves service availability better than 99%. Windows went a different path. Their past build exhaust was so massive that building Windows in the cloud and bringing the build results back for testing on corp.-net. was considered infeasible. So they decided to move to a new build language, codename Concord. By construction, Concord guarantees reliable builds, no over-build, and allows for efficient distribution. Adopting Concord has led to immense performance improvements, we have seen up to 100X speedup for Windows builds. But the path has been long and rocky, since it not only requires a substantial rewrite of existing build logic, but also all related developer and build lab processes have to change. Whether evolution or revolution is the right path forward - the verdict is still out.","conference":"IEEE","terms":",cache storage;cloud computing;parallel machines;program testing;software performance evaluation;specification languages,microsoft build changing;software product testing;software product building;continuous integration scales;short feedback cycles;functions reliably;minimal human intervention;cloudbuild evolution;concord revolution;content based caching;parallel machines;non deterministic build tools;specification languages;performance improvements","keywords":"Build-system;build-language;build-automation;build-as-a-service","startPage":"2","endPage":"2","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582738","citationCount":0,"referenceCount":0,"year":2016,"authors":"W. Schulte","affiliations":"Microsoft, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3886"},"title":"AnModeler: A tool for generating domain models from textual specifications","abstract":"This paper presents AnModeler, a tool for generating analysis models from software requirements specified using use cases. The tool uses the Stanford natural language parser to extract type dependencies (TDs) and parts of speech tags (POS-tags) of sentences from input Use Case Specification (UCS). Then, it identifies sentence structures using a set of rules framed based on Hornby's verb patterns. With the information of the TDs, POS tags, and the identified sentence structures, the tool discovers domain elements, viz.: domain objects (including their attributes and operations) and interactions between them; it consolidates the domain information as a class diagram (as well as a sequence diagram). An experiment conducted on 10 UCSs with two industry experts as subjects showed that the analysis class diagrams generated by AnModeler were remarkably close to those generated by the two industry experts. Being lightweight and easy to use, the tool can also be used to assist students and young developers in acquiring object-oriented domain modeling skills quickly. Link to a short demonstration video: https://youtu.be/_Ct-qF4Y1fU.","conference":"IEEE","terms":"Analytical models;Object oriented modeling;Software;Unified modeling language;Industries;Visualization;Java,diagrams;grammars;natural languages;object-oriented methods;software engineering;text analysis,AnModeler;domain model generating tool;textual specifications;analysis model generation;software requirements;Stanford natural language parser;type dependency extraction;TD extraction;parts of speech tags;POS-tags;use case specification;UCS;Hornby verb patterns;identified sentence structures;domain elements;domain information;sequence diagram;analysis class diagrams generated;object-oriented domain modeling","keywords":"Automated tool for analysis modeling;Model transformation;Analysis class diagram;Problem level sequence diagram;Template code;Automated approach","startPage":"828","endPage":"833","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582823","citationCount":0,"referenceCount":23,"year":2016,"authors":"J. S. Thakur; A. Gupta","affiliations":"Indian Institute of Information Technology, Design and Manufacturing, Jabalpur, India, Jabalpur Engineering College, Jabalpur, India; Indian Institute of Information Technology, Design and Manufacturing Jabalpur, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3887"},"title":"Factoring requirement dependencies in software requirement selection using graphs and integer programming","abstract":"Software requirement selection is to find a subset of requirements (so-called optimal set) that gives the highest customer value for a release of software while keeping the cost within the budget. Several industrial studies however, have demonstrated that requirements of software projects are intricately interdependent and these interdependencies impact the values of requirements. Furthermore, the strengths of dependency relations among requirements vary in the context of real-world projects. For instance, requirements can be strongly or weakly interdependent. Therefore, it is important to consider both the existence and the strengths of dependency relations during requirement selection. The existing selection models however, have ignored either requirement dependencies altogether or the strengths of those dependencies. This research proposes an Integer programming model for requirement selection which considers both the existence and strengths of requirement dependencies. We further contribute a graph-based dependency modeling technique for capturing requirement dependencies and the their corresponding strengths. Automated/semi-automated techniques will also be devised to identify requirement dependencies and the strengths of those dependencies.","conference":"IEEE","terms":"Software;Computational modeling;Planning;Linear programming;Requirements engineering;Complexity theory;Optimization,formal specification;formal verification;graph theory;integer programming;project management;software management;software selection;systems analysis,requirement dependencies;software requirement selection;graphs;optimal set;budgeted cost;software project requirements;dependency relations;integer programming model;graph-based dependency modeling technique;automated techniques","keywords":"Software Requirement Selection;Software Requirement Dependencies;Graph;Integer Programming","startPage":"884","endPage":"887","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582833","citationCount":0,"referenceCount":38,"year":2016,"authors":"D. Mougouei","affiliations":"School of Computer Science, Engineering, and Mathematics, Flinders University, Adelaide, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3888"},"title":"Developer targeted analytics: Supporting software development decisions with runtime information","abstract":"Runtime information of deployed software has been used by business and operations units to make informed decisions under the term “analytics”. However, decisions made by software engineers in the course of evolving software have, for the most part, been based on personal belief and gut-feeling. This could be attributed to software development being, for the longest time, viewed as an activity that is detached from the notion of operating software in a production environment. In recent years, this view has been challenged with the emergence of the DevOps movement, which aim is to promote cross-functional capabilities of development and operations activities within teams. This shift in process and mindset requires analytics tools that specifically target software developers. In this research, I investigate approaches to support developers in their decision-making by incorporating runtime information in source code and provide live feedback in IDEs by predicting the impact of code changes.","conference":"IEEE","terms":"Runtime;Uncertainty;Production;Decision making;Software performance;Computational modeling,business data processing;decision making;software engineering;source code (software),software development decisions;runtime information;business unit;operation unit;developer targeted analytics;decision making;source code;software analytics;performance engineering","keywords":"Developer Targeted Analytics;Software Analytics;Performance Engineering","startPage":"892","endPage":"895","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582835","citationCount":0,"referenceCount":31,"year":2016,"authors":"J. Cito","affiliations":"Software evolution \u0026 architecture lab, University of Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3889"},"title":"Program generation for performance","abstract":"It has become extraordinarily difficult to write software that performs close to optimally on complex modern microarchitectures. Particularly plagued are domains that require complex mathematical computations such as multimedia processing, communication, control, graphics, and machine learning. In these domains, performance-critical components are usually written in C (with possible extensions) and often even in assembly, carefully “tuned” to the platform's architecture and microarchitecture. The result is usually long, rather unreadable code that needs to be re-written or re-tuned with every platform upgrade. On the other hand, the performance penalty for relying on straightforward, non-tuned, “more elegant” implementations can be often a factor of 10, 100, or even more. The overall problem is one of productivity, maintainability, and quality (namely performance), i.e., software engineering. However, even though a large set of sophisticated software engineering theory and tools exist, it appears that to date this community has not focused much on mathematical computations nor performance in the detailed, close-to-optimal sense above. The reason for the latter may be that performance, unlike various aspects of correctness, is not syntactic in nature (and in reality is often even unpredictable and, well, messy). The aim of this talk is to draw attention to the performance/productivity problem for mathematical applications and to make the case for a more interdisciplinary attack. As a set of thoughts in this direction we offer some of the lessons we have learned in the last decade in our own research on Spiral (www.spiral.net), a program generation framework for numerical kernels. Key techniques used in Spiral include staged declarative domain-specific languages to express algorithm knowledge and algorithm transformations, the use of platform-cognizant rewriting systems for parallelism and locality optimizations, and the use of search and machine learning techniques to navigate possible spaces of choices. Experimental results show that the codegenerated by Spiral competes with, and sometimes outperforms, the best available human-written code. Spiral has been used to generate part of Intel's commercial libraries IPP and MKL.","conference":"IEEE","terms":",learning (artificial intelligence);program compilers;rewriting systems;software architecture;software maintenance;software performance evaluation;software quality,program generation framework;software performance;performance-critical component;platform architecture;software maintainability;software quality;software engineering;rewriting system;machine learning","keywords":"Program generation;automatic programming;parallelization;vectorization;rewriting systems;Fourier transform;matrix algebra","startPage":"1","endPage":"1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582737","citationCount":0,"referenceCount":0,"year":2016,"authors":"M. Püschel","affiliations":"Department of Computer Science, ETH Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d388a"},"title":"The power of probabilistic thinking","abstract":"Traditionally, software engineering has dealt in absolutes. For instance, we talk about a system being “correct” or “incorrect”, with the shades of grey in between occasionally acknowledged but rarely dealt with explicitly. And we typically employ logical, algebraic, relational and other representations and techniques that help us reason about software in such absolute terms. There of course have been notable exceptions to this, such as the use of statistical techniques in testing and debugging. But by and large, both researchers and practitioners have favored the relative comfort of an absolutist viewpoint in all aspects of development. In this talk, I will argue the benefits of taking a more thoroughly probabilistic approach in software engineering. Software engineering is rife with stochastic phenomena, and the vast majority of software systems operate in an environment of uncertain, random behavior, which suits an explicit probabilistic characterization. Furthermore, this uncertainty is becoming ever more pronounced in new software systems and platforms, such as the Internet of Things and autonomous vehicles, with their frequent imprecise outputs and heavy reliance on machine learning. To illustrate more deeply some of the considerations involved in taking a probabilistic approach, I will talk about some recent research I have been doing in probabilistic verification.","conference":"IEEE","terms":",learning (artificial intelligence);program debugging;program testing;program verification;uncertainty handling,probabilistic thinking;software engineering;testing;debugging;stochastic phenomena;machine learning;probabilistic verification","keywords":"Probabilistic reasoning;software engineering;stochastic behavior","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582739","citationCount":0,"referenceCount":0,"year":2016,"authors":"D. S. Rosenblum","affiliations":"National University of Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d388b"},"title":"SimilarTech: Automatically recommend analogical libraries across different programming languages","abstract":"Third-party libraries are an integral part of many software projects. It often happens that developers need to find analogical libraries that can provide comparable features to the libraries they are already familiar with. Existing methods to find analogical libraries are limited by the community-curated list of libraries, blogs, or Q\u0026A posts, which often contain overwhelming or out-of-date information. This paper presents our tool SimilarTech (https://graphofknowledge. appspot.com/similartech) that makes it possible to automatically recommend analogical libraries by incorporating tag embeddings and domain-specific relational and categorical knowledge mined from Stack Overflow. SimilarTech currently supports recommendation of 6,715 libraries across 6 different programming languages. We release our SimilarTech website for public use. The SimilarTech website attracts more than 2,400 users in the past 6 months. We observe two typical usage patterns of our website in the website visit logs which can satisfy different information needs of developers. The demo video can be seen at https://youtu.be/ubx8h4D4ieE.","conference":"IEEE","terms":"Libraries;Java;Knowledge based systems;Operating systems;Tagging;Mobile communication,programming languages;software libraries,SimilarTech tool;third-party libraries;programming languages;software projects;analogical libraries;domain-specific relational knowledge;tag embeddings;categorical knowledge","keywords":"","startPage":"834","endPage":"839","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582824","citationCount":0,"referenceCount":19,"year":2016,"authors":"C. Chen; Z. Xing","affiliations":"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d388c"},"title":"Conc-iSE: Incremental symbolic execution of concurrent software","abstract":"Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be re-explored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.","conference":"IEEE","terms":"Software;Testing;Concurrent computing;Algorithm design and analysis;Software algorithms;Computer bugs;Programming,C language;concurrency (computers);multi-threading;program debugging;program testing,Conc-iSE;concurrent software incremental symbolic execution;software updates;bugs;code bases;regression testing tools;test case selection;test case prioritization;sequential software;program versions;interthread change-impact analysis;interprocedural change-impact analysis;execution summaries;multithreaded C programs","keywords":"Symbolic execution;Concurrency;Partial order reduction;Weakest precondition","startPage":"531","endPage":"542","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582788","citationCount":0,"referenceCount":49,"year":2016,"authors":"S. Guo; M. Kusano; C. Wang","affiliations":"Department of ECE, Virginia Tech, Blacksburg, VA, USA; Department of ECE, Virginia Tech, Blacksburg, VA, USA; Department of CS, University of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d388d"},"title":"Identifying domain elements from textual specifications","abstract":"Analysis modeling refers to the task of identifying domain objects, their attributes and operations, and the relationships between these objects from software requirements specifications which are usually written in some natural language. There have been a few efforts to automate this task, but they seem to be largely constrained by the language related issues as well as the lack of a systematic transformation process. In this paper, we propose a systematic, automated transformation approach which first interprets the specification sentences based on the Hornby's verb patterns, and then uses semantic relationships between the words in the sentences, obtained from Type Dependencies using Stanford NL Parser, to identify the domain elements from them. With the help of a controlled experiment, we show that the analysis class diagrams generated by the proposed approach are far more correct, far more complete and less redundant than those generated by the exiting automated approaches.","conference":"IEEE","terms":"Analytical models;Semantics;Unified modeling language;Natural language processing;Object recognition;Software;Marine vehicles,diagrams;formal specification;grammars;natural language processing;text analysis,domain elements;textual specifications;analysis modeling;domain objects identification;software requirements specifications;natural language;systematic automated transformation approach;specification sentences;Hornby verb patterns;semantic relationships;type dependencies;Stanford NL parser;analysis class diagrams","keywords":"Model transformation;Analysis modeling;Analysis class diagram;Automated approach;Natural Language Processing","startPage":"566","endPage":"577","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582792","citationCount":0,"referenceCount":45,"year":2016,"authors":"J. S. Thakur; A. Gupta","affiliations":"Indian Institute of Information Technology, Design and Manufacturing, Jabalpur, India, Jabalpur Engineering College, Jabalpur, India; Indian Institute of Information Technology, Design and Manufacturing, Jabalpur, India","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d388e"},"title":"SOFIA: An automated security oracle for black-box testing of SQL-injection vulnerabilities","abstract":"Security testing is a pivotal activity in engineering secure software. It consists of two phases: generating attack inputs to test the system, and assessing whether test executions expose any vulnerabilities. The latter phase is known as the security oracle problem. In this work, we present SOFIA, a Security Oracle for SQL-Injection Vulnerabilities. SOFIA is programming-language and source-code independent, and can be used with various attack generation tools. Moreover, because it does not rely on known attacks for learning, SOFIA is meant to also detect types of SQLi attacks that might be unknown at learning time. The oracle challenge is recast as a one-class classification problem where we learn to characterise legitimate SQL statements to accurately distinguish them from SQLi attack statements. We have carried out an experimental validation on six applications, among which two are large and widely-used. SOFIA was used to detect real SQLi vulnerabilities with inputs generated by three attack generation tools. The obtained results show that SOFIA is computationally fast and achieves a recall rate of 100% (i.e., missing no attacks) with a low false positive rate (0.6%).","conference":"IEEE","terms":"Security;Testing;Databases;Payloads;Context;Software;Servers,pattern classification;program testing;security of data;SQL,SOFIA;automated security oracle;black-box testing;SQL-injection vulnerabilities;security testing;test executions;programming-language;attack generation tools;one-class classification problem;legitimate SQL statements;SQLi attack statements","keywords":"Security testing;Security oracle;SQL-injection","startPage":"167","endPage":"177","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582755","citationCount":0,"referenceCount":28,"year":2016,"authors":"M. Ceccato; C. D. Nguyen; D. Appelt; L. C. Briand","affiliations":"Fondazione Bruno Kessler, Trento, Italy; SnT Centre, University of Luxembourg, Luxembourg; SnT Centre, University of Luxembourg, Luxembourg; SnT Centre, University of Luxembourg, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d388f"},"title":"An extensible framework for variable-precision data-flow analyses in MPS","abstract":"Data-flow analyses are used as part of many software engineering tasks: they are the foundations of program understanding, refactorings and optimized code generation. Similar to general-purpose languages (GPLs), state-of-the-art domain-specific languages (DSLs) also require sophisticated data-flow analyses. However, as a consequence of the different economies of DSL development and their typically relatively fast evolution, the effort for developing and evolving such analyses must be lowered compared to GPLs. This tension can be resolved with dedicated support for data-flow analyses in language workbenches. In this tool paper we present MPS-DF, which is the component in the MPS language workbench that supports the definition of data-flow analyses for DSLs. Language developers can define data-flow graph builders declaratively as part of a language definition and compute analysis results efficiently based on these data-flow graphs. MPS-DF is extensible such that it does not compromise the support for language composition in MPS. Additionally, clients of MPS-DF analyses can run the analyses with variable precision thus trading off precision for performance. This allows clients to tailor an analysis to a particular use case.","conference":"IEEE","terms":"DSL;Lattices;Syntactics;Algorithm design and analysis;Software;Encoding;Switches,data flow analysis;data flow graphs;program compilers;software maintenance;specification languages,extensible framework;data flow analysis;MPS-DF analysis;software engineering;software refactoring;code generation;general-purpose language;GPL;domain-specific language;DSL;data flow graph","keywords":"Data-flow Analysis;Domain-specific Language;Language Workbench;Inter-procedural Analysis","startPage":"870","endPage":"875","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582830","citationCount":0,"referenceCount":19,"year":2016,"authors":"T. Szabó; S. Alperovich; S. Erdweg; M. Voelter","affiliations":"itemis, Germany / Delft University of Technology, Netherlands; JetBrains, Czechia; Delft University of Technology, Netherlands; independent/itemis, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3890"},"title":"Efficient detection of inconsistencies in a multi-developer engineering environment","abstract":"Software developers work concurrently on different kinds of development artifacts such as requirements, architecture, design, or source code. To keep these development artifacts consistent, developers have a wide range of consistency checking approaches available. However, most existing consistency checkers work best in context of single tools and they are not well suited when development artifacts are distributed among different tools and are being modified concurrently by many developers. This paper presents a novel, cloud-based approach to consistency checking in a multi-developer/-tool engineering environment. It allows instant consistency checking even if developers and their tools are distributed and even if they do not have access to all artifacts. It does this by systematically reusing consistency checking knowledge to keep the memory/CPU cost of consistency checking to a small constant overhead per developer. The feasibility and scalability of our approach is demonstrated through an empirical validation with 22 partly industrial system models. A prototype implementation implementation is available through the DesignSpace Engineering Cloud.","conference":"IEEE","terms":"Unified modeling language;Context;Software systems;Context modeling;Scalability,cloud computing;formal verification;software tools,inconsistency detection;multideveloper engineering environment;software development;consistency checker;cloud-based approach;software tool","keywords":"Incremental Consistency Checking;Multi-Developer Engineering;Model-Driven Engineering","startPage":"590","endPage":"601","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582794","citationCount":0,"referenceCount":40,"year":2016,"authors":"A. Demuth; M. Riedl-Ehrenleitner; A. Egyed","affiliations":"Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3891"},"title":"Local-based active classification of test report to assist crowdsourced testing","abstract":"In crowdsourced testing, an important task is to identify the test reports that actually reveal fault - true fault, from the large number of test reports submitted by crowd workers. Most existing approaches towards this problem utilized supervised machine learning techniques, which often require users to manually label a large amount of training data. Such process is time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labeling while still being able to achieve good performance is crucial. Active learning is one potential technique to address this challenge, which aims at training a good classifier with as few labeled data as possible. Nevertheless, our observation on real industrial data reveals that existing active learning approaches generate poor and unstable performances on crowdsourced testing data. We analyze the deep reason and find that the dataset has significant local biases. To address the above problems, we propose LOcal-based Active ClassiFication (LOAF) to classify true fault from crowdsourced test reports. LOAF recommends a small portion of instances which are most informative within local neighborhood, and asks user their labels, then learns classifiers based on local neighborhood. Our evaluation on 14,609 test reports of 34 commercial projects from one of the Chinese largest crowdsourced testing platforms shows that our proposed LOAF can generate promising results. In addition, its performance is even better than existing supervised learning approaches which built on large amounts of labelled historical data. Moreover, we also implement our approach and evaluate its usefulness using real-world case studies. The feedbacks from testers demonstrate its practical value.","conference":"IEEE","terms":"Testing;Software;Feature extraction;Labeling;Training data;Speech recognition;Manuals,learning (artificial intelligence);program debugging;program testing,local-based active classification;test report;crowdsourced testing;supervised machine learning technique;active learning;LOAF","keywords":"Crowdsourced Testing;Test Report Classification;Active Learning","startPage":"190","endPage":"201","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582757","citationCount":0,"referenceCount":44,"year":2016,"authors":"J. Wang; S. Wang; Q. Cui; Q. Wang","affiliations":"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Electrical and Computer Engineering, University of Waterloo, Canada; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3892"},"title":"Taming Android fragmentation: Characterizing and detecting compatibility issues for Android apps","abstract":"Android ecosystem is heavily fragmented. The numerous combinations of different device models and operating system versions make it impossible for Android app developers to exhaustively test their apps. As a result, various compatibility issues arise, causing poor user experience. However, little is known on the characteristics of such fragmentation-induced compatibility issues and no mature tools exist to help developers quickly diagnose and fix these issues. To bridge the gap, we conducted an empirical study on 191 real-world compatibility issues collected from popular open-source Android apps. Our study characterized the symptoms and root causes of compatibility issues, and disclosed that the patches of these issues exhibit common patterns. With these findings, we propose a technique named FicFinder to automatically detect compatibility issues in Android apps. FicFinder performs static code analysis based on a model that captures Android APIs as well as their associated context by which compatibility issues are triggered. FicFinder reports actionable debugging information to developers when it detects potential issues. We evaluated FicFinder with 27 large-scale open-source Android apps. The results show that FicFinder can precisely detect compatibility issues in these apps and uncover previously-unknown issues.","conference":"IEEE","terms":"Androids;Humanoid robots;Computer bugs;Hardware;Open source software;Ecosystems,Android (operating system);application program interfaces;program debugging;program diagnostics;program testing;public domain software,Android fragmentation;compatibility issue detection;Android ecosystem;device model;operating system version;app testing;fragmentation-induced compatibility issue;open-source Android apps;FicFinder;static code analysis;Android API;actionable debugging information","keywords":"Android fragmentation;compatibility issues","startPage":"226","endPage":"237","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582761","citationCount":0,"referenceCount":70,"year":2016,"authors":"L. Wei; Y. Liu; S. Cheung","affiliations":"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3893"},"title":"GUICat: GUI testing as a service","abstract":"GUIs are event-driven applications where the flow of the program is determined by user actions such as mouse clicks and key presses. GUI testing is a challenging task not only because of the combinatorial explosion in the number of event sequences, but also because of the difficulty to cover the large number of data values. We propose GUICat, the first cloud-based GUI testing framework that simultaneously generates event sequences and data values. It is a white-box GUI testing tool that augments traditional sequence generation techniques with concolic execution. We also propose a cloud-based parallel algorithm for mitigating both event sequence explosion and data value explosion, by distributing the con-colic execution tasks over public clouds such as Amazon EC2. We have evaluated the tool on standard GUI testing benchmarks and showed that GUICat significantly outperforms state-of-the-art GUI testing tools. The video demo URL is https://youtu.be/rfnnQOmZqj4.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Instruments;Cloud computing;Explosions;Java;Computational modeling,cloud computing;graphical user interfaces;parallel algorithms;program testing,GUICat;combinatorial explosion;cloud-based GUI testing framework;event sequences;data values;sequence generation;concolic execution;cloud-based parallel algorithm;con-colic execution tasks","keywords":"Symbolic execution;Test generation;GUI testing;Cloud","startPage":"858","endPage":"863","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582828","citationCount":0,"referenceCount":14,"year":2016,"authors":"L. Cheng; J. Chang; Z. Yang; C. Wang","affiliations":"Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3894"},"title":"Greedy combinatorial test case generation using unsatisfiable cores","abstract":"Combinatorial testing aims at covering the interactions of parameters in a system under test, while some combinations may be forbidden by given constraints (forbidden tuples). In this paper, we illustrate that such forbidden tuples correspond to unsatisfiable cores, a widely understood notion in the SAT solving community. Based on this observation, we propose a technique to detect forbidden tuples lazily during a greedy test case generation, which significantly reduces the number of required SAT solving calls. We further reduce the amount of time spent in SAT solving by essentially ignoring constraints while constructing each test case, but then “amending” it to obtain a test case that satisfies the constraints, again using unsatisfiable cores. Finally, to complement a disturbance due to ignoring constraints, we implement an efficient approximative SAT checking function in the SAT solver Lingeling. Through experiments we verify that our approach significantly improves the efficiency of constraint handling in our greedy combinatorial testing algorithm.","conference":"IEEE","terms":"Linux;Browsers;Approximation algorithms;Software testing;Software algorithms;Software,computability;constraint handling;greedy algorithms;program testing,greedy combinatorial test case generation;unsatisfiable cores;parameter interactions;forbidden tuples;SAT solving community;SAT solving calls;SAT checking function;SAT solver Lingeling;constraint handling","keywords":"Combinatorial testing;test case generation;SAT solving","startPage":"614","endPage":"624","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582796","citationCount":0,"referenceCount":38,"year":2016,"authors":"A. Yamada; A. Biere; C. Artho; T. Kitamura; E. Choi","affiliations":"University of Innsbruck, Austria; Johannes Kepler University, Austria; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan; National Institute of Advanced Industrial Science and Technology (AIST), Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3895"},"title":"CrowdService: Serving the individuals through mobile crowdsourcing and service composition","abstract":"Some user needs in real life can only be accomplished by leveraging the intelligence and labor of other people via crowdsourcing tasks. For example, one may want to confirm the validity of the description of a secondhand laptop by asking someone else to inspect the laptop on site. To integrate these crowdsourcing tasks into user applications, it is required that crowd intelligence and labor be provided as easily accessible services (e.g., Web services), which can be called crowd services. In this paper, we develop a framework named CrowdService which supplies crowd intelligence and labor as publicly accessible crowd services via mobile crowd-sourcing. We implement the proposed framework on the Android platform and evaluate its usability with a user study.","conference":"IEEE","terms":"Crowdsourcing;Web services;Inspection;Portable computers;Mobile communication;Business;Mobile handsets,Android (operating system);mobile computing;outsourcing;Web services,CrowdService;mobile crowdsourcing;service composition;crowdsourcing tasks;crowd intelligence;labor;Web services;publicly accessible crowd services;Android platform","keywords":"mobile crowdsourcing;service composition;reliability","startPage":"214","endPage":"219","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582759","citationCount":0,"referenceCount":14,"year":2016,"authors":"X. Peng; J. Gu; T. H. Tan; J. Sun; Y. Yu; B. Nuseibeh; W. Zhao","affiliations":"School of Computer Science, Fudan University, China; School of Computer Science, Fudan University, China; Singapore University of Technology and Design, Singapore; Singapore University of Technology and Design, Singapore; Department of Computing and Communications, The Open University, UK; Department of Computing and Communications, The Open University, UK; School of Computer Science, Fudan University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3896"},"title":"HybriDroid: Static analysis framework for Android hybrid applications","abstract":"Mobile applications (apps) have long invaded the realm of desktop apps, and hybrid apps become a promising solution for supporting multiple mobile platforms. Providing both platform-specific functionalities via native code like native apps and user interactions via JavaScript code like web apps, hybrid apps help developers build multiple apps for different platforms without much duplicated efforts. However, most hybrid apps are developed in multiple programming languages with different semantics, which may be vulnerable to programmer errors. Moreover, because untrusted JavaScript code may access device-specific features via native code, hybrid apps may be vulnerable to various security attacks. Unfortunately, no existing tools can help hybrid app developers by detecting errors or security holes. In this paper, we present HybriDroid, a static analysis framework for Android hybrid apps. We investigate the semantics of Android hybrid apps especially for the interoperation mechanism of Android Java and JavaScript. Then, we design and implement a static analysis framework that analyzes inter-communication between Android Java and JavaScript. As example analyses supported by HybriDroid, we implement a bug detector that identifies programmer errors due to the hybrid semantics, and a taint analyzer that finds information leaks cross language boundaries. Our empirical evaluation shows that the tools are practically usable in that they found previously uncovered bugs in real-world Android hybrid apps and possible information leaks via a widely-used advertising platform.","conference":"IEEE","terms":"Java;Androids;Humanoid robots;Web pages;Semantics;Bridges;Mobile communication,Android (operating system);Java;mobile computing;program debugging;program diagnostics;security of data;smart phones,HybriDroid;static analysis framework;Android hybrid application;mobile application;security attack;interoperation mechanism;Android Java;JavaScript;bug detector;programmer error identification;taint analyzer","keywords":"Android;hybrid applications;static analysis;multi-language analysis;analysis framework","startPage":"250","endPage":"261","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582763","citationCount":8,"referenceCount":51,"year":2016,"authors":"S. Lee; J. Dolby; S. Ryu","affiliations":"KAIST, South Korea; IBM Research, USA; KAIST, South Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3897"},"title":"Applying combinatorial test data generation to big data applications","abstract":"Big data applications (e.g., Extract, Transform, and Load (ETL) applications) are designed to handle great volumes of data. However, processing such great volumes of data is time-consuming. There is a need to construct small yet effective test data sets during agile development of big data applications. In this paper, we apply a combinatorial test data generation approach to two real-world ETL applications at Medidata. In our approach, we first create Input Domain Models (IDMs) automatically by analyzing the original data source and incorporating constraints manually derived from requirements. Next, the IDMs are used to create test data sets that achieve t-way coverage, which has shown to be very effective in detecting software faults. The generated test data sets also satisfy all the constraints identified in the first step. To avoid creating IDMs from scratch when there is a change to the original data source or constraints, our approach extends the original IDMs with additional information. The new IDMs, which we refer to as Adaptive IDMs (AIDMs), are updated by comparing the changes against the additional information, and are then used to generate new test data sets. We implement our approach in a tool, called comBinatorial big daTa Test dAta Generator (BIT-TAG). Our experience shows that combinatorial testing can be effectively applied to big data applications. In particular, the test data sets created using our approach for the two ETL applications are only a small fraction of the original data source, but we were able to detect all the faults found with the original data source.","conference":"IEEE","terms":"Testing;Databases;Data mining;Research and development;Data models,Big Data;program testing,real-world ETL applications;Medidata;input domain models;adaptive IDM;AIDM;combinatorial big data test data generator;BIT-TAG","keywords":"Big Data Testing;Combinatorial Testing;Input Domain Model;Adaptive Input Domain Model;Test Data Generation","startPage":"637","endPage":"647","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582798","citationCount":0,"referenceCount":27,"year":2016,"authors":"N. Li; Y. Lei; H. R. Khan; J. Liu; Y. Guo","affiliations":"Research and Development, Medidata Solutions, New York, NY, USA; Dept. of Computer Science and Engineering, The University of Texas at Arlington, Arlington, TX, USA; Research and Development, Medidata Solutions, New York, NY, USA; Research and Development, Medidata Solutions, New York, NY, USA; Dept. of Computer Science, George Mason University, Fairfax, VA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3898"},"title":"Move-optimized source code tree differencing","abstract":"When it is necessary to express changes between two source code files as a list of edit actions (an edit script), modern tree differencing algorithms are superior to most text-based approaches because they take code movements into account and express source code changes more accurately. We present 5 general optimizations that can be added to state-of-the-art tree differencing algorithms to shorten the resulting edit scripts. Applied to Gumtree, RTED, JSync, and ChangeDistiller, they lead to shorter scripts for 1898% of the changes in the histories of 9 open-source software repositories. These optimizations also are parts of our novel Move-optimized Tree DIFFerencing algorithm (MTD-IFF) that has a higher accuracy in detecting moved code parts. MTDIFF (which is based on the ideas of ChangeDistiller) further shortens the edit script for another 20% of the changes in the repositories. MTDIFF and all the benchmarks are available under an open-source license.","conference":"IEEE","terms":"Optimization;Open source software;Runtime;Complexity theory;Programming;History,optimisation;public domain software;software maintenance;source code (software),move-optimized source code tree differencing algorithm;source code files;edit actions;edit script;text-based approaches;code movements;source code changes;optimizations;Gumtree;RTED;JSync;ChangeDistiller;open-source software repositories;MTD-IFF;moved code parts;open-source license;software maintenance","keywords":"Tree Differencing;Optimizations;Source Code","startPage":"660","endPage":"671","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582801","citationCount":0,"referenceCount":42,"year":2016,"authors":"G. Dotzler; M. Philippsen","affiliations":"Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany, Programming Systems Group; Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany, Programming Systems Group","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d3899"},"title":"Fine-tuning spectrum based fault localisation with frequent method item sets","abstract":"Continuous integration is a best practice adopted in modern software development teams to identify potential faults immediately upon project build. Once a fault is detected it must be repaired immediately, hence continuous integration provides an ideal testbed for experimenting with the state of the art in fault localisation. In this paper we propose a variant of what is known as spectrum based fault localisation, which leverages patterns of method calls by means of frequent itemset mining. We compare our variant (we refer to it as patterned spectrum analysis) against the state of the art and demonstrate on 351 real faults drawn from five representative open source java projects that patterned spectrum analysis is more effective in localising the fault.","conference":"IEEE","terms":"Spectral analysis;Debugging;Software;Itemsets;Servers;Context;Maintenance engineering,data mining;fault location;program debugging;software engineering,fine-tuning spectrum;item sets;continuous integration;software development teams;project build;fault detection;ideal testbed;spectrum based fault localisation;frequent itemset mining;pattern spectrum analysis;statistical debugging","keywords":"Automated developer tests;Continuous integration;Spectrum based fault localisation;Statistical debugging","startPage":"274","endPage":"285","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582765","citationCount":0,"referenceCount":54,"year":2016,"authors":"G. Laghari; A. Murgia; S. Demeyer","affiliations":"ANSYMO - Universiteit Antwerpen, Belgium; ANSYMO - Universiteit Antwerpen, Belgium; ANSYMO - Universiteit Antwerpen, Belgium","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d389a"},"title":"Automatic runtime recovery via error handler synthesis","abstract":"Software systems are often subject to unexpected runtime errors. Automatic runtime recovery (ARR) techniques aim to recover them from erroneous states and maintain them functional in the field. This paper proposes Ares, a novel, practical approach for ARR. Our key insight is leveraging a system's inherent error handling support to recover from unexpected errors. To this end, we synthesize error handlers in two ways: error transformation and early return. We also equip Ares with a lightweight in-vivo testing infrastructure to select the promising synthesis method and avoid potentially dangerous error handlers. Unlike existing ARR techniques with heavyweight mechanisms (e.g., checkpoint-restart and runtime monitoring), our approach expands the intrinsic capability of runtime error resilience in software systems to handle unexpected errors. Ares's lightweight mechanism makes it practical and easy to be integrated into production environments. We have implemented Ares on top of both the Java HotSpot VM and Android ART, and applied it to recover from 52 real-world bugs. The results are promising - Ares successfully recovers from 39 of them and incurs negligible overhead.","conference":"IEEE","terms":"Runtime;Computer bugs;Java;Software systems;Testing;Resilience;Androids,error handling;Java;program debugging;program testing;virtual machines,automatic runtime recovery;error handler synthesis;software systems;ARR techniques;erroneous states;Ares;error transformation;early return;testing infrastructure;runtime error resilience;production environments;Java HotSpot VM;Android ART;real-world bugs","keywords":"automatic runtime recovery;JVM;exception handling","startPage":"684","endPage":"695","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582803","citationCount":0,"referenceCount":30,"year":2016,"authors":"T. Gu; C. Sun; X. Ma; J. Lü; Z. Su","affiliations":"Department of Computer Science and Technology, Nanjing University, China; Department of Computer Science, University of California, Davis, USA; Department of Computer Science and Technology, Nanjing University, China; Department of Computer Science and Technology, Nanjing University, China; Department of Computer Science, University of California, Davis, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d389b"},"title":"An empirical study on dependence clusters for effort-aware fault-proneness prediction","abstract":"A dependence cluster is a set of mutually inter-dependent program elements. Prior studies have found that large dependence clusters are prevalent in software systems. It has been suggested that dependence clusters have potentially harmful effects on software quality. However, little empirical evidence has been provided to support this claim. The study presented in this paper investigates the relationship between dependence clusters and software quality at the function-level with a focus on effort-aware fault-proneness prediction. The investigation first analyzes whether or not larger dependence clusters tend to be more fault-prone. Second, it investigates whether the proportion of faulty functions inside dependence clusters is significantly different from the proportion of faulty functions outside dependence clusters. Third, it examines whether or not functions inside dependence clusters playing a more important role than others are more fault-prone. Finally, based on two groups of functions (i.e., functions inside and outside dependence clusters), the investigation considers a segmented fault-proneness prediction model. Our experimental results, based on five well-known open-source systems, show that (1) larger dependence clusters tend to be more fault-prone; (2) the proportion of faulty functions inside dependence clusters is significantly larger than the proportion of faulty functions outside dependence clusters; (3) functions inside dependence clusters that play more important roles are more fault-prone; (4) our segmented prediction model can significantly improve the effectiveness of effort-aware fault-proneness prediction in both ranking and classification scenarios. These findings help us better understand how dependence clusters influence software quality.","conference":"IEEE","terms":"Predictive models;Software quality;Data models;Computer science;Servers;Syntactics,pattern classification;pattern clustering;public domain software;software fault tolerance;software quality,dependence cluster;effort-aware fault-proneness prediction;mutually interdependent program element;software quality;open-source system;ranking;classification","keywords":"Dependence clusters;fault-proneness;fault prediction;network analysis","startPage":"296","endPage":"307","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582767","citationCount":0,"referenceCount":42,"year":2016,"authors":"Y. Yang; M. Harman; J. Krinke; S. Islam; D. Binkley; Y. Zhou; B. Xu","affiliations":"Department of Computer Science and Technology, Nanjing University, China; Department of Computer Science, University College London, UK; Department of Computer Science, University College London, UK; School of Architecture, Computing and Engineering, University of East London, UK; Department of Computer Science, Loyola University Maryland, USA; Department of Computer Science and Technology, Nanjing University, China; Department of Computer Science and Technology, Nanjing University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d389c"},"title":"DistIA: A cost-effective dynamic impact analysis for distributed programs","abstract":"Dynamic impact analysis is a fundamental technique for understanding the impact of specific program entities, or changes to them, on the rest of the program for concrete executions. However, existing techniques are either inapplicable or of very limited utility for distributed programs running in multiple concurrent processes. This paper presents DISTIA, a dynamic analysis of distributed systems that predicts impacts propagated both within and across process boundaries by partially ordering distributed method-execution events, inferring causality from the ordered events, and exploiting message-passing semantics. We applied DISTIA to large distributed systems of various architectures and sizes, for which it on average finishes the entire analysis within one minute and safely reduces impact-set sizes by over 43% relative to existing options with run-time overhead less than 8%. Moreover, two case studies initially demonstrated the precision of DISTIA and its utility in distributed system understanding. While conservative thus subject to false positives, DistIA balances precision and efficiency to offer cost-effective options for evolving distributed programs.","conference":"IEEE","terms":"Performance analysis;Servers;Algorithm design and analysis;Clocks;Software;Heuristic algorithms;Message passing,concurrency control;distributed processing;software architecture;system monitoring,DISTIA;dynamic impact analysis;distributed program;concurrent process;dynamic partial ordering;distributed method-execution event;causality inference;distributed system architecture","keywords":"Impact analysis;distributed systems;dynamic partial ordering","startPage":"344","endPage":"355","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582771","citationCount":0,"referenceCount":67,"year":2016,"authors":"H. Cai; D. Thain","affiliations":"School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d389d"},"title":"Battery-aware transformations in mobile applications","abstract":"We present an adaptive binary transformation system for reducing the energy impact of advertisements and analytics in mobile applications. Our approach accommodates both the needs of mobile app developers to obtain income from advertisements and the desire of mobile device users for longer battery life. Our technique automatically identifies recurrent advertisement and analytics requests and throttles these requests based on a mobile device's battery status. Of the Android applications we analyzed, 75% have at least one connection that exhibits such recurrent requests. Our automated detection scheme classifies these requests with 100% precision and 80.5% recall. Applying the proposed battery-aware transformations to a representative mobile application reduces the power consumption of the mobile device by 5.8%, without the negative effect of completely removing advertisements.","conference":"IEEE","terms":"Batteries;Mobile communication;Smart phones;Androids;Humanoid robots;Instruments,advertising;Android (operating system);mobile computing;power aware computing,battery-aware transformations;mobile applications;adaptive binary transformation system;energy impact reduction;advertisements;analytics;mobile device battery status;Android applications","keywords":"Energy efficiency;battery lifetime;mobile advertisements;program analysis","startPage":"702","endPage":"707","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582805","citationCount":0,"referenceCount":17,"year":2016,"authors":"J. Cito; J. Rubin; P. Stanley-Marbell; M. Rinard","affiliations":"University of Zurich, Zurich, Switzerland; MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA; MIT, Cambridge, MA, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d389e"},"title":"IncA: A DSL for the definition of incremental program analyses","abstract":"Program analyses support software developers, for example, through error detection, code-quality assurance, and by enabling compiler optimizations and refactorings. To provide real-time feedback to developers within IDEs, an analysis must run efficiently even if the analyzed code base is large. To achieve this goal, we present a domain-specific language called IncA for the definition of efficient incremental program analyses that update their result as the program changes. IncA compiles analyses into graph patterns and relies on existing incremental matching algorithms. To scale IncA analyses to large programs, we describe optimizations that reduce caching and prune change propagation. Using IncA, we have developed incremental control flow and points-to analysis for C, well-formedness checks for DSLs, and 10 FindBugs checks for Java. Our evaluation demonstrates significant speedups for all analyses compared to their non-incremental counterparts.","conference":"IEEE","terms":"Runtime;DSL;Java;Pattern matching;Program processors;Optimization,Java;program diagnostics;software engineering;specification languages,IncA analysis;domain-specific language;DSL;incremental program analysis;static program analysis;software development;Java","keywords":"Static Analysis;Incremental Computation;Domain-specific Language;Language Workbench","startPage":"320","endPage":"331","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582769","citationCount":0,"referenceCount":41,"year":2016,"authors":"T. Szabó; S. Erdweg; M. Voelter","affiliations":"itemis, Germany / Delft University of Technology, Netherlands; Delft University of Technology, Netherlands; independent / itemis, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d389f"},"title":"LockPeeker: Detecting latent locks in Java APIs","abstract":"Detecting lock-related defects has long been a hot research topic in software engineering. Many efforts have been spent on detecting such deadlocks in concurrent software systems. However, latent locks may be hidden in application programming interface (API) methods whose source code may not be accessible to developers. Many APIs have latent locks. For example, our study has shown that J2SE alone can have 2,000+ latent locks. As latent locks are less known by developers, they can cause deadlocks that are hard to perceive or diagnose. Meanwhile, the state-of-the-art tools mostly handle API methods as black boxes, and cannot detect deadlocks that involve such latent locks. In this paper, we propose a novel black-box testing approach, called LockPeeker, that reveals latent locks in Java APIs. The essential idea of LockPeeker is that latent locks of a given API method can be revealed by testing the method and summarizing the locking effects during testing execution. We have evaluated LockPeeker on ten real-world Java projects. Our evaluation results show that (1) LockPeeker detects 74.9% of latent locks in API methods, and (2) it enables state-of-the-art tools to detect deadlocks that otherwise cannot be detected.","conference":"IEEE","terms":"System recovery;Java;Testing;Analytical models;Computer bugs;Cryptography;Software,application program interfaces;concurrency control;Java;program testing;software fault tolerance;source code (software),LockPeeker;latent locks detection;Java API;lock-related defects detection;software engineering;deadlocks;concurrent software systems;application programming interface;source code;J2SE;black-box testing;locking effects;testing execution","keywords":"Latent lock;deadlock detection;API method","startPage":"368","endPage":"378","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582773","citationCount":0,"referenceCount":34,"year":2016,"authors":"Z. Lin; H. Zhong; Y. Chen; J. Zhao","affiliations":"School of Software, ShanghaiJiao Tong University, China; Department of Computer Science and Engineering, ShanghaiJiao Tong University, China; Department of Computer Science and Engineering, ShanghaiJiao Tong University, China; Department of Advanced Information Technology, Kyushu University, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a0"},"title":"An empirical investigation into the nature of test smells","abstract":"Test smells have been defined as poorly designed tests and, as reported by recent empirical studies, their presence may negatively affect comprehension and maintenance of test suites. Despite this, there are no available automated tools to support identification and repair of test smells. In this paper, we firstly investigate developers' perception of test smells in a study with 19 participants. The results show that developers generally do not recognize (potentially harmful) test smells, highlighting that automated tools for identifying such smells are much needed. However, to build effective tools, deeper insights into the test smells phenomenon are required. To this aim, we conducted a large-scale empirical investigation aimed at analyzing (i) when test smells occur in source code, (ii) what their survivability is, and (iii) whether their presence is associated with the presence of design problems in production code (code smells). The results indicate that test smells are usually introduced when the corresponding test code is committed in the repository for the first time, and they tend to remain in a system for a long time. Moreover, we found various unexpected relationships between test and code smells. Finally, we show how the results of this study can be used to build effective automated tools for test smell detection and refactoring.","conference":"IEEE","terms":"Production;Maintenance engineering;Data mining;Testing;History;Software systems,software maintenance;source code (software),test smells;test suites maintenance;automated tools;source code;production code;code smells","keywords":"Test Smells;Mining Software Repositories;Software Evolution","startPage":"4","endPage":"15","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582740","citationCount":0,"referenceCount":45,"year":2016,"authors":"M. Tufano; F. Palomba; G. Bavota; M. Di Penta; R. Oliveto; A. De Lucia; D. Poshyvanyk","affiliations":"The College of William and Mary, USA; University of Salerno, Italy; Università della Svizzera italiana (USI), Switzerland; University of Sannio, Italy; University of Molise, Italy; University of Salerno, Italy; The College of William and Mary, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a1"},"title":"Mining input grammars from dynamic taints","abstract":"Knowing which part of a program processes which parts of an input can reveal the structure of the input as well as the structure of the program. In a URL http://www.example.com/path/, for instance, the protocol http, the host www.example.com, and the path path would be handled by different functions and stored in different variables. Given a set of sample inputs, we use dynamic tainting to trace the data flow of each input character, and aggregate those input fragments that would be handled by the same function into lexical and syntactical entities. The result is a context-free grammar that reflects valid input structure. In its evaluation, our AUTOGRAM prototype automatically produced readable and structurally accurate grammars for inputs like URLs, spreadsheets or configuration files. The resulting grammars not only allow simple reverse engineering of input formats, but can also directly serve as input for test generators.","conference":"IEEE","terms":"Grammar;Protocols;Uniform resource locators;Java;Instruments;Software;Ports (Computers),context-free grammars;data flow analysis;data mining;file organisation;spreadsheet programs,input grammars mining;dynamic taints;program structure;data flow;context-free grammar;AUTOGRAM prototype;URL;spreadsheets;configuration files","keywords":"Input formats;context-free grammars;dynamic tainting;fuzzing","startPage":"720","endPage":"725","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582807","citationCount":0,"referenceCount":6,"year":2016,"authors":"M. Höschele; A. Zeller","affiliations":"Saarland University, Saarland Informatics Campus, Saarbrücken, Germany; Saarland University, Saarland Informatics Campus, Saarbrücken, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a2"},"title":"Reflection-aware static analysis of Android apps","abstract":"We demonstrate the benefits of DroidRA, a tool for taming reflection in Android apps. DroidRA first statically extracts reflection-related object values from a given Android app. Then, it leverages the extracted values to boost the app in a way that reflective calls are no longer a challenge for existing static analyzers. This is achieved through a bytecode instrumentation approach, where reflective calls are supplemented with explicit traditional Java method calls which can be followed by state-of-the-art analyzers which do not handle reflection. Instrumented apps can thus be completely analyzed by existing static analyzers, which are no longer required to be modified to support reflection-aware analysis. The video demo of DroidRA can be found at https://youtu.be/-HW0V68aAWc.","conference":"IEEE","terms":"Androids;Humanoid robots;Java;Smart phones;Coal;Instruments;Runtime,Android (operating system);Java;program diagnostics;reflection,reflection-aware static analysis;Android apps;DroidRA benefits;taming reflection tool;reflection-related object values extraction;reflective calls;bytecode instrumentation approach;Java method calls;reflection-aware analysis","keywords":"Android;Static Analysis;Reflection;DroidRA","startPage":"756","endPage":"761","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582811","citationCount":3,"referenceCount":30,"year":2016,"authors":"L. Li; T. F. Bissyandé; D. Octeau; J. Klein","affiliations":"SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; CSE, Pennsylvania State University, USA; SnT, University of Luxembourg, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a3"},"title":"Static race detection for device drivers: The Goblint approach","abstract":"Device drivers rely on fine-grained locking to ensure safe access to shared data structures. For human testers, concurrency makes such code notoriously hard to debug; for automated reasoning, dynamically allocated memory and low-level pointer manipulation poses significant challenges. We present a flexible approach to data race analysis, implemented in the open source Goblint static analysis framework that combines different pointer and value analyses in order to handle a wide range of locking idioms, including locks allocated dynamically as well as locks stored in arrays. To the best of our knowledge, this is the most ambitious effort, having lasted well over ten years, to create a fully automated static race detection tool that can deal with most of the intricate locking schemes found in Linux device drivers. Our evaluation shows that these analyses are sufficiently precise, but practical use of these techniques requires inferring environmental and domain-specific assumptions.","conference":"IEEE","terms":"Decision support systems;Servers,authorisation;concurrency (computers);data structures;device drivers;inference mechanisms;Linux;program debugging;program diagnostics;public domain software,static race detection;fine-grained locking;safe access;shared data structures;human testers;concurrency;debug;automated reasoning;low-level pointer manipulation;open source Goblint static analysis;Linux device drivers","keywords":"Concurrency;race condition;abstract interpretation","startPage":"391","endPage":"402","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582775","citationCount":0,"referenceCount":46,"year":2016,"authors":"V. Vojdani; K. Apinis; V. Rõtov; H. Seidl; V. Vene; R. Vogler","affiliations":"University of Tartu, Estonia; University of Tartu, Estonia; University of Tartu, Estonia; Technische Universität, München, Germany; University of Tartu, Estonia; Technische Universität, München, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a4"},"title":"Optimizing customized program coverage","abstract":"Program coverage is used across many stages of software development. While common during testing, program coverage has also found use outside the test lab, in production software. However, production software has stricter requirements on run-time overheads, and may limit possible program instrumentation. Thus, optimizing the placement of probes to gather program coverage is important. We introduce and study the problem of customized program coverage optimization. We generalize previous work that optimizes for complete coverage instrumentation with a system that adapts optimization to customizable program coverage requirements. Specifically, our system allows a user to specify desired coverage locations and to limit legal instrumentation locations. We prove that the problem of determining optimal coverage probes is NP-hard, and we present a solution based on mixed integer linear programming. Due to the computational complexity of the problem, we also provide two practical approximation approaches. We evaluate the effectiveness of our approximations across a diverse set of benchmarks, and show that our techniques can substantially reduce instrumentation while allowing the user immense freedom in defining coverage requirements. When naive instrumentation is dense or expensive, our optimizations succeed in lowering execution time overheads.","conference":"IEEE","terms":"Probes;Software;Optimization;Debugging;Monitoring;Context,computational complexity;integer programming;linear programming;program testing;software engineering,software development;testing;production software;run-time overheads;customized program coverage optimization;legal instrumentation locations;NP-hard;mixed integer linear programming;computational complexity","keywords":"Debugging;mixed integer linear optimization;program coverage","startPage":"27","endPage":"38","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582742","citationCount":0,"referenceCount":40,"year":2016,"authors":"P. Ohmann; D. B. Brown; N. Neelakandan; J. Linderoth; B. Liblit","affiliations":"University of Wisconsin-Madison, Madison, WI, USA; University of Wisconsin-Madison, Madison, WI, USA; University of Wisconsin-Madison, Madison, WI, USA; University of Wisconsin-Madison, Madison, WI, USA; University of Wisconsin-Madison, Madison, WI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a5"},"title":"Practical guidelines for change recommendation using association rule mining","abstract":"Association rule mining is an unsupervised learning technique that infers relationships among items in a data set. This technique has been successfully used to analyze a system's change history and uncover evolutionary coupling between system artifacts. Evolutionary coupling can, in turn, be used to recommend artifacts that are potentially affected by a given set of changes to the system. In general, the quality of such recommendations is affected by (1) the values selected for various parameters of the mining algorithm, (2) characteristics of the set of changes used to derive a recommendation, and (3) characteristics of the system's change history for which recommendations are generated. In this paper, we empirically investigate the extent to which certain choices for these factors affect change recommendation. Specifically, we conduct a series of systematic experiments on the change histories of two large industrial systems and eight large open source systems, in which we control the size of the change set for which to derive a recommendation, the measure used to assess the strength of the evolutionary coupling, and the maximum size of historical changes taken into account when inferring these couplings. We use the results from our study to derive a number of practical guidelines for applying association rule mining for change recommendation.","conference":"IEEE","terms":"Couplings;History;Data mining;Software;Algorithm design and analysis;Guidelines;Size measurement,data mining;recommender systems;unsupervised learning,change recommendation;association rule mining;unsupervised learning technique;evolutionary coupling;recommendation quality;open source systems","keywords":"Evolutionary coupling;association rule mining;parameter tuning;change recommendations;change impact analysis","startPage":"732","endPage":"743","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582809","citationCount":0,"referenceCount":41,"year":2016,"authors":"L. Moonen; S. Di Alesio; D. Binkley; T. Rolfsnes","affiliations":"Simula Research Laboratory, Oslo, Norway; Simula Research Laboratory, Oslo, Norway; Loyola University Maryland, Baltimore, Maryland, USA; Simula Research Laboratory, Oslo, Norway","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a6"},"title":"An end-user oriented tool suite for development of mobile applications","abstract":"In this paper, we show an end-user oriented tool suite for mobile application development. The advantages of this tool suite are that the graphical user interface (GUI), as well as the application logic can both be developed in a rapid and simple way, and web-based services on the Internet can be integrated into our platform by end-users. This tool suite involves three sub-systems, namely ServiceAccess, EasyApp and LSCE. ServiceAccess takes charge of the registration and management of heterogeneous services, and can export different form of services according to the requirements of the other sub-systems. EasyApp is responsible for developing GUI in the form of mobile app. LSCE takes charge of creating the application logic that can be invoked by mobile app directly. Finally, a development case is presented to illustrate the development process using this tool suite. The URL of demo video: https://youtu.be/mM2WkU1_k-w.","conference":"IEEE","terms":"Mobile communication;Libraries;Mobile applications;Servers;Web services;Graphical user interfaces;Visualization,formal logic;graphical user interfaces;mobile computing;software tools,end-user oriented tool suite;mobile application development;graphical user interface;GUI;application logic","keywords":"Mobile application;end-user development;visual development environment;cross-platform","startPage":"768","endPage":"773","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582813","citationCount":0,"referenceCount":21,"year":2016,"authors":"Z. Zhai; B. Cheng; M. Niu; Z. Wang; Y. Feng; J. Chen","affiliations":"State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a7"},"title":"Traceability maintenance: Factors and guidelines","abstract":"Traceability is an important concern for numerous software engineering activities. Establishing traceability links is a challenging and cost-intensive task, which is uneconomical without suitable strategies for maintaining high link quality. Current approaches to Traceability Management (TM), however, often make important assumptions and choices without ensuring that the consequences and implications for trace-ability maintenance are feasible and desirable in practice. In this paper, therefore, we identify a set of core factors that influence how the quality of traceability links can be maintained. For each factor, we discuss relevant challenges and provide guidelines on how best to ensure viable traceability maintenance in a practical TM approach. Our guidelines are meant to be used by tool developers and users to select the most appropriate TM approach for their needs. Our results are based on and supported by data collected from interviews conducted with: (i) 9 of our industrial and academic project partners to elicit requirements for a TM tool, and (ii) 24 software development stakeholders from 15 industrial cases to provide a broader overview of the current state of the practice on TM. To evaluate the feasibility of our guidelines, we investigate a set of existing TM approaches used in industry with respect to our guidelines.","conference":"IEEE","terms":"Unified modeling language;Maintenance engineering;Guidelines;Software;Manuals;Interviews;Computational modeling,software maintenance;software management;software quality,traceability maintenance;traceability quality;software engineering;traceability management;TM","keywords":"traceability quality;traceability maintenance;consistency","startPage":"414","endPage":"425","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582777","citationCount":0,"referenceCount":31,"year":2016,"authors":"S. Maro; A. Anjorin; R. Wohlrab; J. Steghöfer","affiliations":"Chalmers | University of Gothenburg, Sweden; University of Paderborn, Paderborn, Germany; Chalmers | University of Gothenburg, Sweden; Chalmers | University of Gothenburg, Sweden","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a8"},"title":"Inferring annotations for device drivers from verification histories","abstract":"This paper studies and optimizes automated program verification. Detailed reasoning about software behavior is often facilitated by program invariants that hold across all program executions. Finding program invariants is in fact an essential step in automated program verification. Automatic discovery of precise invariants, however, can be very difficult in practice. The problem can be simplified if one has access to a candidate set of assertions (or annotations) and the search for invariants is limited over the space defined by these annotations. Then, the main challenge is to automatically generate quality program annotations. We present an approach that infers program annotations automatically by leveraging the history of verifying related programs. Our algorithm extracts high-quality annotations from previous verification attempts, and then applies them for verifying new programs. We present a case study where we applied our algorithm to Microsoft's Static Driver Verifier (SDV). SDV is an industrial-strength tool for verification of Windows device drivers that uses manually-tuned heuristics for obtaining a set of annotations. Our technique inferred program annotations comparable in performance to the existing annotations used in SDV that were devised manually by human experts over years. Additionally, the inferred annotations together with the existing ones improved the performance of SDV overall, proving correct 47% of drivers more while running 22% faster in our experiments.","conference":"IEEE","terms":"History;Kernel;Manuals;Performance evaluation;Simultaneous localization and mapping;Contracts,device drivers;inference mechanisms;program diagnostics;program verification;software quality,program annotation inference;Windows device driver;program verification;reasoning;software behavior;program invariant;high-quality annotation;Microsoft Static Driver Verifier;SDV","keywords":"Program verification;Invariant generation;Learning invariants;Verification history;Big Code","startPage":"450","endPage":"460","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582781","citationCount":0,"referenceCount":28,"year":2016,"authors":"Z. Pavlinovic; A. Lal; R. Sharma","affiliations":"New York University, USA; Microsoft Research, India; Stanford University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38a9"},"title":"Test case permutation to improve execution time","abstract":"With the growing complexity of software, the number of test cases needed for effective validation is extremely large. Executing these large test suites is expensive, both in terms of time and energy. Cache misses are known to be one of the main factors contributing to execution time of a software. Cache misses are reduced by increasing the locality of memory references. For a single program run, compiler optimisations help improve data locality and code layout optimisations help improve spatial locality of instructions. Nevertheless, cache locality optimisations have not been proposed and explored across several program runs, which is the case when we run several test cases. In this paper, we propose and evaluate a novel approach to improve instruction locality across test case runs. Our approach measures the distance between test case runs (number of different instructions). We then permute the test cases for execution so that the distance between neighboring test cases is minimised. We hypothesize that test cases executed in this new order for improved instruction locality will reduce time consumed. We conduct a preliminary evaluation with four subject programs and test suites from the SIR repository to answer the following questions, 1. Is execution time of a test suite affected by the order in which test cases are executed? and 2. How does time consumed in executing our permutation compare to random test case permutations? We found that the order in which test cases are executed has a definite impact on execution time. The extent of impact varies, based on program characteristics and test cases. Our approach outperformed more than 97% of random test case permutations on 3 of the 4 subject programs and did better than 93% of the random orderings on the remaining subject program. Using the optimised permutation, we saw a maximum reduction of 7.4% over average random permutation execution time and 34.7% over the worst permutation.","conference":"IEEE","terms":"Optimization;Software;Layout;Software testing;Informatics;Complexity theory,cache storage;optimisation;program compilers,cache misses;memory references;compiler optimisations;data locality;code layout optimisations;spatial locality;cache locality optimisations;instruction locality;SIR repository;test case permutations;random orderings;optimised permutation;average random permutation execution time","keywords":"Cache misses;Instruction locality","startPage":"45","endPage":"50","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582744","citationCount":0,"referenceCount":23,"year":2016,"authors":"P. Stratis; A. Rajan","affiliations":"School of Informatics, University of Edinburgh, UK; School of Informatics, University of Edinburgh, UK","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38aa"},"title":"MACKE: Compositional analysis of low-level vulnerabilities with symbolic execution","abstract":"Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.","conference":"IEEE","terms":"Computer bugs;Engines;Software;Security;Memory management;Complex systems;Scalability,program debugging;program diagnostics;program testing;software tools,MACKE tool;compositional analysis;low-level vulnerabilities;concolic execution;concrete+symbolic execution;nontrivial software vulnerabilities;buffer overflows;complex system interactions;scalability issues;constraint solvers;modular interactions;static code analysis;directed interprocedural path exploration;interactive vulnerability report generation;bug fixing;severity scores","keywords":"Symbolic execution;Compositional analysis","startPage":"780","endPage":"785","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582815","citationCount":0,"referenceCount":34,"year":2016,"authors":"S. Ognawala; M. Ochoa; A. Pretschner; T. Limmer","affiliations":"Technical University of Munich, Germany; Singapore University of Technology and Design, Singapore; Technical University of Munich, Germany; Siemens AG, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38ab"},"title":"DSL-maps: From requirements to design of domain-specific languages","abstract":"Domain-Specific Languages (DSLs) are central to Model-Driven Engineering, where they are used for creating models for particular domains. However, current research and tools for building DSLs focus on the design and implementation aspects of the DSL, while the requirements analysis phase, and its automated transition to design is largely neglected. In order to alleviate this situation, we propose DSL-maps, a notation inspired by mind-maps, to represent requirements for DSLs. The notation is supported by a tool, which helps in the automated transition into an initial meta-model design, using a customizable transformation and recommendations from a catalogue of meta-model design patterns.","conference":"IEEE","terms":"DSL;Syntactics;Unified modeling language;Software;Connectors;Computational modeling;Concrete,high level languages;pattern recognition;systems analysis,DSL-maps;domain-specific languages;model-driven engineering;requirements analysis phase;mind-maps;meta-model design patterns","keywords":"Domain Specific Languages;Model-Driven Engineering;Domain Analysis;Meta-Modelling Patterns","startPage":"438","endPage":"443","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582779","citationCount":0,"referenceCount":16,"year":2016,"authors":"A. Pescador; J. de Lara","affiliations":"Computer Science Department, Modelling and Software Engineering Research Group, Universidad Autónoma de Madrid (Spain); Computer Science Department, Modelling and Software Engineering Research Group, Universidad Autónoma de Madrid (Spain)","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38ac"},"title":"APEx: Automated inference of error specifications for C APIs","abstract":"Although correct error handling is crucial to software robustness and security, developers often inadvertently introduce bugs in error handling code. Moreover, such bugs are hard to detect using existing bug-finding tools without correct error specifications. Creating error specifications manually is tedious and error-prone. In this paper, we present a new technique that automatically infers error specifications of API functions based on their usage patterns in C programs. Our key insight is that error-handling code tend to have fewer branching points and program statements than the code implementing regular functionality. Our scheme leverages this property to automatically identify error handling code at API call sites and infer the corresponding error constraints. We then use the error constraints from multiple call sites for robust inference of API error specifications. We evaluated our technique on 217 API functions from 6 different libraries across 28 projects written in C and found that it can identify error-handling paths with an average precision of 94% and recall of 66%. We also found that our technique can infer correct API error specifications with an average precision of 77% and recall of 47%. To further demonstrate the usefulness of the inferred error specifications, we used them to find 118 previously unknown potential bugs (including several security flaws that are currently being fixed by the corresponding developers) in the 28 tested projects.","conference":"IEEE","terms":"Computer bugs;Security;Software;Libraries;Documentation;Algorithm design and analysis;Robustness,C language;error handling;formal specification;program debugging;software tools,APEx;automated inference;error specifications;C APIs;software robustness;software security;software developers;error handling code;bug-finding tools;API functions;C programs;branching points;program statements;software reliability","keywords":"error handling bugs;specification mining;API errors","startPage":"472","endPage":"482","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582783","citationCount":0,"referenceCount":42,"year":2016,"authors":"Y. Kang; B. Ray; S. Jana","affiliations":"Columbia University, USA; University of Virginia, USA; Columbia University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38ad"},"title":"Testing advanced driver assistance systems using multi-objective search and neural networks","abstract":"Recent years have seen a proliferation of complex Advanced Driver Assistance Systems (ADAS), in particular, for use in autonomous cars. These systems consist of sensors and cameras as well as image processing and decision support software components. They are meant to help drivers by providing proper warnings or by preventing dangerous situations. In this paper, we focus on the problem of design time testing of ADAS in a simulated environment. We provide a testing approach for ADAS by combining multi-objective search with surrogate models developed based on neural networks. We use multi-objective search to guide testing towards the most critical behaviors of ADAS. Surrogate modeling enables our testing approach to explore a larger part of the input search space within limited computational resources. We characterize the condition under which the multi-objective search algorithm behaves the same with and without surrogate modeling, thus showing the accuracy of our approach. We evaluate our approach by applying it to an industrial ADAS system. Our experiment shows that our approach automatically identifies test cases indicating critical ADAS behaviors. Further, we show that combining our search algorithm with surrogate modeling improves the quality of the generated test cases, especially under tight and realistic computational resources.","conference":"IEEE","terms":"Testing;Computational modeling;Predictive models;Automobiles;Advanced driver assistance systems;Software,automobiles;digital simulation;driver information systems;mobile robots;neural nets;program testing;search problems,advanced driver assistance system testing;multiobjective search algorithm;neural networks;autonomous cars;sensors;cameras;image processing;decision support software components;warnings;dangerous situation prevention;design time testing;simulated environment;surrogate models;critical ADAS behaviors;computational resources","keywords":"Advanced Driver Assistance Systems;Multi-Objective Search Optimization;Simulation;Surrogate Modeling;Neural Networks","startPage":"63","endPage":"74","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582746","citationCount":0,"referenceCount":59,"year":2016,"authors":"R. Ben Abdessalem; S. Nejati; L. C. Briand; T. Stifter","affiliations":"SnT / University of Luxembourg, Luxembourg; SnT / University of Luxembourg, Luxembourg; SnT / University of Luxembourg, Luxembourg; IEE S.A. Contern, Luxembourg","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38ae"},"title":"Evaluating the evaluations of code recommender systems: A reality check","abstract":"While researchers develop many new exciting code recommender systems, such as method-call completion, code-snippet completion, or code search, an accurate evaluation of such systems is always a challenge. We analyzed the current literature and found that most of the current evaluations rely on artificial queries extracted from released code, which begs the question: Do such evaluations reflect real-life usages? To answer this question, we capture 6,189 fine-grained development histories from real IDE interactions. We use them as a ground truth and extract 7,157 real queries for a specific method-call recommender system. We compare the results of such real queries with different artificial evaluation strategies and check several assumptions that are repeatedly used in research, but never empirically evaluated. We find that an evolving context that is often observed in practice has a major effect on the prediction quality of recommender systems, but is not commonly reflected in artificial evaluations.","conference":"IEEE","terms":"Context;Recommender systems;Software;Proposals;Benchmark testing;MIMICs;History,program testing;software maintenance,code recommender system;method-call completion;code-snippet completion;code search","keywords":"Empirical Study;Artificial Evaluation;IDE Interaction Data","startPage":"111","endPage":"121","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582750","citationCount":0,"referenceCount":29,"year":2016,"authors":"S. Proksch; S. Amann; S. Nadi; M. Mezini","affiliations":"Software Technology Group, Technische Universität Darmstadt, Germany; Software Technology Group, Technische Universität Darmstadt, Germany; Software Technology Group, Technische Universität Darmstadt, Germany; Software Technology Group, Technische Universität Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38af"},"title":"CORRECT: Code reviewer recommendation at GitHub for Vendasta technologies","abstract":"Peer code review locates common coding standard violations and simple logical errors in the early phases of software development, and thus, reduces overall cost. Unfortunately, at GitHub, identifying an appropriate code reviewer for a pull request is challenging given that reliable information for reviewer identification is often not readily available. In this paper, we propose a code reviewer recommendation tool-CORRECT-that considers not only the relevant cross-project work experience (e.g., external library experience) of a developer but also her experience in certain specialized technologies (e.g., Google App Engine) associated with a pull request for determining her expertise as a potential code reviewer. We design our tool using client-server architecture, and then package the solution as a Google Chrome plug-in. Once the developer initiates a new pull request at GitHub, our tool automatically analyzes the request, mines two relevant histories, and then returns a ranked list of appropriate code reviewers for the request within the browser's context. Demo: https://www.youtube.com/watch?v=rXU1wTD6QQ0.","conference":"IEEE","terms":"History;Software;Libraries;Browsers;Collaboration;Authentication;Encoding,client-server systems;DP industry;Internet;recommender systems;software architecture;software reviews;software tools;source code (software),CORRECT;code reviewer recommendation tool;GitHub;Vendasta Technologies;peer code review;software development;client-server architecture;Google Chrome plug-in","keywords":"Code reviewer recommendation;cross-project experience;specialized technology experience;GitHub;pull request","startPage":"792","endPage":"797","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582817","citationCount":0,"referenceCount":18,"year":2016,"authors":"M. M. Rahman; C. K. Roy; J. Redl; J. A. Collins","affiliations":"University of Saskatchewan, Canada; University of Saskatchewan, Canada; Vendasta Technologies, Canada; Google Inc., USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9127eee8435e8e7d38b0"},"title":"Visual contract extractor: A tool for reverse engineering visual contracts using dynamic analysis","abstract":"Visual contracts model the operations of classes, components or services by pre- and post-conditions formalised as graph transformation rules. They provide a precise but intuitive notation to test, document and analyse software systems. However, due to their detailed level of specification of data states and transformations, modelling a real application is a complex and error-prone process. Rather than adopting a top-down modelling approach, we follow a dynamic bottom-up approach to reverse engineer visual contracts from object-oriented programs based on tracing the execution of operations. We developed the Visual Contract Extractor (VCE), a dynamic analysis tool which supports the reverse engineering of visual operation contracts from Java programs. We explore the main features of the tool using two case studies and discuss usage scenarios ranging from traditional program understanding to novel applications in the field of model-based engineering. A screencast demonstrating the tool is provided at https://www.youtube.com/watch?v=VtTx8UHgRGo.","conference":"IEEE","terms":"Contracts;Visualization;Object oriented modeling;Unified modeling language;Java;Reverse engineering;Analytical models,Java;object-oriented programming;program testing;reverse engineering;software tools,visual contract extractor;reverse engineering;dynamic analysis;preconditions;postconditions;graph transformation rules;software system analysis;software system documentation;software system testing;dynamic bottom-up approach;object-oriented programs;operation execution tracing;VCE;dynamic analysis tool;Java programs;model-based engineering","keywords":"Visual contracts;graph transformation;model extraction;dynamic analysis;reverse engineering;specification mining","startPage":"816","endPage":"821","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582821","citationCount":0,"referenceCount":30,"year":2016,"authors":"A. Alshanqiti; R. Heckel; T. Kehrer","affiliations":"Department of Computer Science, University of Leicester, UK; Department of Computer Science, University of Leicester, UK; Department of Electronics, Information and Bioengineering, Politecnico di Milano, Italy","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b1"},"title":"Precise semantic history slicing through dynamic delta refinement","abstract":"Semantic history slicing solves the problem of extracting changes related to a particular high-level functionality from the software version histories. State-of-the-art techniques combine static program analysis and dynamic execution tracing to infer an over-approximated set of changes that can preserve the functional behaviors captured by a test suite. However, due to the conservative nature of such techniques, the sliced histories may contain irrelevant changes. In this paper, we propose a divide-and-conquer-style partitioning approach enhanced by dynamic delta refinement to produce minimal semantic history slices. We utilize deltas in dynamic invariants generated from successive test executions to learn significance of changes with respect to the target functionality. Empirical results indicate that these measurements accurately rank changes according to their relevance to the desired test behaviors and thus partition history slices in an efficient and effective manner.","conference":"IEEE","terms":"History;Semantics;Software;Heuristic algorithms;Runtime;Performance analysis;Organizations,configuration management;program slicing,precise semantic history slicing;dynamic delta refinement;software version histories;static program analysis;dynamic execution tracing;divide and conquer style partitioning approach;software configuration management","keywords":"Semantic history slicing;program analysis;dynamic invariants;software configuration management","startPage":"495","endPage":"506","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582785","citationCount":0,"referenceCount":45,"year":2016,"authors":"Y. Li; C. Zhu; J. Rubin; M. Chechik","affiliations":"University of Toronto, Toronto, ON, Canada; University of Toronto, Toronto, ON, Canada; MIT, Cambridge, MA, USA; University of Toronto, Toronto, ON, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b2"},"title":"Deep learning code fragments for code clone detection","abstract":"Code clone detection is an important problem for software maintenance and evolution. Many approaches consider either structure or identifiers, but none of the existing detection techniques model both sources of information. These techniques also depend on generic, handcrafted features to represent code fragments. We introduce learning-based detection techniques where everything for representing terms and fragments in source code is mined from the repository. Our code analysis supports a framework, which relies on deep learning, for automatically linking patterns mined at the lexical level with patterns mined at the syntactic level. We evaluated our novel learning-based approach for code clone detection with respect to feasibility from the point of view of software maintainers. We sampled and manually evaluated 398 file- and 480 method-level pairs across eight real-world Java systems; 93% of the file- and method-level samples were evaluated to be true positives. Among the true positives, we found pairs mapping to all four clone types. We compared our approach to a traditional structure-oriented technique and found that our learning-based approach detected clones that were either undetected or suboptimally reported by the prominent tool Deckard. Our results affirm that our learning-based approach is suitable for clone detection and a tenable technique for researchers.","conference":"IEEE","terms":"Cloning;Syntactics;Software;Feature extraction;Machine learning;Programming;Transforms,Java;learning (artificial intelligence);software maintenance;source code (software),deep learning code fragments;code clone detection;software maintenance;software evolution;source code;pattern mined linking;Java systems;Deckard tool","keywords":"code clone detection;machine learning;deep learning;neural networks;language models;abstract syntax trees","startPage":"87","endPage":"98","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582748","citationCount":0,"referenceCount":115,"year":2016,"authors":"M. White; M. Tufano; C. Vendome; D. Poshyvanyk","affiliations":"Department of Computer Science, College of William and Mary, Williamsburg, Virginia, USA; Department of Computer Science, College of William and Mary, Williamsburg, Virginia, USA; Department of Computer Science, College of William and Mary, Williamsburg, Virginia, USA; Department of Computer Science, College of William and Mary, Williamsburg, Virginia, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b3"},"title":"Automatic microbenchmark generation to prevent dead code elimination and constant folding","abstract":"Microbenchmarking evaluates, in isolation, the execution time of small code segments that play a critical role in large applications. The accuracy of a microbenchmark depends on two critical tasks: wrap the code segment into a pay-load that faithfully recreates the execution conditions of the large application; build a scaffold that runs the payload a large number of times to get a statistical estimate of the execution time. While recent frameworks such as the Java Microbenchmark Harness (JMH) address the scaffold challenge, developers have very limited support to build a correct payload. This work focuses on the automatic generation of pay-loads, starting from a code segment selected in a large application. Our generative technique prevents two of the most common mistakes made in microbenchmarks: dead code elimination and constant folding. A microbenchmark is such a small program that can be “over-optimized” by the JIT and result in distorted time measures, if not designed carefully. Our technique automatically extracts the segment into a compilable payload and generates additional code to prevent the risks of “over-optimization”. The whole approach is embedded in a tool called AutoJMH, which generates payloads for JMH scaffolds. We validate the capabilities AutoJMH, showing that the tool is able to process a large percentage of segments in real programs. We also show that AutoJMH can match the quality of payloads handwritten by performance experts and outperform those written by professional Java developers without experience in microbenchmarking.","conference":"IEEE","terms":"Payloads;Java;Optimization;Benchmark testing;Distortion measurement;Time measurement;Software,Java;software performance evaluation,microbenchmark generation;dead code elimination prevention;code segment;Java microbenchmark harness;JMH;over-optimization risk prevention;AutoJMH tool","keywords":"Performace evaluation;microbencharking;text tagging","startPage":"132","endPage":"143","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582752","citationCount":0,"referenceCount":36,"year":2016,"authors":"M. Rodriguez-Cancio; B. Combemale; B. Baudry","affiliations":"University of Rennes 1, France; University of Rennes 1/INRIA, France; INRIA, France","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b4"},"title":"CVExplorer: Identifying candidate developers by mining and exploring their open source contributions","abstract":"Open source code contributions contain a large amount of technical skill information about developers, which can help to identify suitable candidates for a particular development job and therefore impact the success of a development team. We develop CVExplorer as a tool to extract, visualize, and explore relevant technical skills data from GitHub, such as languages and libraries used. It allows non-technical users to filter and identify developers according to technical skills demonstrated across all of their open source contributions, in order to support more accurate candidate identification. We demonstrate the usefulness of CVExplorer by using it to recommend candidates for open positions in two companies. A video demonstration of the tool is available at https:// youtu.be/xRxK-wa7PME.","conference":"IEEE","terms":"Java;Tag clouds;Data mining;Software;Aggregates;Data visualization,data visualisation;human resource management;public domain software;software engineering;source code (software),CVExplorer;candidate developers identification;open source code contributions;technical skills data visualization;GitHub","keywords":"Identifying candidate developers;Developer skills identification;Mining software repositories","startPage":"804","endPage":"809","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582819","citationCount":1,"referenceCount":27,"year":2016,"authors":"G. J. Greene; B. Fischer","affiliations":"CAIR, CSIR Meraka, Computer Science Division, University of Stellenbosch, South Africa; CAIR, CSIR Meraka, Computer Science Division, University of Stellenbosch, South Africa","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b5"},"title":"Symbolic execution of stored procedures in database management systems","abstract":"Stored procedures in database management systems are often used to implement complex business logic. Correctness of these procedures is critical for correct working of the system. However, testing them remains difficult due to many possible states of data and database constraints. This leads to mostly manual testing. Newer tools offer automated execution for unit testing of stored procedures but the test cases are still written manually. In this paper, we propose a novel approach of using dynamic symbolic execution to automatically generate test cases and corresponding database states for stored procedures. We treat values in database tables as symbolic, model the constraints on data imposed by the schema and by the SQL statements executed by the stored procedure. We use an SMT solver to find values that will drive the stored procedure on a particular execution path. We instrument the internal execution plans generated by PostgreSQL database management system to extract constraints and use the Z3 SMT solver to generate test cases consisting of table data and procedure inputs. Our evaluation using stored procedures from a large business application shows that this technique can uncover bugs that lead to schema constraint violations and user defined exceptions.","conference":"IEEE","terms":"Remuneration;Testing;Instruments;Servers;Analytical models;Database systems,database management systems;program testing;SQL,stored procedures;database management systems;complex business logic;database constraints;automated execution;unit testing;test cases;dynamic symbolic execution;database tables;SQL statements;execution path;internal execution plans;PostgreSQL;Z3 SMT solver","keywords":"Symbolic Execution;Stored Procedures;SQL","startPage":"519","endPage":"530","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582787","citationCount":0,"referenceCount":36,"year":2016,"authors":"M. S. Mahmood; M. A. Ghafoor; J. H. Siddiqui","affiliations":"Department of Computer Science, LUMS School of Science and Engineering, Lahore, Pakistan; Department of Computer Science, LUMS School of Science and Engineering, Lahore, Pakistan; Department of Computer Science, LUMS School of Science and Engineering, Lahore, Pakistan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b6"},"title":"Towards bounded model checking using nonlinear programming solver","abstract":"Due to their complexity, currently available bounded model checking techniques based on Boolean Satisfiability and Satisfiability Modulo Theories inadequately handle non-linear floating-point and integer arithmetic. Using a numerical approach, we reduce a bounded model checking problem to a constraint satisfaction problem. Currently available techniques attempt to solve the constraint problem but can guarantee neither global convergence nor correctness. Using the IPOPT and ANTIGONE non-linear programming (NLP) solvers, we transform the original constraint satisfaction problem from one having disjunctions of constraints into one having conjunctions of constraints with a few introduced auxiliary variables. The transformation lowers the computing cost and preserves the Boolean structure of the original problem while complying with limits of NLP solvers.","conference":"IEEE","terms":"Model checking;Convergence;Software;Mathematical model;Encoding;Programming,Boolean algebra;computability;formal verification;nonlinear programming,nonlinear programming solver;Boolean satisfiability;Satisfiability Modulo Theories;nonlinear floating point;integer arithmetic;bounded model checking problem;constraint satisfaction problem;constraint problem;global convergence;IPOPT;ANTIGONE nonlinear programming;NLP solvers;auxiliary variables;Boolean structure","keywords":"Bounded Model Checking;Nonlinear Programming;SAT","startPage":"560","endPage":"565","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582791","citationCount":0,"referenceCount":20,"year":2016,"authors":"M. Nishi","affiliations":"Center for Technology Innovation, R\u0026D Group, Hitachi Ltd, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b7"},"title":"Finding access control bugs in web applications with CanCheck","abstract":"Access control bugs in web applications can have dire consequences since many web applications store private and sensitive data. In this paper we present an automated verification technique for access control in Ruby on Rails (Rails) applications. Our technique starts by automatically extracting a model that captures 1) the ways the data is accessed and modified by the application, 2) the access control policy of the application, and 3) the authorization checks used for access control policy enforcement. Then, it automatically translates this model to first order logic and uses automated theorem provers to check whether the declared access control policy is correctly enforced by the implementation. We implemented our technique in a tool called CanCheck. Using CanCheck on open source Rails applications, we found numerous previously unknown exploitable access control bugs as well as several deficiencies in access control policies.","conference":"IEEE","terms":"Rails;Authorization;Computer bugs;Data models;Software;Databases,authorisation;data privacy;formal logic;formal verification;Internet;program debugging;public domain software;theorem proving,access control bugs;Web applications;CanCheck;private data;sensitive data;automated verification;Ruby on Rails;first order logic;automated theorem provers;open source Rails applications","keywords":"Access Control;Logic-based Verification;Web Applications","startPage":"155","endPage":"166","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582754","citationCount":0,"referenceCount":38,"year":2016,"authors":"I. Bocić; T. Bultan","affiliations":"Department of Computer Science, University of California, Santa Barbara, USA; Department of Computer Science, University of California, Santa Barbara, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b8"},"title":"TeeVML: Tool support for semi-automatic integration testing environment emulation","abstract":"Software environment emulation provides a means for simulating an operational environment of a system. This process involves approximation of systems' external behaviors and their communications with a system to be tested in the environment. Development of such an environment is a tedious task and involves complex low level coding. Model driven engineering is an avenue to raise the level of abstraction beyond programming by specifying solution directly using problem domain concepts. In this paper we propose a novel domain-specific modeling tool to generate complex testing environments. Our tool employs a suite of domain-specific visual modeling languages for modeling emulation environment at a high level of abstraction. These high level specifications are then automatically transformed to runtime environment for application integration testing, boosting development productivity and ease of use. The tool demonstration video can be accessed here: https://youtu.be/H3Vg20Juq80.","conference":"IEEE","terms":"Testing;Software;Emulation;Unified modeling language;Protocols;Visualization;Banking,formal specification;program testing;software tools;specification languages,TeeVML;tool support;application integration testing;software environment emulation;model-driven engineering;domain-specific visual modelling language;high level specification","keywords":"Model-driven engineering;domain-specific visual modeling language;software component interface description;testing environment emulation","startPage":"840","endPage":"845","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582825","citationCount":0,"referenceCount":13,"year":2016,"authors":"J. Liu; J. Grundy; I. Avazpour; M. Abdelrazek","affiliations":"School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC 3122, Australia; School of Information Technology, Deakin University, Burwood, VIC 3125, Australia; School of Information Technology, Deakin University, Burwood, VIC 3125, Australia; School of Information Technology, Deakin University, Burwood, VIC 3125, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38b9"},"title":"Model-based whitebox fuzzing for program binaries","abstract":"Many real-world programs take highly structured and complex files as inputs. The automated testing of such programs is non-trivial. If the test does not adhere to a specific file format, the program returns a parser error. For symbolic execution-based whitebox fuzzing the corresponding error handling code becomes a significant time sink. Too much time is spent in the parser exploring too many paths leading to trivial parser errors. Naturally, the time is better spent exploring the functional part of the program where failure with valid input exposes deep and real bugs in the program. In this paper, we suggest to leverage information about the file format and data chunks of existing, valid files to swiftly carry the exploration beyond the parser code. We call our approach Modelbased Whitebox Fuzzing (MoWF) because the file format input model of blackbox fuzzers can be exploited as a constraint on the vast input space to rule out most invalid inputs during path exploration in symbolic execution. We evaluate on 13 vulnerabilities in 8 large program binaries with 6 separate file formats and found that MoWF exposes all vulnerabilities while both, traditional whitebox fuzzing and model-based blackbox fuzzing, expose only less than half, respectively. Our experiments also demonstrate that MoWF exposes 70% vulnerabilities without any seed inputs.","conference":"IEEE","terms":"Data models;Testing;Computer bugs;Libraries;Grammar;Browsers;Media,error handling;program compilers;program debugging;program testing;security of data,model-based whitebox fuzzing;MoWF;program binaries;automated program testing;parser error;symbolic execution-based whitebox fuzzing;error handling code;bugs;path exploration;program vulnerabilities;model-based blackbox fuzzing","keywords":"Symbolic Execution;Program Binaries","startPage":"543","endPage":"553","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582789","citationCount":0,"referenceCount":27,"year":2016,"authors":"V. Pham; M. Böhme; A. Roychoudhury","affiliations":"School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ba"},"title":"Continuous detection of design flaws in evolving object-oriented programs using incremental multi-pattern matching","abstract":"Design flaws in object-oriented programs may seriously corrupt code quality thus increasing the risk for introducing subtle errors during software maintenance and evolution. Most recent approaches identify design flaws in an ad-hoc manner, either focusing on software metrics, locally restricted code smells, or on coarse-grained architectural anti-patterns. In this paper, we utilize an abstract program model capturing high-level object-oriented code entities, further augmented with qualitative and quantitative design-related information such as coupling/cohesion. Based on this model, we propose a comprehensive methodology for specifying object-oriented design flaws by means of compound rules integrating code metrics, code smells and anti-patterns in a modular way. This approach allows for efficient, automated design-flaw detection through incremental multi-pattern matching, by facilitating systematic information reuse among multiple detection rules as well as between subsequent detection runs on continuously evolving programs. Our tool implementation comprises well-known anti-patterns for Java programs. The results of our experimental evaluation show high detection precision, scalability to real-size programs, as well as a remarkable gain in efficiency due to information reuse.","conference":"IEEE","terms":"Motion pictures;Software metrics;Object oriented modeling;Java;Software maintenance,formal specification;Java;object-oriented programming;pattern matching;software architecture;software maintenance;software metrics;software quality,continuous design flaw detection;evolving object-oriented program;incremental multipattern matching;code quality;software maintenance;software evolution;software metrics;locally restricted code smell;coarse-grained architectural antipattern;abstract program model;high-level object-oriented code entities;object-oriented design flaw specification;code metrics;systematic information reuse;Java program","keywords":"design-flaw detection;continuous software evolution;object-oriented software architecture","startPage":"578","endPage":"589","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582793","citationCount":0,"referenceCount":42,"year":2016,"authors":"S. Peldszus; G. Kulcsár; M. Lochau; S. Schulze","affiliations":"Institute for Software Technology, University of Koblenz-Landau, Germany; Real-Time Systems Lab, TU Darmstadt, Germany; Real-Time Systems Lab, TU Darmstadt, Germany; Institute of Software Technology Systems, TU Hamburg-Harburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38bb"},"title":"Supporting oracle construction via static analysis","abstract":"In software testing, the program under test is usually executed with test inputs and checked against a test oracle, which is a mechanism to verify whether the program behaves as expected. Selecting the right oracle data to observe is crucial in test oracle construction. In the literature, researchers have proposed two dynamic approaches to oracle data selection by analyzing test execution information (e.g., variables' values or interaction information). However, collecting such information during program execution may incur extra cost. In this paper, we present the first static approach to oracle data selection, SODS (Static Oracle Data Selection). In particular, SODS first identifies the substitution relationships between candidate oracle data by constructing a probabilistic substitution graph based on the definition-use chains of the program under test, then estimates the fault-observing capability of each candidate oracle data, and finally selects a subset of oracle data with strong fault-observing capability. For programs with analyzable test code, we further extend SODS via pruning the probabilistic substitution graph based on 0-1-CFA call graph analysis. The experimental study on 11 subject systems written in C or Java demonstrates that our static approach is more effective and much more efficient than state-of-the-art dynamic approaches in most cases.","conference":"IEEE","terms":"Software testing;Probabilistic logic;Software;Java;Fault diagnosis,C language;graph theory;Java;probability;program diagnostics;program testing;source code (software),oracle construction;static analysis;software testing;program under test;test oracle;dynamic approach;oracle data selection;test execution information analysis;variable values;interaction information;program execution;SODS;static oracle data selection;substitution relationships;definition-use chains;fault-observing capability estimation;test code analysis;probabilistic substitution graph;0-1-CFA call graph analysis;C language;Java language","keywords":"Test oracle;oracle data selection;static analysis","startPage":"178","endPage":"189","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582756","citationCount":0,"referenceCount":79,"year":2016,"authors":"J. Chen; Y. Bai; D. Hao; L. Zhang; L. Zhang; B. Xie; H. Mei","affiliations":"Key Laboratory of High Confidence Software Technologies (Peking University), MoE; Key Laboratory of High Confidence Software Technologies (Peking University), MoE; Key Laboratory of High Confidence Software Technologies (Peking University), MoE; Department of Computer Science, University of Texas at Dallas, 75080, USA; Key Laboratory of High Confidence Software Technologies (Peking University), MoE; Key Laboratory of High Confidence Software Technologies (Peking University), MoE; Key Laboratory of High Confidence Software Technologies (Peking University), MoE","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38bc"},"title":"QUICKAR: Automatic query reformulation for concept location using crowdsourced knowledge","abstract":"During maintenance, software developers deal with numerous change requests made by the users of a software system. Studies show that the developers find it challenging to select appropriate search terms from a change request during concept location. In this paper, we propose a novel technique-QUICKAR-that automatically suggests helpful reformulations for a given query by leveraging the crowdsourced knowledge from Stack Overflow. It determines semantic similarity or relevance between any two terms by analyzing their adjacent word lists from the programming questions of Stack Overflow, and then suggests semantically relevant queries for concept location. Experiments using 510 queries from two software systems suggest that our technique can improve or preserve the quality of 76% of the initial queries on average which is promising. Comparison with one baseline technique validates our preliminary findings, and also demonstrates the potential of our technique.","conference":"IEEE","terms":"Software;Semantics;Java;Programming;Vocabulary;Databases;Context,query processing;software maintenance,QUICKAR;automatic query reformulation;concept location;crowdsourced knowledge;software maintenance;Stack Overflow programming questions;semantic similarity;semantically relevant queries","keywords":"Query reformulation;crowdsourced knowledge;semantic relevance;word co-occurrence;adjacency list;Stack Overflow","startPage":"220","endPage":"225","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582760","citationCount":0,"referenceCount":24,"year":2016,"authors":"M. M. Rahman; C. K. Roy","affiliations":"Department of Computer Science, University of Saskatchewan, Canada; Department of Computer Science, University of Saskatchewan, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38bd"},"title":"Verifying Simulink Stateflow model: Timed automata approach","abstract":"Simulink Stateflow is widely used for the model-driven development of software. However, the increasing demand of rigorous verification for safety critical applications brings new challenge to the Simulink Stateflow because of the lack of formal semantics. In this paper, we present STU, a self-contained toolkit to bridge the Simulink Stateflow and a well-defined rigorous verification. The tool translates the Simulink Stateflow into the Uppaal timed automata for verification. Compared to existing work, more advanced and complex modeling features in Stateflow such as the event stack, conditional action and timer are supported. Then, with the strong verification power of Uppaal, we can not only find design defects that are missed by the Simulink Design Verifier, but also check more important temporal properties. The evaluation on artificial examples and real industrial applications demonstrates the effectiveness.","conference":"IEEE","terms":"Automata;Software packages;Switches;Clocks;Junctions;Semantics;Synchronization,automata theory;program verification;software tools,Simulink Stateflow model verification;Uppaal timed automata;model-driven development;STU toolkit","keywords":"Simulink Stateflow;Uppaal Timed Automaton;Verification","startPage":"852","endPage":"857","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582827","citationCount":1,"referenceCount":15,"year":2016,"authors":"Y. Yang; Y. Jiang; M. Gu; J. Sun","affiliations":"School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38be"},"title":"Towards efficient and effective automatic program repair","abstract":"Automatic Program Repair (APR) has recently been an emerging research area, addressing an important challenge in software engineering. APR techniques, if effective and efficient, can greatly help software debugging and maintenance. Recently proposed APR techniques can be generally classified into two families, namely search-based and semantics-based APR methods. To produce repairs, search based APR techniques generate huge populations of possible repairs, i.e., search space, and lazily search for the best one among the search space. Semantics-based APR techniques utilize constraint solving and program synthesis to make search space more tractable, and find those repairs that conform to semantics constraints extracted via symbolic execution. Despite recent advances in APR, search-based APR still suffers from search space explosion problem, while the semantics-based APR could be hindered by limited capability of constraint solving and program synthesis. Furthermore, both APR families may be subject to overfitting, in which generated repairs do not generalize to other test sets. This thesis works towards enhancing both effectiveness and efficiency in order for APR to be practically adopted in foreseeable future. To achieve this goal, other than using test cases as the primary criteria for traversing the search space, we designed a new feature used for a new search-based APR technique to effectively traverse the search space, wherein bug fix history is used to evaluate the quality of repair candidates. We also developed a deductive-reasoning-based repair technique that combines search-based and semantics-based approaches to enhance the repair capability, while ensuring the soundness of generated repairs. We also leveraged machine-learning techniques to build a predictive model that predicts whether an APR technique is effective in fixing particular bugs. In the future, we plan to synergize many existing APR techniques, improve our predictive model, and adopt the advances of other fields such as test case generation and program synthesis for APR.","conference":"IEEE","terms":"Maintenance engineering;Computer bugs;History;Software;Semantics;Syntactics;Search problems,inference mechanisms;learning (artificial intelligence);program debugging;semantic networks;software maintenance,automatic program repair;software engineering;software debugging;software maintenance;search-based APR method;semantics-based APR method;semantic constraint extraction;symbolic execution;deductive reasoning;machine learning;bug fixing","keywords":"Automatic Program Repair;Deductive Reasoning;Mining Software Repository;Genetic Programming","startPage":"876","endPage":"879","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582831","citationCount":0,"referenceCount":29,"year":2016,"authors":"X. D. Le","affiliations":"School of Information Systems, Singapore Management University, Singapore","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38bf"},"title":"How good are the specs? A study of the bug-finding effectiveness of existing Java API specifications","abstract":"Runtime verification can be used to find bugs early, during software development, by monitoring test executions against formal specifications (specs). The quality of runtime verification depends on the quality of the specs. While previous research has produced many specs for the Java API, manually or through automatic mining, there has been no large-scale study of their bug-finding effectiveness. We present the first in-depth study of the bug-finding effectiveness of previously proposed specs. We used JavaMOP to monitor 182 manually written and 17 automatically mined specs against more than 18K manually written and 2.1M automatically generated tests in 200 open-source projects. The average runtime overhead was under 4.3x. We inspected 652 violations of manually written specs and (randomly sampled) 200 violations of automatically mined specs. We reported 95 bugs, out of which developers already fixed 74. However, most violations, 82.81% of 652 and 97.89% of 200, were false alarms. Our empirical results show that (1) runtime verification technology has matured enough to incur tolerable runtime overhead during testing, and (2) the existing API specifications can find many bugs that developers are willing to fix; however, (3) the false alarm rates are worrisome and suggest that substantial effort needs to be spent on engineering better specs and properly evaluating their effectiveness.","conference":"IEEE","terms":"Runtime;Computer bugs;Monitoring;Synchronization;Open source software;Java;Testing,application program interfaces;program debugging;program testing;program verification;public domain software,Java API specifications;software development;test executions monitoring;formal specifications;runtime verification;bug-finding effectiveness;Java-MOP;manually written specs;open-source projects;runtime overhead;automatically mined specs;tolerable runtime overhead;false alarm rates","keywords":"runtime verification;specification quality;empirical study","startPage":"602","endPage":"613","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582795","citationCount":0,"referenceCount":63,"year":2016,"authors":"O. Legunsen; W. U. Hassan; X. Xu; G. Roşu; D. Marinov","affiliations":"Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c0"},"title":"Multi-objective test report prioritization using image understanding","abstract":"In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.","conference":"IEEE","terms":"Testing;Mobile communication;Computer bugs;Software;Mobile applications;Image color analysis;Inspection,image classification;image matching;mobile computing;natural language processing;program testing;text analysis,multiobjective test report prioritization;image understanding;crowdsourced software testing;software maintenance task;text-based test-report classification;mobile testing domain;screenshots;descriptive text;natural-language text information;mobile applications;spatial pyramid matching;SPM","keywords":"Crowdsourced Testing;Test Report Prioritization;Image Understanding;Multi-Objective Optimization","startPage":"202","endPage":"213","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582758","citationCount":0,"referenceCount":41,"year":2016,"authors":"Y. Feng; J. A. Jones; Z. Chen; C. Fang","affiliations":"Department of Informatics, University of California, Irvine, USA; Department of Informatics, University of California, Irvine, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c1"},"title":"Automated model-based Android GUI testing using multi-level GUI comparison criteria","abstract":"Automated Graphical User Interface (GUI) testing is one of the most widely used techniques to detect faults in mobile applications (apps) and to test functionality and usability. GUI testing exercises behaviors of an application under test (AUT) by executing events on GUIs and checking whether the app behaves correctly. In particular, because Android leads in market share of mobile OS platforms, a lot of research on automated Android GUI testing techniques has been performed. Among various techniques, we focus on model-based Android GUI testing that utilizes a GUI model for systematic test generation and effective debugging support. Since test inputs are generated based on the underlying model, accurate GUI modeling of an AUT is the most crucial factor in order to generate effective test inputs. However, most modern Android apps contain a number of dynamically constructed GUIs that make accurate behavior modeling more challenging. To address this problem, we propose a set of multi-level GUI Comparison Criteria (GUICC) that provides the selection of multiple abstraction levels for GUI model generation. By using multilevel GUICC, we conducted empirical experiments to identify the influence of GUICC on testing effectiveness. Results show that our approach, which performs model-based testing with multi-level GUICC, achieved higher effectiveness than activity-based GUI model generation. We also found that multi-level GUICC can alleviate the inherent state explosion problems of existing a single-level GUICC for behavior modeling of real-world Android apps by flexibly manipulating GUICC.","conference":"IEEE","terms":"Graphical user interfaces;Testing;Androids;Humanoid robots;Space exploration;Analytical models;Mobile communication,Android (operating system);fault diagnosis;graphical user interfaces;mobile computing;program debugging;program testing,automated model-based Android GUI testing;multilevel GUI comparison criteria;multilevel GUICC;automated graphical user interface testing;fault detection;mobile applications;functionality testing;usability testing;application under test;mobile OS platforms;systematic test generation;debugging support;AUT GUI modeling;GUI model generation;single-level GUICC;Android apps behavior modeling","keywords":"GUI testing;Android application testing;GUI model generation;GUI comparison criteria;Model-based test input generation","startPage":"238","endPage":"249","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582762","citationCount":2,"referenceCount":27,"year":2016,"authors":"Y. Baek; D. Bae","affiliations":"Korea Advanced Institute of Science and Technology (KAIST) Daeieon. Republic of Korea; Korea Advanced Institute of Science and Technology (KAIST) Daeieon. Republic of Korea","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c2"},"title":"An automated collaborative requirements engineering tool for better validation of requirements","abstract":"This demo introduces an automated collaborative requirements engineering tool, called TestMEReq, which is used to promote effective communication and collaboration between client-stakeholders and requirements engineers for better requirements validation. Our tool is augmented with real time communication and collaboration support to allow multiple stakeholders to collaboratively validate the same set of requirements. We have conducted a user study focusing on validating requirements using TestMEReq with a few groups of requirements engineers and client stakeholders. The study shows that our automated tool support is able to assist requirements engineers to effectively communicate with client-stakeholders to better validate the requirements virtually in real time. (Demo video: https://www.youtube.com/watch?v=7sWLOx-N4Jo).","conference":"IEEE","terms":"Collaboration;Stakeholders;Software;Testing;User interfaces;Libraries;Prototypes,formal specification;formal verification;groupware;program testing,automated collaborative requirement engineering tool;requirements validation;TestMEReq","keywords":"Abstract test;Essential Use Cases;Essential User Interface;requirement-based testing;requirements validation;communication and collaboration","startPage":"864","endPage":"869","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582829","citationCount":0,"referenceCount":12,"year":2016,"authors":"N. A. Moketar; M. Kamalrudin; S. Sidek; M. Robinson; J. Grundy","affiliations":"Innovative Software System and Services Group, Universiti Teknikal Malaysia Melaka, Melaka, Malaysia; Innovative Software System and Services Group, Universiti Teknikal Malaysia Melaka, Melaka, Malaysia; Innovative Software System and Services Group, Universiti Teknikal Malaysia Melaka, Melaka, Malaysia; Fulgent Corporation, USA; Faculty of Science Engineering and Built Environment, School of Information Technology, Melbourne Burwood Campus, Deakin University, Victoria 3125, Australia","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c3"},"title":"Towards automatically generating descriptive names for unit tests","abstract":"During maintenance, developers often need to understand the purpose of a test. One of the most potentially useful sources of information for understanding a test is its name. Ideally, test names are descriptive in that they accurately summarize both the scenario and the expected outcome of the test. Despite the benefits of being descriptive, test names often fall short of this goal. In this paper we present a new approach for automatically generating descriptive names for existing test bodies. Using a combination of natural-language program analysis and text generation, the technique creates names that summarize the test's scenario and the expected outcome. The results of our evaluation show that, (1) compared to alternative approaches, the names generated by our technique are significantly more similar to human-generated names and are nearly always preferred by developers, (2) the names generated by our technique are preferred over or are equivalent to the original test names in 83% of cases, and (3) our technique is several orders of magnitude faster than manually writing test names.","conference":"IEEE","terms":"Testing;Maintenance engineering;Software maintenance;Natural languages;Semantics;Prototypes,natural language processing;program testing;software maintenance,descriptive name;unit testing;software maintenance;test name;natural-language program analysis;text generation","keywords":"Unit testing;Descriptive names;Maintenance","startPage":"625","endPage":"636","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582797","citationCount":0,"referenceCount":34,"year":2016,"authors":"B. Zhang; E. Hill; J. Clause","affiliations":"University of Delaware, Newark, DE, USA; Drew University, Madison, NJ, USA; University of Delaware, Newark, DE, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c4"},"title":"Automatic test image generation using procedural noise","abstract":"It is difficult to test programs that input images, due to the large number of (pixel) values that must be chosen and the complex ways these values interact. Typically, such programs are tested manually, using images that have known results. However, this is a laborious process and limited in the range of tests that can be applied. We introduce a new approach for testing programs that input images automatically, using procedural noise and spatial statistics to create inputs that are both realistic and can easily be tuned to have specific properties. The effectiveness of our approach is illustrated on an epidemiological simulation of a recently introduced tree pest in Great Britain: Oriental Chestnut Gall Wasp. Our approach produces images that match the real landscapes more closely than other techniques and can be used (alongside metamorphic relations) to detect smaller (artificially introduced) errors with greater accuracy.","conference":"IEEE","terms":"Software;Testing;Genetic algorithms;White noise;Histograms;Imaging;Image generation,image denoising;image matching;program testing,automatic test image generation;procedural noise;image pixels;program testing;spatial statistics;epidemiological simulation;tree pest;Great Britain;Oriental chestnut gall wasp;image matching;real landscapes","keywords":"software testing;image processing;test data generation","startPage":"654","endPage":"659","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582800","citationCount":0,"referenceCount":18,"year":2016,"authors":"M. Patrick; M. D. Castle; R. O. J. H. Stutt; C. A. Gilligan","affiliations":"Department of Plant Sciences, University of Cambridge, United Kingdom; Department of Plant Sciences, University of Cambridge, United Kingdom; Department of Plant Sciences, University of Cambridge, United Kingdom; Department of Plant Sciences, University of Cambridge, United Kingdom","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c5"},"title":"Locus: Locating bugs from software changes","abstract":"Various information retrieval (IR) based techniques have been proposed recently to locate bugs automatically at the file level. However, their usefulness is often compromised by the coarse granularity of files and the lack of contextual information. To address this, we propose to locate bugs using software changes, which offer finer granularity than files and provide important contextual clues for bug-fixing. We observe that bug inducing changes can facilitate the bug fixing process. For example, it helps triage the bug fixing task to the developers who committed the bug inducing changes or enables developers to fix bugs by reverting these changes. Our study further identifies that change logs and the naturally small granularity of changes can help boost the performance of IR-based bug localization. Motivated by these observations, we propose an IR-based approach Locus to locate bugs from software changes, and evaluate it on six large open source projects. The results show that Locus outperforms existing techniques at the source file level localization significantly. MAP and MRR in particular have been improved, on average, by 20.1% and 20.5%, respectively. Locus is also capable of locating the inducing changes within top 5 for 41.0% of the bugs. The results show that Locus can significantly reduce the number of lines needing to be scanned to locate the bug compared with existing techniques.","conference":"IEEE","terms":"Computer bugs;Software;Debugging;History;Information retrieval;Natural languages;Manuals,information retrieval;program debugging,Locus;software changes;information retrieval;bug-fixing;bug inducing changes;IR-based bug localization;source file level localization;MAP;MRR","keywords":"bug localization;software changes;information retrieval;software analytics","startPage":"262","endPage":"273","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582764","citationCount":0,"referenceCount":46,"year":2016,"authors":"M. Wen; R. Wu; S. Cheung","affiliations":"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c6"},"title":"Generating test cases to expose concurrency bugs in android applications","abstract":"Mobile systems usually support an event-based model of concurrent programming. This model, although advantageous to maintain responsive user interfaces, may lead to subtle concurrency errors due to unforeseen threads interleaving coupled with non-deterministic reordering of asynchronous events. These bugs are very difficult to reproduce even by the same user action sequences that trigger them, due to the undetermined schedules of underlying events and threads. In this paper, we proposed RacerDroid, a novel technique that aims to expose concurrency bugs in android applications by actively controlling event schedule and thread interleaving, given the test cases that have potential data races. By exploring the state model of the application constructed dynamically, our technique starts first to generate a test case that has potential data races based on the results obtained from existing static or dynamic race detection technique. Then it reschedules test cases execution by actively controlling event dispatching and thread interleaving to determine whether such potential races really lead to thrown exceptions or assertion violations. Our preliminary experiments show that RacerDroid is effective, and it confirms real data races, while at the same time eliminates false warnings for Android apps found in the wild.","conference":"IEEE","terms":"Message systems;Androids;Humanoid robots;Concurrent computing;Computer bugs;Instruments;Data models,Android (operating system);concurrency control;mobile computing;program debugging;program testing;smart phones;user interfaces,test case generation;concurrency bug;Android application;mobile system;event-based model;concurrent programming;user interface;RacerDroid technique;smart phone","keywords":"record/replay;data race;mobile application;Android;testing","startPage":"648","endPage":"653","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582799","citationCount":0,"referenceCount":28,"year":2016,"authors":"H. Tang; G. Wu; J. Wei; H. Zhong","affiliations":"State key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China; State key Laboratory of Computer Sciences, Institute of Software, Chinese Academy of Sciences, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c7"},"title":"Migrating cascading style sheets to preprocessors by introducing mixins","abstract":"Cascading Style Sheets (CSS) is the standard language for styling web documents and is extensively used in the industry. However, CSS lacks constructs that would allow code reuse (e.g., functions). Consequently, maintaining CSS code is often a cumbersome and error-prone task. Preprocessors (e.g., Less and Sass) have been introduced to fill this gap, by extending CSS with the missing constructs. Despite the clear maintainability benefits coming from the use of preprocessors, there is currently no support for migrating legacy CSS code to preprocessors. In this paper, we propose a technique for automatically detecting duplicated style declarations in CSS code that can be migrated to preprocessor functions (i.e., mixins). Our technique can parameterize differences in the style values of duplicated declarations, and ensure that the migration will not change the presentation semantics of the web documents. The evaluation has shown that our technique is able to detect 98% of the mix-ins that professional developers introduced in websites and Style Sheet libraries, and can safely migrate real CSS code.","conference":"IEEE","terms":"Cascading style sheets;Syntactics;HTML;Browsers;Software;Semantics;Libraries,document handling;hypermedia markup languages;Internet;programming languages;software maintenance;software reusability,cascading style sheet;CSS;mixin;preprocessor function;Web document styling;code reuse;code maintenance;hypermedia markup language;HTML","keywords":"Cascading style sheets;refactoring;duplication;migration","startPage":"672","endPage":"683","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582802","citationCount":0,"referenceCount":44,"year":2016,"authors":"D. Mazinanian; N. Tsantalis","affiliations":"Computer Science and Software Engineering, Concordia University, Montreal, Canada; Computer Science and Software Engineering, Concordia University, Montreal, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c8"},"title":"Recommending relevant classes for bug reports using multi-objective search","abstract":"Developers may follow a tedious process to find the cause of a bug based on code reviews and reproducing the abnormal behavior. In this paper, we propose an automated approach to finding and ranking potential classes with the respect to the probability of containing a bug based on a bug report description. Our approach finds a good balance between minimizing the number of recommended classes and maximizing the relevance of the proposed solution using a multi-objective optimization algorithm. The relevance of the recommended classes (solution) is estimated based on the use of the history of changes and bug-fixing, and the lexical similarity between the bug report description and the API documentation. We evaluated our system on 6 open source Java projects, using the version of the project before fixing the bug of many bug reports. The experimental results show that the search-based approach significantly outperforms three state-of-the-art methods in recommending relevant files for bug reports. In particular, our multi-objective approach is able to successfully locate the true buggy methods within the top 10 recommendations for over 87% of the bug reports.","conference":"IEEE","terms":"Computer bugs;Search problems;Software engineering;Optimization;Software;History;Documentation,application program interfaces;estimation theory;Java;optimisation;probability;program debugging;public domain software;search problems;software maintenance,class recommendation;bug report description;multiobjective search;probability;multiobjective optimization algorithm;relevance estimation;API documentation;open source Java project;software maintenance","keywords":"Search-based software engineering;bug reports;multi-objective optimization;software maintenance","startPage":"286","endPage":"295","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582766","citationCount":0,"referenceCount":28,"year":2016,"authors":"R. Almhana; W. Mkaouer; M. Kessentini; A. Ouni","affiliations":"Computer and Information Science Department, University of Michigan, Dearborn, MI, USA; Computer and Information Science Department, University of Michigan, Dearborn, MI, USA; Computer and Information Science Department, University of Michigan, Dearborn, MI, USA; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38c9"},"title":"What developers want and need from program analysis: An empirical study","abstract":"Program Analysis has been a rich and fruitful field of research for many decades, and countless high quality program analysis tools have been produced by academia. Though there are some well-known examples of tools that have found their way into routine use by practitioners, a common challenge faced by researchers is knowing how to achieve broad and lasting adoption of their tools. In an effort to understand what makes a program analyzer most attractive to developers, we mounted a multi-method investigation at Microsoft. Through interviews and surveys of developers as well as analysis of defect data, we provide insight and answers to four high level research questions that can help researchers design program analyzers meeting the needs of software developers. First, we explore what barriers hinder the adoption of program analyzers, like poorly expressed warning messages. Second, we shed light on what functionality developers want from analyzers, including the types of code issues that developers care about. Next, we answer what non-functional characteristics an analyzer should have to be widely used, how the analyzer should fit into the development process, and how its results should be reported. Finally, we investigate defects in one of Microsoft's flagship software services, to understand what types of code issues are most important to minimize, potentially through program analysis.","conference":"IEEE","terms":"Software;Security;Interviews;Companies;Ecosystems;Software engineering;Pain,program diagnostics;software engineering,high quality program analysis tools;multimethod investigation;functionality developers;software developers;code issues;Microsoft software services;software defect analysis","keywords":"program analysis;code defects","startPage":"332","endPage":"343","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582770","citationCount":0,"referenceCount":52,"year":2016,"authors":"M. Christakis; C. Bird","affiliations":"Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ca"},"title":"Mining revision histories to detect cross-language clones without intermediates","abstract":"To attract more users on different platforms, many projects release their versions in multiple programming languages (e.g., Java and C#). They typically have many code snippets that implement similar functionalities, i.e., cross-language clones. Programmers often need to track and modify cross-language clones consistently to maintain similar functionalities across different language implementations. In literature, researchers have proposed approaches to detect cross-language clones, mostly for languages that share a common intermediate language (such as the .NET language family) so that techniques for detecting single-language clones can be applied. As a result, those approaches cannot detect cross-language clones for many projects that are not implemented in a .NET language. To overcome the limitation, in this paper, we propose a novel approach, CLCMiner, that detects cross-language clones automatically without the need of an intermediate language. Our approach mines such clones from revision histories, which reflect how programmers maintain cross-language clones in practice. We have implemented a prototype tool for our approach and conducted an evaluation on five open source projects that have versions in Java and C#. The results show that CLCMiner achieves high accuracy and point to promising future work.","conference":"IEEE","terms":"Cloning;Java;C# languages;History;Grammar;Software,data mining;programming languages;software maintenance,revision histories mining;cross-language clones detection;intermediate language;.NET language family;single-language clones detection;CLCMiner;open source projects;Java language;C# language","keywords":"cross-language clone;diff;revision history","startPage":"696","endPage":"701","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582804","citationCount":1,"referenceCount":21,"year":2016,"authors":"X. Cheng; Z. Peng; L. Jiang; H. Zhong; H. Yu; J. Zhao","affiliations":"Department of Computer Science and Engineering, ShanghaiJiao Tong University, China; School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; Department of Computer Science and Engineering, ShanghaiJiao Tong University, China; School of Software, ShanghaiJiao Tong University, China; Department of Advanced Information Technology, Kyushu University, Japan","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38cb"},"title":"StraightTaint: Decoupled offline symbolic taint analysis","abstract":"Taint analysis has been widely applied in ex post facto security applications, such as attack provenance investigation, computer forensic analysis, and reverse engineering. Unfortunately, the high runtime overhead imposed by dynamic taint analysis makes it impractical in many scenarios. The key obstacle is the strict coupling of program execution and taint tracking logic code. To alleviate this performance bottleneck, recent work seeks to offload taint analysis from program execution and run it on a spare core or a different CPU. However, since the taint analysis has heavy data and control dependencies on the program execution, the massive data in recording and transformation overshadow the benefit of decoupling. In this paper, we propose a novel technique to allow very lightweight logging, resulting in much lower execution slowdown, while still permitting us to perform full-featured offline taint analysis. We develop StraightTaint, a hybrid taint analysis tool that completely decouples the program execution and taint analysis. StraightTaint relies on very lightweight logging of the execution information to reconstruct a straight-line code, enabling an offline symbolic taint analysis without frequent data communication with the application. While StraightTaint does not log complete runtime or input values, it is able to precisely identify the causal relationships between sources and sinks, for example. Compared with traditional dynamic taint analysis tools, StraightTaint has much lower application runtime overhead.","conference":"IEEE","terms":"Runtime;Security;Performance analysis;Reverse engineering;Registers;Malware,program diagnostics;security of data,StraightTaint;decoupled offline symbolic taint analysis;ex post facto security applications;attack provenance investigation;computer forensic analysis;reverse engineering;program execution strict coupling;taint tracking logic code;lightweight logging;full-featured offline taint analysis;hybrid taint analysis tool;straight-line code reconstruction","keywords":"Taint analysis;Decoupling;Offline;Symbolic taint analysis","startPage":"308","endPage":"319","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582768","citationCount":0,"referenceCount":54,"year":2016,"authors":"J. Ming; D. Wu; J. Wang; G. Xiao; P. Liu","affiliations":"College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA 16802, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38cc"},"title":"Radius aware probabilistic testing of deadlocks with guarantees","abstract":"Concurrency bugs only occur under certain interleaving. Existing randomized techniques are usually ineffective. PCT innovatively generates scheduling, before executing a program, based on priorities and priority change points. Hence, it provides a probabilistic guarantee to trigger concurrency bugs. PCT randomly selects priority change points among all events, which might be effective for non-deadlock concurrency bugs. However, deadlocks usually involve two or more threads and locks, and require more ordering constraints to be triggered. We interestingly observe that, every two events of a deadlock usually occur within a short range. We generally formulate this range as the bug Radius, to denote the max distance of every two events of a concurrency bug. Based on the bug radius, we propose RPro (Radius aware Probabilistic testing) for triggering deadlocks. Unlike PCT, RPro selects priority change points within the radius of the targeted deadlocks but not among all events. Hence, it guarantees larger probabilities to trigger deadlocks. We have implemented RPro and PCT and evaluated them on a set of real-world benchmarks containing 10 unique deadlocks. The experimental results show that RPro triggered all deadlocks with higher probabilities (i.e., \u003e7.7x times larger on average) than that by PCT. We also evaluated RPro with radius varying from 1 to 150 (or 300). The result shows that the radius of a deadlock is much smaller (i.e., from 2 to 114 in our experiment) than the number of all events. This further confirms our observation and makes RPro meaningful in practice.","conference":"IEEE","terms":"Computer bugs;System recovery;Concurrent computing;Probabilistic logic;Instruction sets;Benchmark testing,concurrency control;probability;program debugging;program testing;system recovery,radius aware probabilistic testing;RPro;deadlock triggering;concurrency bug;probabilistic guarantee","keywords":"Deadlock;random testing;bug radius;multithreaded program","startPage":"356","endPage":"367","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582772","citationCount":0,"referenceCount":66,"year":2016,"authors":"Y. Cait; Z. Yang","affiliations":"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38cd"},"title":"Bugram: Bug detection with n-gram language models","abstract":"To improve software reliability, many rule-based techniques have been proposed to infer programming rules and detect violations of these rules as bugs. These rule-based approaches often rely on the highly frequent appearances of certain patterns in a project to infer rules. It is known that if a pattern does not appear frequently enough, rules are not learned, thus missing many bugs. In this paper, we propose a new approach - Bugram - that leverages n-gram language models instead of rules to detect bugs. Bugram models program tokens sequentially, using the n-gram language model. Token sequences from the program are then assessed according to their probability in the learned model, and low probability sequences are marked as potential bugs. The assumption is that low probability token sequences in a program are unusual, which may indicate bugs, bad practices, or unusual/special uses of code of which developers may want to be aware. We evaluate Bugram in two ways. First, we apply Bugram on the latest versions of 16 open source Java projects. Results show that Bugram detects 59 bugs, 42 of which are manually verified as correct, 25 of which are true bugs and 17 are code snippets that should be refactored. Among the 25 true bugs, 23 cannot be detected by PR-Miner. We have reported these bugs to developers, 7 of which have already been confirmed by developers (4 of them have already been fixed), while the rest await confirmation. Second, we further compare Bugram with three additional graph- and rule-based bug detection tools, i.e., JADET, Tikanga, and GrouMiner. We apply Bugram on 14 Java projects evaluated in these three studies. Bugram detects 21 true bugs, at least 10 of which cannot be detected by these three tools. Our results suggest that Bugram is complementary to existing rule-based bug detection approaches.","conference":"IEEE","terms":"Computer bugs;Software;Programming;Java;Buildings;Semantics;Software reliability,inference mechanisms;Java;knowledge based systems;natural language processing;probability;program debugging;public domain software;software reliability,Bugram;bug detection;n-gram language model;software reliability;rule-based technique;programming rule inference;probability sequence;open source Java project","keywords":"Bug Detection;Static Code Analysis;N-gram Language Model","startPage":"708","endPage":"719","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582806","citationCount":0,"referenceCount":57,"year":2016,"authors":"S. Wang; D. Chollak; D. Movshovitz-Attias; L. Tan","affiliations":"Electrical and Computer Engineering, University of Waterloo, Canada; Electrical and Computer Engineering, University of Waterloo, Canada; Computer Science Department, Carnegie Mellon University, USA; Electrical and Computer Engineering, University of Waterloo, Canada","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ce"},"title":"Learning a dual-language vector space for domain-specific cross-lingual question retrieval","abstract":"The lingual barrier limits the ability of millions of non-English speaking developers to make effective use of the tremendous knowledge in Stack Overflow, which is archived in English. For cross-lingual question retrieval, one may use translation-based methods that first translate the non-English queries into English and then perform monolingual question retrieval in English. However, translation-based methods suffer from semantic deviation due to inappropriate translation, especially for domain-specific terms, and lexical gap between queries and questions that share few words in common. To overcome the above issues, we propose a novel cross-lingual question retrieval based on word embed-dings and convolutional neural network (CNN) which are the state-of-the-art deep learning techniques to capture word- and sentence-level semantics. The CNN model is trained with large amounts of examples from Stack Overflow duplicate questions and their corresponding translation by machine, which guides the CNN to learn to capture informative word and sentence features to recognize and quantify semantic similarity in the presence of semantic deviations and lexical gaps. A uniqueness of our approach is that the trained CNN can map documents in two languages (e.g., Chinese queries and English questions) in a dual-language vector space, and thus reduce the cross-lingual question retrieval problem to a simple k-nearest neighbors search problem in the dual-language vector space, where no query or question translation is required. Our evaluation shows that our approach significantly outperforms the translation-based method, and can be extended to dual-language documents retrieval from different sources.","conference":"IEEE","terms":"Semantics;Neural networks;Context;Google;Inspection;Predictive models;Convolution,language translation;learning (artificial intelligence);natural language processing;neural nets;query processing;question answering (information retrieval);search problems;text analysis,dual-language vector space;domain-specific cross-lingual question retrieval;lingual barrier;nonEnglish speaking developers;Stack Overflow;translation-based method;monolingual question retrieval;semantic deviation;domain-specific terms;lexical gap;word embeddings;convolutional neural network;deep learning technique;word-level semantics capture;sentence-level semantics capture;CNN model training;machine translation;semantic similarity;document mapping;Chinese queries;English questions;k-nearest neighbor search problem;dual-language document retrieval","keywords":"Cross-lingual question retrieval;Word embeddings;Convolutional Neural Network;Dual-Language Vector Space","startPage":"744","endPage":"755","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582810","citationCount":0,"referenceCount":47,"year":2016,"authors":"G. Chen; C. Chen; Z. Xing; B. Xu","affiliations":"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Computer Science and Technology, Zhejiang University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38cf"},"title":"Sound static deadlock analysis for C/Pthreads","abstract":"We present a static deadlock analysis for C/Pthreads. The design of our method has been guided by the requirement to analyse real-world code. Our approach is sound (i.e., misses no deadlocks) for programs that have defined behaviour according to the C standard and the Pthreads specification, and is precise enough to prove deadlock-freedom for a large number of such programs. The method consists of a pipeline of several analyses that build on a new context- and thread-sensitive abstract interpretation framework. We further present a lightweight dependency analysis to identify statements relevant to deadlock analysis and thus speed up the overall analysis. In our experimental evaluation, we succeeded to prove deadlock-freedom for 292 programs from the Debian GNU/Linux distribution with in total 2.3 MLOC in 4 hours.","conference":"IEEE","terms":"System recovery;Instruction sets;Pipelines;Computer bugs;Standards;Concurrent computing;Scalability,C language;formal specification;Linux;program diagnostics,sound static deadlock analysis;C/Pthreads;real-world code analysis;C standard;Pthreads specification;deadlock-freedom;context-sensitive abstract interpretation framework;thread-sensitive abstract interpretation framework;lightweight dependency analysis;Debian GNU/Linux distribution;time 4 hour","keywords":"deadlock analysis;static analysis;abstract interpretation","startPage":"379","endPage":"390","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582774","citationCount":0,"referenceCount":45,"year":2016,"authors":"D. Kroening; D. Poetzl; P. Schrammel; B. Wachter","affiliations":"University of Oxford, Oxford, UK; University of Oxford, Oxford, UK; University of Sussex, Brighton, UK; SSW-Trading GmbH, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d0"},"title":"Evaluating non-adequate test-case reduction","abstract":"Given two test cases, one larger and one smaller, the smaller test case is preferred for many purposes. A smaller test case usually runs faster, is easier to understand, and is more convenient for debugging. However, smaller test cases also tend to cover less code and detect fewer faults than larger test cases. Whereas traditional research focused on reducing test suites while preserving code coverage, recent work has introduced the idea of reducing individual test cases, rather than test suites, while still preserving code coverage. Other recent work has proposed non-adequately reducing test suites by not even preserving all the code coverage. This paper empirically evaluates a new combination of these two ideas, non-adequate reduction of test cases, which allows for a wide range of trade-offs between test case size and fault detection. Our study introduces and evaluates C%-coverage reduction (where a test case is reduced to retain at least C% of its original coverage) and N-mutant reduction (where a test case is reduced to kill at least N of the mutants it originally killed). We evaluate the reduction trade-offs with varying values of C% and N for four real-world C projects: Mozilla's SpiderMonkey JavaScript engine, the YAFFS2 flash file system, Grep, and Gzip. The results show that it is possible to greatly reduce the size of many test cases while still preserving much of their fault-detection capability.","conference":"IEEE","terms":"Debugging;Size measurement;Computer science;Electrical engineering;Fault detection;Engines;Software,fault diagnosis;fault tolerant computing;program debugging;program testing,nonadequate test-case reduction;software debugging;code coverage;fault detection;test suite reduction;test case size;N-mutant reduction;Mozilla SpiderMonkey JavaScript engine;YAFFS2 flash file system;Grep;Gzip;software testing","keywords":"test reduction;test adequacy;coverage;mutation testing","startPage":"16","endPage":"26","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582741","citationCount":0,"referenceCount":27,"year":2016,"authors":"M. A. Alipour; A. Shi; R. Gopinath; D. Marinov; A. Groce","affiliations":"School of Electrical Engineering and Computer Science, Oregon State University, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA; Department of Computer Science, University of Illinois at Urbana-Champaign, USA; School of Electrical Engineering and Computer Science, Oregon State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d1"},"title":"Phrase-based extraction of user opinions in mobile app reviews","abstract":"Mobile app reviews often contain useful user opinions like bug reports or suggestions. However, looking for those opinions manually in thousands of reviews is ineffective and time-consuming. In this paper, we propose PUMA, an automated, phrase-based approach to extract user opinions in app reviews. Our approach includes a technique to extract phrases in reviews using part-of-speech (PoS) templates; a technique to cluster phrases having similar meanings (each cluster is considered as a major user opinion); and a technique to monitor phrase clusters with negative sentiments for their outbreaks over time. We used PUMA to study two popular apps and found that it can reveal severe problems of those apps reported in their user reviews.","conference":"IEEE","terms":"Artificial neural networks;Mobile communication;Facebook;Games;Monitoring;Batteries;Computer bugs,feature extraction;mobile computing;pattern clustering;sentiment analysis;software reviews,phrase-based extraction;user opinion;mobile app review;PUMA;part-of-speech template;PoS template;phrase clustering","keywords":"Opinion Mining;Review Analysis;Phrase Extraction","startPage":"726","endPage":"731","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582808","citationCount":0,"referenceCount":19,"year":2016,"authors":"P. M. Vu; H. V. Pham; T. T. Nguyen; T. T. Nguyen","affiliations":"Computer Science Department, Utah State University; Computer Science Department, Utah State University; Computer Science Department, Utah State University; Computer Science Department, Utah State University","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d2"},"title":"Relda2: An effective static analysis tool for resource leak detection in Android apps","abstract":"Resource leak is a common bug in Android applications (apps for short). In general, it is caused by missing release operations of the resources provided by Android (like Camera, Media Player and Sensors) that require programmers to explicitly release them. It might lead to several serious problems for the app and system, such as performance degradation and system crash. This paper presents Relda2, a light-weight, scalable and practical static analysis tool, for detecting resource leaks in the byte-code of Android apps automatically. It supports two analysis techniques (flow-insensitive for quick scanning and flow-sensitive for accurate scanning), and performs inter-procedural analysis to get more precise bug reports. In addition, our tool is practical to analyze real-world apps, and has been applied to 103 Android apps, including industry applications and open source programs. We have found 67 real resource leaks in these apps, which we confirmed manually. A demo video of our tool can be found at the website: https://www.youtube.com/watch?v=Mk-MFcHpTds.","conference":"IEEE","terms":"Androids;Humanoid robots;Computer bugs;Smart phones;Software;Leak detection,Android (operating system);program diagnostics,Relda2;static analysis tool;resource leak detection;Android apps;light-weight static analysis;scalable static analysis;interprocedural analysis;open source program;byte-code","keywords":"Android apps;resource leak;static analysis;byte-code","startPage":"762","endPage":"767","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582812","citationCount":0,"referenceCount":14,"year":2016,"authors":"T. Wu; J. Liu; X. Deng; J. Yan; J. Zhang","affiliations":"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d3"},"title":"An empirical evaluation of two user interfaces of an interactive program verifier","abstract":"Theorem provers have highly complex interfaces, but there are not many systematic studies of their usability and effectiveness. Specifically, for interactive theorem provers the ability to quickly comprehend intermediate proof situations is of pivotal importance. In this paper we present the (as far as we know) first empirical study that systematically compares the effectiveness of different user interfaces of an interactive theorem prover. We juxtapose two different user interfaces of the interactive verifier KeY: the traditional one which focuses on proof objects and a more recent one that provides a view akin to an interactive debugger. We carefully designed a controlled experiment where users were given various proof understanding tasks that had to be solved with alternating interfaces. We provide statistical evidence that the conjectured higher effectivity of the debugger-like interface is not just a hunch.","conference":"IEEE","terms":"User interfaces;Usability;Inspection;Standards;Java,interactive systems;program debugging;program verification;theorem proving;user interfaces,empirical evaluation;user interface;interactive program verifier;theorem prover;proof object;interactive debugger","keywords":"Verification;Proof Understanding;Empirical Evaluation","startPage":"403","endPage":"413","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582776","citationCount":0,"referenceCount":14,"year":2016,"authors":"M. Hentschel; R. Hähnle; R. Bubel","affiliations":"TU Darmstadt, Dept. of Computer Science, Darmstadt, Germany; TU Darmstadt, Dept. of Computer Science, Darmstadt, Germany; TU Darmstadt, Dept. of Computer Science, Darmstadt, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d4"},"title":"The IDE as a scriptable information system","abstract":"Software engineering is extremely information-intensive. Every day developers work with source code, version repositories, issue trackers, documentation, web-based and other information resources. However, three key aspects of information work lack good support: (i) combining information from different sources; (ii) flexibly presenting collected information to enable easier comprehension; and (iii) automatically acting on collected information, for example to perform a refactoring. Poor support for these activities makes many common development tasks time-consuming and error-prone. We propose an approach that directly addresses these three issues by integrating a flexible query mechanism into the development environment. Our approach enables diverse ways to process and visualize information and can be extended via scripts. We demonstrate how an implementation of the approach can be used to rapidly write queries that meet a wide range of information needs.","conference":"IEEE","terms":"Visualization;Heating;Data visualization;Information services;Software;Engines;Computer bugs,data visualisation;information needs;programming environments;query processing;software maintenance;source code (software),IDE;scriptable information system;software engineering;source code;version repositories;issue trackers;documentation;Web-based system;information resources;refactoring;query mechanism;development environment;information processing;information visualization;information needs","keywords":"code queries;software visualization;refactoring","startPage":"444","endPage":"449","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582780","citationCount":0,"referenceCount":19,"year":2016,"authors":"D. Asenov; P. Müller; L. Vogel","affiliations":"Dept. of Computer Science, ETH Zurich, Switzerland; Dept. of Computer Science, ETH Zurich, Switzerland; Ergon Informatik AG, Zurich, Switzerland","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d5"},"title":"What makes killing a mutant hard","abstract":"Mutation operators have been studied at length to determine which ones are the “best” at some metric (for example creates the least equivalent mutants, creates hard-to-kill mutants, etc.). These studies though have focused on specific test suites, where the test inputs and oracles are fixed, which leads to results that are strongly influenced by the test suites and thus makes the conclusions potentially less general. In this paper we consider all test inputs and we assume we have no prior knowledge about the likelihood of any specific inputs. We will also show how varying the strength of the oracle have a big impact on the results. We only consider a few mutation operators (mostly relational), only a handful of programs to mutate (amenable to probabilistic symbolic execution), and only consider how likely it is that a mutant is killed. A core finding is that the likelihood of reaching the source line where the mutation is applied, is an important contributor to the likelihood of killing the mutant and when we control for this we can see which operators create mutations that are too easy versus very hard to kill.","conference":"IEEE","terms":"Java;Testing;Radiation detectors;Probabilistic logic;Measurement;Software,probability;program testing;source code (software),mutation operator;test suite;test input;probabilistic symbolic execution;source line","keywords":"Mutation Testing;Probabilistic Symbolic Execution","startPage":"39","endPage":"44","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582743","citationCount":0,"referenceCount":16,"year":2016,"authors":"W. Visser","affiliations":"Department of Computer Science, Stellenbosch University, South Africa","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d6"},"title":"Model driven design of heterogeneous synchronous embedded systems","abstract":"Synchronous embedded systems are becoming more and more complicated and are usually implemented with integrated hardware/software solutions. This implementation manner brings new challenges to the traditional model-driven design environments such as SCADE and STATEMATE, that supports pure hardware or software design. In this paper, we propose a co-design tool Tsmart-Edola to facilitate the system developers, and automatically generate the executable VHDL code and C code from the formal verified SyncBlock computation model. SyncBlock is a lightweight high-level system specification model with well defined syntax, simulation and formal semantics. Based on which, the graphical model editor, graphical simulator, verification translator, and code generator are implemented and seamlessly integrated into the Tsmart-Edola. For evaluation, we apply Tsmart-Edola to the design of a real-world train controller based on the international standard IEC 61375. Several critical ambiguousness or bugs in the standard are detected during formal verification of the constructed system model. Furthermore, the generated VHDL code and C code of Tsmart-Edola outperform that of the state-of-the-art tools in terms of synthesized gate array resource consumption and binary code size. The abstract demo video address is : https://youtu.be/D9ROyJmKZ4s The tool, user manual and examples can be downloaded: http://sts.thss.tsinghua.edu.cn/Tsmart-Edola/.","conference":"IEEE","terms":"Computational modeling;Hardware;Ports (Computers);Software;Generators;Graphical models;Semantics,embedded systems;formal specification;formal verification;hardware-software codesign,model driven design;heterogeneous synchronous embedded system;SCADE;STATEMATE;Tsmart-Edola;VHDL code;C code;SyncBlock computation model;lightweight high-level system specification;formal semantics;graphical model editor;graphical simulator;verification translator;code generator;real-world train controller;formal verification","keywords":"model driven development;computation model;hardwaresoftware co-design","startPage":"774","endPage":"779","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582814","citationCount":0,"referenceCount":19,"year":2016,"authors":"H. Zhang; Y. Jiang; H. Liu; H. Zhang; M. Gu; J. Sun","affiliations":"School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; Department of Computer Science, University of Illinois at Urbana-Champaign, Illinois, USA; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China; School of Software, Tsinghua University, TNLIST, KLISS, Beijing, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d7"},"title":"Usage, costs, and benefits of continuous integration in open-source projects","abstract":"Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI.","conference":"IEEE","terms":"Open source software;Automation;History;Tunneling;Buildings;Testing,costing;program compilers;program testing;public domain software;software engineering,open-source projects;continuous integration systems;CI systems;software compilation;software building;software testing;automated software engineering;GitHub;CI usage;CI costs;CI benefits","keywords":"continuous integration;mining software repositories","startPage":"426","endPage":"437","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582778","citationCount":2,"referenceCount":56,"year":2016,"authors":"M. Hilton; T. Tunnell; K. Huang; D. Marinov; D. Dig","affiliations":"Oregon State University, USA; University of Illinois, USA; University of Illinois, USA; University of Illinois, USA; Oregon State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d8"},"title":"Array length inference for C library bindings","abstract":"Simultaneous use of multiple programming languages (polyglot programming) assists in creating efficient, coherent, modern programs in the face of legacy code. However, manually creating bindings to low-level languages like C is tedious and error-prone. We offer relief in the form of an automated suite of analyses, designed to enhance the quality of automatically produced bindings. These analyses recover high-level array length information that is missing from C's type system. We emit annotations in the style of GObject-Introspection, which produces bindings from annotations on function signatures. We annotate each array argument as terminated by a special sentinel value, fixed-length, or of length determined by another argument. These properties help produce more idiomatic, efficient bindings. We correctly annotate at least 70% of all arrays with these length types, and our results are comparable to those produced by human annotators, but take far less time to produce.","conference":"IEEE","terms":"Libraries;High level languages;Data mining;Memory management;Production;Safety;Programming,,","keywords":"FFI;foreign function interfaces;bindings;libraries;static analysis;type inference","startPage":"461","endPage":"471","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582782","citationCount":0,"referenceCount":24,"year":2016,"authors":"A. J. Maas; H. Nazaré; B. Liblit","affiliations":"University of Wisconsin-Madison, Madison, WI, USA; Universidade Federal de Minas Gerais, Belo Horizonte, Brazil; University of Wisconsin-Madison, Madison, WI, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38d9"},"title":"Predicting semantically linkable knowledge in developer online forums via convolutional neural network","abstract":"Consider a question and its answers in Stack Overflow as a knowledge unit. Knowledge units often contain semantically relevant knowledge, and thus linkable for different purposes, such as duplicate questions, directly linkable for problem solving, indirectly linkable for related information. Recognising different classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base. Existing methods focus on binary relatedness (i.e., related or not), and are not robust to recognize different classes of semantic relatedness when linkable knowledge units share few words in common (i.e., have lexical gap). In this paper, we formulate the problem of predicting semantically linkable knowledge units as a multiclass classification problem, and solve the problem using deep learning techniques. To overcome the lexical gap issue, we adopt neural language model (word embeddings) and convolutional neural network (CNN) to capture word- and document-level semantics of knowledge units. Instead of using human-engineered classifier features which are hard to design for informal user-generated content, we exploit large amounts of different types of user-created knowledge-unit links to train the CNN to learn the most informative wordlevel and document-level features for the multiclass classification task. Our evaluation shows that our deep-learning based approach significantly and consistently outperforms traditional methods using traditional word representations and human-engineered classifier features.","conference":"IEEE","terms":"Semantics;Knowledge engineering;Software;Uniform resource locators;Complex networks;Machine learning;Social network services,data mining;learning (artificial intelligence);neural nets;pattern classification;social networking (online),semantically linkable knowledge prediction;developer online forums;convolutional neural network;Stack Overflow;knowledge unit;information needs;binary relatedness;multiclass classification problem;deep learning techniques;neural language model;word embeddings;CNN;word-level semantics;document-level semantics;human-engineered classifier features;informal user-generated content;word representation","keywords":"Link prediction;Semantic relatedness;Multiclass classification;Deep learning;Mining software repositories","startPage":"51","endPage":"62","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582745","citationCount":0,"referenceCount":51,"year":2016,"authors":"B. Xu; D. Ye; Z. Xing; X. Xia; G. Chen; S. Li","affiliations":"College of Computer Science and Technology, Zhejiang University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Computer Science and Technology, Zhejiang University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Computer Science and Technology, Zhejiang University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38da"},"title":"BovInspector: Automatic inspection and repair of buffer overflow vulnerabilities","abstract":"Buffer overflow is one of the most common types of software vulnerabilities. Various static analysis and dynamic testing techniques have been proposed to detect buffer overflow vulnerabilities. With automatic tool support, static buffer overflow detection technique has been widely used in academia and industry. However, it tends to report too many false positives fundamentally due to the lack of software execution information. Currently, static warnings can only be validated by manual inspection, which significantly limits the practicality of the static analysis. In this paper, we present BovInspector, a tool framework for automatic static buffer overflow warnings inspection and validated bugs repair. Given the program source code and static buffer overflow vulnerability warnings, BovInspector first performs warning reachability analysis. Then, BovInspector executes the source code symbolically under the guidance of reachable warnings. Each reachable warning is validated and classified by checking whether all the path conditions and the buffer overflow constraints can be satisfied simultaneously. For each validated true warning, BovInspector fix it with three predefined strategies. BovInspector is complementary to prior static buffer overflow discovery schemes. Experimental results on real open source programs show that BovInspector can automatically inspect on average of 74.9% of total warnings, and false warnings account for about 25% to 100% (on average of 59.9%) of the total inspected warnings. In addition, the automatically generated patches fix all target vulnerabilities. Further information regarding the implementation and experimental results of BovInspector is available at http://bovinspectortool.github.io/project/. And a short video for demonstrating the capabilities of BovInspector is now available at https://youtu.be/IMdcksROJDg.","conference":"IEEE","terms":"Maintenance engineering;Software;Engines;Testing;Inspection;Reachability analysis;Explosions,inspection;program debugging;program diagnostics;program testing;public domain software;source code (software),BovInspector;buffer overflow vulnerabilities repair;software vulnerabilities;static analysis;dynamic testing techniques;static buffer overflow detection technique;tool framework;automatic static buffer overflow warnings inspection;bugs repair;program source code;warning reachability analysis;path conditions;buffer overflow constraints;static buffer overflow discovery schemes;open source programs","keywords":"Buffer Overflow;Symbolic Execution;Validation;Automatic Repair","startPage":"786","endPage":"791","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582816","citationCount":0,"referenceCount":21,"year":2016,"authors":"F. Gao; L. Wang; X. Li","affiliations":"State Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, 210023, China; State Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, 210023, China; State Key Laboratory of Novel Software Technology, Nanjing University, Nanjing, 210023, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38db"},"title":"Lightweight collection and storage of software repository data with DataRover","abstract":"The ease of setting up collaboration infrastructures for software engineering projects creates a challenge for researchers that aim to analyze the resulting data. As teams can choose from various available software-as-a-service solutions and can configure them with a few clicks, researchers have to create and maintain multiple implementations for collecting and aggregating the collaboration data in order to perform their analyses across different setups. The DataRover system presented in this paper simplifies this task by only requiring custom source code for API authentication and querying. Data transformation and linkage is performed based on mappings, which users can define based on sample responses through a graphical front end. This allows storing the same input data in formats and databases most suitable for the intended analysis without requiring additional coding. Furthermore, API responses are continuously monitored to detect changes and allow users to update their mappings and data collectors accordingly. A screencast of the described use cases is available at https: //youtu.be/mt4ztff4SfU.","conference":"IEEE","terms":"Software;Databases;Data collection;Programming;Collaboration;Couplings;Monitoring,application program interfaces;database management systems;project management;query processing;software engineering;software management;source code (software);storage management,software repository data collection;software repository data storage;DataRover system;software engineering project;source code;API authentication;API querying;data transformation;data linkage;graphical front end","keywords":"Data Collection;Data Mapping Definition;Link Discovery;API monitoring;Data Storage","startPage":"810","endPage":"815","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582820","citationCount":0,"referenceCount":7,"year":2016,"authors":"T. Kowark; C. Matthies; M. Uflacker; H. P. H. Plattner","affiliations":"Institute, University of Potsdam, August-Bebel-Str. 88 Potsdam, Germany; Institute, University of Potsdam, August-Bebel-Str. 88 Potsdam, Germany; Institute, University of Potsdam, August-Bebel-Str. 88 Potsdam, Germany; Institute, University of Potsdam, August-Bebel-Str. 88 Potsdam, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38dc"},"title":"On essential configuration complexity: Measuring interactions in highly-configurable systems","abstract":"Quality assurance for highly-configurable systems is challenging due to the exponentially growing configuration space. Interactions among multiple options can lead to surprising behaviors, bugs, and security vulnerabilities. Analyzing all configurations systematically might be possible though if most options do not interact or interactions follow specific patterns that can be exploited by analysis tools. To better understand interactions in practice, we analyze program traces to characterize and identify where interactions occur on control flow and data. To this end, we developed a dynamic analysis for Java based on variability-aware execution and monitor executions of multiple small to medium-sized programs. We find that the essential configuration complexity of these programs is indeed much lower than the combinatorial explosion of the configuration space indicates. However, we also discover that the interaction characteristics that allow scalable and complete analyses are more nuanced than what is exploited by existing state-of-the-art quality assurance strategies.","conference":"IEEE","terms":"Meteorology;Complexity theory;Quality assurance;Testing;Extraterrestrial measurements;Computer bugs;Java,Java;program diagnostics;software quality,configuration complexity;highly-configurable system;quality assurance;program trace analysis;dynamic analysis;Java;variability-aware execution;execution monitoring","keywords":"Feature Interaction;Configurable Software;Variability-Aware Execution","startPage":"483","endPage":"494","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582784","citationCount":0,"referenceCount":70,"year":2016,"authors":"J. Meinicke; C. Wong; C. Kästner; T. Thüm; G. Saake","affiliations":"University of Magdeburg, Germany; Carnegie Mellon University, USA; Carnegie Mellon University, USA; TU Braunschweig, Germany; University of Magdeburg, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38dd"},"title":"Privacy preserving via interval covering based subclass division and manifold learning based bi-directional obfuscation for effort estimation","abstract":"When a company lacks local data in hand, engineers can build an effort model for the effort estimation of a new project by utilizing the training data shared by other companies. However, one of the most important obstacles for data sharing is the privacy concerns of software development organizations. In software engineering, most of existing privacy-preserving works mainly focus on the defect prediction, or debugging and testing, yet the privacy-preserving data sharing problem has not been well studied in effort estimation. In this paper, we aim to provide data owners with an effective approach of privatizing their data before release. We firstly design an Interval Covering based Subclass Division (ICSD) strategy. ICSD can divide the target data into several subclasses by digging a new attribute (i.e., class label) from the effort data. And the obtained class label is beneficial to maintaining the distribution of the target data after obfuscation. Then, we propose a manifold learning based bi-directional data obfuscation (MLBDO) algorithm, which uses two nearest neighbors, which are selected respectively from the previous and next subclasses by utilizing the manifold learning based nearest neighbor selector, as the disturbances to obfuscate the target sample. We call the entire approach as ICSD\u0026MLBDO. Experimental results on seven public effort datasets show that: 1) ICSD\u0026MLBDO can guarantee the privacy and maintain the utility of obfuscated data. 2) ICSD\u0026MLBDO can achieve better privacy and utility than the compared privacy-preserving methods.","conference":"IEEE","terms":"Decision support systems;Data privacy;Servers;Radio frequency,data handling;data privacy;estimation theory;learning (artificial intelligence),interval covering based subclass division;ICSD strategy;manifold learning based bidirectional obfuscation;MLBDO algorithm;effort estimation;training data utilization;software development organization privacy;software engineering;privacy-preserving data sharing problem;data privatization;effort data digging;class label;target data distribution;nearest neighbor selection;manifold learning based nearest neighbor selector;target sample obfuscation;ICSD\u0026MLBDO;public effort datasets","keywords":"Effort estimation;Privacy-preserving;Locality preserving projection;Subclass division","startPage":"75","endPage":"86","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582747","citationCount":0,"referenceCount":66,"year":2016,"authors":"F. Qi; X. Jing; X. Zhu; F. Wu; L. Cheng","affiliations":"State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China; State Key Laboratory of Software Engineering, School of Computer, Wuhan University, China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38de"},"title":"Too much automation? The bellwether effect and its implications for transfer learning","abstract":"“Transfer learning”: is the process of translating quality predictors learned in one data set to another. Transfer learning has been the subject of much recent research. In practice, that research means changing models all the time as transfer learners continually exchange new models to the current project. This paper offers a very simple “bellwether” transfer learner. Given N data sets, we find which one produces the best predictions on all the others. This “bellwether” data set is then used for all subsequent predictions (or, until such time as its predictions start failing-at which point it is wise to seek another bellwether). Bellwethers are interesting since they are very simple to find (just wrap a for-loop around standard data miners). Also, they simplify the task of making general policies in SE since as long as one bellwether remains useful, stable conclusions for N data sets can be achieved just by reasoning over that bellwether. From this, we conclude (1) this bellwether method is a useful (and very simple) transfer learning method; (2) “bellwethers” are a baseline method against which future transfer learners should be compared; (3) sometimes, when building increasingly complex automatic methods, researchers should pause and compare their supposedly more sophisticated method against simpler alternatives.","conference":"IEEE","terms":"Software;Learning systems;Testing;Manuals;Buildings;Predictive models;Complexity theory,data mining;learning (artificial intelligence);software quality,bellwether effect;transfer learning;software quality predictor;data mining","keywords":"Defect Prediction;Data Mining;Transfer learning","startPage":"122","endPage":"131","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582751","citationCount":0,"referenceCount":50,"year":2016,"authors":"R. Krishna; T. Menzies; W. Fu","affiliations":"Computer Science, North Carolina State University, USA; Computer Science, North Carolina State University, USA; Computer Science, North Carolina State University, USA","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38df"},"title":"ProcessPAIR: A tool for automated performance analysis and improvement recommendation in software development","abstract":"High-maturity software development processes can generate significant amounts of data that can be periodically analyzed to identify performance problems, determine their root causes and devise improvement actions. However, conducting that analysis manually is challenging because of the potentially large amount of data to analyze and the effort and expertise required. In this paper, we present ProcessPAIR, a novel tool designed to help developers analyze their performance data with less effort, by automatically identifying and ranking performance problems and potential root causes, so that subsequent manual analysis for the identification of deeper causes and improvement actions can be properly focused. The analysis is based on performance models defined manually by process experts and calibrated automatically from the performance data of many developers. We also show how ProcessPAIR was successfully applied for the Personal Software Process (PSP). A video about ProcessPAIR is available in https://youtu.be/dEk3fhhkduo.","conference":"IEEE","terms":"Calibration;Unified modeling language;Sensitivity;Software;Performance analysis;Statistical distributions;Estimation,data analysis;software performance evaluation;software process improvement;software tools,ProcessPAIR;software tool;performance analysis;improvement recommendation;software development;data analysis;personal software process;PSP","keywords":"software process;performance analysis;improvement recommendation","startPage":"798","endPage":"803","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582818","citationCount":0,"referenceCount":14,"year":2016,"authors":"M. Raza; J. P. Faria","affiliations":"INESC TEC/University of Porto - Faculty of Engineering, Rua Dr. Roberto Frias, s/n, 4200-465 Porto Portugal; INESC TEC/University of Porto - Faculty of Engineering, Rua Dr. Roberto Frias, s/n, 4200-465 Porto Portugal","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e0"},"title":"SuperMod: Tool support for collaborative filtered model-driven software product line engineering","abstract":"The increase in productivity implied by model-driven software product line engineering is weakened by the complexity exposed to the user having to manage a multi-variant model. Recently, a new paradigm has emerged: filtered software product line engineering transfers the established check-out/modify/commit workflow from version control to variability management, allowing to iteratively develop the multi-variant model in a single-variant view. This paper demonstrates SuperMod, a tool that supports collaborative filtered model-driven product line engineering, implemented for and with the Eclipse Modeling Framework. Concerning variability management, the tool offers capabilities for editing feature models and specifying feature configurations, both being well-known formalisms in product line engineering. Furthermore, collaborative editing of product lines is provided through distributed version control. The accompanying video shows that SuperMod seamlessly integrates into existing tool landscapes, reduces the complexity of multi-variant editing, automates a large part of variability management, and ensures consistency. A tool demonstration video is available here: http://youtu.be/5XOk3x5kjFc.","conference":"IEEE","terms":"Unified modeling language;Software product lines;Collaboration;Control systems;Computational modeling;Software;Metadata,collaborative filtering;software product lines,SuperMod tool support;collaborative filtering;model-driven software product line engineering;check-out-modify-commit workflow;version control;variability management;Eclipse modeling framework;distributed version control","keywords":"Model-driven software engineering;software product line engineering;version control;filtered editing","startPage":"822","endPage":"827","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582822","citationCount":0,"referenceCount":22,"year":2016,"authors":"F. Schwäger; B. Westfechtel","affiliations":"Applied Computer Science, University of Bayreuth, 95440 Bayreuth, Germany; Applied Computer Science, University of Bayreuth, 95440 Bayreuth, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e1"},"title":"Goal-conflict detection based on temporal satisfiability checking","abstract":"Goal-oriented requirements engineering approaches propose capturing how a system should behave through the specification of high-level goals, from which requirements can then be systematically derived. Goals may however admit subtle situations that make them diverge, i.e., not be satisfiable as a whole under specific circumstances feasible within the domain, called boundary conditions. While previous work allows one to identify boundary conditions for conflicting goals written in LTL, it does so through a pattern-based approach, that supports a limited set of patterns, and only produces pre-determined formulations of boundary conditions. We present a novel automated approach to compute boundary conditions for general classes of conflicting goals expressed in LTL, using a tableaux-based LTL satisfiability procedure. A tableau for an LTL formula is a finite representation of all its satisfying models, which we process to produce boundary conditions that violate the formula, indicating divergence situations. We show that our technique can automatically produce boundary conditions that are more general than those obtainable through existing previous pattern-based approaches, and can also generate boundary conditions for goals that are not captured by these patterns.","conference":"IEEE","terms":"Boundary conditions;Software;Cost accounting;Requirements engineering;Computational modeling;Methane;Standards,computability;formal specification;pattern recognition;temporal logic,goal-conflict detection;temporal satisfiability checking;goal-oriented requirements engineering;boundary conditions;pattern-based approach;tableaux-based LTL satisfiability;finite representation","keywords":"Goal Conflicts;Satisfiability Checking;Tableaux Method","startPage":"507","endPage":"518","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582786","citationCount":0,"referenceCount":47,"year":2016,"authors":"R. Degiovanni; N. Ricci; D. Alrajeh; P. Castro; N. Aguirre","affiliations":"Departamento de Computación, Universidad Nacional de Río Cuarto and CONICET, Argentina; Departamento de Computación, Universidad Nacional de Río Cuarto and CONICET, Argentina; Department of Computing, Imperial College London, UK; Departamento de Computación, Universidad Nacional de Río Cuarto and CONICET, Argentina; Departamento de Computación, Universidad Nacional de Río Cuarto and CONICET, Argentina","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e2"},"title":"Symbolic execution of complex program driven by machine learning based constraint solving","abstract":"Symbolic execution is a widely-used program analysis technique. It collects and solves path conditions to guide the program traversing. However, due to the limitation of the current constraint solvers, it is difficult to apply symbolic execution on programs with complex path conditions, like nonlinear constraints and function calls. In this paper, we propose a new symbolic execution tool MLB to handle such problem. Instead of relying on the classical constraint solving, in MLB, the feasibility problems of the path conditions are transformed into optimization problems, by minimizing some dissatisfaction degree. The optimization problems are then handled by the underlying optimization solver through machine learning guided sampling and validation. MLB is implemented on the basis of Symbolic PathFinder and encodes not only the simple linear path conditions, but also nonlinear arithmetic operations, and even black-box function calls of library methods, into symbolic path conditions. Experiment results show that MLB can achieve much better coverage on complex real-world programs.","conference":"IEEE","terms":"Optimization;Libraries;Engines;Machine learning algorithms;Java;Transforms;Algorithm design and analysis,arithmetic;constraint handling;learning (artificial intelligence);optimisation;program diagnostics,complex program;machine learning;constraint solving;program analysis;constraint solvers;complex path conditions;nonlinear constraints;symbolic execution tool;MLB;optimization problems;dissatisfaction degree;optimization solver;symbolic PathFinder;linear path conditions;nonlinear arithmetic operations;black-box function calls;library methods;symbolic path conditions","keywords":"Symbolic Execution;Machine Learning;Complicated Path Condition;Constraint Solving","startPage":"554","endPage":"559","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582790","citationCount":0,"referenceCount":26,"year":2016,"authors":"X. Li; Y. Liang; H. Qian; Y. Hu; L. Bu; Y. Yu; X. Chen; X. Li","affiliations":"State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, P.R. China","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e3"},"title":"Automatically recommending code reviewers based on their expertise: An empirical comparison","abstract":"Code reviews are an essential part of quality assurance in Free, Libre, and Open Source Software (FLOSS) projects. However, finding a suitable reviewer can be difficult, and delayed or forgotten reviews are the consequence. Automating reviewer selection with suitable algorithms can mitigate this problem. We compare empirically six algorithms based on modification expertise and two algorithms based on review expertise on four major FLOSS projects. Our results indicate that the algorithms based on review expertise yield better recommendations than those based on modification expertise. The algorithm Weighted Review Count (WRC) recommends at least one out of five reviewers correctly in 69 % to 75 % of all cases, which is one of the best results achieved in the comparison.","conference":"IEEE","terms":"Measurement;Prediction algorithms;Software;Software algorithms;Algorithm design and analysis;History;Machine learning algorithms,project management;public domain software;software quality,code reviewer automatic recommendation;quality assurance;free, libre, and open source software projects;FLOSS projects;reviewer selection automation;modification expertise;review expertise;weighted review count;WRC","keywords":"Code reviewer recommendation;code reviews;expertise metrics;issue tracker;open source;patches;recommendation system","startPage":"99","endPage":"110","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582749","citationCount":0,"referenceCount":57,"year":2016,"authors":"C. Hannebauer; M. Patalas; S. Stünkelt; V. Gruhn","affiliations":"paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Germany; paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Germany; paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Germany; paluno - The Ruhr Institute for Software Technology, University of Duisburg-Essen, Germany","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e4"},"title":"Visualization of combinatorial models and test plans","abstract":"Combinatorial test design (CTD) is an effective and widely used test design technique. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. One challenge for successful application of CTD in practice relates to this manual model definition and maintenance process. Another challenge relates to the comprehension and use of the test plan generated by CTD for prioritization purposes. In this work we introduce the use of visualizations as a means to address these challenges. We apply three different forms of visualization, matrices, graphs, and treemaps, to visualize the relationships between the different elements of the model, and to visualize the strength of each test in the test plan and the relationships between the different tests in terms of combinatorial coverage. We evaluate our visualizations via a user survey with 19 CTD practitioners, as well as via two industrial projects in which our visualization was used and allowed test designers to get vital insight into their models and into the coverage provided through CTD generated test plans.","conference":"IEEE","terms":"Visualization;Data visualization;Atmospheric modeling;Manuals;Computational modeling;Testing;Image color analysis,program testing;program visualisation,combinatorial models visualization;test plans visualization;combinatorial test design;automatic test plan generation;matrices;graphs;treemaps;combinatorial coverage;software visualization;CTD generated test plans","keywords":"Combinatorial Testing;Software Visualization","startPage":"144","endPage":"154","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582753","citationCount":0,"referenceCount":25,"year":2016,"authors":"R. Tzoref-Brill; P. Wojciak; S. Maoz","affiliations":"School of Computer Science, Tel Aviv University and IBM Research, Israel; IBM Systems, USA; School of Computer Science, Tel Aviv University, Israel","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e5"},"title":"ASE 2019 Organization","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"26","endPage":"27","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952202","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e6"},"title":"ASE 2015 Organization","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xvii","endPage":"xvii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371982","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e7"},"title":"ASE Steering Committee and ASE Fellows","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxii","endPage":"xxii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371987","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e8"},"title":"Message from the Chairs","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"20","endPage":"25","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952558","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38e9"},"title":"Program Committees and Reviewers","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"28","endPage":"33","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952425","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ea"},"title":"Steering Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"34","endPage":"34","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952537","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38eb"},"title":"Sponsors and Supporters","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"35","endPage":"35","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952508","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ec"},"title":"Doctoral Symposium and Tool Demonstrations Committees","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xxi","endPage":"xxi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371986","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ed"},"title":"Organization Committee","abstract":"Provides a listing of current committee members and society officers. The conference also offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"vii","endPage":"xii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115608","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ee"},"title":"Silver sponsors","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"xiii","endPage":"xiii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115609","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ef"},"title":"Publisher's Information","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"910","endPage":"910","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372091","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f0"},"title":"Message from the Chairs","abstract":"Presents the welcome message from the conference proceedings.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"viii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693057","citationCount":0,"referenceCount":0,"year":2013,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f1"},"title":"Message from the Chairs","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"xv","endPage":"xvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371981","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f2"},"title":"Program Committee","abstract":"Provides a listing of current committee members and society officers.","conference":"IEEE","terms":"","keywords":"","startPage":"xviii","endPage":"xviii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371983","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f3"},"title":"Message from the chairs","abstract":"Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"xi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115607","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f4"},"title":"[Copyright notice]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"4","endPage":"4","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952247","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f5"},"title":"[Title page i]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1","endPage":"1","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952307","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f6"},"title":"[Title page iii]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"3","endPage":"3","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952325","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f7"},"title":"Author Index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1289","endPage":"1296","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952195","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f8"},"title":"Table of contents","abstract":"Presents the table of contents/splash page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"5","endPage":"19","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8952268","citationCount":0,"referenceCount":0,"year":2019,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38f9"},"title":"Table of contents","abstract":"Presents the table of contents/splash page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"v","endPage":"xiv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371980","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38fa"},"title":"[Title page iii]","abstract":"Presents the title page of the proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"iii","endPage":"iii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371978","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38fb"},"title":"[Front matter]","abstract":"Conference proceedings front matter may contain various advertisements, welcome messages, committee or program information, and other miscellaneous conference information. This may in some cases also include the cover art, table of contents, copyright statements, title-page or half title-pages, blank pages, venue maps or other general information relating to the conference that was part of the original conference proceedings.","conference":"IEEE","terms":"","keywords":"","startPage":"i","endPage":"ii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693055","citationCount":0,"referenceCount":0,"year":2013,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38fc"},"title":"Expert Review Panel","abstract":"The conference offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"xix","endPage":"xix","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371984","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38fd"},"title":"[Front matter]","abstract":"Conference proceedings front matter may contain various advertisements, welcome messages, committee or program information, and other miscellaneous conference information. This may in some cases also include the cover art, table of contents, copyright statements, title-page or half title-pages, blank pages, venue maps or other general information relating to the conference that was part of the original conference proceedings.","conference":"IEEE","terms":"","keywords":"","startPage":"1","endPage":"2","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115604","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38fe"},"title":"Author index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"906","endPage":"909","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372090","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9128eee8435e8e7d38ff"},"title":"[Copyright notice]","abstract":"Presents the copyright information for the conference. May include reprint permission information.","conference":"IEEE","terms":"","keywords":"","startPage":"iv","endPage":"iv","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371979","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3900"},"title":"Contents","abstract":"The following topics are dealt with: concurrency; dynamic analysis; testing and verification; evolution; generation and synthesis; recommendations; security; debugging; resources; specification mining; models and complexity; software analysis; adaptation and transformation; and models and requirements.","conference":"IEEE","terms":",concurrency control;data mining;program debugging;program testing;security of data;software engineering,automated software engineering;software concurrency;dynamic analysis;software testing;software verification;software evolution;software generation;software synthesis;software security;software debugging;software resources;specification mining;software models;software complexity;software analysis","keywords":"","startPage":"1","endPage":"6","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693056","citationCount":0,"referenceCount":0,"year":2013,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3901"},"title":"Additional reviewers","abstract":"The conference offers a note of thanks and lists its reviewers.","conference":"IEEE","terms":"","keywords":"","startPage":"xx","endPage":"xx","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371985","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3902"},"title":"Sponsors and supporters","abstract":"The conference organizers greatly appreciate the support of the various corporate sponsors listed.","conference":"IEEE","terms":"","keywords":"","startPage":"xxvi","endPage":"xxvi","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371989","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3903"},"title":"Author index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"1034","endPage":"1036","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115605","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3904"},"title":"Contents","abstract":"The following topics are dealt with: test generation; developer practice and behavior; documentation; formal verification; security of data; mobile development; binary analysis; program comprehension; reliability and bugs; source code analysis; symbolic execution; program repair; recommender systems; concurrency; program synthesis; and data visualization.","conference":"IEEE","terms":",concurrency (computers);data visualisation;formal verification;mobile computing;program debugging;program diagnostics;program testing;recommender systems;security of data;software reliability;source code (software);symbol manipulation;system documentation,test generation;developer practice;developer behavior;documentation;formal verification;security of data;mobile development;binary analysis;program comprehension;bugs;reliability;source code analysis;symbolic execution;program repair;recommender systems;concurrency;program synthesis;data visualization","keywords":"","startPage":"xiv","endPage":"xx","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115606","citationCount":0,"referenceCount":0,"year":2017,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3905"},"title":"[Title page i]","abstract":"The following topics are dealt with: automated development support; formal verification; specification mining; search-based software testing; concurrency bugs; concurrency analysis; automatic test generation; mobile applications; program repair; program synthesis; software performance; product lines; configurable software systems; defect prediction; debugging; concurrent programming; parallel programming; program analysis; program translations; software evolution; data mining and tool demonstrations.","conference":"IEEE","terms":",concurrency control;data mining;formal specification;mobile computing;parallel programming;program debugging;program diagnostics;program testing;program verification;software maintenance;software product lines,tool demonstrations;data mining;software evolution;program translations;program analysis;parallel programming;concurrent programming;debugging;defect prediction;configurable software systems;product lines;software performance;program synthesis;program repair;mobile applications;automatic test generation;concurrency analysis;concurrency bugs;search-based software testing;specification mining;formal verification;automated development support","keywords":"","startPage":"i","endPage":"i","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371977","citationCount":0,"referenceCount":0,"year":2015,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3906"},"title":"[Front matter]","abstract":"Conference proceedings front matter may contain various advertisements, welcome messages, committee or program information, and other miscellaneous conference information. This may in some cases also include the cover art, table of contents, copyright statements, title-page or half title-pages, blank pages, venue maps or other general information relating to the conference that was part of the original conference proceedings.","conference":"IEEE","terms":"","keywords":"","startPage":"i","endPage":"xviii","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582736","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
{"_id":{"$oid":"5e5b9129eee8435e8e7d3907"},"title":"Author index","abstract":"Presents an index of the authors whose articles are published in the conference proceedings record.","conference":"IEEE","terms":"","keywords":"","startPage":"900","endPage":"901","pdfLink":"https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582837","citationCount":0,"referenceCount":0,"year":2016,"authors":"","affiliations":"","_class":"com.example.oasisdocument.docs.Paper"}
